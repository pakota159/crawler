title,paragraph
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"The manufacturing business faces huge transformations nowadays. Due to rapid development of digital world and broad application of data science, various fields of human activity seek improvement. Modern manufacturing is often referred to as industry 4.0 that is the manufacturing under conditions of the fourth industrial revolution that has brought robotization, automation and broad application of data"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"The amount of data to be stored and processed is growing every day. Therefore, today's manufacturing companies need to find new solutions and use cases for this data. Of course, data brings its benefits to manufacturing companies as it allows to automate large-scale processes and speed up execution time"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"Predictive analytics is the analysis of present data to forecast and avoid problematic situations in advance. Manufacturers are deeply interested in monitoring the company functioning and its high performance. Finding the best possible way to hold problematic issues, overcoming difficulties or preventing them from happening at all are marvelous opportunities for the manufacturers using predictive analytics. The implementation of predictive analytics allows dealing with waste (overproduction, idle time, logistics, inventory, etc. Therefore, let's concentrate on the possible solutions brought by predictive analytics"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"Both these prediction models are aimed at forecasting the moment when the equipment fails to perform the task. As a result, the secondary goal may be achieved  - to prevent these failures from happening or at least to reduce their number. This becomes possible due to the numerous predictive techniques"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"Preventive maintenance is usually applied to the piece of equipment that is still working to lessen the likelihood of its failing. There are 2 major types of preventive maintenance: time-based and usage-based. The biggest strength of preventive maintenance is planning. Having at hand the prediction concerning future troubles with the equipment, the manufacturer may plan a break or a shut down for repairing. Such breaks are usually made to avoid considerable delays and failures, which are often caused by more significant problems that may arise"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"Demand forecasting is a complex process involving analysis of data and massive work of the accountants and specialists. Moreover, it appears to have strong relations with inventory management. A simple fact may explain this interrelation - demand forecasting uses the data of the supply chain"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"There are a lot of benefits of demand forecasting for the manufacturers. First of all, it gives the opportunity to control inventory better and reduce the need to store significant amounts of useless products. Besides, the online inventory management software helps to collect data that may be of great use for further analysis. One more critical factor is that the data input for the demand forecasting may be continually updated. Thus, relevant forecasts may be made. Additional benefits lie in the improvement of the supplier-manufacturer relations, as both can efficiently regulate their stocks and supply process"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,Manufacturing and selling the product involves taking into account numerous factors and criteria influencing the product price. All the elements starting with the initial price of the raw material and up to the distribution costs contribute in the final product price. And what happens when the customer finds this price too high or too low?
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"Price optimization is the process of finding the best possible price both for manufacturer and customer, not too high and not to low. Modern price optimization solutions can increase your profit efficiently. These tools aggregate and analyze pricing and cost data both from the internal sources and those of your competitors and derive optimized price variants"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,The manufacturers spend a considerable amount of money every year on supporting warranty claims. Warranty claims disclose valuable information on the quality and reliability of the product. They help to reveal early warnings or defects of the product
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"The manufacturers tend to invest more and more money into robotization of their enterprises every year. The AI-powered robot models help to satisfy the ever-increasing demand. Moreover, industrial robots largely contribute to increasing of quality of a product. Every year, the upgraded models come to the production floor to revolutionize the production lines. They are straightforward. Moreover, manufacturing robots are more affordable for enterprises than ever before"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"Big Data has brought big opportunities to manufacturing companies regarding product development. The manufacturers use the advantage of Big Data to understand their customers better, to meet the demand and to satisfy their needs. Thus, data may be used to develop new products or to improve the existing ones"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"Using Big Data for product development, the manufacturers can design a product with increased customer value and minimize the risks connected to introduction of a new product to the market. Actionable insights are taken into account while modeling and planning. This data can strengthen the decision-making process. Also, data management tools are widely applied to optimize the operational aspects of the distribution chain"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"AI-powered technologies and computer vision applications found their usage in manufacturing at the stage of quality control. In this respect object identification and object detection and classification proved to be very efficient. Usually, quality control monitoring was performed by people. However, now it is more common to rely on computer vision rather than on human vision. These monitoring systems usually consist of computer hardware and software, cameras, and lighting for image capturing. After that, these images are algorithmically compared to the standards to identify discrepancies"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"Supply chains have always been complex and unpredictable. Risk has always been a part of the manufacturing processes and product delivery. Using big data analytics for managing supply chain risk may be quite beneficial for the manufacturers. With the help of analytics, the companies can predict potential delays and calculate probabilities of the problematic issues. The companies use analytics to identify backup suppliers and develop contingency plans"
Top 8 Data Science Use Cases in Manufacturing - KDnuggets,"Along with forecasting possible risks, demand and the requirements of the market, data analytics can help to keep up with high-quality standards and quality metrics. Moreover, incorporating smart data techniques into manufacturing may help to forecast unexpected wastes or problems. Big data can help to achieve many of the business goals set by the manufacturers having spending less time and money as ever before"
My Best Tips for Agile Data Science Research - KDnuggets,Every machine learning project should start by defining the goals of the project. We must define what is a good result in order to know when to stop the research and move forward to the next problem. This phase is usually done with the business stakeholders
My Best Tips for Agile Data Science Research - KDnuggets,"What is a good performance is a pretty hard question which is heavily based on how hard is the problem and what are the business needs. My advice is to start your modeling by building a simple baseline model, it can be a simple machine learning model with basic features or even a business rule (heuristics) like the average label in an important category. This way we can measure our performance in comparison to the baseline and monitor our improvement in the task"
My Best Tips for Agile Data Science Research - KDnuggets,"Iterations are one of the core characteristics of agile development. In a data science project, we don’t iterate on features like the engineering team, we iterate on models. Starting with a simple model with a small number of features and making it more and more complex iteratively has many advantages. You can stop at any point when your model is good enough and save time and complexity. You know exactly how every change you made has affected the model performance and this gives you intuition for your next experiments and maybe most importantly, by adding complexity iteratively you can debug your model for bugs and data leakages much easier and faster"
My Best Tips for Agile Data Science Research - KDnuggets,"Planning research projects is hard because they have a very large amount of uncertainty. From my experience, it is best to plan your projects using subgoals, for example, data exploration, data cleaning, dataset building, feature engineering, and modeling are small parts of the research that you can plan at least a few weeks forward. These sub-goals can bring value on their own without the final model. For example, after data exploration, the data scientist can bring actionable insights for the business people and data set cleaning and building can help other data scientist and analysts for their own projects immediately"
My Best Tips for Agile Data Science Research - KDnuggets,"Failing fast is maybe my most important point and probably the hardest to do. At each iteration, you must ask yourself what is the probability that the model performance will reach the minimum valuable KPI? I think that making the model more complex iteratively really helps in this part. Adding more features and trying more models usually gives incremental improvements. If your model performance is 70% and your minimum valuable KPI is 90% you are probably not going to get there, so, you need to stop your project and move to the next problem or change something drastic like changing your label or tagging much more data. I am not saying that you shouldn’t try to solve very hard problems, just make sure that you are not wasting time on methods that probably won’t reach your project goals"
My Best Tips for Agile Data Science Research - KDnuggets,"My last advice is deploying your model in production at the earliest point or a little after the point that your model is valuable. I know that maybe your final model will have totally different features and a lot of the work will be wasted. But, first, your model gives value, why wait? Secondly and more importantly, in many cases, the production has its own constraints, some features are not available at the production systems, some features are in different formats, maybe your model is to slow or uses to much RAM etc. Solving these problems early can save a lot of unrealistic modeling time"
How to Capture Data to Make Business Impact - KDnuggets,"A data universe is formed by information and noise: Information is data that can be transformed into knowledge; noise is data from which no knowledge can be extracted (of course, noise data can also be treated and lead to “findings” which are false knowledge and not truly useful). Note that data being noise or information depends on the business challenge under discussion. In other words, the same datum could be noise in one business case and information in another"
How to Capture Data to Make Business Impact - KDnuggets,"Consider the historic data set of ice cream consumption in Madagascar: It would be next to common sense that this data has no relevance in this sports case so is clearly noise. But notice that only data from 1995 on is used. Indeed, there was data since 1975. So why the full record was not used? In the season 1995-1996, Spain adopted the rule of giving 3 points by won match instead of just 2. A"
How to Capture Data to Make Business Impact - KDnuggets,"And it tested positive. So, data from 1975 to 1994 was noise and not information. In this case, noticing it requested from a deep knowledge of the underlying field of study (and a bit of luck too)"
How to Capture Data to Make Business Impact - KDnuggets,"Capturing noise has a side effect: the more noise we store, the more difficult it is to find information in the captured data. It is like a needle (information) in a haystack (noise). Even if we would easily tell the difference between a needle and thatch, it could be very difficult to recognize the needle if it is surrounded by a big amount of thatch. The Big Data rule-of-thumb of storing as much data as you can and for as large longitude as possible would be a false friend"
How to Capture Data to Make Business Impact - KDnuggets,"The overall accuracy of the model was 91%. We could take this value as a proxy for the percentage of information captured. On the other hand, we know with certainty which data we used for the treatment from the total available. So, we could calculate it as follows:"
How to Capture Data to Make Business Impact - KDnuggets,"Now that we have a definition for efficiency, we can introduce a new concept, Smart Data. In IoT, smart Data is often described as data coming from smart sensors. Some others define it as Big Data that has been cleansed, filtered, and prepared for analysis. There is no agreed definition for “Smart Data. The higher the efficiency, the smarter data is. Regardless of the source or cleaning process of the data, the highest efficiency (or “smartness”) of the data is 100%"
How to Capture Data to Make Business Impact - KDnuggets,"To optimize data capturing, any Big Data initiative must be an iterative process guided by the business needs. Different teams must communicate and collaborate closely to iteratively develop a solution around the business challenge. Bring in the analytics team early to help the IoT team capture data in the way that the data will be used. Separate information from noise in the existing data to improve"
"How To Work In Data Science, AI, Big Data - KDnuggets",There are many facets to working in Data Science. Your role will depend greatly on the industry you pick and the area of Data Science you want to pursue. A Data Science career is very dynamic and requires a team effort to succeed
"How To Work In Data Science, AI, Big Data - KDnuggets",Data preparation is particularly important. Data scientists typically spend about 80% of their time on it. Having access to data shaped in the right way makes them more productive and happier
"How To Work In Data Science, AI, Big Data - KDnuggets","I then joined PageGroup. There, I worked as an lead developer and architect on a global transformation programme across 34 countries. I led the technical delivery of search, multi-channel communication, business intelligence, text analytics, job board integration, and advertising solutions"
"How To Work In Data Science, AI, Big Data - KDnuggets",Now I am a lead big data and machine learning engineer at JustGiving. JustGiving is a tech-for-good company that’s helped 26 million users in 164 countries raise $5 billion for good causes. It was acquired in 2017 by
"How To Work In Data Science, AI, Big Data - KDnuggets","JustGiving is still a start-up at heart, so there is no typical day. I get involved in various tasks, such as data and report requirements capture, engineering new data pipeline, investigating operational issues, running data experiments, analysing unstructured data looking for useful patterns, exploring new ways to use the data to answer questions, presenting a data story, and sharing my knowledge and experience. This means that I work closely with marketing, product managers and product analysts to understand their data needs and what metrics and predictions are important for them"
"How To Work In Data Science, AI, Big Data - KDnuggets","Building and running large-scale data infrastructure and distributed model training are active areas in academia and industry. They are evolving at a fast pace, with new tooling being introduced every few months. I like to use cloud solutions in an innovative way to improve our in-house data science platform, enhance our business processes, and make data insights available to internal and external users"
"How To Work In Data Science, AI, Big Data - KDnuggets","In addition KOALA allows us to make predictions from streaming data. It is extremely costs effective solution compared to paying for a vendor product. If you compare it to a vendor solution based on the same web traffic, KOALA is 10x cheaper, more developer friendly, and we get the raw streamed data back in real-time, rather than in batches or having to use a propitiatory locked down querying or reporting system"
"How To Work In Data Science, AI, Big Data - KDnuggets","On the infrastructure side, I see serverless computing and Platform as a Service (PaaS) infrastructure in the public cloud such as AWS and Azure becoming more prominent. Functions in serverless computing are particularly interesting for me, as they can auto-scale in less than a 100 milliseconds, are highly available and are low cost. They are low cost as you only pay for the time your code is executed, rather than for an always-on machine or container like in more traditional cloud infrastructure. I’ve even shown that you can implement most of the existing container-based"
"How To Work In Data Science, AI, Big Data - KDnuggets","The open source frameworks and programming languages will also continue to grow compared to closed vendor specific products and languages, e. Apache Spark framework, Python, R, SQL. The same goes for data storage and access: cloud storage, data warehouses and data lakes will store data in more open rather than proprietary formats, and this will be more accessible over standard APIs or open protocols"
"How To Work In Data Science, AI, Big Data - KDnuggets","It’s about self-motivation: when things move at such a fast pace, you can look broadly across the sector to gain a general understanding. But you also need to focus your energy on one specific course or project and complete it. The industry also tends to repackage old technologies with some improvements as new trending ones, like cyber security, cognitive computing, chatbots, virtual reality and deep learning at the moment. So I would follow your heart for the areas you are truly interested in and want to focus on rather than the latest trend"
"How To Work In Data Science, AI, Big Data - KDnuggets","In terms of gaining the knowledge, it is a lot easier than it used to be. For example in the past you had to pay for specific vendor training and there was the cost of the product itself. You can now access the"
"How To Work In Data Science, AI, Big Data - KDnuggets","Some are graphical, but in my view you should learn to program in SQL, Python, or R. All three have the ability to do data science at scale thanks to frameworks like Apache Spark. I particularly like Python as it benefits from being an efficient development language with a solid test framework and numerous data science packages"
"How To Work In Data Science, AI, Big Data - KDnuggets","As an ML engineer or data scientist, expect to spend a lot of time on data preparation. This is an important process to master, which involves the cleaning, parsing, enriching and shaping the data so that it can be used in the ML algorithms and experiments. Overall, remember that the processes, tools and data sources are always evolving, so there is no one-off unicorn training course you can do. You will need be self-motivated and open to constantly learn and adapt to the data ecosystem"
"How To Work In Data Science, AI, Big Data - KDnuggets","I would recommend that you learn another language such as Mandarin (1.1 billion speakers) or Spanish (0.5 billion speakers), to remain mobile, get more career opportunities, and be competitive within this interconnected world. This will also open your mind and give you an insight into other cultures and values, and how they use their data"
"How To Work In Data Science, AI, Big Data - KDnuggets","Some jobs and professions will be replaced, and some human expertise will be lost, but we will still rely on the data and algorithms. For example, once driverless transportation is widely adopted and considered safer, cheaper and more convenient than human drivers, future generations may not wish to drive a car or even have a driving license. However humans will still be involved in the systems that automate the driving, the creative analysis of the telemetry and IoT data, the supervision and monitoring of the ecosystem, and the wider participation in the transport industry and sharing economy"
Top R Packages for Data Cleaning - KDnuggets,"One of the most common and powerful data programming tools is R, an open source language and environment for statistical computing and graphics. R provides uses with all the tools needed to create data science projects but with anything, it is only as good as the data that feeds into it. With that, there are a number of libraries within the R environment that help with data cleaning and manipulation before the start of any project"
Top R Packages for Data Cleaning - KDnuggets,This will create a visualisation of your data to spot any anomalies quickly for. A boxplot visualisation uses the same package but splits into quartiles for outlier detection. Both of these combined will quickly tell you if you need to limit the dataset or only use certain segments of it within any algorithms or statistical modelling
Top R Packages for Data Cleaning - KDnuggets,"This package is able to find duplicates by multiple columns and make friendly columns with ease from your dataframe. It even has a get_dupes() function for finding duplicate values amongst multiple rows of data. If you are looking to dedupe your data in a more advanced manner, for example, finding different combinations or using fuzzy logic, you may want to look into a"
The Pareto Principle for Data Scientists - KDnuggets,"More than a century ago, Vifredo Pareto, a professor of Political Economy, published the results of his research on the distribution of wealth in society. The dramatic inequalities he observed, e. 20% of the people owned 80% of the wealth, surprised economists, sociologists, and political scientists. Over the last century, several pioneers in varied fields observed this disproportionate distribution in several situations, including business. The theory that a vital few inputs/causes (e. 20% of inputs) directly influence a significant majority of the outputs/effects (e. 80% of outputs) came to be known as the Pareto Principle – also referred to as the 80-20 rule"
The Pareto Principle for Data Scientists - KDnuggets,"The Pareto Principle is a very simple yet extremely powerful management tool. Business executives have long used it for strategic planning and decision making. Observations such as 20% of the stores generate 80% of the revenue, 20% of software bugs cause 80% of the system crashes, 20% of the product features drive 80% of the sales etc. This way they are able to plan and prioritize their actions. In fact, today, data science plays a big role in sifting through tons of complex data to help identify future Paretos"
The Pareto Principle for Data Scientists - KDnuggets,"While data science is helping predict new Paretos for businesses, data science can benefit from taking a look internally, searching for Paretos within. Exploiting these can make data science significantly more efficient and effective. In this article, I’ll share a few ways in which we, as data scientists, can use the power of the Pareto Principle to guide our day-to-day activities"
The Pareto Principle for Data Scientists - KDnuggets,"If you are a data science leader/manager, you’d inevitably need to help develop the analytics strategy for your organization. While different business leaders can share their needs, you have to articulate all these organizational (or business unit) needs and prioritize them into an analytic roadmap. A simple approach is to quantify the value of solving each analytic need, and sort them in the decreasing order of value. You’ll often notice that the top few problems/use-cases are disproportionately valuable (Pareto Principle), and should be prioritized above the others. In fact, a better approach would be to quantify the complexity of solving/implementing each problem/use-case, and prioritizing them based on a trade-off between value and complexity (e"
The Pareto Principle for Data Scientists - KDnuggets,"Business problems tend to be vague and unstructured, and a data scientist’s job involves identifying the right scope. Scoping often requires keeping the focus on the most important aspects of the problem and deprioritizing aspects that are of less value. To start with, looking at the distribution of outputs/effects over inputs/causes will help us understand if high level Paretos exist in the problem space. Subsequently, we can choose to look at only certain inputs/outputs or causes/effects. For example, if 20% of stores generate 80% of sales, we can group rest of the stores into a cluster and do the analysis instead of evaluating them individually"
The Pareto Principle for Data Scientists - KDnuggets,"Complex business problems require data beyond what is readily available in analytic data marts. We need to request access, purchase, fetch, scrape, parse, process, and integrate data from internal/external sources. These come in different shapes, sizes, health, complexity, costs etc. Waiting for the entire data plan to fall in place can cause project delays that are not in our control. One simple approach could be to categorize these data needs based on their value to the end solution, e. Absolute must have, Good to have, and Optional (the Pareto Principle). This will help us focus on the Absolute must haves, and not get distracted or delayed by the Optional items. In addition to value, considering aspects of cost, time, and effort of data acquisition will help us better prioritize our data planning exercise"
The Pareto Principle for Data Scientists - KDnuggets,"It’s anecdotally said that a craftsman completes 80% of their work using only 20% of their tools. This holds true for us data scientists as well. We tend to use few analyses and models for a significant part of our work (the Pareto Principle), while the other techniques get used much less frequently. Typical examples during exploratory analysis include, variable distributions, anomaly detection, missing value imputation, correlation matrices etc. Similarly, examples during the modeling phase include k-fold cross-validation, actual vs. Building mini automations (e"
The Pareto Principle for Data Scientists - KDnuggets,"During the modeling phase, it doesn’t take us long to arrive upon a reasonable working model early in the process. Majority of the accuracy gains have been made by now (the Pareto Principle). The rest of the process is about fine-tuning the models and pushing for the incremental accuracy gains. Sometimes, the incremental accuracy gains are required to make the solution viable for business. On other occasions, the model fine-tuning doesn’t add much value to the eventual insight/proposition. As data scientists, we need to be cognizant of these situations, so that we know where to draw the line accordingly"
The Pareto Principle for Data Scientists - KDnuggets,"Today’s data science ecosystem is very multi-disciplinary. Teams include business analysts, machine learning scientists, big data engineers, software developers and multiple business stakeholders. A key driver of success of a such teams is communication. As someone who is working hard, you might be tempted to communicate all the work – challenges, analyses, models, insights etc. However, in today’s world of information overload, taking such an approach will not help. We will need to realize that there are ‘useful many but a vital few’ (the Pareto Principle) and use this understanding to simplify the amount of information we communicate. Similarly, what we present and highlight needs to be customized based on the target audience (business stakeholders vs"
Another 10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"We are in the process of writing a book on Mathematics for Machine Learning that motivates people to learn mathematical concepts. The book is not intended to cover advanced machine learning techniques because there are already plenty of books doing this. Instead, we aim to provide the necessary mathematical skills to read those other books"
Another 10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"By Aston Zhang, Zack C. Lipton, Mu Li, and Alex J. Smola"
Another 10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"You should look at your data. Graphs and charts let you explore and learn about the structure of the information you collect. Good data visualizations also make it easier to communicate your ideas and findings to other people. Beyond that, producing effective plots from your own data is the best way to develop a good eye for reading and understanding graphs—good and bad—made by others, whether presented in research articles, business slide decks, public policy advocacy, or media reports. This book teaches you how to do it"
Another 10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations"
Another 10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"The purpose of the book is to consider large and challenging multistage decision problems, which can be solved in principle by dynamic programming and optimal control, but their exact solution is computationally intractable. We discuss solution methods that rely on approximations to produce suboptimal policies with adequate performance. These methods are collectively referred to as reinforcement learning, and also by alternative names such as approximate dynamic programming, and neuro-dynamic programming"
What no one will tell you about data science job applications - KDnuggets,"For every person who has a question, and asks it, there are ten people who have the same question, but don’t ask it. If you’re one of those ten, then this post is for you. Hopefully you’ll find it helpful"
What no one will tell you about data science job applications - KDnuggets,I think my lack of academic pedigree is really what is killing me. It is not really skills (though they really need a lot of work and I am doing that). I am not even getting the interviews to show my skills so that’s why I say that
What no one will tell you about data science job applications - KDnuggets,"For example, if you apply for a technical job through Indeed, you’re very unlikely to get anywhere. Everybody knows Indeed, and it’s easy to apply that way. That means the average person who applies to a job on Indeed is, most likely, an average person. So a hiring manager will spend less time looking at a resume that comes from Indeed, because she expects it to be average"
What no one will tell you about data science job applications - KDnuggets,"[3] By using websites that most people don’t know about yet, you’re marking yourself as someone who seeks out opportunities intentionally. The average person who applies to a job on those websites is, most likely, above average. That’s why companies pay more attention to applicants who apply through less known channels"
What no one will tell you about data science job applications - KDnuggets,"Luckily there’s good news: most engineering teams understand that their HR can’t screen for talent. So the best engineering teams hire through networks and back channels instead of job boards. As a result, my best advice to you is:"
What no one will tell you about data science job applications - KDnuggets,"There are lots of ways to get better at interviewing, but the best way is to do it a lot. So my advice to go to meetups will help you here, too: the more you interview, the better you’ll get at it. Even if you bomb your first ones, it’s a skill like anything else and you’ll pick it up as you go"
What no one will tell you about data science job applications - KDnuggets,"Lastly, I’m sorry this system is so badly broken. I know it’s especially hard for beginners. It isn’t fair, but there’s light at the end of the tunnel: after you have 1–2 years’ experience, the companies start chasing you. The reward is worth the effort"
Data Science for Decision Makers: A Discussion with Dr Stelios Kampakis - KDnuggets,"While data science is a field showing immense growth at present, it’s somewhat nebulous in its description. I think there’s a lot of uncertainty as to exactly what it is and how to apply it. Fortunately, Stelios is an expert data scientist with a mission to educate the public about the power of data science and AI. He is a member of the"
Data Science for Decision Makers: A Discussion with Dr Stelios Kampakis - KDnuggets,"Well, that’s an interesting question. Many people these days end up being data scientists solely because there are great job prospects. You see people having studied something completely different (e"
Data Science for Decision Makers: A Discussion with Dr Stelios Kampakis - KDnuggets,"My case was quite different. My first degree was in Cognitive Psychology, and I was planning to become an academic in the area of cognitive science, AI and neuroscience. I ended up spending more and more time in data analysis, until I decided to do an applied PhD in machine learning. This was the transition point for me, as I moved away from academia and into the industry. However, I would say that my involvement with the wider area of data and AI, has lasted more than 10 years, and started in my early university days"
Data Science for Decision Makers: A Discussion with Dr Stelios Kampakis - KDnuggets,"I am one of the few people that have received training in all of these areas (and some more, e. I don’t have any particular preferences, and I’ve done work using methods from all of them. However, if I had the possibility, I believe that the most fascinating opportunity would be to work on general AI"
Data Science for Decision Makers: A Discussion with Dr Stelios Kampakis - KDnuggets,"Hm, I think that getting into data science now is the easiest it has ever been. I know some people will disagree with me. I also know, that me and other people have spent countless hours in our education and training before we called ourselves data scientists, so it might not seem fair to say this. However, let me explain"
Data Science for Decision Makers: A Discussion with Dr Stelios Kampakis - KDnuggets,"We had seen a similar trend with software development. You had to really know about computer science in the past, if you were to code the simplest thing. Now, many software projects can be accomplished by people who have just spent a few months going through tutorials and using high level frameworks and platforms"
Data Science for Decision Makers: A Discussion with Dr Stelios Kampakis - KDnuggets,"I believe we are going to see more of the same trends. Data science will expand across more and more verticals and more and more countries. I’ve been doing work with companies from all over the world (from Egypt and Cyprus, to the US and Germany). Industries in all countries are trying to catch up with the trend, collect more data and make better use of it"
Data Science for Decision Makers: A Discussion with Dr Stelios Kampakis - KDnuggets,"Much like the workshop, it is a high level overview of data science, and covers everything a decision maker needs to know about data science. It includes many case studies from my experience, as well as various examples from multiple industries. I also hand it over for free to the participants of the data science workshop"
Data Science for Decision Makers: A Discussion with Dr Stelios Kampakis - KDnuggets,"I am reading the blogs of all major companies doing research in AI (Uber, Google, Facebook, etc. I am also reading the papers from all major conferences (ICML, NeurIPS, AI & Stats, etc. Finally, there are some blogs and newsletters I am following, such as"
Top 7 Data Science Use Cases in Travel - KDnuggets,Data science has brought new marvelous opportunities to many industries. Along with these possibilities it has also brought constant changes and challenges. Travel and tourism industry is no exception here
Top 7 Data Science Use Cases in Travel - KDnuggets,"Travel is on its rise nowadays. This may be explained by the fact that it has become affordable to a broader audience. Thus, the target market has changed dramatically by getting more extensive than ever before. It is no more a privilege of the rich and noble. Moreover, travel and tourism have become a worldwide trend"
Top 7 Data Science Use Cases in Travel - KDnuggets,"To satisfy all the needs of the growing number of consumers and process enormous data chunks the data science algorithms are vital. Big data becomes a critical tool as far as airlines, hotels, reservation and booking websites and many others are striving to improve their services every day. Let’s consider several of widespread and efficient data science use cases in the travel industry"
Top 7 Data Science Use Cases in Travel - KDnuggets,"People in some way tend to appreciate travel experience personalization. Customer segmentation entails dividing all your customers according to their preferences and adaptation of the general stack of services to satisfy the needs of every group. Thus, the key idea is to find one solution that would fit all cases. In its turn, personalization is a trick that allows providing a specific service to a particular person. Thus, personalization makes this process deeper"
Top 7 Data Science Use Cases in Travel - KDnuggets,"Sentiment analysis is a branch of unsupervised learning aimed at analyzing textual data and recognizing emotional elements in the text. Sentiment analysis allows the company owner or the service provider to learn about the real attitude of the customers towards their brands. Regarding the travel industry, customers reviews play a huge role. Travelers often read reviews posted on various web platforms and websites and make decisions on their basis. That is why a lot of modern booking website offer sentiment analysis as a part of their service package for those travel agencies, hotels and hostels eager to cooperate with"
Top 7 Data Science Use Cases in Travel - KDnuggets,"These recommendations are often provided by matching the client's wishes and needs with the available offers. Generally speaking, applying the data-powered recommendation engines solutions the travel and tourism companies can offer the rental deals, alternative travel dates, new routes, destinations and attractions based on previous search and preferences. Due to recommendation engines the travel agencies and booking service providers can make suitable offers to all their customers"
Top 7 Data Science Use Cases in Travel - KDnuggets,"Route optimization plays a significant role in the travel and tourism industry. Trip planning, taking into account different destinations, schedules, working hours and distances may be quite challenging. Here comes travel route optimization"
Top 7 Data Science Use Cases in Travel - KDnuggets,"Nowadays, travel bots are truly changing the travel industry by providing exceptional assistance in travel arrangements and support for the clients. An AI-powered travel bot can answer questions, save user’s time and money, organize the trip and suggest new places to visit. The 24/7 accessibility mode and support of multiple languages make a travel bot the best possible solution for customers support"
Top 7 Data Science Use Cases in Travel - KDnuggets,"The most important factor to mention here is that these bots are constantly learning, therefore they become smarter and even more helpful every day. Thus, chatbot is capable of solving major travel and tourism tasks. Integration of a bot into your website would prove to be very beneficial. Such companies like JetBlue, Marriott, Ryanair, Hyatt, Hipmunk, Kayak, Booking and many others know this for sure"
Top 7 Data Science Use Cases in Travel - KDnuggets,In getting competitive advantages the companies seek to use big data with maximum benefit. In making decisions and actions travel and tourism companies largely rely on analytics. Both real-time and predictive analytics have many applications in the travel industry
Top 7 Data Science Use Cases in Travel - KDnuggets,"One of the most vivid use cases of real-time analytics in travel is tourism analytics. Tourism forecasting models allow predicting travel activity for specific periods and customer segments. Their principal task is to identify long-term and short-term opportunities for new deals. Due to the analysis of the previous clients’ activities, preferences and purchases the companies can predict future opportunities for business expansion"
Top 7 Data Science Use Cases in Travel - KDnuggets,Predictive analytics finds its implementation in dynamic pricing and fair forecasting. The practices of dynamic pricing and fair forecasting are not new to the travel industry. Every year more and more companies apply this technique to attract as many clients as it is possible
Top 7 Data Science Use Cases in Travel - KDnuggets,"As we all know the prices are the subject for the continuous changes depending on the season, weather, provider and the availability of places, seats, and rooms. With the help of smart tools, simultaneous monitoring of these price changes on multiple websites becomes possible. Self-learning algorithms are capable to collect historical data and predict future price movements taking into account all the external factors"
Top 7 Data Science Use Cases in Travel - KDnuggets,"Data science is changing the face of the travel industry. It helps travel and tourism businesses to provide unique travel experience and high satisfaction rates, preserving personal touch. In recent years data science has become one of the most promising technologies bringing changes to various industries. It has shifted the way we travel and our attitude toward traveling arrangements. The use cases presented in this article are only the tip of an iceberg. With a vast variety of solutions provided by the application of data science and machine learning, travel business can learn their clients’ needs and preferences to provide them with the best possible services and offers"
Asking Great Questions as a Data Scientist - KDnuggets,"Any of the questions above could yield a variety of answers, so it is imperative that you’re asking questions. Just because you have something in your mind that is an awesome idea for approaching the problem, does not mean that other people don’t similarly have awesome ideas that need to be heard an discussed. At the end of the day, data science typically functions as a support function to other areas of the business. Meaning we can’t just go rogue"
Asking Great Questions as a Data Scientist - KDnuggets,"Even the most seasoned data scientist will still find themselves creating a methodology or solution that isn’t in their area of expertise or is a unique use case of an algorithm that would benefit from the thoughts of other data subject matter experts. Often times the person listening to your proposed methodology will just give you the thumbs up, but when you’ve been staring at your computer for hours there is also a chance that you haven’t considered one of the underlying assumptions of your model or you’re introducing bias somewhere. Someone with fresh eyes can give a new perspective and save you from realizing your error AFTER you’ve presented your results"
Asking Great Questions as a Data Scientist - KDnuggets,"Great questions are the ones that get asked. However, there is an art and science to asking good questions and also a learning process involved. Especially when you’re starting at a new job, ask everything. Even if it’s something that you believe you should already know, it’s better to ask and course-correct, than to not ask. You could potentially lose hours working on an analysis and then have your boss tell you that you misunderstood the request"
Asking Great Questions as a Data Scientist - KDnuggets,"I’ve experienced what Karlo mentioned myself. Being direct can sometimes come off as judgement.  We definitely need to put on our “business acumen” hats on to the best of our ability to come across as someone who is genuinely trying to understand and deliver to their needs. I’ve found that if I can pose the question as “looking for their valuable feedback”, it’s a win-win for everyone involved"
Asking Great Questions as a Data Scientist - KDnuggets,"Follow-up questions feel good. When a question prompts another question you feel like you’re really getting somewhere. Peeling back another layer of the onion if you will. You’re collaborating, you’re listening, you’re in the zone"
Asking Great Questions as a Data Scientist - KDnuggets,Questioning has been instrumental to my career. An additional benefit is that I’ve found my ‘voice’ over the years. I feel heard in meetings and my opinion is valued. A lot of this growth has come from getting comfortable asking questions and I’ve also learned a ton about a given business/industry through asking these questions
Asking Great Questions as a Data Scientist - KDnuggets,I’ve learned a lot about diversity of viewpoints and that people express information in different ways. This falls under the “business acumen” piece of data science that we’re not often taught in school. But I hope you can go forward and fearlessly ask a whole bunch of questions
Python Data Science for Beginners - KDnuggets,Python’s syntax is very clean and short in length. Python is open-source and a portable language which supports a large standard library. Buy why Python for data science? Read on to find out more
Python Data Science for Beginners - KDnuggets,"Python is a popular high-level object-oriented programming language which is used widely by a huge number of software developers. Guido van Rossum designed this in 1991, and Python software foundation has further developed it. But the question is, with dozens of programming languages based on OOP concepts already available, why this new one? So, the main purpose to develop this language is to emphasize code readability and scientific and mathematical computing (e. NumPy, SymPy, Orange)"
Python Data Science for Beginners - KDnuggets,"It is a popular Python library which is useful in scientific calculations which provide array objects, as well as tools to integrate C and C++. NumPy provides a powerful N dimensional array which is in the form of rows and columns. These can be initialized from a Python list. To use this, first you just need to install the library using the command prompt by typing:"
Python Data Science for Beginners - KDnuggets,"Python is an incredible language for data science and those who want to start in the field of data science. It supports a huge number of array libraries and frameworks to give a choice for working with data science in a clean and efficient way. The various frameworks and libraries come with a specific purpose for use, and must be chosen according to your requirement. Here we have listed some of the"
Python Data Science for Beginners - KDnuggets,"As we have summarized before, NumPy is short for Numerical Python. It is the most popular library and base for higher level tools in Python programming for data science. An in-depth understanding of NumPy arrays helps in using Pandas effectively for data scientists. NumPy is versatile in that you can work with multi-dimensional arrays and matrices. NumPy has many built-in functions related to statistical, numerical computation, linear algebra, Fourier transform, etc. NumPy is the standard library for scientific computing with powerful tools to integrate with C and C++. If you want to master data science then NumPy is the must learn library"
Python Data Science for Beginners - KDnuggets,"Pandas is popularly known for providing data frames in Python. This is a powerful library for data analysis, compared to other domain-specific languages like R. By using Pandas it’s easier to handle missing data, supports working with differently indexed data gathered from multiple different resources, and supports automatic data alignment. It also provides tools for data analysis and data structures like merging, shaping, or slicing datasets, and it is also very effective in working with data related to time series by providing robust tools for loading data from Excel, flat files, databases and fast HDF5 format"
Python Data Science for Beginners - KDnuggets,"Matplotlib stands for Mathematical Plotting Library in Python. This is a library which is mostly used for data visualization, including 3D plots, histograms, image plots, scatterplots, bar charts, and power spectra with interactive features for zooming and panning for publication in different hard copy formats. It supports almost all platforms such as Windows, Mac, and Linux. This library also serves as an extension for the NumPy library. Matplotlib has a module"
Python Data Science for Beginners - KDnuggets,"After working for a decade in Infosys and Sapient, he started his first startup, Leno, to solve a hyperlocal book-sharing problem. He is interested in product marketing, and analytics. His latest venture"
Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019 - KDnuggets,"2018 has passed. The insane pre-holiday shopping is behind us, along with celebrations, and personal to-do lists for the next 12 months. But it’s a great time for a retrospective. So, let’s"
Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019 - KDnuggets,"While the number of “We’re hiring” posts from tech companies, other businesses, and government agencies increases, fewer specialists are available to fill the new vacancies. And machine learning tasks still must be performed, at least the routine ones. Vendors introduced tools for automating machine learning workflows (autoML) to solve this problem. While"
Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019 - KDnuggets,Model interpretability is not only important for companies that need to fulfill legal obligations to customers. It serves a technical purpose as well. Every ML model considers input features (problem properties) to predict results (outputs). The more relevant features we create and use to train an ML model during
Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019 - KDnuggets,"435 percent, which is close to the human average performance (86.831 percent). The model was trained on the latest version of"
Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019 - KDnuggets,"SQuAD2.0 is a reading comprehension dataset that consists of more than 100,000 question-answer pairs from its previous version and over 50,000 unanswerable questions. The dataset publishers’ goal is to force models to understand when a question can’t be answered given a context"
Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019 - KDnuggets,"Brands are looking for ways to engage with clients in relationships that would be mutually beneficial. Businesses strive to personalize interaction with customers showing content that may be meaningful for each of them. Data on customer preferences, their individual details, online behavior, or device must be collected and processed to deliver a personalized experience"
Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019 - KDnuggets,"However, building and developing a recommender system for the travel sector poses several challenges. One of the problems is caused by the exactitude of services/products provided via an online travel agency. Unlike media content (songs or movies), accommodation options are limited and dynamic: “"
Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019 - KDnuggets,"Moreover, recommending a wrong hotel or an apartment may cause more customer frustration than suggesting a video, for instance. So, many attributes for booking options (location, facilities, refund policies, etc. Data sparsity is another problem:"
Robust Quality – Powerful Integration of Data Science and Process Engineering. - KDnuggets,"With rapid growth in data and its usage, data quality is becoming quite important. It is important to connect these two aspects of quality to ensure better performance. The book by Rajesh Jugulum provides a strong connection between the concepts in data science and process engineering that is necessary to ensure better quality levels and takes you through a systematic approach to measure holistic quality with several case studies"
Learn How to Listen: One of the hardest parts of being a data scientist - KDnuggets,"These meetings are important. Very important. There you can understand the business, the goals of the area, their KPIs, and what are the requirements for the work they want you to do"
Learn How to Listen: One of the hardest parts of being a data scientist - KDnuggets,"Careful here, that doesn’t mean that if you are right, and you have the means to prove it, you should just stay there and agree to whatever. The concept of idea-meritocracy is important here. Everyone has a point of view, one better than the other, being able to discern and find the best solution to a problem is possible. Here you can see a great video that explain this in a more graphical way:"
Learn How to Listen: One of the hardest parts of being a data scientist - KDnuggets,"Don’t just be there staring at the presentation, or at people faces, your phone or anything else. Take these 30–45 mins to have a productive meeting and focus. Take notes if you need them, but pay attention, they deserve it"
Learn How to Listen: One of the hardest parts of being a data scientist - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"In the course of time, data science has proved its high value and efficiency. Data scientists find more and more new ways to implement big data solutions in daily life. Nowadays data is a fuel needed for a successful company"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"Telecommunication companies are not an exception. Due to these circumstances, they cannot afford not to use data science. Within the telecom industry data science applications are widely used to streamline the operations, to maximize profits, to build effective marketing and business strategies, to visualize data, to perform data transfer and for many other cases. Key activities of the companies working in the telecommunication sector are strongly related to data transfer, exchange, and import. The amounts of data passing through various communication channels are getting larger every minute. Therefore, old techniques and methods are no longer relevant"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"Telecommunication industry being the one attracting almost the most significant number of users every day is a vast field for fraudulent activity. The most widespread cases of fraud in the telecom area are illegal access, authorization, theft or fake profiles, cloning, behavioral fraud, etc. Fraud has a direct influence on the relationship established between the company and the user"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"Therefore, fraud detection systems, tools, and techniques found wide usage. By applying unsupervised machine learning algorithms to an immense amount of customer and operator data to spot the characteristics of normal traffic you can prevent fraud. The algorithms define the anomalies and with the help of data visualization techniques present them as alerts to the analysts in real time. The efficiency of this technique is very high because it allows to provide an almost real-time response to the suspicious activity"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"Predictive analytics is applied by the telecommunication companies to get valuable insights to become faster, better and make data-driven decisions. Knowledge of customer preferences and needs gives a better understanding of the customer. Predictive analytics uses historical data to build forecasts. The better is the quality of the data and the longer they are historically, the better is predictability"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"The key to success for the telecommunication companies is to segment their market and target the content according to each group. This golden rule is relevant to the various areas of business. Speaking about telecommunication, there are four segmentation schemes of primary importance: customer value segmentation, customer behaviour segmentation, customer lifecycle segmentation, and customer migration segmentation"
Top 10 Data Science Use Cases in Telecom - KDnuggets,Acquiring a customer is a challenging task. Keeping the customer engaged requires a lot of effort as well. Accurate diagnosis of the customer's behavior and enabling alerts highlight the customers at a risk defecting. Smart data platforms can bring together customer transactions data and data from real-time communication streams to disclose the insights concerning customers feelings about the services. This allows immediate addressing the satisfaction-related issues and churn prevention
Top 10 Data Science Use Cases in Telecom - KDnuggets,"Customer lifetime value is a discounted value of all the future profits and revenues generated by the customer. The CLV model is concentrated on customer purchasing behavior, activity, services utilized, and average customer value. Smart solutions process real-time insights distinguishing between profitable, nearly profitable, and unprofitable segments of customers predicting future cash flows"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"Telecommunication companies tend to regard the customer's engagement process and internal channels as a guarantee of smooth functioning of the operations. Network management and optimization gives an opportunity to define the score points in operations to identify the root causes of these complications. Looking into historical data and predicting possible future problems or, on the contrary, beneficial scenarios is a great benefit for the telecom providers"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"Product development is a complex process that needs control and thoughtful management starting from the stage of concept development till ongoing lifecycle management and maintenance. Ensuring the high-quality performance of the product according to the customer’s requirements is impossible without applying smart data solutions. Data-driven product development process should take into consideration not only customer needs but the results of digital analytics implementation, internal feedbacks, and marketing intelligence"
Top 10 Data Science Use Cases in Telecom - KDnuggets,Recommendation engines are present in all the spheres of our digital life. Telecommunication sphere is among these aspects. Ignoring the enormous data sets concerning customers preferences would become a significant loss for the telecom. Prediction of the future needs becomes possible to the availability of data
Top 10 Data Science Use Cases in Telecom - KDnuggets,Customer sentiment analysis is a set of methods applied for information processing. This analysis allows assessment of the customer positive or negative reaction to the service or product. Analysis of the aggregated data also allows revealing recent trends and reacting to the customers’ problematic issues in real-time. Customer sentiments analysis largely relies on text analysis techniques. Modern tools collect feedback from various social media sources conduct analysis and provide an opportunity of utilizing mechanisms for direct responding
Top 10 Data Science Use Cases in Telecom - KDnuggets,"The telecommunication industry is famous for its long-term experience in dealing with significant data streams for years. Due to rapid development of the internet and the evolving of 3G, 4G, and even 5G connections, telecommunication companies face the challenge of the constantly changing customer requirements. The subscribers are becoming more and more demanding, and the traffic gets more active every day"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"Real-time streaming analytics can deal with this task. Modern streaming analytic solutions are specially tailored to continuously ingest, analyze and correlate data gained from multiple sources and generate response action in real-time mode. Real-time analytics combines the data related to customer profiles, network, location, traffic, and usage to create a 360-degree user-centric view of the product or service. It also captures and analyzes the interaction and communication between the customers"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"The telecommunication sphere belongs to highly competitive industries. Acquiring as many subscribers as possible remains a critical goal, anyway. Due to the fact that in recent years the number of users has been growing extremely fast, pricing emerged as a tool to limit congestion and increase revenue at the same time"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"Telecommunication industry has been boosted by the active use of machine learning and data science. This step was made only for the better. A great many of aspects and issues became much easier to resolve, control or even prevent from happening"
Top 10 Data Science Use Cases in Telecom - KDnuggets,"The telecommunication sphere had to adopt modern technologies and techniques to stay relevant and not to lose positions under severe circumstances of the global market. Telecom companies operate with vast communication networks and infrastructures with the intense data flow. Processing and analyzing this data with the help of data science algorithms, methodologies and tools find practical application. Therefore, we attempted to specify several of these use cases and to demonstrate real benefits one can get"
"REV 2: Next Data Science Leaders Summit, NYC, May 23-24 - KDnuggets","Quite apt since 2018 represented a worldwide reality check for the risk landscape related to data: GDPR and data privacy compliance, widespread security breaches and leaked data, cyber threats specifically targeting machine learning models, news unfolding about Cambridge Analytica, plus the growing recognition of our responsibilities for social impact through ethical data science. We reviewed the problems and solutions. We revised our ideas about priorities for data science. We looked ahead"
"REV 2: Next Data Science Leaders Summit, NYC, May 23-24 - KDnuggets","Looking beyond problems and solutions, how do we manage the team within that complex landscape? A decade has passed since industry first began to embrace the practice of data science. We have great examples about learning data science, and how people can upskill to join industry teams – as those inspiring “Data 8” courses at UC Berkeley with thousands of students illustrate. While acquiring skills to become an individual contributor is incredibly valuable and so much in demand, the practice of leading data science teams in enterprise represents an entirely different matter"
"REV 2: Next Data Science Leaders Summit, NYC, May 23-24 - KDnuggets","Consider: Why do we need models? Where are companies failing to use models appropriately, and why? What are some practical steps data science organizations should take to apply models successfully in their business? Much of Dr. Kahneman’s life and work has been devoted to the science of human decision making. In this era of AI applications, unbundling the process of decision-making represents a key challenge in enterprise, and Dr. Kahneman brings crucial insights"
Preparing for the Unexpected - KDnuggets,"In some domains, new values appear all the time, so it's crucial to handle them in a good way. Using deep learning, one can learn a special Out-of-Vocabulary embedding for these new values. But how can you train this embedding to generalize well to any unseen value? We explain one of the methods employed at Taboola"
Preparing for the Unexpected - KDnuggets,"Unseen values, also called OOV (Out of Vocabulary) values, must be handled properly. Different algorithms have different methods to deal with OOV values. Different assumptions on the categorical features should be treated differently as well"
Preparing for the Unexpected - KDnuggets,"In this post, I’ll focus on the case of deep learning applied to dynamic data, where new values appear all the time. I’ll use Taboola’s recommender system as an example. Some of the inputs the model gets at inference time contain unseen values – this is common in recommender systems. Examples include:"
Preparing for the Unexpected - KDnuggets,"One simple solution is to replace all the rare values with a special OOV token before training. Since all OOV values are the same from the model’s point of view, we’ll replace them with the OOV token at inference time. This solution has two positive outcomes:"
Preparing for the Unexpected - KDnuggets,"The interesting thing is that even if we don’t use the item id at all during training, the model still performs worse on rare items! This is because they come from a distribution different than that of the general population. They have specific characteristics – maybe they performed poorly online, which caused Taboola’s recommender system to recommend them less, and in turn – they became rare in the dataset. So why does this distribution difference matter?"
Preparing for the Unexpected - KDnuggets,"If we learn the OOV embedding using this special distribution, it won’t generalize to the general population. Think about it this way – every item was a new item at some point. At that point, it was injected with the OOV token. So the OOV embedding should perform well for all possible items"
Preparing for the Unexpected - KDnuggets,"How can we use lots of examples to train the OOV embedding while at the same time use the same examples to train the non-OOV embeddings? Instead of randomly injecting the OOV token before starting to train, we chose the following approach: in each epoch the model trains using all of the available values (the OOV token isn’t injected). At the end of the epoch we sample a random set of examples, inject the OOV token, and train the model once again. This way, we enjoy both worlds!"
Preparing for the Unexpected - KDnuggets,"Our model had been used in production for a long time before we thought of the new approach.  It could have been easy to miss this potential performance gain, since the model performed well overall. It just stresses the fact that you always have to look for the unexpected!"
Upskill your Data Science Career. Attend ODSC East in person or Live Online. 45% off Ends Friday - KDnuggets,"ODSC East is the top conference for data science practitioners and AI engineers: 300+ talks, full and half-day expert-led trainings, and shorter hands-on workshops. KDnuggets subscribers save 45% with code KDN45. Register now!"
4 Reasons Why Your Machine Learning Code is Probably Bad - KDnuggets,"Your current ML workflow probably chains together several functions executed linearly. Instead of linearly chaining functions, data science code is better written as a set of tasks with dependencies between them. That is your data science workflow should be a DAG"
4 Reasons Why Your Machine Learning Code is Probably Bad - KDnuggets,Below is a stylized example of a machine learning flow which is expressed as a DAG. In the end you just need to run TaskTrain() and it will automatically know which dependencies to run. For a full example see
10 Trending Data Science Topics at ODSC East 2019 - KDnuggets,"ODSC East 2019, Boston, Apr 30 - May 3, will host over 300+ of the leading experts in data science and AI. Here are a few standout topics and presentations in this rapidly evolving field. Register for ODSC East at 50% off till Feb 8"
10 Trending Data Science Topics at ODSC East 2019 - KDnuggets,"In this talk, Ben explains the concept of compositional machine learning, which is the nesting of two or more functions to form a single new function. For example, looking at cats and dogs and being able to recognize specific parts, such as paws, tails, whiskers, and more. By developing a model like this, you will be able to develop much more sophisticated models that can understand more complicated concepts in your data"
10 Trending Data Science Topics at ODSC East 2019 - KDnuggets,"Here, Yong will focus on the KafkaDataset module in TensorFlow. KafkaDataset processes Kafka streaming data directly to TensorFlow's graph. As a part of Tensorflow (in `tf. The module exposes a machine learning friendly Python interface through Tensorflow's `tf. It could be directly fed to `tf"
10 Trending Data Science Topics at ODSC East 2019 - KDnuggets,"Recommendation engines (also known as recommendation systems) are an integral part of many online platforms and retail companies. You’ve likely encountered countless in the past, such as in an online store’s “You may also be interested in…” or “Customers also purchased…” features. What’s the magic behind the scenes that make them work so well?"
10 Trending Data Science Topics at ODSC East 2019 - KDnuggets,Open-source libraries like Pandas are essential tools for making data science more accessible to the greater data science community. Python is one of the more popular languages data scientists use due to the availability of tools like pandas and scikit-learn. Thus the two together make the perfect pair
10 Trending Data Science Topics at ODSC East 2019 - KDnuggets,"In this workshop, you will learn how to accelerate your data analyses using the Python language and Pandas, a library specifically designed for interactive data analysis. Pandas is a massive library, so Daniel will focus on its core functionality, specifically, loading, filtering, grouping, and transforming data. Having completed this workshop, you will understand the fundamentals of Pandas, be aware of common pitfalls, and be ready to perform your own analyses"
10 Trending Data Science Topics at ODSC East 2019 - KDnuggets,"Anyone involved in marketing knows how much work goes into it. Many marketing pros would much rather handle the decision making, strategic side of things, rather than the tedious repetitive emailing and social posting. How great would it be if you had an AI that could do the grunt work for you?"
10 Trending Data Science Topics at ODSC East 2019 - KDnuggets,"In this talk, Ilya will discuss automatic decision-making and AI techniques for promotional campaigns. First, they will present a methodology to develop highly automated promotion management systems. Next, Ilya will walk through practical examples of how advanced customer and content signals can be generated using predictive models and then be used in the automation of targeting, budgeting, and pricing decisions"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"Agile software development has taken over the hi-tech industry. Whether implemented as Scrum, Kanban or Scrumban, these methods were created in order to be flexible and allow quick changes by working in short cycles. While these implementations are well suited for development, they collide in certain aspects with research, therefore in order to be agile in research, we need to adapt the core values of Agile and reconcile them with research methodologies, I. The following is a method I developed, which is based on my personal experience managing a data-science-research team and was tested with multiple projects. In the next sections, I’ll review the different types of research from a time point-of-view, compare development and research workflow approaches and finally suggest my work methodology"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"Short or medium-term projects, in my opinion, apply to any project with some affinity to academia or a company-related project that aims to create a novel algorithm or implements a new feature but with industry constraints such as time, resources or money. In contrast, long-term research in the industry is usually the most feared by (although there are exceptions). For example, in many interviews, as a PhD graduate, I was asked if I could work in a fast-paced startup-mentality company and deliver results on a short-term basis"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"You try to design a generic architecture that is clear, reusable and that can be easily maintained in the future. In research, the process can be compared to a prototyping stage where we need a lot of flexibility, this allows us to try many ideas as fast as possible. For me and others, this boils down to using “Notebooks” — an interactive python environment that allows you to"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"In traditional IDEs, you are forced to reload your dataset and waste valuable time as many times as you restart your debugging process, in notebooks the dataset is persistent and is kept in memory for the entire duration (as long as the kernel is not reset). In a notebook, the debug process consists of using print()’s, therefore, the debugging stage is really quite simplistic. Eventually, when the algorithm is finished, we restructure it using traditional software development tools such as PyCharm using OOP, Design-patterns and finally we write input-output tests"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"These small tasks are pulled by team members until completion, trying to finish all tasks until the cycle ends. The cycle is reset every X weeks. In general, research tasks are longer and they don’t always correspond well with short cycle methodology. For example, when we kick-start a model, it may take a few weeks to get the coveted accuracy metric. However, this doesn’t mean that we don’t see measurable results during those weeks in other ways, i"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"In research, we look at the product demands, assign features, think of possible algorithmic solutions, define goals and KPIs. The truth is that we don’t have a crystal-clear path toward that goal, in other words, we don’t know what the exact road to completion in terms of tasks is. Algorithmic development is not merely production, it is much more about understanding the problem, assessing options, validation, etc. In practice, we test many different hypotheses and ideas, based on intuition and experience, some may help, others may not"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"We first decide on a sensible deadline for a project, whether two-weeks, a month or more, basically as much time as you think it should take based on your experience or guesstimate. Deadlines between different projects are not aligned, therefore hard to place in rigid cycles. It’s important to keep in mind that these deadlines can change, projects may be extended or end earlier than we anticipated"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"In each stage, I create as many ideas, hypotheses or tasks, i. For example, in the ‘literature review’ stage you could have several tasks such as looking for papers on Google Scholar, searching Github. In the ‘data exploration’ stage you could possibly explore feature engineering, selection or all the embedding methods available today, from word2vec, phrase2vec, sent2vec to Elmo, Bert, etc. In the ‘algorithm’ stage we can test several classic machine-learning algorithms, try some neural net ideas (CNN, LSTM, BI-GRU, multi-input networks), stacking algorithms, ensembles, etc. In the ‘result analysis’ stage, we can explore many metrics such as accuracy, F1, explore the correctness of the model by looking at the content, etc. In the ‘review’ stage, a team member reviews our algorithm. Finally in the ‘deployment stage’, we convert our notebooks into a clear class-based API, exposing init(), train(), predict(), upload_model() and download_model() to the DevOps team,"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"I do not assign estimations on each deliverable as this adds planning overhead, installs a rigid work-plan that I want to avoid and disrupts creativity in the research process, i. I want my team to explore different solutions that appear and are thought of during the creative process and not stick a predetermined plan that is basically just a wish-list. In other words, data, results, and insights from the process yields many brilliant ideas that will allow my team to solve novel business problems"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"The list of tasks is contained within each project box, the appropriate task can be tagged for ‘completion’ or ‘in progress’ in the UI, obviously, you only work on the subtask list in the stage that the project box resides in. As your team grows, you may have a dedicated person whose job is to maintain every sub-task in extreme detail and you may feel the need to use Jira’s built-in sub-task management which adds some overhead to subtask management. I Personally feel that the majority of our sub-tasks do not need to be tracked in full and the board view should be as clean and grouped as possible"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"Project management doesn’t end with a management board, a daily meeting is also a very important aspect of project management. My team has a one-hour daily meeting, usually in the morning and similarly to development we try to brainstorm, synchronize, talk about what we did yesterday, discuss issues that we are stuck at and talk about the next steps. Due to the relatively longer nature of our talks, I found that a sit-down daily is more appropriate, in contrast to a stand-up daily, i. This allows us to share ideas, be creative and constantly improve without waiting for feedback at the end of a cycle"
Data-science? Agile? Cycles? My method for managing data-science projects in the Hi-tech industry. - KDnuggets,"Hopefully, You can adapt or use some of the ideas mentioned here for managing data-science-research projects. Keep in mind that for this methodology to work, your company must understand that research is an uncertain process, some results can’t be guaranteed, but with the right project management methodology, the process can be successfully controlled in order to achieve our goals. Finally, if you are interested in project workflows from product design until model completion and maintenance, Shay Palachy wrote an excellent"
6 Books About Open Data Every Data Scientist Should Read - KDnuggets,"If some of that sounded like Greek to you, that’s OK. This is an introduction, after all — and the book makes sure to include Java source code for each major component. Readers should find this an accessible, practical and mathematically focused way to ease into this complex but fascinating topic. It’s intended for all self-learners as well as undergraduate students"
6 Books About Open Data Every Data Scientist Should Read - KDnuggets,"Globalization is a fraught and controversial concept, but nobody questions how mobile and global our data has become. For anybody who wants an extremely modern take on the intersection of globalization and open data analysis, it makes sense to turn to thought leaders. Hailing from NYU’s GovLab, authors Verhulst and Young have put together"
6 Books About Open Data Every Data Scientist Should Read - KDnuggets,"Gone are the days when citizens remained in the dark about how their digital lives get surveilled, mined for profitable insights and even sold to unknown third parties. It’s not just our web browsing habits, either — the panoply of connected devices we rely on represent, essentially, a global surveillance network. Whether or not people use this network for pro-social objectives relies on the intentions of those tasked with building it"
6 Books About Open Data Every Data Scientist Should Read - KDnuggets,"That means data scientists. Weigend himself has been a consultant for businesses, financial and health care entities and even the educational community. He argues that big data and data science are tools for positive change, but that we haven’t yet laid down a common framework to reconcile the needs of big business with the right to online privacy. This book proposes ways to “make data work for us"
6 Books About Open Data Every Data Scientist Should Read - KDnuggets,"Data scientists don’t just need a variety of concrete skills to draw from. They also need to know how those skills can apply to making more data-driven decisions in investing, developing startups, R&D, community organizing, interfacing with the public and much more. This book is ideal for decision-makers and data scientists alike, since it thinks “big picture” and presents credible strategies for engaging in data analysis in a collaborative, ethical and consequential manner"
How I used NLP (Spacy) to screen Data Science Resumes - KDnuggets,"A friend of mine has his own Data Science consultancy. He recently bagged a good project which required him to hire 2 Data Scientist. He had put a job posting on LinkedIn and to his surprise he received close to 200 resumes. When I met him in person he remarked, “"
How I used NLP (Spacy) to screen Data Science Resumes - KDnuggets,I had been working on few NLP projects both as part of work and as a hobby for past 2 years. I decided to take a shot at my friend’s problem. I told my friend perhaps we can solve this problem or at least bring down the time of manual scanning through some NLP techniques
How I used NLP (Spacy) to screen Data Science Resumes - KDnuggets,I was on a look out for a library that kind of does ‘phrase/word matching’. My search requirement was satisfied by Spacy. Spacy has a feature called ‘Phrase Matcher’. You can read more about it
How I used NLP (Spacy) to screen Data Science Resumes - KDnuggets,"There are many off the shelf packages which help in reading the resume. Luckily all the resume that my friend had got was of the PDF format. So, I decided to explore PDF packages like PDFminer or PyPDF2. I chose PyPDF2"
How I used NLP (Spacy) to screen Data Science Resumes - KDnuggets,"A typical Data Scientist has two options either position himself/herself as a generalist or come across as an expert in one area say ‘NLP’. Based on the job requirement, a Data Scientist can run this code against his/her resume and get to know which keywords are appearing more and whether he/she looks like a ‘Generalist’ or ‘Expert’. Based on the output you can further tweak your resume to position yourself accordingly"
"From Good to Great Data Science, Part 1: Correlations and Confidence - KDnuggets","As a data scientist you'll spend a lot of time answering questions with data. I currently work as a data scientist in the healthcare industry, providing metrics and building models for hospitals and healthcare related organizations. In my practice most of my time is spent doing two things:"
"From Good to Great Data Science, Part 1: Correlations and Confidence - KDnuggets","What they both have in common is that there's a right way and a wrong way to do them. Further, it's very easy to answer these questions the wrong way and have your mistakes go unnoticed. As you'll see, the difference between great and average answers to these questions is a having a little bit of a mathematical background"
"From Good to Great Data Science, Part 1: Correlations and Confidence - KDnuggets","As you can see, each row represents a hospital. Each hospital has a mortality rate and quality score. The premise is to"
"From Good to Great Data Science, Part 1: Correlations and Confidence - KDnuggets",Not quite. We've made a huge assumption about our data. We've assumed that our data was anything
"From Good to Great Data Science, Part 1: Correlations and Confidence - KDnuggets",Much better. Now the task at hand is to rank prescribers on the basis of how many opioids they prescribe. But prescribers prescribe different amounts of drugs. This hints that we should now take each prescriber's opioid prescription ratio. Let's do just that
"From Good to Great Data Science, Part 1: Correlations and Confidence - KDnuggets",The formula looks intimidating but it's intuitive if you take the time to walk through it. Explaining the mathematics behind why this formula works is worth an entire discussion itself and is thus out of scope. So we'll just focus on applying it below
"From Good to Great Data Science, Part 1: Correlations and Confidence - KDnuggets","As both of these examples have demonstrated, sometimes it’s not such an easy task and many data scientists often fall short in this regard. It’s very easy to fall into the trap of using parametric correlation in a non-parametric scenario. And it’s very easy to naively sort a list of Bernoulli trials without taking into account the number of observations for each trial. In fact, it happens more often than you’d think in practice"
The Essential Data Science Venn Diagram - KDnuggets,"It was helpful, and we all were enlightened.  Thank you, Dr. Conway!"
The Essential Data Science Venn Diagram - KDnuggets,"First off, let me say that my Venn diagram, like all the Venn diagrams, is inherently flawed. Why? Because it is a two-dimensional diagram illustrating what is inherently a multi-dimensional dynamic. Part of this is reflected in the disclaimer in the lower corner that the dimensions of communication (audial + visual) and soft skills are not shown"
The Essential Data Science Venn Diagram - KDnuggets,"One implication of this delineation is that “traditional research,” for the most part, has not made sufficient use of multivariate statistics. Depending on your discipline, this implication may not hold true. Unfortunately, for many disciplines, it does hold true. It could be said that this fact is part of why so many industries are susceptible to disruption"
The Essential Data Science Venn Diagram - KDnuggets,"An example of this is the persistent over-reliance on the coefficient of determination (R-squared) that continues in numerous circles. R-squared is not resistant to outliers, is only relative to the target variable, may inflate with serially-correlated data, and in many other ways can be unreliable. Even so, it is heavily utilized — in part because Excel and nearly every other graphing tool makes it readily available. While R-squared can be a useful tool, it is still a tool that must be used correctly (e"
The Essential Data Science Venn Diagram - KDnuggets,"Multivariate problems require multivariate tools, and a majority of business challenges are inherently multivariate. Many analysts and technical professionals still have a long way to go in learning how to properly apply computational multivariate statistics ( ⊆ “machine learning”). As such, they remain susceptible to disruption from competitors who harness better insights from data using multivariate algorithms … or disruption from themselves by adopting invalid strategies ("
The Essential Data Science Venn Diagram - KDnuggets,"As portrayed, insight can also come simply from the overlap of having valid intuition (automation is not a prerequisite). Such has certainly been the case for much of history. The current hype of advanced analytics is largely about accelerating insights by automating data collection, processing, and analysis"
The Essential Data Science Venn Diagram - KDnuggets,"The danger of “bias” exists in the “traditional research” zone and is the main inspiration for this update of the Data Science Venn Diagram. Statistical “bias” is the exclusion or ignoring of significant variables, not unlike the colloquial meaning. Since most people are not familiar with handling multivariate analyses, the danger of bias most readily creeps in when multivariate problems are treated as bivariate or univariate problems. As noted above, the presence of bias makes organizations susceptible to disruption by newcomers whose multivariate approaches make them more insightful competitors"
The Essential Data Science Venn Diagram - KDnuggets,"The area labeled as “traditional software” is problematic to describe. This ties back to the limitations of trying to summarize a multivariate system with a two-dimensional graphic. This overlap could also have been used to characterize mechanical automation (which certainly constitutes a significant portion of the global economy). For the purposes of this discussion, and in the context of most white-collar intellectual endeavors, suffice it to say that this area can represent a lack of rigor when performing risk assessments"
The Essential Data Science Venn Diagram - KDnuggets,Are there exceptions to the sequence proposed above? Certainly! It is not uncommon for machine learning outputs to bring an association to a domain expert’s attention that they had previously dismissed or were unaware of. There are also plenty of cases where domain experts’ heuristics were already implemented in automated systems and later validated once data collection became possible. Let us also not forget the role of automation in helping us collect data in the first place that can later be evaluated with domain expertise and statistical rigor
The Essential Data Science Venn Diagram - KDnuggets,"To sum it up, this new Venn diagram tells us things we already knew, but perhaps had not formally verbalized. It is a simple schema, and as such can help people better prioritize their workflows. For example, it encourages data scientists to start their work by first talking to domain experts — something that is already touted as a best practice. If you are the type of person who is not content with a list of"
The Essential Data Science Venn Diagram - KDnuggets,S. Analytics from Texas A&M University and a M.S. Earth Science from Rice University. He currently consults in the upstream energy sector
What Is Dimension Reduction In Data Science? - KDnuggets,We have access to a large amounts of data now. The large amount of data can lead us to situations where by we take every possible data that is available to us and feed it into a forecasting model to predict our target variable. This article aims to explain the common issues associated with introduction of large set of features and provides solutions which we can utilise to resolve those problems
What Is Dimension Reduction In Data Science? - KDnuggets,Occasionally we gather data for our data science project and end up gathering a large set of features. Some of these features (known as variables) are not as important as others. Sometimes the features themselves are correlated with each other. And occasionally we end up over-fitting the problem by introducing too many features. The large number of features make the data set sparse
What Is Dimension Reduction In Data Science? - KDnuggets,Imagine you want to e-mail a large set of files to your friend. Uploading and sending the files might take a longer time. You can speed up the process of uploading of the files by zipping the files and e-mail the zipped file instead. Zipping the file compresses large quantity of data into smaller equivalent sets
What Is Dimension Reduction In Data Science? - KDnuggets,Firstly the eigenvectors of the variance-covariance matrix are calculated. The vector represents the directions of maximum variance which are known as the principal components. The eigenvalues are then created that define magnitude of the principal components
What Is Dimension Reduction In Data Science? - KDnuggets,"Essentially, non-linear data is mapped and transformed onto a higher-dimensional space. Then PCA is used to reduce the dimensions. However, one downside of this approach is that it is computationally very expensive"
What Is Dimension Reduction In Data Science? - KDnuggets,We then compute kernel matrix. This requires us to construct a similarity matrix. The matrix is then decomposed via creating eigen values and eigen vectors
What Is Dimension Reduction In Data Science? - KDnuggets,"We have access to a large set of data now. When we are building forecasting models that are trained on images, sound and/or textual contents then the input feature sets can end up having a large set of features. It increases space, further adds over-fitting and slows down the time to train the models. Occasionally features are introduced that end up adding more noise than expected"
DATAx Singapore – meet Data Science Leaders – special year of the Pig offer - KDnuggets,"Copyright © 2018 The Innovation Enterprise Ltd. All Rights Reserved. Innovation Enterprise Ltd is a division of Argyle Executive Forum. Registered in England and Wales, Company Registered Number 6982151, 131 Finsbury Pavement London EC2A 1NT"
Free eBook: Practical Data Science Cookbook – Second Edition - KDnuggets,"Data is the backbone of any modern business or organization. As increasing amounts of data are generated each year, the need to analyze and create value out of it is more important than ever. Companies that know what to do with their data and how to do it well will have a competitive advantage over companies that don’t. Because of this, there will be an increasing demand for people that possess both the analytical and technical abilities to extract valuable insights from data and create valuable solutions that put those insights to use"
The Data Science Gold Rush: Top Jobs in Data Science and How to Secure Them - KDnuggets,"Of course, there are plenty of other positions related to the field of data and analytics, with graduates, job seekers, and those seeking career changes alike all vying for an opportunity to execute them. Because big data touches almost every industry across the board, those who aren’t already working in data and analytics will soon be utilizing the technology for its undeniable business benefits. Whichever way you slice it, the future of work is through data"
The Data Science Gold Rush: Top Jobs in Data Science and How to Secure Them - KDnuggets,"5 percent in the 10-year period between 2014 and 2024. This is well ahead of the 6.5 percent growth projected for the rest of the job market, adding a potential 488,500 jobs to the workforce"
Data Science Project Flow for Startups - KDnuggets,"I have divided the process into three aspects that run in parallel: product, data science and data engineering. In many cases (including most of the places I worked for), there might not be a data engineer to perform these duties. In this case the data scientist is usually in charge of working with developers to help with these aspects. Alternatively, the data scientist might do these preparations, if they happen to be the rarest of all of God’s beasts:"
Data Science Project Flow for Startups - KDnuggets,"This can sometime entail dumping large data sets from production databases into their staging/exploration counterparts, or to colder storage (for example, object storage) if its time availability is not critical in the research phase. Conversely, it can mean pulling large data dumps from very cold storage back into table or document form to enable fast querying and complex computations. Whatever the case, this phase is required for the research phase to start and frequently ends up taking more time than expected, and so that’s the right time to initiate it"
Data Science Project Flow for Startups - KDnuggets,"Nevertheless, the metric-to-product-value function might be a step function, meaning that any model performing under some X value has no use for the customer; in these cases, we will prefer iterating until that threshold is suppressed. However, while this X might be very high in some cases, I believe that both product/business people and data scientists tend to overestimate the height of this step; it’s very easy to state that anything under 95% accuracy (for example) provides no value and can’t be sold. In many cases, however, careful examination and challenging of product assumptions can lead to very valuable products that might not be as demanding technically (at least for the first iteration of the product)"
Data Science Project Flow for Startups - KDnuggets,"Finally, the product person in charge needs to approve the scope and KPIs defined. It is the data scientist’s job to make sure everybody understand the implications of the scope — what was included and what was prioritized — and the relation between the product KPIs and the harder metrics that will guide her during model development, including the extent to which the letter approximate the former. Stating this explicitly can prevent cases where the consumers of the models being developed — product and business people — understand only during or after model development that the wrong metric was optimized"
Data Science Project Flow for Startups - KDnuggets,"By now the initial set of required data should have been made available by data engineering. However, some deficiencies in the explored data will often be discovered during this phase, and additional data sources might be added to the working set. The data engineer should be prepared for this"
Data Science Project Flow for Startups - KDnuggets,"This is an important check to perform at this stage because some data and software engineering can begin in parallel to model development. Additionally, a suggested solution might turn out to be inadequate or too costly in engineering terms, in which case this should be identified and dealt with as soon as possible. When technical issues are considered before model development starts, the knowledge gained during the research phase can then be used to suggest an alternate solution that might better fit technical constraints. This is another reason why the research phase must also result in some overview of the solution landscape, and not just in a single solution direction"
Data Science Project Flow for Startups - KDnuggets,"With the required infrastructure in place, actual model development can begin in earnest. The extent of what is considered the model to be developed here varies by company, and depends on the relation, and the divide, between the model to be delivered by the data scientist and the service or feature to be deployed in production. The various type of approaches to this divide can perhaps be captured somewhat by considering a spectrum"
Data Science Project Flow for Startups - KDnuggets,"While developing the model, different versions of it (and the data processing pipeline accompanying it) should be continuously tested against the predetermined hard metric(s). This gives a rough estimate of progress and also allows the data scientist to decide when the model seems to be working well enough to warrant the overall KPI check. Do note that this can be misleading, as getting from 50% to 70% accuracy, for example, is in many cases much easier than getting from 70% to 90% accuracy"
Data Science Project Flow for Startups - KDnuggets,"In the more common case, the hard metric is a good approximation of the actual product needs, but not a perfect one. This phase is thus an opportunity to make sure that the softer metrics, that cannot be checked automatically, are also satisfied. This is done together with product and customer success. If you can additionally check the actual value to a customer directly— e"
Data Science Project Flow for Startups - KDnuggets,"Scaleable data ingestion and processing also need to be set up, in the (quite common) case where this was not part of the model. This can mean, for example, turning Python functions that ran on a single core to a pipeline streaming data goes through, or into batch jobs running periodically. In the case of significant data re-use, a caching layer is sometimes set up"
Data Science Project Flow for Startups - KDnuggets,"Users and customers are happy. Product people have managed to build or adapt the product they wanted around the model. We’re done. Toasts are toasted, cheers are cheered, and all is well"
Data Science Project Flow for Startups - KDnuggets,"This is a suggestion for the flow of data science projects. It is also very specific, limited in scope — for the sake of simplicity and visibility — and obviously cannot cover the many variations on this flow that exist in practice. It also represents my experience"
2018’s Top 7 R Packages for Data Science and AI - KDnuggets,"Disclaimer: This list is based on the libraries and packages I reviewed in my personal newsletter. All of them were trending in one way or another among programmers, data scientists, and AI enthusiasts. Some of them were created before 2018, but if they were trending, they could be considered"
2018’s Top 7 R Packages for Data Science and AI - KDnuggets,"We’ll try hypothesis testing. Here, a hypothesis is proposed so that it’s testable on the basis of observing a process that’s modeled via a set of random variables. Normally, two statistical data sets are compared, or a data set obtained by sampling is compared against a synthetic data set from an idealized model"
2018’s Top 7 R Packages for Data Science and AI - KDnuggets,"Here, we first specify the response and explanatory variables, then we declare a null hypothesis. After that, we generate resamples using bootstrap and finally calculate the median. The result of that code is:"
2018’s Top 7 R Packages for Data Science and AI - KDnuggets,"This will allow you to visualize the distribution of the simulation-based inferential statistics or the theoretical distribution (or both). For an example, let’s use the flights data set. First, let’s do some data preparation:"
How AI and Data Science is Changing the Utilities Industry - KDnuggets,"Then, those entities can prioritize certain areas over others. The created model also accepts information from several sources. For example, a utility brand can populate the system with operational data or known statistics about vegetation and growth patterns"
How AI and Data Science is Changing the Utilities Industry - KDnuggets,"However, it's not easy to spot problems if they relate to hidden leaks. An initiative from the University of Waterloo applies AI to find them in water pipes by using sound-processing capabilities to monitor for sounds that could indicate dribbles of water. This system is especially helpful because it can even locate slow"
Ontology and Data Science - KDnuggets,"Quantum mechanics opened a new view of reality and what “exists” in nature. For some physicists in the 1900s there was simply no reality expressed in the quantum formalism. At the other extreme, there were many quantum physicists who took the diametrically opposite view: that the unitarily evolving quantum state completely describes actual reality, with the alarming implication that practically all quantum alternatives must always continue to coexist (in superposition). And thus opening the whole world to a new view and understanding of the “things” that “exists” in nature"
Ontology and Data Science - KDnuggets,"If we bring back the definition of formal ontology from above, and then we think of data and information, it’s possible to set up a framework to study data and its relation to other data. In this framework we represent information in an especially useful way. Information represented in a particular formal ontology can be more easily accessible to automated information processing, and how best to do this is an active area of research in computer science like data science. The use of the formal ontology here is"
Ontology and Data Science - KDnuggets,I talked about the concept of Linked Data. The goal of Linked Data is to publish structured data in such a way that it can be easily consumed and combined with other Linked Data. And also I discussed the concept of the knowledge graph which consists in integrated collections of data and information that also contains huge numbers of links between different data
Ontology and Data Science - KDnuggets,"Well the missing concept in all those definition was ontology. Because that’s the way we can connect entities and understand their relationships. With ontology one can enable such a description, but first we need to formally specify components such as individuals (instances of objects), classes, attributes and relations as well as restrictions, rules and axioms"
Ontology and Data Science - KDnuggets,"For example, suppose you want to change a property in a relational database. You had previously thought that the property was single-valued, but now it needs to be multi-valued. For almost all modern relational databases, this change would require you to delete the entire column for that property and then create an entirely new table that holds all of those property values plus a foreign key reference"
Ontology and Data Science - KDnuggets,"This is not only a lot of work, but it will also invalidate any indices that deal with the original table. It will also invalidate any related queries that your users have written. In short, making that one change can be very difficult and complicated. Often, such changes are so troublesome that they are simply never made"
Ontology and Data Science - KDnuggets,Inside of that you have ontologies representing the relations between entities. Maybe an example of such an ontology would be good here. This is taken from
Ontology and Data Science - KDnuggets,"In this example, Kat Thomas is a consultant who is working with Bob Jones on a Sales Process Redesign project. Kat works for Consult, Inc and Bob reports to Alice Reddy. We can infer a lot of information through this ontology. Since the Sales Process Redesign is about sales we can infer that Kat Thomas and Bob Jones have expertise in sales. Consult, Inc must provide expertise in this area as well. We also know that Alice Reddy is likely responsible for some aspect of sales at Widgets, Inc because her direct report is working on the Sales Process Redesign project"
Ontology and Data Science - KDnuggets,"This is one of the reasons why I’m creating this article, trying to follow what’s happening across the industry, and you should be aware of this. We will program less, and will use semantics technologies more in the near future. It’s closer to the way we think. I mean do you think in relational data bases? I’m not saying we think in graphs, but it’s much easier to pass information between our heads and a knowledge graph than creating weird data base models"
Ontology and Data Science - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Data Science For Our Mental Development - KDnuggets,"Emotion is a fundamental element of human society. If you think about it, everything worth analyzing is influenced by human behavior. Cyber attacks are highly impacted by disgruntled employees who may either ignore due diligence or engage in"
Data Science For Our Mental Development - KDnuggets,"Entire fields of psychology and behavioral economics are dedicated to this field. That being said, the ability to measure and analyze emotions effectively will enable us to improve society in remarkable ways. For example, a psychology professor at the University of California, San Francisco,"
Data Science For Our Mental Development - KDnuggets,"Patients lying to their doctors is not uncommon. The trust barrier for both men and women is fueled by embarrassment and too little face time with doctors. One digital health platform,"
Data Science For Our Mental Development - KDnuggets,Around a third say they withheld details because they couldn’t find the right opportunity or didn’t have enough time during the appointment (27%) or because the doctor didn’t ask any questions or specifically if anything was bothering them (32%). A major impact of this is in the field of suicide. According to the
Data Science For Our Mental Development - KDnuggets,"Cognitive abilities are brain-based skills we need to carry out any task from the simplest to the most complex. They have more to do with the mechanisms of how we learn, remember, problem-solve, and pay attention, rather than with any actual knowledge. People are keen to improve cognition. Who wouldn’t want to remember names and faces better, to be able more quickly to grasp difficult abstract ideas, and to be able to “see connections” better?"
Data Science For Our Mental Development - KDnuggets,"Augmented Reality allows us to feel as though we have teleported to another world. Computational material science and biology are fields that come to my mind instantly when thinking about this problem. Being a computational material scientist in the past, I know that visualizing complex molecular structures is a challenge for many researchers"
Data Science For Our Mental Development - KDnuggets,"Emotional Intelligence is the capacity to be aware of, control, and express one’s emotions, and to handle interpersonal relationships judiciously and empathetically. All people experience emotions, but it is a select few who can accurately identify them as they occur. It could either be a lack of self-awareness or simply our limited emotional vocabulary. Many times, we don’t even know what we want. We strive to connect with those around us in a specific way or consume a particular product, just so we could feel a very unique emotion that we fail to describe. We feel much more than just happiness, sadness, anger, anxiety or fear. Our emotions are complex combinations of all of the above. The ability to understand our own emotions as well as those of people around us is vital for the emotional well being and maintaining positive relationships"
Data Science For Our Mental Development - KDnuggets,"Supervised learning has already given access to some common emotions. Our complex emotional modes may be better understood by performing unsupervised learning on brain waves. For example, a simple outlier detection algorithm can perhaps reveal new emotional modes or significant emotional stressors to pay attention to. Such studies can potentially reveal innovative ways of improving our emotional intelligence"
Data Science For Our Mental Development - KDnuggets,"Emotional intelligence includes both understanding our own emotions and being more sensitive towards those around us. With deeper studies into our emotional states, we can perhaps unlock new emotions that we have never experienced before. In the right hands, AI can act as our extensions to help us form meaningful bonds with the people we value in our lives"
Data Science For Our Mental Development - KDnuggets,"Let’s assume that the modes of experience we have today is represented in space X. 10 years from now, let’s say that the modes of experience are represented in space Y. The space Y will be significantly bigger than space X. This futuristic space of Y may have access to new types of emotions other than our conventional happy, sad and mad. This new space of Y can even allow us to comprehend abstract thoughts that reflect what we wish to express more accurately. Each of us could potentially see the world in ways even"
Data Science For Our Mental Development - KDnuggets,"When you lack emotional intelligence, it’s hard to understand how you come across to others. You feel misunderstood because you don’t deliver your message in a way that people can understand. Even with practice, emotionally intelligent people know that they don’t communicate every idea perfectly. AI has the potential to change that with enhanced self-expression"
Data Science For Our Mental Development - KDnuggets,"10 years ago, most of our communication was restricted to phones and emails. Today, we have access to video conferences, Augmented Reality and a wide array of applications on social media. As we enhance our cognitive abilities and emotional intelligence, we can express ourselves through idioms of far greater resolution and lower levels of abstractions"
Data Science For Our Mental Development - KDnuggets,"We even have access to vibrating gaming consoles that take advantage of our sense of touch for making that Mario Kart game that much more realistic. Augmented Realities of today only limit us to our vision and sense of hearing. In the future, Augmented Realities might actually allow us to smell, taste and touch our virtual environment. Along with access to our 5 senses, our emotional reaction to certain situations might be fine-tuned and optimized with the power of AI. This might mean sharing the fear of our main characters on"
How to go from Zero to Employment in Data Science - KDnuggets,"You can aim for a certification as a fixed goal for this step. For example, when I started learning about big data technologies, I signed up for Cloudera’s Spark and Hadoop developer exam. Some professionals don’t like certificates and swear on practical work experience, but I consider them very helpful especially in the beginning, as a goal, an orientation on what to learn and when to mark this first step as “done”"
How to go from Zero to Employment in Data Science - KDnuggets,"Many people stay stuck in step 1 - this is a dangerous trap. Instead, try to move away from courses as quickly as possible and create some project you're passionate about. Courses can teach you the basics, but they're usually not very good at actually motivating you. But if you're stuck in something that's dear to you, you'll solve the problem much faster and thus learn faster"
How to go from Zero to Employment in Data Science - KDnuggets,"If you can't think of a good project yet, look around for existing projects, read blogs, and expand your knowledge of what's already ""out there"". This could very well take some time, but it's an effort that's well worth it. Of course, you should also orient the technologies used towards the kind of job you're aiming at"
How to go from Zero to Employment in Data Science - KDnuggets,"And don't be afraid of your first one or two projects sucking. They probably will. I'm sure mine did. And those of the ""big guys"" out there probably did, too. The only way to make that third really cool project is to make the first two projects as well. These first two are the ones where most of your learning happens"
How to go from Zero to Employment in Data Science - KDnuggets,"After two or three personal projects, think about contributing to existing larger open-source projects. Contributing your code to those projects is the only way to get feedback from the very smartest people. They rarely if ever do one-on-one tutoring. This is the best way to keep learning, but it takes a while to be able to work at that level"
How to go from Zero to Employment in Data Science - KDnuggets,"Add your projects and talks to your LinkedIn and/or Github profile, and demonstrate why your projects are useful. You have to find out who will be reading your profile. For recruiters, just mention the project as if it was a ""normal"" job. For domain experts, just state that it was an unpaid project, but link to your GitHub repository and maybe mention how many stars it got"
The SIAM Book Series on Data Science - KDnuggets,"As the Book Series Editor-in-Chief, Ilse C.F. Ipsen, notes:"
The SIAM Book Series on Data Science - KDnuggets,Please contact Ilse C.F. Ipsen (
The SIAM Book Series on Data Science - KDnuggets,"Ilse C.F. Ipsen is Professor of Mathematics at North Carolina State University, and a Fellow of both the AAAS and SIAM. Her main areas of research include numerical linear algebra, randomized algorithms, probabilistic numerics, and numerical analysis"
"Data Science in the Real World – Meet Netflix, Google and Amazon at DATAx Singapore - KDnuggets","Innovation Enterprise (Argyle Executive Forum) is registered with the National Association of State Boards-of-Accountancy (NASBA) as a sponsor of continuing professional education on the National Registry of CPE Sponsors (Sponsor ID 103072). State boards of accountancy have final authority on the acceptance of individual courses for CPE credit. Complaints regarding registered sponsors may be addressed to the National Registry of CPE Sponsors, 150 Fourth Avenue North, Suite 700, Nashville, TN, 37219-2417. Website:"
The Role of the Data Engineer is Changing - KDnuggets,"In 2012, if you wanted to have a sophisticated analytics practice at your VC-backed startup, you needed one or more data engineers. These engineers were responsible for extracting data from your operational systems and piping it somewhere that analysts and business users could get at it. Often they would do some transformation work to make the data easier to analyze. Without the data engineers, analysts and scientists didn’t have any data to work with, so frequently engineers were the very first members of a new data team"
The Role of the Data Engineer is Changing - KDnuggets,"Unless you need to push the boundaries of what these technologies are capable of, you probably don’t need a highly specialized team of dedicated engineers to build solutions on top of them. If you manage to hire them, they will be bored. If they are bored, they will leave you for Google, Facebook, LinkedIn, Twitter, … — places where their expertise is actually needed. If they are not bored, chances are they are pretty mediocre. Mediocre engineers really excel at building enormously over complicated, awful-to-work-with messes they call “solutions”"
The Role of the Data Engineer is Changing - KDnuggets,"They’ll find reasons why off-the-shelf pipelines won’t actually suit your very custom data needs, and reasons why analysts shouldn’t actually be building their own data transformations. They’ll write code that is fragile, hard to maintain, and non-performant. And you’ll come to rely on this code because it’s underneath everything else your team does"
The Role of the Data Engineer is Changing - KDnuggets,"While data engineers no longer need to manage Hadoop clusters or scale hardware for Vertica at VC-backed startups, there is still real engineering to do in this area. Making sure that your data technology is operating at its peak results in massive improvements to performance, cost, or both. That typically involves:"
The Role of the Data Engineer is Changing - KDnuggets,"Finally, data engineers at leading companies are often also involved in building tooling that doesn’t exist off-the-shelf. For instance, data engineers at Airbnb built Airflow because they didn’t have a way to effectively build and schedule DAGs. And data engineers at Netflix are responsible for building and maintaining a sophisticated infrastructure for"
The Role of the Data Engineer is Changing - KDnuggets,"You can get most of your core infrastructure off-the-shelf today, but someone still needs to monitor it and make sure it’s performing. And if you’re truly a cutting-edge data organization, you’ll likely want to push the boundaries on existing tooling. Data engineers can help with both"
The Role of the Data Engineer is Changing - KDnuggets,"In practice, integrations are implemented in waves. Typically, the first phase includes core application database and event tracking, with the second phase including marketing systems like an ESP and advertising platforms. These first two phases are available completely off the shelf today. Once you go deeper into your more domain-specific SaaS vendors, you’ll need data engineers to build and maintain these more niche data ingestion pipelines"
The Role of the Data Engineer is Changing - KDnuggets,"For example, ecommerce companies end up dealing with a ton of different products in the ERP / logistics / shipping domain. Many of these products are very specific to particular verticals, and almost none of them are available off the shelf. Expect your data engineers to build these for the foreseeable future"
The Role of the Data Engineer is Changing - KDnuggets,"Building and maintaining reliable ingestion pipelines is hard. If you decide to expend the resources to build one out, expect it to take longer than you initially budgeted for, and expect it to require more maintenance than you’d like. Getting to V1 is easy, but getting a pipeline to consistently deliver data to your warehouse is hard. Don’t make the commitment to supporting a custom data ingestion pipeline until you’re sure the business case is there. Once you do, invest the time and build it to be robust. Consider using Stitch’s open source"
The Role of the Data Engineer is Changing - KDnuggets,"If you’re writing Scalding code to scan terabytes of event data in S3 and aggregating it to a session level so that it can be loaded into Vertica, you’re probably going to need a data engineer to write that job. But if your events data is already in BigQuery (loaded by Google Analytics 360), then it’s already fully addressable in a performant, scalable environment. The difference is that"
The Role of the Data Engineer is Changing - KDnuggets,"It also means that data teams without any data engineers can still get a long way with data transformation tools built for analysts. Data engineers still have a meaningful role to play in building these transformation pipelines, however. There are two key areas where data engineers should get involved:"
The Role of the Data Engineer is Changing - KDnuggets,"While SQL can natively accomplish most data transformation needs, it can’t handle everything. One common need is to do geo enrichment by taking a lat/long and assigning a particular region. At the moment, this is not widely supported on modern MPP analytic databases (although this is"
The Role of the Data Engineer is Changing - KDnuggets,"This change in role also informs a rethinking of the sequencing of data engineer hires. The previous accepted wisdom was that you needed data engineers first, because data analysts and scientists had nothing to work with if there wasn’t a data platform in place. Today, data analysts and scientists should self-serve and build the first version of their data stack using off-the-shelf tools"
The Role of the Data Engineer is Changing - KDnuggets,"The way I think about this shift is a change in data engineering’s role on the team. It’s gone from a builder-of-infrastructure to a supporting-the-broader-data-team role. That’s actually a pretty huge shift, and one that some data engineers (who want to focus on building infrastructure) aren’t always excited about"
The Role of the Data Engineer is Changing - KDnuggets,I could not agree more with this sentiment. The best data engineers at startups today are support players that are involved in almost everything the data team does. They should be excited about that collaborative role and motivated to make the entire team successful
"Core Principles of Sustainable Data Science, Machine Learning and AI Product Development: Research as a core driver - KDnuggets","Regardless of the size of your organisation, if you are developing machine learning or AI products, the core asset you have is a research professional, data scientist or AI scientist, regardless of their academic background. Developing a model using software libraries blindly won't resolve issues you might encounter after deployment of the product. For example, even if you need to do a simple hyperparameter"
"Core Principles of Sustainable Data Science, Machine Learning and AI Product Development: Research as a core driver - KDnuggets","Software development is an integral part of ML product development. However, during research, a code development can go very wild and a scientist, even if they are very good software developers, would end up creating hard to follow and poor code. Once there is a confidence in reproducibility and robustness of results, the production code should be re-written with high-quality software engineering principles"
"Core Principles of Sustainable Data Science, Machine Learning and AI Product Development: Research as a core driver - KDnuggets","A cold start problem for ML products is to release and design data-sets before even doing any research like work. This, of course, has to be aligned with industrial requirements. Imagine datasets like MINST or imagenet for benchmarking. Released sets will be the first step for any model building or product development, and would constitute a data product themselves. Data versioning is also a must"
"Core Principles of Sustainable Data Science, Machine Learning and AI Product Development: Research as a core driver - KDnuggets","There is no such thing as a universal or generic workflow. A workflow depends on a human understanding of processes and steps. Human understanding is based on language and linguistically there is no such thing as universal language, at least it isn't practical yet c"
"Core Principles of Sustainable Data Science, Machine Learning and AI Product Development: Research as a core driver - KDnuggets","Sprints or Agile is not suitable for AI research and research environment, it is a different kind of innovation than software engineering. Thinking that Agile is a cure to do scientific innovation is naive wishful thinking. Structuring a research group,  periodic reviews and releases of the results via presentations and detailed technical reports are much more suitable for data science on top of mini-workshops. A simple proposal runs can also be made to decide which direction to invest, akin to research proposals"
"Core Principles of Sustainable Data Science, Machine Learning and AI Product Development: Research as a core driver - KDnuggets","A service using ML technologies should produce more data. The very first service monitoring is A/Null testing, meaning that what would happen in the absence of the AI product. Detailed analysis of the service data would bring more insights both for business and to research"
"Core Principles of Sustainable Data Science, Machine Learning and AI Product Development: Research as a core driver - KDnuggets","Tools may improve the productivity immensely but AI replacing a data scientist or AI scientist is far from reality, at least for now. If you are investing in AI products, basically you are investing in research at the core, missing that important point may cost organisations a lot. The basic core principles or variation of them may help in sustaining AI products longer and form your teams accordingly"
The Data Science Event You Need in 2019 - KDnuggets,"MADS Can Help You Achieve Your 2019 Goals! Marketing Analytics and Data Science is coming to San Francisco, Apr 8-10. KDnuggets readers save 20% with VIP Code MADS19KDN. Register Today and Save!"
"Beginner to Advanced. Improve your skills with training, workshops and more. - KDnuggets","ODSC East in Boston will be the top global event in 2019 that gets you ahead. We offer an unparalleled 350 hours of talks, workshops and training sessions across 14 tracks. Register now with code KDN55 to save 55%!"
2018’s Top 7 Python Libraries for Data Science and AI - KDnuggets,"Disclaimer: This list is based on the libraries and packages I reviewed in my personal newsletter. All of them were trending in one way or another among programmers, data scientists, and AI enthusiasts. Some of them were created before 2018, but if they were trending, they could be considered"
2018’s Top 7 Python Libraries for Data Science and AI - KDnuggets,7. You must install or upgrade your TensorFlow package to at least 1.7:
2018’s Top 7 Python Libraries for Data Science and AI - KDnuggets,"Explaining machine learning models isn’t always easy. Yet it’s so important for a range of business applications. Luckily, there are some great libraries that help us with this task. In many applications, we need to know, understand, or prove how input variables are used in the model, and how they impact final model predictions"
A Guide to Decision Trees for Machine Learning and Data Science - KDnuggets,"Decision Trees are a class of very powerful Machine Learning model cable of achieving high accuracy in many tasks while being highly interpretable. What makes decision trees special in the realm of ML models is really their clarity of information representation. The “knowledge” learned by a decision tree through training is directly formulated into a hierarchical structure. This structure holds and displays the knowledge in such a way that it can easily be understood, even by non-experts"
A Guide to Decision Trees for Machine Learning and Data Science - KDnuggets,"It might depend on whether or not you feel like going out with your friends or spending the weekend alone; in both cases, your decision also depends on the weather. If it’s sunny and your friends are available, you may want to play soccer. If it ends up raining you’ll go to a movie. And if your friends don’t show up at all, well then you like playing video games no matter what the weather is like!"
A Guide to Decision Trees for Machine Learning and Data Science - KDnuggets,"The concept is the same for decision trees in Machine Learning. We want to build a tree with a set of hierarchical decisions which eventually give us a final result, i. The decisions will be selected such that the tree is as small as possible while aiming for high classification / regression accuracy"
A Guide to Decision Trees for Machine Learning and Data Science - KDnuggets,"Decision Tree models are created using 2 steps: Induction and Pruning. Induction is where we actually build the tree i. Because of the nature of training decision trees they can be prone to major overfitting. Pruning is the process of removing the unnecessary structure from a decision tree, effectively reducing the complexity to combat overfitting with the added bonus of making it even easier to interpret"
A Guide to Decision Trees for Machine Learning and Data Science - KDnuggets,"For step 2, the selection of which feature to use and the specific split is commonly chosen using a greedy algorithm to minimise a cost function. If we think about it for a second, performing a split when building a decision tree is equivalent to dividing up the feature space. We will iteratively try out different split points and then at the end select the one that has the lowest cost. Of course we can do a couple of smart things like only splitting within the range of values in our dataset. This will keep us from wasting computations on testing out split points that are trivially poor"
A Guide to Decision Trees for Machine Learning and Data Science - KDnuggets,"The most common stopping method is to use a minimum count on the number of training examples assigned to each leaf node. If the count is less than some minimum value then the split is not accepted and the node is taken as a final leaf node. If all of our leafs nodes become final, the training stops. A smaller minimum count will give you finer splits and potentially more information, but is also prone to overfitting on your training data. Too large of a min count and you might stop to early. As such, the min value is usually set based on the dataset, depending on how many examples are expected to be in each class"
A Guide to Decision Trees for Machine Learning and Data Science - KDnuggets,"Because of the nature of training decision trees they can be prone to major overfitting. Setting the correct value for minimum number of instances per node can be challenging. Most of the time, we might just go with a safe bet and make that minimum quite small, resulting in there being many splits and a very large, complex tree. The key is that many of these splits will end up being redundant and unnecessary to increasing the accuracy of our model"
Think Twice Before You Accept That Fancy Data Science Job - KDnuggets,"The combination of the two can produce some scary results, as what generally happens is they find some fancy DS/ML job descriptions online and instead of attracting senior data scientists (the ones who may actually be up for the challenge of a nascent data environment*, but usually prefer established teams), they attract junior data scientists (who don't have the experience to discern whether this is a fitting role or not). The junior data scientist mentions her interest in neural networks and drops the term ""heteroskedasticity"" and (BOOM) she has the job. And now she's expected to produce results from a data environment that isn't quite data-science-ready"
Think Twice Before You Accept That Fancy Data Science Job - KDnuggets,"If the firm doesn't have an established data environment, then guess what? Nine times out of ten, you'll be responsible for building this, instead of working on actual data science projects. This is one thing I see over and over. RUN"
Think Twice Before You Accept That Fancy Data Science Job - KDnuggets,"Senior data scientists may be up for this challenge, but expecting a junior data scientist to be able to perform well in this environment is unrealistic. Having no data engineers or no machine learning engineers means you'll be building data pipelines and focusing on writing SQL queries or moving data around. And when you do finish a data science project, since the firm doesn't have any machine learning engineers, you'll be expected to put this project into production, which can be overwhelming unless you have prior software engineering experience. RUN"
Think Twice Before You Accept That Fancy Data Science Job - KDnuggets,"This one can probably be lumped in with #2, but it goes further than merely not having defined objectives. This one speaks more to the data science manager. I've been lucky to have great managers in my career, ones that defined a comprehensive vision for the team in terms of projects and personal growth, both of which are important. If the hiring manager doesn't have a clear vision for the team, then, junior data scientists, beware. You can spot this when the hiring manager says something along the lines of ""data science is so new and we're all just figuring it out"" or ""we're still trying to prove the value of data science"""
Think Twice Before You Accept That Fancy Data Science Job - KDnuggets,The other side of this is growth. You want to make sure your manager cares about your long-term growth. Ask if they have one-on-ones with their team. Ask about their long-term team goals. Ask about long-term projects. Ask about a mentorship program. Usually the answers to these questions will give you an idea for the vision of the team and whether or not you'll grow to your potential in this role
Think Twice Before You Accept That Fancy Data Science Job - KDnuggets,"This is a tricky one, because everyone wants to use the fancy tools and solve the biggest problems, so when a hiring manager mentions these, we're instantly attracted. But if the team and manager focus on tools over problems, then this may signal that they are more concerned with building cool stuff instead of providing practical value. In my experience, managers that focus on problems over tools will ensure that data science projects are providing value to business owners. Let me go ahead and break something to you early on: no one outside of your team cares what tools you're using; they just want the job done"
Think Twice Before You Accept That Fancy Data Science Job - KDnuggets,"When I graduated and got my first job, I worked directly with a senior software engineer. Every single line of code I wrote crossed his eyes and although this was frustrating at times, every minute was a learning experience. I felt like I was maturing in dog years. Every month I learned more than I did in the prior five months. I was a sponge"
Think Twice Before You Accept That Fancy Data Science Job - KDnuggets,"This is where a lot of young candidates go wrong. They don't ask enough questions. I see it all the time. Junior candidates just don't know that they're supposed to ask questions, but this is one of the most important steps in finding the right fit. Ask about the team. Ask about the company. Ask about the possibilities to be mentored or mentor others (equally important). Ask about inter-department collaboration. Ask about the types of data science projects you'll be working on. Ask about the successes of the team. Ask about the failures (diplomatically, of course). Ask about the vision of the team. Anything you're interested in, ask about"
Think Twice Before You Accept That Fancy Data Science Job - KDnuggets,"This section is such an important sub-section of the previous one, that I decided to dedicate an entire section to it. It's the most important thing I focus on when I go on interviews. More than work-life balance, or what time everyone arrives at the office, or group dynamics; more than all these things, I care about the role itself. My biggest goal when interviewing for a new job is to determine what I'll be working on and sharpening the skills I care about most. Since this is the most important thing to me, I make sure to leave no stone unturned. Failing to ask these questions is how most data scientists get into a situation where the job they're doing isn't the job they thought they'd be doing; that is, they're playing the part of data analyst or data engineer instead of data scientist or machine learning engineer"
Introduction to Statistics for Data Science - KDnuggets,"The first step of every statistical analysis you will perform is to determine whether the dataset you are dealing with is a population or a sample. As you might recall, a population is a collection of all items of interest in your study whereas a sample is a subset of data points from that population. Let’s take a short refresher!"
Introduction to Statistics for Data Science - KDnuggets,"First we take several samples (heights of different man) from our population and for each sample group we calculate the respective mean. For example, we can have groups where the height is 176 cm, others with 182cm, others with 172cm, and so on. We then plot this sample mean distribution. The following picture depicts the distribution of our several samples with the x mark, in each, referring to the mean value"
Introduction to Statistics for Data Science - KDnuggets,Let’s look at a more visual example with this GIF. The population already showed a normal distribution but later you can try with other shapes and even draw your own. We start by taking samples of size n=5 from the population and calculating the respective mean. As you increase the number of samples with n= 5 you see that the distribution of the means starts to be shaped like a normal distribution. When we increase the process several times in the thousands we get a normal distribution with a mean equal to the population’s mean. The more samples you get the more narrower the Normal distribution will be
"Top Active Blogs on AI, Analytics, Big Data, Data Science, Machine Learning – updated - KDnuggets","January 2019. If we have missed any popular active blogs, please suggest them in the comments below. Enjoy!"
"Industry Predictions: AI, Machine Learning, Analytics & Data Science Main Developments in 2018 and Key Trends for 2019 - KDnuggets","While substantial investments are being  made into data science across many industries, the scarcity of data science skills and resources limits the advancement of AI and ML projects within organizations. In addition, one data science team is only able to execute several projects a year given the iterative nature of the process and the manual work that goes into data preparation and feature engineering. In 2019, data science automation platforms will capture much of the mind share. Data science automation will cover much wider areas than machine learning automation, including data preparation, feature engineering, machine learning and the production of data science pipelines. These platforms will accelerate data science, execute more business initiatives whilst maintaining the current investments and resources"
"Industry Predictions: AI, Machine Learning, Analytics & Data Science Main Developments in 2018 and Key Trends for 2019 - KDnuggets","Gone are the days of waiting several months for a data science project.  In 2019, we are going to see a transformation in how businesses implement and optimize their AI and machine learning initiatives. New data science automation platforms offer a single, seamless platform that enables companies to accelerate, democratize, and operationalize the entire data science process – from raw data through feature engineering to machine learning – eliminating the most time-consuming and labor- and skill-intensive tasks of data science. As a result, what once took months to complete, now will take only days, significantly accelerating time to value for AI and machine learning initiatives"
"Industry Predictions: AI, Machine Learning, Analytics & Data Science Main Developments in 2018 and Key Trends for 2019 - KDnuggets","The data scientist, once the sexiest job of the 21st century, will become very different from what we know today. As analytics are pushed to the end user, self-service becomes routine, and data prep tools become more powerful, the data scientist will transform into more of a consultant than a data sourcing and preparation expert. They will be charged with helping the business make sense of data, understanding how to interpret results, and what courses of action might be warranted. It’s a higher value role for the data scientist and ultimately, a better use of their skills"
"Industry Predictions: AI, Machine Learning, Analytics & Data Science Main Developments in 2018 and Key Trends for 2019 - KDnuggets","Two trends that I see continue are Automation and Interpretability. The former will continue to hype a bit throughout 2019 but then face the problem that only fairly well-defined types of data science problems lend themselves to full automation. Much more powerful are environments where the data scientist can mix automation with interaction, truly allowing them to deploy data science to others without having to outsource all of it to self-declared experts. Interpretability will turn into a larger problem for deep learning for all types of data where understanding (or controlling) the underlying decision is important. We will never accept ""human failure"" from AI for safety critical decisions"
"Industry Predictions: AI, Machine Learning, Analytics & Data Science Main Developments in 2018 and Key Trends for 2019 - KDnuggets","The long-promised enterprise AI transformation is poised to begin in earnest in 2019. Most enterprises have reached a point of digital maturity, ensuring access to quality data at scale. With mature data sets, AI providers can offer lower cost, easier to use AI tools for specific business use cases. The effect of enterprise AI at scale will be significant. Gartner expects the business value of AI to hit nearly $3.9 trillion in by 2022. Consumers also stand to benefit in almost every sector. They’ll see more innovative products and services, smarter homes, factories and cities, improved health, and a higher quality of life"
"Industry Predictions: AI, Machine Learning, Analytics & Data Science Main Developments in 2018 and Key Trends for 2019 - KDnuggets","AI will augment – not replace – the workforce. AI applications will be transformational, improving efficiency and performance, generating huge cost savings, and giving rise to more innovative products and services. However, the future of work will involve humans and AI. The most innovative companies have already started planning how to best make this symbiotic future a reality. In the shorter term – this means the gap in skilled workers in data science will persists and demand will remain very high. Educational opportunities will expand to help address the need. In the long term – as we develop the skills and technology to unlock AI – society stands to benefit enormously from AI-augmented workers, homes, vehicles, power grids, factories, cities, more"
"Industry Predictions: AI, Machine Learning, Analytics & Data Science Main Developments in 2018 and Key Trends for 2019 - KDnuggets","We’ll have no choice – data growth is exponential and increasingly structured because of IoT (see chart). Like how consumers today can view their home’s energy consumption and look at comparisons to neighbors’ energy use, we’ll start to see this bleed (finally!) into the supply chain, as example. Automated analytics will find something interesting, create a visual representation of that item – then it shows it to a human so action can be taken. Data will be pulled from the whole pie, allowing people running the enterprise to see the forest for the trees like never before"
Learning Machine Learning vs Learning Data Science - KDnuggets,"Machine learning has seen much hype from journalists who are not always careful with their terminology. In popular discourse, it has taken on a wide swath of meanings and implications well beyond its scope to practitioners. Machine learning refers to a specific form of mathematical optimization: getting a computer to perform better at some task, through training data or experience, without explicit programming. This often takes the form of building a model based on past cases with known outcomes, and applying the model to make predictions for future cases, finding ways to minimize a numerical “error” or “cost” function representing how much the predictions mismatch reality"
Learning Machine Learning vs Learning Data Science - KDnuggets,"Machine learning is an important skill for data scientists, but it is one of many. Thinking of machine learning as the whole of data science is akin to thinking of accounting as the entirety of running a profitable company. Further, the skills gap in data science is largely in areas complementary to machine learning — business sensibility, statistics, problem framing, and communication"
Learning Machine Learning vs Learning Data Science - KDnuggets,"We see this as a significant problem. Many student have focused far too heavily on machine learning education over a more balanced curriculum. This has unfortunately led to a glut of underprepared early-career professionals seeking data science roles. Both of the authors, and several other data science hiring managers with whom they spoke when preparing this article, have interviewed numerous candidates who advertise their knowledge of machine learning but who can say little about basic statistics, bias and variance, or data quality, much less present a coherent project proposal to achieve a business objective"
Learning Machine Learning vs Learning Data Science - KDnuggets,"In the authors’ experience, software engineers seem especially susceptible to the siren’s call of an education too rich in machine learning. We speculate that this is because machine learning uses the same type of thinking that already comes easily to software developers: algorithmic, convergent thinking with clearly defined objectives. An education that is hyper-specialized in machine learning offers the false promise of more interesting work without demanding any fundamental cognitive shifts. Sadly, the job market rarely delivers on this promise, and many who follow this path find that they are unable to make the career shift from engineer to scientist"
Learning Machine Learning vs Learning Data Science - KDnuggets,"Data science demands learning a different style of thought: often divergent, poorly defined, and requiring constant translation in and out of the technical sphere. Data scientists are fundamentally generalists, and benefit from a broad education over a deep one. Interdisciplinary study is a far better bet than a narrow concentration"
Learning Machine Learning vs Learning Data Science - KDnuggets,"Creating general purpose machine learning algorithms is a scalable job — once somebody has designed and implemented an algorithm, everybody can use it with virtually no cost of replication. Of course everyone will want to use the best algorithms, created by the best researchers. Most organizations cannot afford to hire top-tier algorithm designers, many of whom receive seven-figure salaries. Thankfully, much of their work is available to the public in research papers, open source libraries, and cloud APIs. Thus the world’s best ML algorithm designers have an outsized impact, and their work enables the generalist data scientists who use their algorithms to have a large impact in turn"
Learning Machine Learning vs Learning Data Science - KDnuggets,"Of course, there are many highly worthwhile and interesting paths for a career other than data science. In case you are thinking of a career more specifically in machine learning, here's one of the dirty secrets of the industry: Machine Learning Engineers at large companies actually do very little machine learning themselves. Instead, they spend most of their time building data processing pipelines and model deployment infrastructure. If you do want one of these (often excellent) jobs, we still recommend focusing a minority of your education on machine learning algorithms, in favor of general engineering, DevOps practices, and data pipeline infrastructure"
Learning Machine Learning vs Learning Data Science - KDnuggets,"While the world’s best machine learning expert may be able to contribute more to the grand sum of human knowledge than the world’s best data scientist, a skilled data scientist can have an outsized impact in a much broader range of situations. The job market reflects this. If you are seeking employment, you will likely do best by consuming machine learning education as just one part of a balanced diet. And, if you are looking to make your company more data driven, you will likely do best by hiring a generalist"
Why You Shouldn’t be a Data Science Generalist - KDnuggets,"To see why, just imagine that you’re a company trying to hire a data scientist. You almost certainly have a fairly well-defined problem in mind that you need help with, and that problem is going to require some fairly specific technical know-how and subject matter expertise. For example, some companies apply simple models to large datasets, some apply complex models to small ones, some need to train their models on the fly, and some don’t use (conventional) models at all"
Why You Shouldn’t be a Data Science Generalist - KDnuggets,"Those of us who work in the industry bear a lot of the blame for this. We tend to lump an excessive number of things into the “data science” bucket in casual conversations, blog posts and presentations. Building a robust data pipeline for production? That’s a “data science problem"
Why You Shouldn’t be a Data Science Generalist - KDnuggets,"If you’ve only ever worked with relatively small (<5 Gb) datasets stored in . Here are a couple of reasons: 1) A 50 Gb dataset won’t fit in your computer’s RAM, so you generally need other ways to feed it into your model, and 2) that much data can take a ridiculous amount of time to process, and often has to be stored redundantly. Managing that storage takes specialized technical know-how"
Why You Shouldn’t be a Data Science Generalist - KDnuggets,"Your job will be to translate data into actionable business insights. You’ll often be the go-between for technical teams and business strategy, sales or marketing teams. Data visualization is going to be a big part of your day-to-day"
Why You Shouldn’t be a Data Science Generalist - KDnuggets,"Highly technical people often have a hard time understanding why data analysts are so important, but they really are. Someone needs to convert a trained and tested model and mounds of user data into a digestible format so that business strategies can be designed around them. Data analysts help to make sure that data science teams don’t waste their time solving problems that don’t deliver business value"
Why You Shouldn’t be a Data Science Generalist - KDnuggets,"The five job descriptions I’ve laid out here definitely don’t stand alone in all cases. At an early-stage startup, for instance, a data scientist might have to be a data engineer and/or a data analyst, too. But most jobs will fall more neatly into one of these categories than the others — and the larger the company, the more these categories will tend to apply"
Should you become a data scientist? - KDnuggets,"In 2009 and 2012, articles were published by McKinsey and the Harvard Business Review, hyping up the role of the data scientist, showing how they were revolutionizing the way businesses are operating and how they would be critical to future business success. They not only saw the advantage of a data-driven approach, but also the importance of utilizing predictive analytics into the future in order to remain competitive and relevant. Around the same time in 2011, Andrew Ng came out with a free online course on machine learning, and the curse of AI FOMO (fear of missing out) kicked in"
Should you become a data scientist? - KDnuggets,"We have the demand and we have the supply, so what’s the problem? Well, the problem isn’t a shortage of programs to support that demand and capitalize on the hype. It feels like every day there are new courses being developed to satisfy the cravings from aspiring data scientist to break into the field: master’s programs, boot camps and online courses. It’s an arms race to make the right courses with the promise of a Machine Learning job at the end of it. Just three to six months and a small investment of ~10-15k and you’ll be guaranteed a well-paid job upon graduation"
Should you become a data scientist? - KDnuggets,"These programs are designed to be a one-stop-shop for everything data science: you learn the programming, the visualization, the modeling-- it’s all there. What you soon discover is that many (surely, not all) of the business problems being faced can be solved using similar approaches, so if you’re looking to apply some algorithm, chances are there’s a library that already exists to help you do just that. Simple right?"
Should you become a data scientist? - KDnuggets,"Second, automation of AI models will come. Don’t believe me? It’s already happening with products like Google AutoML and software packages like TPot. What seemed like the most technically challenging and appealing part of the job will be automated. As they say now: “No ML expertise? No problem!”"
Common mistakes when carrying out machine learning and data science - KDnuggets,"Assume, in our rental data, that we have an apartment-type column with the following values: [ground floor, loft, maisonette, loft, loft, ground floor]. LabelEncoder can turn this into [3,2,1,2,2,1], introducing ordinality, which means that ground_floor >loft > maisonette. For some algorithms like decision trees, and its deviations, this type of encoding for this feature would be fine, but applying regressions and SVM might not make that much sense"
Common mistakes when carrying out machine learning and data science - KDnuggets,"The rental price is in Euros so the fitted coefficient would be approximately 100 times larger than the fitted coefficient if the price was in cents. L1 and L2 penalize the larger coefficients more, meaning it will penalize the features in smaller scales more. To prevent this, the features should be standardized before applying L1 or L2"
Common mistakes when carrying out machine learning and data science - KDnuggets,"There were three different algorithms I wanted to explore, comparing characterstics such as performance differences and speed. These three were gradient boosted trees with different implementations (XGBoost and LightGMB), Random Forest (FR, scikit-learn) and 3-layer Neuronal Networks (NN, Tensorflow). I selected RMSLE (root mean squared logarithm error) to be the metric for the optimization of the process. I used RMSLE because I derived the logarithm of the target variable"
Common mistakes when carrying out machine learning and data science - KDnuggets,Feature importance provides a score that indicates how informative each feature was in the construction of the decision trees within the model. One of the ways to calculate this score is to count how many times a feature is used to split the data across all trees. This score can be computed in
Common mistakes when carrying out machine learning and data science - KDnuggets,"The same applies to the number of stations within one kilometer from the apartment. Many metro stations around would, in general, increase the rental price. However, it also had a negative effect — more noise"
Common mistakes when carrying out machine learning and data science - KDnuggets,"The idea behind stacked models is to create several base models and a meta model on top of the results from the base models in order to produce final predictions. However, it is not so obvious how to train the meta model because it can be biased towards the best of the base models. A very good explanation of how to do it correctly can be found in the post"
How to build a data science project from scratch - KDnuggets,"However, in order to start practising data science, it is better if you challenge a real-life problem. Digging into the data in order to find deeper insights. Carrying out feature engineering using additional sources of data and building stand-alone machine learning pipelines"
How to build a data science project from scratch - KDnuggets,"However, I suggest not only to concentrate on your interests but also to listen to what people around you are talking about. What bothers them? What are they complaining about? This can be another good source of ideas for a data science project. In those cases where people are still complaining about it, this may mean that the problem wasn’t solved properly the first time around. Thus, if you challenge it with data, you could provide an even better solution and have an impact in how this topic is perceived"
How to build a data science project from scratch - KDnuggets,"This is just one of the things I heard from people who had recently moved to Berlin for work. Most newcomers complained that they hadn’t imagined Berlin to be so expensive, and that there were no statistics about possible price ranges of the apartment. If they had known this it beforehand, they could have asked for a higher salary during the job application process or could have considered other options"
How to build a data science project from scratch - KDnuggets,"I wanted to gather the data, build an interactive dashboard where you could select different options such as a 40m2 apartment situated in Berlin Mitte with a balcony and equipped kitchen, and it would show you the price ranges. This, alone, would help people understand apartment prices in Berlin. Also, by applying machine learning, I would be able to identify the drivers of the rental prices and practise with different machine learning algorithms"
How to build a data science project from scratch - KDnuggets,"Once you starting getting the data, it is very important to have a look at it as early as possible in order to find any possible issues. For instance, if you web scrape, you could have missed some important fields. If you use a comma separator while saving data into a file, and one of the fields also contains commas, you can end up having files which are not separated very well"
How to build a data science project from scratch - KDnuggets,"Additionally, some agencies would increase or decrease the price for the same apartment after a month. I was told that if nobody wanted this apartment, the price would decrease. Conversely, I was told that, if there were so many requests for it, that the agencies increased the price. These sounds like good explanations"
How to build a data science project from scratch - KDnuggets,"Already from this data visualization you can see that the price distribution of 2.5 rooms falls into the distribution of 2 room apartment. The reason for this is that most of the 2.5 room apartments aren’t situated in the center of the city which, of course, reduces the price"
How to build a data science project from scratch - KDnuggets,"Visualization helps you to identify important attributes, or “features,” that could be used by these machine learning algorithms. If the features you use are very uninformative, any algorithm will produce bad predictions. With very strong features, even a very simple algorithm can produce pretty decent results"
How to build a data science project from scratch - KDnuggets,"However, there was one feature that was problematic, namely the address. There were 6.6K apartments and around 4.4K unique addresses of different granularity. There were around 200 unique postcodes which could be converted into the dummy variables but then very precious information of a particular location would be lost"
6 Step Plan to Starting Your Data Science Career - KDnuggets,"However, it's also possible to take a less formalized approach. People could go to data science conferences in their areas and sign up for workshops or intimate Q&A sessions with experts. There are also opportunities to get insights through message boards and Skype conversations"
6 Step Plan to Starting Your Data Science Career - KDnuggets,"However, it's necessary to be mindful that although this list provides suggested foundational components, everyone is different. People may not progress in this order, and they may not perform all the steps. That's OK, as long as they keep their objectives in sight"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"Yet, most of state-of-art end-to-end solutions use attention-based models. Attention mechanism summarizes encoder features relevant to predict next label. Most of the modern architectures are improvements on"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"The LAS model consists of an encoder (similar to an acoustic model), which has pyramidal structure to reduce the time step, an attention (alignment) model, and a decoder — an analogue to a pronunciation or a language model. LAS offers good results without an additional language model, and is able to recognize out-of-vocabulary words. However, to decrease word error rate (WER), special techniques are used, such as shallow fusion, which is integration of separately trained LM and is used as input to decoder and as additional input to final output layer"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"Most of the CALL solutions use ASR at their back-end. However, a conventional ASR system trained on native speech is not suitable for this task, due to students’ accent, language errors, lots of incorrect words or out-of-vocabulary words (OOV). Therefore, techniques from Natural Language Processing (NLP) and Natural Language Understanding (NLU) should be applied to determine the meaning of the student’s utterance and detect errors. Most of the systems are trained on non-native speech corpora with a fixed native language, using in-house corpora"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"Most of CAPT papers use ASR models in a specific way, for forced alignment. A student’s waveform is aligned in time with the textual prompt, and the confidence score for each phone is used to estimate the quality of pronunciation of this phone by the user. However, some novel approaches were presented, where, for example, relative distance between different phones is used to assess student’s language proficiency, and involves end-to-end training"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"Participants from both academia and industry presented their solutions which were benchmarked on an opened dataset consisting of two parts: speech processing and text processing. They contain German prompts and English answers by a student. Language (vocabulary, grammar) and meaning of the responses have been assessed independently by human experts. The task is open-ended, i"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"This year, A. Zeyer and colleagues presented a new ASR model showing the best ever results on LibriSpeech corpus (1000 hours of clean English speech) — the reported WER is 3.82%. This is another example of an end-to-end model, an improvement of LAS. It uses special Byte-Pair-Encoding subword units, having 10K subword targets in total"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,For a smaller English corpus — Switchboard (300 hours of telephone-quality speech) the best result is shown by a modification of Lattice-free MMI (Maximum Mutual Information) approach by H. Hadian et.5% WER
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"Despite the success of end-to-end neural network approaches, one of their main shortcomings is that they need huge databases for their training. For endangered languages with few native speakers, creating such database is close to impossible. This year, traditionally, there was a session on ASR for such languages. The most popular approach for this task is transfer learning, i. Unsupervised (sub)word units discovery is another widely used approach"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"A bit different task is ASR for under-resourced languages. In this case, a relatively small dataset (dozens of hours) is usually available. This year, Microsoft organized a challenge on Indian languages ASR, and even shared a"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"This year we have seen many presentations on voice conversion. For example, trained on VCTK corpus (40 hours of native English speech), a voice conversion tool computes the speaker embedding or i-vector of a new target speaker using only a single target speaker’s utterance. The results sound a bit robotic, yet the target voice is recognizable"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"Another interesting approach for word-level speech processing is Speech2Vec. It resembles Word2Vec widely used in the field of natural language processing, and lets learn fixed-length embeddings for variable length word speech segments. Under the hood, Speech2Vec uses encoder-decoder model with attention"
Interspeech 2018: Highlights for Data Scientists - KDnuggets,"With the development of Deep Learning, Interspeech conference, originally intended for the speech processing and DSP community, gradually transforms to a broader platform for communication of machine learning scientists irrespective of their field of interest. It becomes the place to share common ideas across different areas of machine learning, and to inspire multi-modal solutions where speech processing occurs together (and sometimes in the same pipeline) with video and natural language processing. Sharing the ideas between fields, undoubtedly, speeds up the progress; and this year’s Interspeech conference has shown several examples of such sharing"
Exploring the Data Jungle Free eBook - KDnuggets,"Some people like to believe that all data is ready to be used immediately. But that's just not true! Data in the wild is hard to track and even harder to understand, and the first job of data scientists is to identify and prepare data so it can be used. With the right perspective and guidance, you'll have everything you need to get hacking at and taming that wild data!"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"I would say, if you are enrolling yourself in masters or PhD programs in any area - you are brave. You are showing a courage to learn and grow. So, what goes wrong after this step? Once you start studying, you get lots of assignments, lots of things to self-learn and explore, do work on research topics etc. Students start worrying about completing assignments On TIME to get maximum score, completing assignments 100% to get maximum score, completing assignment ACCURATE for maximum score. Which in turn counts towards their GPA. So, they start ignoring the fact of learning things and run behind scoring and getting good GPA. I would like to tell you, if you are worrying about getting maximum grades and for that if you are asking somebody else’s work to complete your assignment, this is going to give you nothing more than score. The consequences of it you realize when you start looking for a job. If you are maintaining your GPA equal to or greater than 3.0 [whatever standard, you have in your university], you are good to go. GPA doesn’t matter, what matters is how much knowledge you gained. GPA is just a number, if you are having minimum criteria required to be eligible for applying to any job in the market, you are doing good. And if you follow the learning and gaining knowledge you will definitely end up scoring good"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"So, don’t limit yourself in exploring things during your academic years. Treat yourself as a blank slate and explore that area as much as you can within your academics. This is your time to explore being new bee in the area. Don’t hesitate to audit the courses apart from your required semester credits. The best part of universities here is you can audit courses for free. For that sometimes you may need to approach to a particular professor to audit his/her class. You are gaining knowledge by auditing too. If you are not allowed to audit any class, I am sure you should be knowing someone else taking that class. So, approach and ask that friend about what’s going in that class, what are they learning. This way you will get to know what’s going on around. What skills are in demand in the market. What’s going on in other areas too. Once you start exploring, you will get to know more and more about it and start getting to know your areas of interest too"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"Talking specifically for data – take / audit maximum courses provided in your universities related to data analysis, data mining, machine learning, deep learning, data science, cognitive, data visualization, programming [ R, python, Scala etc.], big data, business intelligence, probability, statistics, and many more. Don’t just fall for completing credits, graduating and getting degree. You are enough grown up to figure out what works for you. Try to find time. Try to do your assignments your own. Don’t worry about getting score. If you want to really worry about something then worry about understanding concepts right, worry about utilizing your university time effectively, worry about helping yourself on time. So please, “Be an explorer!!”"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"Take an advantage of each facility you are getting. You can learn and explore lots of tools free of cost. Group study rooms help you and guide on team work when you do your projects, prepare for your presentations. Visiting tours to employer’s work place gives you motivation to work harder as it might be your dream job to get into it. It’s not that due to these things you get into a job, but its understanding importance of each facility that university provides to you. It’s up to you how you utilize those"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"Make use of those too along with studies, but you are enough grown up to understand how much time you would like to spend on those. These non-technical events help you to make friends, have social life, gives an opportunity to lead or organize some events. So, every facility you are getting has its own purpose. Its only your job to figure out the priority of each one of those. You are a student, and it’s your job to take right advantage of any facility that university provides to you"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"Talking specifically to Data field, learn and explore maximum tools and software you are getting in university. Read books on stats, probability, mathematics, machine learning etc. Take the prints of book pages you want to refer in future too or write notes for your reference. Use group study rooms to discuss and understand your data. Do presentations. Attend company seminars or presentations to see their case studies. Join data clubs, if it’s not there form one and organize events/meetings. Participate in competitions, it’s easy to form a group in university as you will find lots of people having common interest in your class. Take an advantage and be wise to utilize the facilities provided by University"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"Apart from University activities there are other ways to do networking and connect with people. Like you can join technical meet ups in data [ there are lots of available in every area], show up there, interact with people, ask them what they are working on, and show what you can do contribute. There are couple meetups run specifically to collaborate and work on Kaggle competitions, it’s a great way to form group and work"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"Apart from meet ups, there are conference like Open Data Science, Machine Learning, Robotics and AI, Other university events – like MIT, Harvard etc. If you are not able to purchase the ticket, advantage of being student is these conferences promote students and provide them discounts on tickets or you can join them to volunteer. That way you can again listen to and connect with lots of leaders and learn about their work in Data"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"So, what I feel is there are many ways you find, only thing you require is to seek always what you are looking for. It’s important to approach people and ask questions. Always keep in mind, no question is right or wrong, may be sometimes it can be easy one or hard one. But never hesitate to find answer by yourself or to ask someone. Never fall for finding any purpose talking to any person, you never know that contact can help you further in your career change. So, please “Seek, Network and Showcase”"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"We talked a lot about, exploring, learning things, networking, showing knowledge or interest. Though the keywords sound very easy, it’s a very hard and continuous process. Most of the people have tendency to give up in between. I don’t think it’s their fault, but it’s an attitude or a human tendency, where everybody needs or seeks a success and a fame overnight. Bitter part is, it never happens so. Always we see success of a person but never see how long it took for that person to get that success or a fame. There might be some exceptions who became billionaire in early age, but life wasn’t easy for any one of them after that too, even though they achieved so much success, maintaining, digesting and further growing that success is an important factor"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"As we know, data field is in a boom right now, in the job market and there are lots of job opportunities/openings in similar area, salaries are pretty fat too. No doubt, every individual wants to build their career into it. In result it’s competitive field. And considering the kind of  job application process we have, - it’s not effective, it’s almost outdated in turn people hardly get calls. Lots of profiles get filtered out with automated parsing system. So, students might find it challenging to get interview calls, but its important to be patient and persistent. You should not lose your interest and patience if you are really looking to get into any specific job role. Keep applying to jobs through online job boards. People with whom you are networking always handover your resume. Its important not to give up and if you don’t then definitely you will get what you want"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"I do remember, when I started my job search in Data science, my first concern was, why do they need PhD candidates? How come degree can be a filter to any job role? How experience doesn’t take priority over the degree? No doubt you are going to be in similar situation now or later. But one very good quality I have is, "" I never give up "". I was persistently applying to jobs – all the platforms – LinkedIn, Glass door or Monster [whichever you feel or all]. And finally got into one opportunity"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"If you are getting your masters or PhD in any specific area, it is required to work hard and get into same area of interest. There might be more challenges being an immigrant or being new in that field, but definitely these three qualities helps to overcome those challenges.  So, keep applying. It’s not good to compromise before putting on efforts to achieve it. Be persistent & patient. I am sure things will work out for you one day, the way it worked for me"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"One needs to find out what he/she good at. It might be your writing, communication or leadership skills. Or you might be more innovative or creative. Answer to it is that every quality gives you an opportunity to be outstanding in area you are interested in"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"Talking specific about data, you might find lots of people doing and using Kaggle projects, but you might believe and showcase your own interesting projects apart from academics to show your skills and creativity. You might find lots of people going to technical meetups, but adding value to those meetups, showing knowledge and sharing it, also holding and organizing those might out-stand you from others. You might find people following lots of leaders to gain knowledge, but there will be only few who believes in sharing knowledge and enhancing skills. Only thing you need to find out which side you would like to prefer to out-stand yourself"
Kick Start Your Data Career! Tips From the Frontline - KDnuggets,"I have seen lots of people changing their resume as per data job roles, does that really a need? The answer is may be for automatic parsing system sometimes it is a need, but trust me, if you are good in any specific field or area, those obstacles don’t affect much.  Being creative at making your resume, is another separate topic, I will not go much in details for this blog, but rather than that important factor is “Finding out your strengths and utilizing those to out-stand yourself in the job market”. Hope you are following 😊. Remember ""Everyone doesn't born as a SMART, but people definitely learn to be SMART"""
Data Science Projects Employers Want To See: How To Show A Business Impact - KDnuggets,"To increase customer retention, we need to identify potentially unhappy customers. If we can intervene early in the customer lifecycle, we can offer things like discounts or alternative services to try and prevent unhappy customers from leaving. Since we have access to customer data, we can build a machine learning model to try and predict unhappy customers that would likely churn. To keep things simple, we’ll just look at using a logistic regression model"
Data Science Projects Employers Want To See: How To Show A Business Impact - KDnuggets,"To do this, we’ll make predictions using the “test” dataset. We’ll pass in the “fit” model from the previous section. To predict the probabilities, we’ll specify “type=response”"
Data Science Projects Employers Want To See: How To Show A Business Impact - KDnuggets,"We’ll set the response threshold at 0.5, so if a predicted probability is above 0.5, we’ll convert this response to “Yes”"
Data Science Projects Employers Want To See: How To Show A Business Impact - KDnuggets,"A useful way to use this plot is to take the area under the curve, also known as the AUC. The AUC can take on any value between 0 and 1, with 1 being the best. Here’s the R code for computing the AUC:"
Data Science Projects Employers Want To See: How To Show A Business Impact - KDnuggets,"Our model has an AUC of 0.85, which is pretty good. If we were to just make random guesses, our ROC would be a 45-degree line. This would correspond to an AUC of 0.5. At the very least, we’re outperforming random guessing, so we know that our model is at least adding some value!"
Data Science Projects Employers Want To See: How To Show A Business Impact - KDnuggets,"Let’s start by making some assumptions about cost. We’ll assume that it costs $300 to acquire a new customer in the telecommunications industry. We previously stated that our data shows it’s five times more expensive to acquire new customers than retain existing ones, so our retention cost will be $60"
Data Science Projects Employers Want To See: How To Show A Business Impact - KDnuggets,"Let’s calculate the cost per customer using various thresholds (0.1, 0.2, 0.3,…,0.9, 1.0). After initializing a threshold vector “thresh” I can loop through each and make predictions. Since I’m calculating cost on a per customer basis, I’ll divide by the total number of datapoints in my test set"
Data Science Projects Employers Want To See: How To Show A Business Impact - KDnuggets,"Finally, I’ll put the results in a dataframe, along with what I’m calling a “simple” model. This is our logistic regression model from before that defaults to a threshold of 0.5"
How will automation tools change data science? - KDnuggets,"So what is preventing the adoption and acceleration of data science in enterprises? A typical enterprise data science project is highly complex and involves many steps, including data collection, last-mile ETL* (data wrangling), feature engineering, machine learning, visualization and production (see illustration below). A traditional data science project takes several months to complete even for an experienced team. It is a highly involved and collaborative process that requires a wide range of specialized skill sets, such as domain experts, data engineers, data scientists, business intelligence engineers, and software architects. In addition, the outcome of most enterprise data science projects are hard to interpret, making it difficult for business users to implement the results"
How will automation tools change data science? - KDnuggets,"Playing with machine learning (ML) models is considered to be the fun part, but the real pain point of any data science project is often last-mile ETL and feature engineering. As illustrated below, machine learning requires a single flat table called a feature table. Given a feature table, data scientists can play with ML algorithms. But actual enterprise data is never a single flat table. Instead, it’s a collection of many data tables with complex relationships"
How will automation tools change data science? - KDnuggets,Will data scientists or domain experts be replaced with automation tools? The answer is obviously no. No tool can truly replace skilled experts. Instead it makes them more productive. Automation will affect data science in three major ways:
Linking Data Science Activities to Business Initiatives Using the Hypothesis Development Canvas - KDnuggets,"We’ll continue to test and make refinements to the Hypothesis Development Canvas. I also encourage others to test, stretch, bend and abuse the canvas as a way to ensure that it reflects the best thinking of all parties. Thanks for your help!"
3 Challenges for Companies Tackling Data Science - KDnuggets,"The pace of innovation in the data science space is very fast, and each new piece of technology has its own learning curve. In many cases, the original technology is developed by computer scientists, with the intended audience also being someone with very strong programming skills. These software packages are implemented in many different programming languages, so the learning curve is very steep for those who do not write code full-time"
3 Challenges for Companies Tackling Data Science - KDnuggets,"Engineers and scientists who do not program full-time should look for tools that enable them to get up and running quickly, preferably within computational platforms that they’re already familiar with. Point-and-click apps like those found in MATLAB can serve as an easy starting point for learning the technology. Beyond that, a programmatic interface is typically required to fine-tune analytics to improve robustness and accuracy. Mature programming tools will have consistent APIs that make it easy to swap in different data science techniques. If businesses are serious about data science, they should also look for training courses that can help employees ramp up much faster than learning from trial and error"
3 Challenges for Companies Tackling Data Science - KDnuggets,"A common compromise is to pair up engineers who have domain knowledge with data scientists to leverage each of their strengths, but this may not be possible in many cases because there are far more domain experts than data scientists. Another solution is to adopt tools that simultaneously lower the bar for machine learning (for the domain experts) and provide flexibility and extensibility (for the data scientists). In practice, this means adopting a tool that has both a graphical interface (i"
3 Challenges for Companies Tackling Data Science - KDnuggets,"Platforms for developing analytics offer ways to package the algorithm to run in different production environments. Look for a tool that provides integration paths and application servers for use with common IT systems, as well as the ability to target embedded devices. For example, MATLAB provides deployment paths for integrating analytics with programming languages commonly used in IT systems (e.NET), as well as for converting analytics to standalone C code that can be run on embedded devices. Both of these deployment options are accessed through point-and-click interfaces, further reducing the time spent on conversion. By automating the process of converting the analytic to run in production systems, these tools significantly reduce the time it takes to get a new analytic in production"
3 Challenges for Companies Tackling Data Science - KDnuggets,Technologies that enable domain experts to apply machine learning and other data science techniques to their work are here to stay. They provide exciting opportunities for teams to innovate—in both their design workflows and the products they create. It does not appear that the shortage of data scientists will be addressed anytime soon. Domain experts will play a crucial role in filling this gap. Their knowledge of the business and the products it produces positions them well to find innovative ways to apply data analytics technologies
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"But, the problem with these plots is that they are created using a trained model. If we could create these plots from train data directly, it could help us understand the underlying data better. In fact, it can help you with all the following things:"
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"If dependent variable (target) is binary, scatter plots don’t work because all points lie either at 0 or 1. For continuous target, too many data points make it difficult to understand the target vs. Featexp creates better plots which help with this problem. Let’s try it out!"
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"Featexp creates equal population bins (X-axis) of a numeric feature. It then calculates target’s mean in each bin and plots it in the left-hand side plot above. In our case, target’s mean is nothing but default rate. The plot tells us that customers with high negative values for DAYS_BIRTH (higher age) have lower default rates. This makes sense since younger people are usually more likely to default. These plots help us understand what the feature is telling about customers and how it will affect the model. The plot on the right shows number of customers in each bin"
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"Noisy features lead to overfitting and identifying them isn’t easy. In featexp, you can pass a test set and compare feature trends in train/test to identify noisy ones. This test set is not the actual test set. Its your local test set/validation set for which you know target"
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,Dropping low trend-correlation features works well when there are a lot of features and they are correlated with each other. It leads to less overfitting and other correlated features avoid information loss. It’s also important to not drop too many important features as it might lead to a drop in performance
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,Not dropping important features further improves LB AUC to 0.74. It’s also interesting and concerning that test AUC doesn’t change as much as LB AUC. Getting your validation strategy right such that local test AUC follows LB AUC is also important. Whole code can be found in
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"The insights that you get by looking at these plots help with creating better features. Just having a better understanding of data can lead to better feature engineering. But, in addition to this, it can also help you in improving the existing features. Let’s look at another feature EXT_SOURCE_1:"
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"Customers having a high value of EXT_SOURCE_1 have low default rates. But, the first bin (~8% default rate) isn’t following the feature trend (goes up and then down). It has only negative values around -99.985 and a large population. This probably implies that these are special values and hence, don’t follow the feature trend. Fortunately, non-linear models won’t have a problem learning this relationship. But, for linear models like logistic regression, such special values and nulls (which will be shown as a separate bin) should be imputed with a value from a bin with similar default rate instead of simply imputing with feature mean"
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"Featexp also helps you with gauging feature importance. DAYS_BIRTH and EXT_SOURCE_1 both have a good trend. But, population for EXT_SOURCE_1 is concentrated in special value bin implying that feature has the same information for most of the customers and hence, can’t differentiate them well. This tells that it might not be as important as DAYS_BIRTH. Based on XGBoost model’s feature importance, DAYS_BIRTH is actually more important than EXT_SOURCE_1"
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"Data leakage from target to features leads to overfitting. Leaky features have high feature importance. But, understanding why leakage is happening in a feature is difficult. Looking at featexp plots can help you with that"
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"The feature below has 0% default rate in ‘Nulls’ bin and 100% in all other bins. Clearly, this is an extreme case of leakage. This feature has a value only when the customer has defaulted. Based on what the feature is, this could be because of a bug or the feature is actually populated only for defaulters (in which case it should be dropped)"
My secret sauce to be in top 2% of a Kaggle competition - KDnuggets,"Since featexp calculates trend correlation between two data sets, it can be easily used for model monitoring. Every time the model is re-trained, the new train data can be compared with a well-tested train data (typically train data from the first time you built the model). Trend correlation can help you monitor if anything has changed in feature w"
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,"These multiple facets of strategy did play a significant part in defining my perspectives on strategy.There is no doubt that works of other greats in the field like Peter Drucker and Michael Porter  did shape my thinking process and my perspectives on strategic management. However what made this book on top of my favourite list is the different angles through which the field of strategic management was looked at by the authors. The title of this post is derived by drawing inspiration from Mintzberg’s seminal work. In this post, I am attempting to take you on a safari through the data science strategy formulation process"
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,"When formulating a data science strategy, a pertinent question one can ask is this. With tremendous strides data science is making in influencing business outcomes, should data science strategy lead organisation strategy or like any other functional strategy, should it be aligned to Organisational strategies ? Well, in my opinion, like any other functional strategy, data science strategy should also be aligned to organisational strategy. Data science domain would have no meaning if it is not used to support the organisation in meeting its overall objectives. And for this very reason I strongly believe that data science strategy has to be derived out of organisation strategy. So the next question is how do we define a data science strategy which is aligned to organisation strategy ? To answer that question let us decipher the strategic alignment framework"
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,"Strategic alignment is the process by which an organisation’s competencies,resources and actions are aligned to the planned organisational objectives. Data science has become a very critical competency an organisation have to build, to have an edge in this digitally connected era. However it is equally important that the output from a data science engagement i. This can be achieved by traversing the processes of the alignment framework. Figure 2 is the depiction of data science strategic alignment framework"
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,"The trail for analysis for the customer management function is as depicted in figure 3 above. To ensure that data science strategy is aligned to organisational goals, the first step of the process is to identify Key Performance Indicators ( KPI’s )  for each function within the value chain. For the function ‘customer management’ which we are analysing, one critical KPI which has substantial impact on the top line and bottom line is"
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,"Having identified a critical performance indicator, alignment to it would entail deriving data science use cases which will help in achievement of this performance indicator . For customer management function a  use case which will help in improving customer retention rate would be to predict probability of premium renewal. The output from this use case can be used for targeted campaigns towards customers who have low probability of renewing premiums, there by enabling achievement of the KPI.  In addition to use cases which are directly related to the KPI we should also derive related use cases which will enable the process of achieving that KPI. For example having known which customer to be targeted, it would also be valuable to know specifics of how to target them,like predicting right time and channel to reach out to target customers or predicting right price point for giving them specific offers"
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,"In a similar fashion, we have to look across all the functions,critical metrics within each function and derive all primary and related predictive use cases. These use cases can be formed into an interconnected web called Strategic Alignment Map ( SAM). Figure 4 below  is a representative SAM depicting the business value chain,its critical functions, interconnected web of use cases and its corresponding category ( Natural Language Processing, Inferential , Machine Learning/Deep Learning, Other AI etc). A comprehensive SAM would form the blue print for aligning data science projects to organisational strategy and also in indicating inter dependencies between different use cases / models. In addition, it will also be an aid to get a view on various data science competencies which are required to add value to an organisation"
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,"Once we have an interconnected web of use cases critical for the organisation, the next task would be in getting data acquisition and integration strategies aligned to the overall strategy. To align data acquisition strategies to overall strategy we first have to know what kind of data points are required for implementing the use cases depicted in the SAM and also the characteristics of the data points like formats, velocity, frequency, data systems which generate them etc. A good approach to derive those details is to look at each use case, identify business factors influencing  each of them and then working our way downwards"
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,A comprehensive list of  factors like the above have to be identified through close discussions with business/domain teams. Having identified various factors affecting each use case the next task is to identify data points related to each factor. Some of the major data points related to factors influencing renewal rate is depicted in figure 5 below
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,"The requirement for data points related to each factor governs data sourcing and integration strategies. From the various data points depicted above we can see that data requirements can be from within the organisation and also from external sources. For example, data points related to competition in all probability will have to be acquired from external sources. Other data points predominantly can be acquired from various systems within the organisation"
Data Science Strategy Safari: Aligning Data Science Strategy to Org Strategy - KDnuggets,"Having seen the data science strategic alignment framework in action one can not help but wonder if we can draw parallels from the framework to some of the perspectives of Mintzberg’s “Strategy Safari”. The process steps encapsulated within this framework have elements of the Learning, Cognitive and Planning Schools of strategy formulation. However at the end of the day this framework, like any other framework, is aimed at structuring one’s though process towards achievement of certain objectives. The objective it aims to accomplish is to ensure that your data science efforts are aligned to overall Organisational goals and strategies"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"Various organizations predict when you’ll die. Insurance companies, police departments, doctors -- and in some cases, this actually helps you. The fact is, there are computers out there calculating the probability you will die in the relatively near future. These predictions serve a variety of purposes, such as trying to medically allay your demise, or at least administratively manage it"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"People do try to gloss over this little detail. For example, the publisher of my book, ""Predictive Analytics,"" was initially quite resistant to authorize the book's subtitle ‘cause it has the word “die” in it. The book title’s an informal definition of the field, by the way. The full title is, ""Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"But we shouldn’t gloss over death — not only for the obvious reason of its unavoidability, but also ‘cause death matters. I mean, every important thing a person does can be valuable to predict -- for all kinds of business, government, and healthcare operations. And this definitely applies for the final thing we will each do. To put it gently, I'm talking about, you know, keeling over"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"How does it work? Well, the same machine learning methods apply for this use of predictive analytics as for predicting any other kind of behavior or outcome. What makes the difference for what it's predicting is just the training data. For predicting death, we supply data that includes examples of A) people who died -- say, within a time window of 12 months -- and examples of B) people who did not die within that time frame, so that our trusty machine can derive patterns that discern group A from group B. Practically speaking, for the machine to learn is for it to find trends in how those two groups differ"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"The basis for differentiating these two groups will be any and all things recorded about individuals, including their demographics and their previous behavior. Whichever such attributes are included in the training data end up serving as inputs to the predictive model. For example, in healthcare this can be all kinds of clinical features, test results, and even MRIs. But then it goes beyond all that to some pretty surprising stuff. All kinds of unexpected factors you wouldn't immediately think of might help predict. For example, early retirement is a risk factor. For a group of Austrian men in one study, early retirement decreased life expectancy. And, on the boat the Titanic in 1912, women were almost four times more likely to survive than men. Rock stars die younger, and solo rock stars die even younger than those in bands, according to public health offices in the U.K"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"But that’s far from certain. As with most outcomes we may wish to predict, death is impossible for anyone or any technology to predict with high accuracy in general. But it’s still useful. I should be quick to point that, like many other outcomes, it's definitely possible for machine learning to predict death considerably better than guessing -- and often better than human experts. I'll explain why that limited level of prediction is still extremely useful -- but first a little more on what it means to estimate these probabilities"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"The trends learned from data are together known as a predictive model. For death prediction, this is often called a mortality model. Or, the more genteel among us might call it a croakometer"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"Just kidding. Now, when the model predicts for an individual person today whether they'll live or die, only time will tell for sure if it turns out to be correct. To be more specific, some models calculate, for example, the probability you will die within, say, one year. So, among those assigned around a, say, 30% probability, about 30% of those will indeed die within a year. But we don’t have certainty about any one of them as an individual -- on a case-by-case basis, we don't know for sure one way or the other. You can think of probabilities as just a level of predictive confidence that death will happen — a risk level — which is basically never full on 100%. There’s never total certainty — always some shade of gray"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"Now, just to clarify, even though the model just gives us probabilities, the word ""prediction"" does still apply. If you say, ""Let's put our money on everyone assigned a 90% or greater risk and bet they'll die. So beware any claims of ""high accuracy"" in machine learning. It’s not a magic crystal ball"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"Number one: Healthcare providers predict death to help prevent it. Patients with a high mortality risk are flagged accordingly, and this guides clinical treatment decisions. For example, Riskprediction"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"Number two: End-of-life counseling, palliative care, and hospice care can be more effectively targeted when we know which patients are the most likely to pass away. Many who are nearing the end of life will greatly benefit from these offerings, especially if well-timed. For this reason, there's an active trend among healthcare institutions, insurance companies, and research labs to apply machine learning for this purpose"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"And finally, number five: Life insurance. Now, the central thing life insurance companies do is predict life expectancy -- that is, they predict when you're going to die. That's how they set premiums and manage coverage. To improve their predictions, life insurance companies more and more these days go beyond conventional actuarial look-up tables and integrate machine learning methods in order to improve predictive performance"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"And, hold on, we're not quite finished yet -- as a special bonus, there's actually one more thing to predict even after death. The Chicago Police Department predicts whether a murder can be solved, based on the characteristics of a homicide and its victim. The objective is to better manage and triage investigations, which improves overall enforcement, which, one hopes, could help reduce the number of future murders"
Dr. Data Show Video: Five Reasons Computers Predict When You’ll Die - KDnuggets,"And that wraps up another deadly episode of the Dr. Data Show. I’m Eric Siegel; thanks for watching. Hit “like” and share this video if you think your friends would also be interested in the very morbid but practical topic of death prediction. And for access to the entire web series, go to TheDoctorDataShow"
Intro to Data Science for Managers - KDnuggets,"Let’s take a closer look. The branches of our mindmap are grouped in a particular way. In the middle, there are two sections of asic knowledge on which any data science projects are built, namely"
6 Goals Every Wannabe Data Scientist Should Make for 2019 - KDnuggets,"A person could apply OKRs to a data science project by choosing the most meaningful metric associated with it. That metric shapes the objective, and the key results take a deep dive into the processes the individual must go through to make the project fruitful. It's best if each key result has a date associated with it"
Expand Your Data Science Knowledge with Hands-on Labs and Bootcamp - KDnuggets,"Elevate your data skills with hands-on labs and bootcamp at TDWI Las Vegas, Feb 10-15. Super Early Bird ends Dec 14. KDnuggets readers save up to $915 with code KD20!"
What is the Best Python IDE for Data Science? - KDnuggets,"Created by Guido van Rossum, Python was first released back in 1991. The interpreted high-level programming language is developed for general-purpose programming. Python interpreters are available on several operating systems, including Linux, MacOS, and Windows"
What is the Best Python IDE for Data Science? - KDnuggets,"In such a case, you need to use an IDE (Integrated Development Environment) or a dedicated code editor. As Python is one of the leading programming languages, there is a multitude of IDEs available. So the question is, “Which is the best IDE for Python?”"
What is the Best Python IDE for Data Science? - KDnuggets,"Apparently, there is no single IDE or code editor for Python that can be crowned with “THE BEST” label. This is because each of them has their own strengths and weaknesses. Furthermore, choosing among the vast number of IDEs might be time-consuming"
What is the Best Python IDE for Data Science? - KDnuggets,"Worry not though, as we’ve got you covered. In order to help you pick the right one, we’ve sorted out some of the prominent IDEs for Python, specifically created for working with data science projects. These are:"
What is the Best Python IDE for Data Science? - KDnuggets,"Atom is a free, open-source text and source code editor available for a number of programming languages, including Java, PHP, and Python. The text editor supports plugins written in Node. Although Atom is available for a number of programming language, it shows an exceptional love for Python with its interesting data science features"
What is the Best Python IDE for Data Science? - KDnuggets,"One of the greatest features that Atom brings to the table is support for SQL queries. However, you need to install the Data Atom plugin first to access the feature. It provides support for Microsoft SQL Server, MySQL, and PostgreSQL. Furthermore, you can visualize results in Atom without the need of opening any other window"
What is the Best Python IDE for Data Science? - KDnuggets,"Born out of IPython in 2014, Jupyter Netbook is a web application based on the server-client structure. It allows you to create as well as manipulate notebook documents called notebooks. For Python data scientists, Jupyter Notebook is a must-have as it offers one of the most intuitive and interactive data science environments"
What is the Best Python IDE for Data Science? - KDnuggets,"In addition to operating as an IDE, Jupyter Notebook also works as an education or presentation tool. Moreover, it is a perfect tool for those just starting out with data science. You can easily see and edit the code with Jupyter Notebook, allowing you to create impressive presentations"
What is the Best Python IDE for Data Science? - KDnuggets,"By using visualization libraries like Matplotlib and Seaborn, you can display the graphs in the same document as the code is in. Also, you can export your entire work to a PDF, HTML or . Like IPython, Project Jupyter is an umbrella term for a bunch of projects, including Notebook itself, a Console, and a Qt console"
What is the Best Python IDE for Data Science? - KDnuggets,"PyCharm is a dedicated IDE for Python. PyCharm to Python is what Eclipse is to Java. The full-featured Integrated Development Environment is available both in free and paid versions, dubbed Community and Professional editions, respectively. It is one of the quickest IDEs to install with a simplistic set up thereafter, and is preferred by data scientists"
What is the Best Python IDE for Data Science? - KDnuggets,"For those with a likeness for IPython or Anaconda distribution, know that PyCharm easily integrates tools like Matplotlib and NumPy. This means you can work easily with array viewers and interactive plots while working on data science projects. Other than that, the IDE extends support for JavaScript, Angular JS, etc. This makes it opportune for web development too"
What is the Best Python IDE for Data Science? - KDnuggets,"Once you finish the installation, PyCharm can be readily used for editing, running, writing, and debugging the Python code. To start with a new Python project, you need to simply open a fresh file and start writing down the code. In addition to offering direct debugging and running features, PyCharm also offers support for source control and full-sized projects"
What is the Best Python IDE for Data Science? - KDnuggets,"What’s best about Rodeo is that it offers the same level of convenience to both beginners and veterans. As the Python IDE allows you to see and explore while creating simultaneously, Rodeo is undoubtedly one of the best IDEs for those starting out with data science using Python. The IDE also boasts built-in tutorials and comes with helper material"
What is the Best Python IDE for Data Science? - KDnuggets,"Spyder is an open-source, dedicated IDE for Python. What’s unique about the IDE is that it is optimized for data science workflows. It comes bundled with the Anaconda package manager, which is the standard distribution of"
What is the Best Python IDE for Data Science? - KDnuggets,"Build especially for data science projects, Spyder flaunts a smooth learning curve allowing you to learn it in a flash. The online help option allows you to look for specific information about libraries while side-by-side developing a project. Moreover, the Python-specific IDE shares resemblance with RStudio. Hence, it is opportune to go for when switching from R to Python"
What is the Best Python IDE for Data Science? - KDnuggets,"Spyder’s integration support for Python libraries, such as Matplotlib and SciPy, further testifies to the fact that the IDE is meant especially for data scientists. Other than the appreciable IPython/Jupyter integration, Spyder has a unique “variable explorer” feature at its disposal. It allows displaying data using a table-based layout"
What is the Best Python IDE for Data Science? - KDnuggets,"After working for a decade in Infosys and Sapient, he started his first startup, Leno, to solve a hyperlocal book-sharing problem. He is interested in product marketing, and analytics. His latest venture"
How Data Scientists Can Train and Updates Models to Prepare for COVID-19 Recovery - KDnuggets,"Even if we can quickly contain the virus, recovery will take time. The ongoing fear of the virus, broken businesses, and millions of unemployed people will have long-term impacts. Data-driven companies will be able to navigate this, but effective strategies cannot be built on incorrect assumptions"
How Data Scientists Can Train and Updates Models to Prepare for COVID-19 Recovery - KDnuggets,"The most valuable source of insight for the recovery rate in key markets will be tracking how demand returns in economies that are further along in their recovery journey. There are many variables, but only as more countries recover will we begin to gather insights into what revised baselines should be. A few key inputs for identifying your business's recovery rate include:"
How Data Scientists Can Train and Updates Models to Prepare for COVID-19 Recovery - KDnuggets,Enterprises need to be able to focus your efforts on the surges of demand that occur. Events drive these so tracking events from massive down to minor is key. Even the small events can cluster to create significant impact – these are often referred to as
How Data Scientists Can Train and Updates Models to Prepare for COVID-19 Recovery - KDnuggets,"The coming recovery period is going to be a dynamic time for the events industry as a whole. Before the pandemic, thousands of high-impact events worldwide took place every week. This frequency will nonetheless resume with time, but the level of rescheduling and changes will make a recovery particularly intense"
How Data Scientists Can Train and Updates Models to Prepare for COVID-19 Recovery - KDnuggets,"She brings comprehensive data science and advanced machine learning expertise, as well as industry-leading intelligence product R&D insights after more than a decade of experience of leading R&D teams at Baidu in Beijing and Workday in California. Within her role with PredictHQ, she is responsible for data intelligence R&D, machine learning features, and product R&D and intelligence product development. She both grows and leads a team of data scientists, engineers, and analysts that work on complex challenges such as event impact prediction models to make sense of millions of events worldwide. Dr. Wang is passionate about using data, algorithms, and building intelligence to make an influence on the world. Outside of work, she is an avid lover of hiking, history, and food"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"It is one thing to build machine learning models that are interesting on an experimental and exploratory level but it is another thing to build models that drive action and disrupt your business. Many teams use machine learning as decision support divisions: the data scientists analyze data and generate ad-hoc insights that a manager uses to manually make decisions. There are many challenges with this model, including lack of scalability and efficiency, and inability of data science teams to measure their generated value. Without a value measurement system, data scientists might not be able to justify their existence and, more importantly, won’t be able to self-correct and improve their practices. In order to disrupt business, machine learning models must adopt a product-focused approach, which is  a much more significant undertaking"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"For a product-driven approach to use machine learning, it is important to think about the problem you are trying to solve from the beginning and to have some initial idea of how the machine learning solution might be used. The first step is to understand what pain points you are trying to tackle, and what kind of service-level agreement in terms of quality, availability and responsibility you need. This guides the overall structure of the product, what approaches you might take, and what constraints exist. Based on the business needs and other requirements, the delivery patterns- batch, real-time, or streaming- and infrastructure can be decided"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"Other than the exploratory phase - feature engineering and machine learning model exploration and selection - the rest of the pipeline is no different from typical software products and therefore all the good practices like DevOps and cookie cutter project structures can be used and, to a large extent, automated. Once you know the scope and the constraints of the problem, you can start playing with the data. The goal of this stage is to determine, as quickly as possible, if you understand the problem sufficiently and if you can come up with a solution. Jupyter notebooks are a great medium at this stage and you can do rapid prototyping before you have to think about everything else that needs to be taken into account. However, if any piece of the exploratory work becomes needed for the final product then “you need to enforce all of the software engineering practices like unit and integration testing and deployment and performance tests to ensure that the data pipelines are reliable and meet necessary standards as early as possible,” said Solmaz. At this stage, it is also important to know what exceptions might appear and how to handle them without a need for manual intervention"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"Once the design stage is concluded, the ingestion, feature preparation, modeling scripts, and their corresponding tests can be promoted to a test/QA environment, before going to a production environment. Having everything run in production is the ultimate goal and the limitations of that environment sets the constraint for other stages. For example, if the production environment does not support software packages or hardware that your model relies on, using those in the design stage is not warranted. In some cases, there is no need for re-training models in the production environment which means only the requirements of the inference part need to be considered. A good practice is to obtain the production environment limitations at the beginning and limit all experimentation based on those. Using containerized solutions is one way to achieve consistency here (see"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"Once the product is live, it is important to monitor the pipeline. This includes the machine learning model for problems like drift in the input data distribution or instability of inferred scores or clusters. Another important piece to monitor is the interaction of the end-user with the product. This allows you to measure the value generated and to self-correct the pipeline if needed"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"In order to achieve a highly efficient end-to-end pipeline, you need to create a multi-faceted team and establish the right  dynamics. Depending on the complexity of the projects and the availability of resources, one person might take all these roles, aka a full-stack data scientist, or they might be taken on by multiple people. However, you should have automated data collection about the user behavior throughout your product. Solmaz believes that “data doesn’t always tell you why people are doing stuff and therefore you should have a UX research team to help you understand the user behaviour"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"The product features that you need, and therefore the data that is required to be used, is guided by what problem you are trying to solve for the end user. This needs to be clarified in the problem definition stage through user interviews. Amir points out that “unlike the traditional way of building products where your R&D team is sitting somewhere far from the customers what you should do when you build a machine learning solution is to put the user in the center of your design. Sometimes talking to domain experts can be a good proxy for what users want but it is crucial to talk to the end users directly as well. A typical mistake is that you might think that you are the user of a certain service or product and therefore you know what it should be like. However, it is important to realize that you are not the “typical user” in most cases and therefore still need to hear people out before you know what you need to build. Inmar said that you can’t say, “I buy books, therefore I understand people who are going to buy books"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"In many cases your data science product and the context it is placed in are very dynamic and you need to design accordingly. These dynamic designs need to be done “in a non-invasive way in order to avoid disturbing the workflow that the user is experiencing,” said Amir. As the machine learning solutions are built on the patterns found in data, any changes in it, including user behavior as a result of the introduction of the product, means that you might have to retrain/redesign the model"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"Another important design decision is determining how to collect the right data, especially when you are dealing with a cold start problem which can exist on many levels. For example, you might not have any data at all, or you have some data but no relevant/reliable labels, or you have all the data you need and have built a model and have evaluated it on historical data but need to know how it performs in real life. In fact, Amir thinks that “the people who build the most successful products (and get rich) are the ones who design the most creative ways to collect data   combined with innovative machine learning models"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"Using that model, you can complete and test your end-to-end solution, and if you have done your homework in identifying a user problem, this solution can solve that problem to some extent. Once you have a product with some basic functionality in the hands of the users, you have the opportunity to gather explicit and implicit feedback from the users to improve your product. The design of the pipeline has to be flexible enough that you can substitute in more complex machine learning solutions as soon as you have enough information"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,Taking the minimum viable machinery approach also allows you to establish a baseline for your work. Inmar believes that “usually the biggest bang for the buck could be achieved with the most basic algorithms within an end-to-end solution and then you can measure it using A/B testing or by evaluating the monetary implications of your model to make incremental improvements. Solmaz warned us that “in many organizations it is the first time that machine learning is being tried so there is this doubt if it actually works and it is important for those few first projects to make sure they get some sort of result
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"A lot of what we talked about so far applies to the situation where you have autonomy to decide the structure of the team and the infrastructure you use. Tackling implementation and scaling issues is more straightforward with all features that public clouds offer. However, not all companies have that luxury, for example larger enterprises that have regulatory limitations about where their data can be or smaller ones that have limited capital and operational budgets. In those scenarios what you can do is limited by the organizational and legacy considerations and platforms. Sometimes “you have to deal with clients that have their data in good shape but there's a lot of fragmentation that comes from different silos and you have to engineer to get the datasets together,” said Vincent. Another aspect to consider is that these companies have been doing things in certain ways for a long time and the domain experts in these companies really know a lot about their subject matter"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"Sometimes managers at these companies are overly excited and expect you to do magic for them and you need to have that humbleness to listen to their problems and transparency to do careful feasibility analysis and give them the best recommendations. Educating technical and non-technical people in the company about the possibilities presented by machine learning can also provide the right context for successful work without which good ideas face a lot of resistance and expectations are set at wrong levels. For example, if you work with good product managers who know the domain and industry well and your first model can basically be understanding what's going on in their brain and just automating that,” said Solmaz"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"I did my PhD at the University of Toronto specializing in machine learning. Previously, I worked at Kobo on things like recommendations, search, NLP on the text of the books, experience optimization and so on. Afterwards I led the software and machine learning groups at Kindred which is a company that is building robotic solutions for operations in warehouses. Now at Uber my focus is on productionizing the algorithms that come out of a research team on the cars and the software stack"
Machine Learning in Action: Going Beyond Decision Support Data Science - KDnuggets,"I'm currently the vice-president of Data Science and Engineering at Shopify. My team is distributed across multiple  data-driven products, such as order fraud detection, Shopify capital, forecasting future sales, and logistics and fulfillment. I completed a Masters in machine learning, and a Masters in bioinformatics where I used predictive models to predict the outcome of breast cancer. During my degrees, I got a sense of joy and frustration of real solutions that work outside of lab and outside of your own shell script. After that I worked at Morgan Stanley before joining Shopify in 2013"
The Big Data Game Board™ - KDnuggets,"While the Big Data Storymap has recently gained lots of attention, it’s getting old and blasé.  It needs to be freshened up. So with a nudge from my friend Frederick Lardaro (on twitter as @FLardaro), I have created the “"
The Big Data Game Board™ - KDnuggets,"Move away from Data Lake 2.0, where data is randomly stored for yet-to-be determined purposes, and instead develop Data Lake 3.0 that becomes the ultimate repository for the organization’s key digital assets – data and analytics.  Data Lake 3.0 becomes the organization’s collaborative value creation platform, facilitating the collaboration between the business constituents and the data science team to leverage data and analytics to uncover new sources of customer, product, service, channel and operational value (see Figure 4)"
The Big Data Game Board™ - KDnuggets,"Organizations don’t fail at Big Data because of a lack of use cases; they fail because they have too many.  Organizations need a formal, collaborative process to identify, validate, vet, value and prioritize the use cases, and ensure IT and Business alignment in pursuing those use cases with the optimal mix of high business value and high implementation feasibility (think “low hanging fruit”).  Invest the time upfront of a Big Data and Data Science initiative to understand in details the financial, customer and operational drivers that constitute a use case, and flag any potential inhibitors and implementation risks. Come into this game with eyes wide open (see Figure 5)!"
The Big Data Game Board™ - KDnuggets,"Over-hyped technology innovations provide the ultimate “Field of Dreams” moment with non-scientists cranking away on science experiments.  While there is value in organizations getting familiar with new technologies, the technology Proofs of Concepts should NOT be inflicted upon business stakeholders with over-fabricated expectations of grandeur.  Business users long ago stopped believing in the technology “Silver Bullet” solution – a simple and seemingly magical solution to a complicated problem (see Figure 6)"
Learning during a crisis (Data Science 90-day learning challenge) - KDnuggets,"Almost every one of us is contained inside our homes because of the COVID-19 pandemic. We rarely go outside, and I am spending an awful lot of time on the internet, in front of my desktop computer, and I am doing a lot of reading online. With so much reading going on, there were two articles I read recently, and I could not get them off my mind. First, was"
Learning during a crisis (Data Science 90-day learning challenge) - KDnuggets,"There are many times in my life that I have figured out the hoaxes and fake emails, messages or calls because I have come across them while reading. Places where people lost money to fraud, I was saved because I either already knew or I wanted to search it first and read on it before clicking or replying to anything. Scott has mentioned three roads to learning:"
Learning during a crisis (Data Science 90-day learning challenge) - KDnuggets,"So what do we learn from this? There is one very well known fact in personal development that the quality of your life is directly proportional to the quality of questions you ask yourself. In fact, most people never ask any questions to themselves, or they keep on asking the same questions for years. There is even a book on this, and I haven’t read it yet:"
Learning during a crisis (Data Science 90-day learning challenge) - KDnuggets,"With all the negative news going around, how all the world is in a downward spiral with the corona pandemic, with all the conspiracy theories going around of the source of the corona, sometimes you may feel depressed and become downright negative. This is what news does to you. Yes, it happened to me a few times last week. And with the real threat of this pandemic and the countrywide lockdowns across the world, you may absorb some of it, and during those times it may be very difficult for you to find happiness, to focus on your learning challenge. This is when you need to understand that the world has never stopped, no matter how tough the times have been. We had threats way more serious than current pandemic:"
Learning during a crisis (Data Science 90-day learning challenge) - KDnuggets,"No matter what has happened, humanity always went on, and we are still here, growing and expanding. Look on the positive side of life. Yes, do take all the physical precautions you need because looking at the positive side does not mean being reckless about your life. I said at the beginning of this article: there is a thin line between being positive and kidding yourself. What you need to do is to understand facts,"
Learning during a crisis (Data Science 90-day learning challenge) - KDnuggets,"So, take precautions, practice social distancing and keep your mind focused on the thought that when this pandemic is over, you will come out with one of the greatest learning periods of your life, in such a short time. Do what you have never done before, look inside your heart for guidance. It always guides you"
Should Data Scientists Model COVID19 and other Biological Events - KDnuggets,The role of a data scientist has been expanding for years. It went from crunching numbers with statistics to building scalable databases to building production ready machine learning or deep learning models. Biostatistics and Epidemiology are highly specialized fields of statistics that universities offer different degrees for them
Should Data Scientists Model COVID19 and other Biological Events - KDnuggets,"Biostatisticians use statistical techniques that your current everyday data scientists have probably never heard of. This is a great example where lack of domain knowledge exposes you as someone that does not know what they are doing and are merely hopping on a trend. While it is known among the community to build a prediction model to see who is more likely to have survived the titanic or classify an iris plant to being your data science journey, perhaps more caution should be given to more serious matters such as a global pandemic that is killing hundreds of thousands and potentially millions of people"
Should Data Scientists Model COVID19 and other Biological Events - KDnuggets,"Epidemiology is the study of the frequency and distribution of diseases within human populations and environments. Epidemiology is an important aspect of public health as it relates to the understanding of a disease in the population and assess its risk.  Typically epidemiologists have an experienced science background in areas such as biology, medicine and virology etc. This is how an epidemiologist builds their domain knowledge to actually be able to understand what they are modelling"
Should Data Scientists Model COVID19 and other Biological Events - KDnuggets,"Biostatistics is the application of statistical techniques to scientific research in health-related fields, including medicine, epidemiology, and public health. Someone who has a degree in statistics could probably become an analyst with data from retail, demographics, real estate, economics, finance etc. The biological sciences are a whole different space and require a separate qualification altogether"
Should Data Scientists Model COVID19 and other Biological Events - KDnuggets,Now a data scientist could come from a non statistical/mathematical background and suddenly start modelling disease data to show their skills. This is not the right type of data to to show your knowledge. It is up to each individual to know whether they have the ability to properly handle the data. So much false and misleading content has been published that further stains the profession of a data scientist as it shows that there are still people who are ignorant to the data and only care about using a random forest of xgboost model from Python instead of R (because R is not as cool apparently as it once was to some people) and promote it on LinkedIn hoping a recruiter or a senior data scientist will be impressed
Should Data Scientists Model COVID19 and other Biological Events - KDnuggets,"For example, forecasts of tomorrow’s stock prices are much less accurate because factors 1 and 3 above are not satisfied. First, the factors that contribute to changes in stock prices are not particularly well understood and depend at least partly on human psychology. Second, well-publicised forecasts of the stock market can directly affect the behaviour of many investors"
Should Data Scientists Model COVID19 and other Biological Events - KDnuggets,"The above 3 factors are not all applicable to diseases but we can see how number 2 is a problem because of underestimation of the actual number of cases. The second problem is that the forecasts of COVID-19 can affect the thing we are trying to forecast because governments are reacting, some better than others. A simple model using the available data will be misleading unless it can incorporate the various steps being taken to slow transmission"
Should Data Scientists Model COVID19 and other Biological Events - KDnuggets,"While a data scientist is tasked with analysing data to bring us insights, it is our responsibility to realize where our talents are to be used and when to take a step back and let the actual experts lead the charge. Infectious disease modelling is just too much of a specialized and sensitive area to blindly give your two cents. We need to be aware of the situations when we are needed and when we are not needed"
Fast Track Your Data Science Career - KDnuggets,"Student can also gain relevant, hands-on experience using real-world data sets. Online students can earn their degree on a part-time basis while they continue to work. As a result, they are better able to apply these new skills to their current job for a faster return-on-investment"
LinkedIn Top Voices 2018: Data Science & Analytics - KDnuggets,Meet 10 must-know writers and creators discussing everything from the prominence of Python to the ethical implications of artificial intelligence. Hint - you already know no. 1
Can Java Be Used for Machine Learning and Data Science? - KDnuggets,"Machine learning, data science, and artificial intelligence have been some of the most talked-about technologies in recent years, and rightfully so. These advancements in the world of tech have taken automation and business processes to the next level. Organizations of all sizes are investing millions of dollars into research and people to build these incredibly powerful data-driven applications"
Can Java Be Used for Machine Learning and Data Science? - KDnuggets,"Java is an incredibly useful, speedy, and reliable programming language that helps development teams build a multitude of projects. From data mining and data analysis to the building of Machine Learning applications, Java is more than applicable to the field of data science. It’s one of the most preferred languages for these tasks and has plenty of reasons why. If you’re about to tackle a machine learning project, consider using it. You’ll be surprised how much you can get out of it"
Free Workshop Preview: Data Thinking with Martin Szugat - KDnuggets,"As anticipation grows for Predictive Analytics World’s virtual conferences (PAW for Industry 4.0, PAW for Healthcare and Deep Learning World on 11-12 May 2020) and virtual workshops (13 May 2020), here is a chance to start familiarising yourself with the quality of the content and of the virtual networking. Gain an insight into how to apply design thinking for data science & analytics. Reserve your spot"
Peer Reviewing Data Science Projects - KDnuggets,"In any technical development field, having other practitioners review your work before shipping code off to production is a valuable support tool to make sure your work is error-proof. Even through your preparation for the review, improvements might be discovered and then other issues that escaped your awareness can be spotted by outsiders. This peer scrutiny can also be applied to Data Science, and this article outlines a process that you can experiment with in your team"
Peer Reviewing Data Science Projects - KDnuggets,"Peer review is an important part of any creative activity. It is used in research — both inside and outside academia — to ensure the correctness of results, adherence to the scientific method, and quality of output. In engineering, it is used to provide outside scrutiny and to catch costly errors early on in the process of technology development. Everywhere it is used to improve decision making"
Peer Reviewing Data Science Projects - KDnuggets,"Those of us working in the tech industry are familiar with one particular and very important instance of peer review — the code review process. If you’ve ever received a review, you should know that despite being slightly uncomfortable — as putting our creations under scrutiny always is — it can be highly valuable. This process forces us to prepare and, as a result, find numerous improvements to make even before the review. This often raises small but significant issues that have escaped our gaze but are easy for an outsider to spot, and sometimes it can provide very profound insights"
Peer Reviewing Data Science Projects - KDnuggets,"As in the research review, the motivation here is that model development phase errors can also be costly. These errors are almost always discovered in production and may require active model monitoring to catch. Otherwise, they are caught late in the productization phase, while requiring a developer/data engineer and exposing gaps between implicit assumptions made about engineering capabilities or the production environment; the technical validity check done at the end of the research phase is meant to mitigate some of these errors. However, since many of them are (1) technical, (2) implementation-based, and (3) model-specific, they can often only be examined during and after this stage, and so the model development review also handles them"
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,"Statistics can be a powerful tool when performing the art of Data Science (DS). From a high-level view, statistics is the use of mathematics to perform technical analysis of data. A basic visualisation such as a bar chart might give you some high-level information, but with statistics we get to operate on the data in a much more information-driven and targeted way. The math involved helps us form concrete conclusions about our data rather than just guesstimating"
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,"Statistical features is probably the most used statistics concept in data science. It’s often the first stats technique you would apply when exploring a dataset and includes things like bias, variance, mean, median, percentiles, and many others. It’s all fairly easy to understand and implement in code! Check out the graphic below for an illustration"
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,There are many more distributions that you can dive deep into but those 3 already give us a lot of value. We can quickly see and interpret our categorical variables with a Uniform Distribution. If we see a Gaussian Distribution we know that there are many algorithms that by default will perform well specifically with Gaussian so we should go for those. And with Poisson we’ll see that we have to take special care and choose an algorithm that is robust to the variations in the spatial spread
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,We have a dataset and we would like to reduce the number of dimensions it has. In data science this is the number of feature variables. Check out the graphic below for an illustration
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,"The cube represents our dataset and it has 3 dimensions with a total of 1000 points. Now with today’s computing 1000 points is easy to process, but at a larger scale we would run into problems. However, just by looking at our data from a"
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,"With feature pruning we basically want to remove any features we see will be unimportant to our analysis. For example, after exploring a dataset we may find that out of the 10 features, 7 of them have a high correlation with the output but the other 3 have very low correlation. Then those 3 low correlation features probably aren’t worth the compute and we might just be able to remove them from our analysis without hurting the output"
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,The most common stats technique used for dimensionality reduction is PCA which essentially creates vector representations of features showing how important they are to the output i. PCA can be used to do both of the dimensionality reduction styles discussed above. Read more about it in
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,"Over and Under Sampling are techniques used for classification problems. Sometimes, our classification dataset might be too heavily tipped to one side. For example, we have 2000 examples for class 1, but only 200 for class 2. That’ll throw off a lot of the Machine Learning techniques we try and use to model the data and make predictions! Our Over and Under Sampling can combat that. Check out the graphic below for an illustration"
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,"Let’s look at an example. Suppose I gave you a die and asked you what were the chances of you rolling a 6. Well most people would just say that it’s 1 in 6. Indeed if we were to do a frequency analysis we would look at some data where someone rolled a die 10,000 times and compute the frequency of each number rolled; it would roughly come out to 1 in 6!"
The 5 Basic Statistics Concepts Data Scientists Need to Know - KDnuggets,"If our frequency analysis is very good then it’ll have some weight in saying that yes our guess of 6 is true. At the same time we take into account our evidence of the loaded die, if it’s true or not based on both its own prior and the frequency analysis. As you can see from the layout of the equation Bayesian statistics takes everything into account. Use it whenever you feel that your prior data will not be a good representation of your future data and results"
"Has AI Come Full Circle?  A data science journey, or why I accepted a data science job - KDnuggets","Personal journeys in Data Science can vary greatly between individuals. Some are just getting starting and wading into this vast ocean of opportunity, and others have been involved during its decades-long evolution as a professional field. This review of a longer journey can provide a broader perspective of how you might fit into this interesting career"
"Has AI Come Full Circle?  A data science journey, or why I accepted a data science job - KDnuggets","After coding 2 or 3 applications for different clients, we wanted to make the process easier by creating re-usable modules.  This was also the early days of graphical user interfaces, and we thought that a visual way of connecting these modules might be helpful.  This was the origin of Clementine, invented by Colin Shearer; Clementine quickly sparked more commercial interest than the rest of our products put together"
"Has AI Come Full Circle?  A data science journey, or why I accepted a data science job - KDnuggets","However, my experience with clients had also revealed a glitch in our “democratisation” of machine learning.  Although Clementine applications could indeed be built without programming or deep knowledge of machine learning, this did not mean that they were as easy to build as a simple spreadsheet.  Applying machine learning to business problems requires conceptual leaps: this showed up in our training courses, where the machine learning aspect was always the hardest part for students to understand. The transformation of data for analysis was also significant: each step was easy to understand, but a complex transformation process remains complex, and in some ways, just like programming, even when using a higher level toolkit.  Consider the difference between coding in assembler and Python; both are programming, but no one would choose assembler over Python without a good reason (except for fun)"
"Has AI Come Full Circle?  A data science journey, or why I accepted a data science job - KDnuggets","At the end of 1998, ISL was acquired by SPSS.  For Clementine as a product, this was a game-changer, opening up a global market and leading to a vast array of applications.  This was a period of expansion, during which the immense value of machine learning applications became apparent; we published case studies of industry-changing applications and ROI in billions of dollars. We applied Clementine to a wide range of data, including unstructured forms. The term “Predictive Analytics” was coined: a more understandable phrase with a clearer link to potential benefits.  Machine learning was becoming mainstream, and the main vehicle for this was commercial software packages and their associated methodologies (this description applies equally well to SAS and Clementine, now renamed “SPSS Modeler”)"
"Has AI Come Full Circle?  A data science journey, or why I accepted a data science job - KDnuggets","Over the last 10 years, the growth of mainstream machine learning has continued hand-in-hand with the growth of open source software.  The association of machine learning applications with open source was surprising to many in the world of commercial software, but perhaps it should not have been.  The high price of commercial software created a barrier, not so much for organisations adopting machine learning as for individuals learning the tools of the trade.  It’s much easier for a university to teach or a curious individual to learn Python than a commercial package, so it’s easier to hire people with this kind of knowledge"
"Has AI Come Full Circle?  A data science journey, or why I accepted a data science job - KDnuggets","On the other hand, the outstanding benefits to be gained from machine learning applications made commercial software a victim of its own success.  What does it matter if developing a predictive model takes a little longer if the ROI is 10,000%?  Compared to the benefits, both software and development costs were insignificant, and in any case, developing a machine learning model is only a small part of the complete solution.  More complex development processes and a lack of systematic methodology may slow down development and reduce the quality of results, but the returns are still enormous, and, well, everybody’s doing it"
"Has AI Come Full Circle?  A data science journey, or why I accepted a data science job - KDnuggets","So it doesn’t bother me that AI applications are now built using tools like those of 30 years ago, or that more efficient tools are widely ignored.  It does bother me when the methodology is also 30 years out of date; exclusive emphasis on low-level technical issues can sideline business requirements and the business significance of intermediate findings.  Technical problems must be solved, but if we do so at the expense of business relevance, then we doom our solutions to fail"
KNIME Spring Summit Online Edition - KDnuggets,"The KNIME Summits, in spring and fall, have been taking place since 2008 in Europe and the US. In light of the coronavirus, this year’s KNIME Spring Summit moved online. Not too late to participate: KNIME Spring Summit continues online. Check out the extended summit program now"
KNIME Spring Summit Online Edition - KDnuggets,"The KNIME Summits, in spring and fall, have been taking place since 2008 in Europe and the US. In light of the coronavirus, this year’s KNIME Spring Summit moved online. More than 2000 people dialed in from their home offices to take part in a course or joined the interactive summit sessions"
KNIME Spring Summit Online Edition - KDnuggets,"This approach removes the gap between creating data science and putting data science into production. Dean Abbott (Smarter HQ) gave the keynote speech. You can see a video of his talk (and the other talks, too!) on the"
KNIME Spring Summit Online Edition - KDnuggets,"More talks by invited speakers and a wide range of workshops are being streamed live as free webinars in the coming weeks. Presentations of KNIME in Action are brought to you by speakers from Chiesi Farmaceutici, Redfield, and Queen’s University, Canada. The workshop program includes time series analysis, guided labeling, machine learning, RDKit in KNIME, GDPR compliance and anonymization techniques, to name just a few"
How Data Science Is Being Used to Understand COVID-19 - KDnuggets,"Many of these sources pull from data provided by trusted bodies such as the U.S Centers for Disease Control and Prevention (CDC) and the World Health Organization (WHO). They also include direct links to those places so that people have quick, easy access to reliable information"
How Data Science Is Being Used to Understand COVID-19 - KDnuggets,"Contact tracing is an effective way to slow COVID-19. It involves getting in touch with a person's close contacts after that individual tests positive for the virus and telling them to self-isolate. Contact tracing is time-consuming, although it's getting easier as more people take social distancing seriously"
How Data Science Is Being Used to Understand COVID-19 - KDnuggets,"They created a mobile phone-based solution to eliminate the need for people to call the contacts manually. Instead, those parties get text messages confirming the need for self-isolation. The researchers clarify that their approach would be most effective if it gets support from national leaders and is not an effort primarily spearheaded by independent app developers"
How Data Science Is Being Used to Understand COVID-19 - KDnuggets,"Many people with COVID-19 have only mild symptoms or none at all. Plus, the classic symptoms include a fever and a cough — two issues not restricted to the coronavirus. These things could make it easier for people to unknowingly spread the disease. But, developers created an app that"
How Data Science Is Being Used to Understand COVID-19 - KDnuggets,"200,000 users. People can and should interact with the app even if they are asymptomatic or do not think their symptoms are COVID-19-related. The more researchers know about the coronavirus, the better equipped they are to tackle it"
How Data Science Is Being Used to Understand COVID-19 - KDnuggets,"People interact with the app to do a short daily symptom check-in. They also give their age and postal code, plus disclose any preexisting conditions. That information helps scientists determine the groups that are most affected or at risk. The app does not take user data for commercial purposes, but it gives it to people who are working to stop the coronavirus, including some at health organizations"
How Data Science Is Being Used to Understand COVID-19 - KDnuggets,"Besides the race to restrict the COVID-19 spread, scientists are working as quickly as possible to uncover effective treatments. Two graduates of the data science program at Columbia University have turned to machine learning to help. The typical process of antibody discovery in a lab takes years. This approach, however,"
How Data Science Is Being Used to Understand COVID-19 - KDnuggets,"The team taking this approach says this method is less costly than traditional ones, too. Humans are still part of the process because they have to test the gene sequences identified as most promising by the machine learning algorithm. However, using this expedited method could be crucial in efficiently finding interventions that work for coronavirus patients"
How Data Science Is Being Used to Understand COVID-19 - KDnuggets,"Data science specialists have also concluded that graph databases are instrumental in showing them how COVID-19 spreads. A graph database shows links between people, places or things. Scientists refer to each of those entities as a node, and the connections between them are the ""edges"
3 Best Sites to Find Datasets for your Data Science Projects - KDnuggets,"You should be very familiar with Kaggle by now. Companies have been releasing their data in Kaggle to harness the strength of the community and solve their real-life problems. This makes Kaggle the perfect place to find datasets with real problem statements to solve. If you want to practice building machine learning models without the hassle of generating or labeling data, Kaggle is the best place for you. Furthermore, the notebooks section of Kaggle allows users to share their codes and models, which serve as a great learning resource. I highly recommend beginners to find their first data science project in Kaggle"
3 Best Sites to Find Datasets for your Data Science Projects - KDnuggets,"Just out of beta early this year (2020), the Google Dataset Search is the most comprehensive Dataset search engine available. It claims to index more than 25 million datasets online and has helped scientists and researchers to better locate datasets since its inception in Sep 2018. Armed with the function to filter according to data types, date updated, and more, the Google Dataset Search has become the favorite for most of us"
3 Best Sites to Find Datasets for your Data Science Projects - KDnuggets,"When looking for data science datasets, you might want to look at what your government has made publicly available. These data, when put into good use, might result in solutions that benefit your community as a whole. Data.S. Government, where the government’s data are released to promote research and development within the scientific communities. At Data"
3 Best Sites to Find Datasets for your Data Science Projects - KDnuggets,"What if you are not a resident in the U.S. More often than not, you will find sites where your local government publishes its data. For example,"
3 Best Sites to Find Datasets for your Data Science Projects - KDnuggets,"Using these sites, you will be able to find any datasets that interest you. Remember, practicing data science is the best way to learn. So keep these sites handy as you will definitely need it"
Why you should NOT use MS MARCO to evaluate semantic search - KDnuggets,"Looking at steps 3 and 4 (and maybe 5), it is not surprising to find bias in the dataset. And to be fair, I think the bias is recognized as an issue in the literature. The surprise was the degree of the bias that we observed and how this might affect experiments involving semantic search"
Why you should NOT use MS MARCO to evaluate semantic search - KDnuggets,"We started with a reasonable baseline involving only term-matching signals. Next, we got promising results when we used only semantic signals in the application, just to sanity check the setup and to confirm that there was indeed relevant information contained in the embeddings. After that, the obvious follow up was to combine both signals"
Why you should NOT use MS MARCO to evaluate semantic search - KDnuggets,"To better investigate what was going on, we collected query-document data from Vespa about both relevant and random documents. For example, the next graph shows the empirical distribution of the sum of dot-products between the query and title embeddings and between the query and body embeddings. The blue histogram shows the distribution for random (and therefore likely non-relevant to the queries) documents. The red histogram shows the same information but now conditioned on the fact that the documents are relevant to the queries"
Why you should NOT use MS MARCO to evaluate semantic search - KDnuggets,"As expected, we got much higher scores on average for relevant documents. Great. Now, let’s look at a similar graph for the BM25 scores. The results are similar but much more extreme in this case. Relevant documents have much higher BM25 scores, to the point where almost no relevant document has low enough signal to be excluded from being retrieved by term-matching signals. This means that, after accounting for term-matching, there are almost no relevant documents left to be matched by semantic signals. This is true even if the semantic embeddings are informative"
Why you should NOT use MS MARCO to evaluate semantic search - KDnuggets,"At this point, a reasonable observation would be that we are talking about pre-trained embeddings and that we could get better results if we fine-tuned the embeddings to the specific application at hand. This might very well be the case but there are at least two important considerations to be taken into account: cost and overfitting. The resource/cost consideration is important but more obvious to be recognized. You either have the money to pursue it or not. If you do, you still should check to see if the improvement you get is worth the cost"
Why you should NOT use MS MARCO to evaluate semantic search - KDnuggets,"The main issue, in this case, relates to overfitting. It is not easy to avoid overfitting when using big and complex models such as Universal Sentence Encoder and sentence BERT. Even if we use the entire MS MARCO dataset, which is considered a big and important recent developments to help advance the research around NLP tasks, we only have around 3 million documents and 300 thousand labeled queries to work with. This is not necessarily big relative to such massive models"
SIGKDD Community Impact Program (Deadline Jun. 15) - KDnuggets,"SIGKDD is announcing a funding opportunity through its Community Impact Program.The goal of the program is to support projects that promote data science and help the data science community to grow, broaden, and diversify. Read more here"
SIGKDD Community Impact Program (Deadline Jun. 15) - KDnuggets,"The goal of the program is to support projects that promote data science and help the data science community to grow, broaden, and diversify. Maximum project duration is one year. Funded projects will be required to present results of their work and outcomes at the annual KDD conference"
SIGKDD Community Impact Program (Deadline Jun. 15) - KDnuggets,It focuses on projects that benefit the community and society. Research projects of any kind will not be funded. See examples of previously funded projects below
SIGKDD Community Impact Program (Deadline Jun. 15) - KDnuggets,"The KDD Community Impact Program intends to fund upto 10 projects with a funding range between $10k-$50k. All project costs must be fully justified. In exceptional cases, more funding may be allocated based on justification. Funding will be given as unrestricted funds (which usually means that minimal institutional overheads will be applied)"
SIGKDD Community Impact Program (Deadline Jun. 15) - KDnuggets,"Project proposals are limited to at most 5 pages, single-spaced, using 12-point font and 1-inch margins, inclusive of illustrations and references. Pages beyond the page limit will not be read. Each proposal should include:"
Making sense of ensemble learning techniques - KDnuggets,"For many companies/data scientists that specialize or work with machine learning (ML), ensemble learning methods have become the weapons of choice. As ensemble learning methods combine multiple base models, together they have a greater ability to produce a much more accurate ML model. For example, at Bigabid we’ve been ensemble learning to successfully solve a variety of problems ranging from optimizing LTV ("
Making sense of ensemble learning techniques - KDnuggets,"Ensemble learning is an ML paradigm where numerous base models (which are often referred to as “weak learners”) are combined and trained to solve the same problem. This method is based on the theory that by correctly combining several base models together, these weak learners can be used as building blocks for designing more complex ML models. Together these ensemble models (aptly called “strong learners”) achieve better, more accurate results"
Making sense of ensemble learning techniques - KDnuggets,"In other words, a weak learner is merely a base model that alone performs rather poorly. In fact, its accuracy level is barely above chance, meaning that it predicts the outcome only slightly better than a random guess would. These weak learners will often be computationally simple as well. Typically, the reason base models don’t perform very well by themselves is because they either have a high bias or too much variance, which makes them weak"
Making sense of ensemble learning techniques - KDnuggets,This is where ensemble learning comes in. This method attempts to try to reduce the general error by combining several weak learners together. Think of ensemble models as the data science version of the expression “two heads are better than one
Making sense of ensemble learning techniques - KDnuggets,"It’s important to understand the concept of a weak learner and why it earned this name in the first place, as the reason comes down to either bias or variance. More specifically, the prediction error of an ML model, namely the difference between the trained model and the ground truth, can be broken down into the sum of the following: the bias and the variance. For instance:"
Making sense of ensemble learning techniques - KDnuggets,"If a model is too simplistic and doesn’t have many parameters, then it may have high bias and low variance. In contrast, if a model has many parameters, then it may have high variance and low bias. As such, it’s necessary to find the right balance without underfitting or overfitting the data, as this tradeoff in complexity is the reason why there exists a tradeoff between variance and bias. Simply put, an algorithm can’t simultaneously be more complex and less complex at the same time"
Making sense of ensemble learning techniques - KDnuggets,"Bagging attempts to incorporate similar learners on small-sample populations and calculates the average of all the predictions. Generally, bagging allows you to use different learners in different populations. By doing so, this method helps to reduce the variance error"
Making sense of ensemble learning techniques - KDnuggets,"Boosting is an iterative method that fine-tunes the weight of an observation according to the most recent classification. If an observation was incorrectly classified, this method will increase the weight of that observation in the next round (in which the next model will be trained) and will be less prone to misclassification. Similarly, if an observation was classified correctly, then it will reduce its weight for the next classifier. The weight represents how important the correct classification of the specific data point should be, as this enables the sequential classifiers to focus on examples previously misclassified. Generally, boosting reduces the bias error and forms strong predictive models, but at times they may overfit on the training data"
Making sense of ensemble learning techniques - KDnuggets,"Stacking is a clever way of combining the information provided by different models. With this method, a learner of any sort can be used to combine different learners’ outputs. The result can be a decrease in bias or variance determined by which combined models are used"
"Diffusion Map for Manifold Learning, Theory and Implementation - KDnuggets","There is a definite shape to this dataset. Now, if we have to measure the similarity of two points in this set, we measure the Euclidean Distance. If this distance is small, then we say points are similar or if this is large, otherwise. The following figure represents this scenario"
"Diffusion Map for Manifold Learning, Theory and Implementation - KDnuggets","The core idea is a time-dependent diffusion process, which is nothing but a random walk on the dataset where each hop has a probability associated with it. When the diffusion process runs for a time t, we get different probabilities of various paths it can take to calculate the distance over the underlying geometric structure. Mathematically, we call this the steady-state probability of the Markov Chain"
"Diffusion Map for Manifold Learning, Theory and Implementation - KDnuggets","Now we define a row-normalized diffusion matrix, P. Mathematically, this is equivalent to the transition matrix in the Markov chain. While P denotes the probability (or connectivity in this case) of single hopping from point x to point y, P² denotes the probability of reaching y from x in two hops and so on. As we increase the number of hops or Pᵗ for increasing values of t we observe that the diffusion process runs forward. Or in other words, the probability of following the geometric structure increases"
"Diffusion Map for Manifold Learning, Theory and Implementation - KDnuggets","To demonstrate the algorithm, we begin with a dataset having a definite geometric structure. Let us begin by creating a 2D figure, as shown earlier. Our main aim is to find out whether the diffusion map unravels the underlying geometric structure of data or not"
"Diffusion Map for Manifold Learning, Theory and Implementation - KDnuggets","As you might have noticed, the 3D dataset preserves the ‘S’ shape in one angle, and that is correct. Now our goal is to flatten the dimensions to 2 while preserving this shape. After applying the Diffusion Map, hopefully, we should see the ‘S’ like structure in 2D"
Free High-Quality Machine Learning & Data Science Books & Courses: Quarantine Edition - KDnuggets,"Do you have the luxury of being stuck at home right now? Due to COVID-19, many of us are relegated to being locked down, quarantined, sheltered in place, or the like for the time being. If you find yourself in this situation and are looking for free learning materials in the way of books and courses in order to take advantage and sharpen your data science and machine learning skills, this collection of articles I have previously written curating such things is for you. Altogether, you will find links to smaller collections of just such materials, totalling more than 100 high quality books and courses"
Free High-Quality Machine Learning & Data Science Books & Courses: Quarantine Edition - KDnuggets,"Let's also take the opportunity to say that we salute all of the essential workers who do not have this luxury at the moment. KDnuggets wants to say thank you to all people working in the trenches during this pandemic: doctors, nurses, medical researchers, mail carriers, police, firefighters, first responders, pharmacy workers, supermarket clerks, food service providers, truck drivers, everyone in the supply chain, and all others who provide essential services in order to protect the vulnerable, the frontline healthcare workers, and to allow the rest of us to stay and work from home. Thank you!"
Free High-Quality Machine Learning & Data Science Books & Courses: Quarantine Edition - KDnuggets,"Here is a quick collection of such books to start your fair weather study off on the right foot. The list begins with a base of statistics, moves on to machine learning foundations, progresses to a few bigger picture titles, has a quick look at an advanced topic or 2, and ends off with something that brings it all together. A mix of classic and contemporary titles, hopefully you find something new (to you) and of interest here"
Free High-Quality Machine Learning & Data Science Books & Courses: Quarantine Edition - KDnuggets,"Summer, summer, summertime. Time to sit back and unwind. Or get your hands on some free machine learning and data science books and get your learn on. Check out this selection of free quality, curated books to get you started"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"First, ""data scientist"" is redundant. It's like calling a librarian a ""book librarian. Duh!"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"Furthermore, don't tell anyone I said this, but real sciences like physics and chemistry don't have ""science"" in their name. Your science is trying too hard if it has to call itself a science: Social science, political science, data science, and I gotta say -- even though I have three degrees in it and was a professor of it -- computer science is an arbitrarily defined field. It's just the amalgam of everything to do with computers -- as a concept and as an appliance -- from the engineering of how to build them and the deep mathematics about their theoretical limitations to how to make them more user friendly, and even business strategies for managing a team of programmers"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"But I digress. Ok, next buzzword: “Big data. It’s like looking at the Pacific Ocean and saying “big water"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"But the real problem with ""big data"" is that it emphasizes the size. There’s always so much more data today than there was yesterday. So we're gonna run out of adjectives really quickly: “big data,” “bigger data,” “even bigger data,” “the biggest data. I’m not joking. That's before the first Star Wars movie came out!"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"Now, in some cases, people use the terms data science and big data just to refer to machine learning, i. That's the topic of most episodes of this program, The Dr. Data Show. It’s a show about machine learning -- which is a well-defined field and by the way is also often called predictive analytics, especially when you're talking about its deployment in the private or public sector. I would urge folks to use the well-defined terms machine learning or predictive analytics if in fact that's what you’re specifically talking about"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"But as for data science and big data, in their general usage they suffer from a terrible case of vagueness. The have a wide range of subjective definitions, which compete and conflict. Basically, they're often used to mean nothing more specific than ""some clever use of data. They're just plain subjective -- you can use them to mean whichever technology you'd like: machine learning, data visualization, or even just basic reporting"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"But much worse than that, this vagueness often serves to mislead and misrepresent by alluding to capabilities that don't exist. For example, the popular press -- as well certain analytics vendors -- sometimes use ""data science"" to denote some whole collection of methods that includes machine learning as well as some other advanced methods. The problem is, those other advanced methods are implied but often actually just don't really exist. They're vaporware. This confusion is sometimes inadvertent -- such as when journalists aren’t fully knowledgeable of the topic yet want it to sound as powerful as possible -- but, either way, the end result is souped-up hype that overpromises and circulates misinformation"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"So now you're probably asking yourself, ""How could Dr. Data come down so hard on these words if he loves data so much?"" Well, no, Dr. Data doesn't hate these words, only the misleading ways in which they're often used. Dr. Data's love for data is fully intact. After all, he named himself after it. Anyway, let's talk data for a moment"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"These buzzwords are all ""data this"" and ""data that"" -- so what exactly is all the fuss about data? I mean, most people couldn't be less interested in data. The non-geeks out there think it's the driest, most boring word ever. The word ""data"" is a deal-killer at cocktail parties. I know from personal experience. I have the data"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"But, no! That’s wrong! Let me make a correction. It isn't indiscriminate. The stuff logged into all these memory banks are exactly the things that matter. That's why they're being recorded. People think data’s boring because they’re overlooking the fact that data is experience -- it’s a long list of prior events from which it’s possible to analytically learn"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"In fact, we could say that data is powerful and all-encompassing for the very same reason that it's misconstrued as boring, which is that it's very abstract. Data can mean anything and everything. In its most abstract, it means nothing in particular, but in the particular, it always means something valuable and interesting"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"There are ""quintillions and quintillions"" of bytes. That's my Carl Sagan impersonation. Data grows by an estimated 2.5 quintillion bytes per day. A quintillion is a 1 with 18 zeros after it"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"And here's the big win: We can improve everything with this data. All the main functions and day-to-day operational decisions of companies and governments are exactly what these data streams are recording. Therefore, data records exactly the right, relevant experience to apply predictive analytics where it’s needed most. We have just the right data for this technology to learn how to streamline the major operations behind financial risk management, fraud detection, marketing, law enforcement, healthcare, and manufacturing. Boom!"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"And so data isn't the most boring after all. In fact, it's the most. It's hip to be square. You know, I had always assumed the sexiest profession was firefighters. But who knows. Maybe it's just the hard hat. This is a picture of me dressed up as a data miner for Halloween"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"Anyway, we actually produced a rap music video about predictive analytics and how being a data geek affects your social life. It's the the best ever educational predictive analytics rap music video ever created ever, period. And also the only one. Just three and a half minutes long, you can check it out at PredictThis"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"In conclusion, there's a lot to be excited about when it comes to the data explosion and what we can do with it. The buzzwords are kinda inane when viewed up close -- perhaps an equally appropriate and less misleading buzzword for all this would be ""datapalooza"" -- but, in any case, the terms really allude to a culture of smart people doing creative things to make value of all this data. Today’s totally historical advent of having data about everything and using data for everything is mind-blowingly profound and important"
Dr. Data Show Video:  What the Hell Does “Data Science” Really Mean? - KDnuggets,"I’m Eric Siegel; thanks for watching. Hit “like” and share this video if you think your friends were also wondering what the hell ""data science"" and ""big data"" really mean. And for access to the entire web series, go to TheDoctorDataShow"
Scaling Your Data Strategy - KDnuggets,"Companies growing at a fast pace enjoy two unique advantages simultaneously. They are able to utilize small but mighty teams with a can-do attitude and deliver product features quickly to the market. At the same time, in order to maintain rapid expansion, they have a tangible incentive to pay just as much attention to setting up the fundamental practices for sustainable growth. This means managing digital transformation to take advantage of the benefits of new technology, and managing increasing simultaneous and interconnected projects. Not to mention that at the intersection of all these projects and processes, there is often a lot of data zipping around! As data grows exponentially, this unique position can be leveraged to craft a robust, long term data strategy"
Scaling Your Data Strategy - KDnuggets,"At SSENSE, we attempt to address large-scale problems with data-driven solutions. In this context, business relationships can quickly become complex, and identifying patterns and behaviors around your data can become incrementally challenging. If you are regularly starting new projects that are deeply intertwined with existing features and processes at your organization, you are no longer able to build isolated, one-off built-from-scratch solutions. Your operation has grown to the point that it pays off to have a corporate data strategy. In this article, I will present my vision for a cohesive data strategy based on my prior professional experiences. While the purpose of this article is not to outline our exact data strategy at SSENSE, a lot of the main principles draw inspiration from it"
Scaling Your Data Strategy - KDnuggets,"No matter where you are in your data-driven journey, having a data strategy helps unlock the power of data and allows your organization to treat data as a critical asset. A data strategy is a plan designed to improve the methods, practices, and processes of data used across the organization, and to ensure that data is being used in a sustainable and reproducible way. Because data is generated and used by diverse business units with different practices and responsibilities, having a committee to oversee a data strategy across the organization becomes central to business success. Not having a data strategy is okay depending on where you are in your journey, but it becomes more likely that different departments will solve data issues on their own, resulting in wasted resources, units operating in silos, and a growing lack of cohesiveness in the organization. I managed to encapsulate that grim possibility in one sentence, so let’s focus on positive and constructive ideas henceforth"
Scaling Your Data Strategy - KDnuggets,"Newer methods of combining data under a single interface such as a data lake, help to avoid a lot of the problems and complexity of managing and using data in organizations. Even so, consolidating the terminology, metadata, and relationships under one catalog makes accessing and using data much simpler. For each field of data, such a catalog might include definitions, sources, locations, domains, use-cases, stakeholders, etc"
Scaling Your Data Strategy - KDnuggets,"A strong governance model outlines security details, access rights, high-level transformation logic, naming conventions, and rules for how the data should be used. In a data-driven organization, a strong governance model should not be thought of as an overwhelming barrier to entry for users, or as a means to limit access to data. Rather, it should empower all members of the organization to use data responsibly and effectively"
Scaling Your Data Strategy - KDnuggets,"Companies are constantly in a state of digital transformation. We should consider this the norm as we seek to take advantage of the benefits of new technology, and we cannot ignore that we are generating more and more data in different systems across the organization. Data storage is one of the basic capabilities of technology stacks. Identifying the right data store for an application or business process is important to ensure that the storage technology is used correctly for its purpose"
Scaling Your Data Strategy - KDnuggets,"For data and business analysts, raw data generated from applications is a gold mine of knowledge. If the business impact cannot be identified today, it likely makes sense to store and transform the data so that it can be used later on. Processing data is a vital component of a data strategy as it turns raw data into a finished good. As such, it turns data generated from business applications into an asset that can be used for data-driven decision making"
Scaling Your Data Strategy - KDnuggets,"If you didn’t have a data committee before, these concerns were likely being handled by smaller units of software or data leaders. This may work for smaller and simpler operations but it does not scale well. Depending on where you are in your growth, your committee can be much smaller or much bigger. It should, however, represent or cover the concerns of the above teams and their responsibilities"
Scaling Your Data Strategy - KDnuggets,"Like implementing new business strategies or technical applications, implementing a data strategy is an evolving process. Understanding the need for a data strategy and a committee to oversee its implementation is a good start. Members of the organization start to think about data as a strategic asset that enables better understanding of their business, and how they can use it to drive value"
Scaling Your Data Strategy - KDnuggets,"The process of identifying all data entities, data sources, definitions, and relationships should be the first point of entry. The next step is defining the governance standards to use data, and a system to link all data sources together. Each process in the data strategy should have key stakeholders to drive that part of the process. For example, a member from the data reporting and operations team is most likely suited to drive the data governance standards. Taking advantage of the expertise of your data committee ensures that you are able to deliver and implement a data strategy quickly and effectively"
Hilary Mason and Gilad Lotan to Keynote at MADS 2019 - KDnuggets,"The 2019 Marketing Analytics and Data Science conference, Apr 8-10 in San Francisco, will empower you with practical applications to achieve better bottom line results in a data-driven future. Save 20% with VIP Code MADS19KDN. Register now and save!"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"Data science is an inter-disciplinary field which contains methods and techniques from fields like statistics, machine learning, Bayesian etc. They all aim to generate specific insights from the data. In this article, we are listing down some excellent data science books which cover the wide variety of topics under Data Science"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"This book gives an overview of Data Science. Data Science is a very large umbrella term and this book is good for anyone trying to get their feet wet in the field for the first time. Read it to understand what Data Science is, what are some general tasks and algorithms, and some general tips and tricks"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"Based on Stanford courses CS246 and CS35A, the book helps users learn topics to do Data Mining on large datasets. Often a very common problem a data scientist has to solve is to perform simple numerical tasks (which you can do by writing small pieces of programs) on a very large dataset. MMDS works exactly towards that. Added to that, you have topics like Dimensionality Reduction and Recommendation Systems which help you learn about the application of linear algebra and metric distances in the real world. An absolute must-read for all Data Scientists"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,Bayesian Statistics works somewhat differently from normal statistics. The concepts of uncertainty and fitting distributions to real-world datasets make Bayesian methods more fitting to learn about real-world datasets. Prof. Downey’s extremely cool “learn by programming it in Python” style makes the book a treat for those getting started with Bayesian Methods
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"This book teaches applied Linear Algebra in real-world systems. The applications involve circuits, signal processing, communications, and control systems. Link to previous years’ course notes by professor Boyd can be found"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"Metaheuristics are quick learning probabilistic ways to do tasks that would require you to otherwise write programs to search using Brute Force. For maybe small data, Brute Force approaches take lesser effort to implement, but they exhaust very fast with the amount of data added. This book is probably the best introduction to metaheuristic methods like Genetic Algorithms, Hill Climbing, Co-Evolution, and (basic) Reinforcement Learning"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"A good overview of Python tools in data Science. A very good document for a senior Python developer wanting to get into Data Science or someone moving into Python from R for Data Science. Overall if you want to understand what Python can do for Data Science, you should read this article"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,Optimal Transport is the mathematics of assignment from one set of distributions into another. This is probably one of the few fields in Data Science that has won more than one Fields Medals (the highest honor in Mathematics).  The mathematical concepts are used in many Machine Learning and Deep Learning algorithms as distance metrics and for assignment problem-solving
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"Data Mining, as you might have seen in the more famous MMDS book mentioned earlier, is a method to do computations effectively on a large dataset. These computations can be done by brute force methods, and might work well on small datasets, but might take really really long to run on large datasets. A good introductory and reference book for Data Mining"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"Information theory is one of the four mathematical theories you will find in Data Science along with Linear Algebra, Convex Optimization, and Statistics. This is a good tutorial to understand the theory. The good thing is that the tutorial is accessible to beginners"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"I found it really good, it’s like showing you multiple solved problems to make you learn. Not as much rigor as earlier books and more learning-by-showing. Good refresher for people who have not touched Linear Algebra for a long time"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,Another free book for college-level Linear Algebra. Good for beginners. It also comes with homework problems if you want to practice
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,This huge tutorial is by the Pandas development team to learn and understand the library. Pandas is a must-learn library if you are working in Data Science. There is no escape
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"Kalman Filters and other Bayesian Filters are useful when working with noisy data coming with time which can be fitted to a certain model with parameters to be deduced. The twofold thing these models do is deduce the parameters as well as model the noise. Though most commonly used examples are location data, similar filters can work things well in forecasting too"
50 Must-Read Free Books For Every Data Scientist in 2020 - KDnuggets,"A general introduction to different concepts of Data Science. This includes causal models, regression models, factor models and so on. The sample programs are in R"
"Covid-19, your community, and you — a data science perspective - KDnuggets","We are data scientists—that is, our job is to understand how to analyze and interpret data. When we analyze the data around covid-19, we are very concerned. The most vulnerable parts of society, the elderly and the poor, are most at risk, but controlling the spread and impact of the disease requires us"
"Covid-19, your community, and you — a data science perspective - KDnuggets","Wash your hands thoroughly and regularly, avoid groups and crowds, cancel events, and don’t touch your face. In this post, we explain why we are concerned, and you should be too. For an excellent summary of the key information you need to know, read"
"Covid-19, your community, and you — a data science perspective - KDnuggets","Anyone is welcome to translate this article, to help their local communities understand these issues. Please link back to here with appropriate credit. Let us know on"
"Covid-19, your community, and you — a data science perspective - KDnuggets","Just over 2 years ago one of us (Rachel) got a brain infection which kills around 1/4 of people who get it, and leaves 1/3 with permanent cognitive impairment. Many others end up with permanent vision and hearing damage. Rachel was delirious by the time she crawled across the hospital parking lot. She was lucky enough to receive prompt care, diagnosis, and treatment. Up until shortly before this event Rachel was in great health. Having prompt access to the emergency room almost certainly saved her life"
"Covid-19, your community, and you — a data science perspective - KDnuggets","Now, let’s talk about covid-19, and what might happen to people in Rachel’s situation in the coming weeks and months. The number of people found to be infected with covid-19 doubles every 3 to 6 days. With a doubling rate of three days, that means the number of people found to be infected can increase 100 times in three weeks (it’s not actually quite this simple, but let’s not get distracted by technical details). One in 10 infected people requires hospitalization for many weeks, and most of these require oxygen. Although it is very early days for this virus, there are already regions where hospitals are entirely overrun, and people are no longer able to get the treatment that they require (not only for covid-19, but also for anything else, such as the life-saving care that Rachel needed). For instance, in Italy, where just a week ago officials were saying that everything was fine, now sixteen million people have been put on lock-down ("
"Covid-19, your community, and you — a data science perspective - KDnuggets","The flu has a death rate of around 0.1% of infections. Marc Lipsitch, the director of the Center for Communicable Disease Dynamics at Harvard,"
"Covid-19, your community, and you — a data science perspective - KDnuggets","For each person that has the flu, on average, they infect 1.3 other people. That’s called the “R0” for flu. If R0 is less than 1.0, then an infection stops spreading and dies out. If it’s over 1.0, it spreads. R0 currently is 2-3 for covid-19 outside China. The difference may sound small, but after 20 “generations” of infected people passing on their infection, an R0 of 1.3 would result in 146 infections, but an R0 of 2.5 would result in 36 million infections! (This is, of course, very hand-wavy and ignores many real-world impacts, but it’s a reasonable illustration of the"
"Covid-19, your community, and you — a data science perspective - KDnuggets","One common response we’ve seen on social media to people that are pointing out the reasons to be concerned, is “don’t panic” or “keep calm”. This is, to say the least, not helpful. No-one is suggesting that panic is an appropriate response. For some reason, however, “keep calm” is a very popular reaction in certain circles (but not amongst any epidemiologists, whose job it is to track these things). Perhaps “keep calm” helps some people feel better about their own inaction, or makes them feel somehow superior to people who they imagine are running around like a headless chicken"
"Covid-19, your community, and you — a data science perspective - KDnuggets","But “keep calm” can easily lead to a failure to prepare and respond. In China, tens of millions were put on lock-down and two new hospitals were built by the time they reached the statistics that the US has now. Italy waited too long, and just today (Sunday March 8) they reported 1492 new cases and 133 new deaths, despite locking down 16 million people. Based on the best information we’re able to ascertain at this stage, just 2-3 weeks ago Italy was in the same position that the US and UK are in today (in terms of infection statistics)"
"Covid-19, your community, and you — a data science perspective - KDnuggets","Note that nearly everything about covid-19 at this stage is up in the air. We don’t really know it’s infection speed or mortality, we don’t know how long it remains active on surfaces, we don’t know whether it survives and spreads in warm conditions. Everything we have is current best guesses based on the best information people are able to put together. And remember, the vast majority of this information is in China, in Chinese. Currently, the best way to understand the Chinese experience so far is to read the excellent"
"Covid-19, your community, and you — a data science perspective - KDnuggets","That would be enormously speculative and not an optimal response under any threat modeling scenario. It also seems extremely unlikely that countries like Italy and China would effectively shut down large parts of their economy for no good reason. It’s also not consistent with the actual impacts we’re seeing on the ground in infected areas, where the medical system is unable to cope (for instance, Italy is using 462 tents for “pre-triage”, and still has to"
"Covid-19, your community, and you — a data science perspective - KDnuggets","If you are under 50, and do not have risk factors such as a compromised immune system, cardiovascular disease, a history of previous smoking, or other chronic illnesses, then you can have some comfort that covid-19 is unlikely to kill you. But how you respond still matters very much. You still have just as much chance of getting infected, and if you do, just as much chance of infecting others. On average, each infected person is infecting over two more people, and they become infectious before they show symptoms. If you have parents that you care about, or grandparents, and plan to spend time with them, and later discover that you are responsible for infecting them with covid-19, that would be a heavy burden to live with"
"Covid-19, your community, and you — a data science perspective - KDnuggets","And of course, it is not just about the people immediately around you. This is a highly significant ethical issue. Each person who does their best to contribute to controlling the spread of the virus is helping their whole community to slow down the rate of infection. As Zeynep Tufekci"
"Covid-19, your community, and you — a data science perspective - KDnuggets","We should prepare, not because we may feel personally at risk, but so that we can help lessen the risk for everyone. We should prepare not because we are facing a doomsday scenario out of our control, but because we can alter every aspect of this risk we face as a society. That’s right, you should prepare because your neighbors need you to prepare—especially your elderly neighbors, your neighbors who work at hospitals, your neighbors with chronic illnesses, and your neighbors who may not have the means or the time to prepare because of lack of resources or time"
"Covid-19, your community, and you — a data science perspective - KDnuggets","This has impacted us personally. The biggest and most important course we’ve ever created at fast. Last Wednesday (March 4), we made the decision to move the"
"Covid-19, your community, and you — a data science perspective - KDnuggets","We were one of the first large courses to move online. Why did we do it? Because we realized early last week that if we ran this course, we were implicitly encouraging hundreds of people to get together in an enclosed space, multiple times over a multi-week period. Bringing groups together in enclosed spaces is the single worst thing that can be done. We felt ethically obliged to ensure that, at least in this case, this didn’t happen. It was a heart-breaking decision. Our time spent working directly with our students has been one of the great pleasures and most productive periods every year. And we had students planning to fly in from all over the world, who we really didn’t want to let down"
"Covid-19, your community, and you — a data science perspective - KDnuggets","That means that in the next two weeks the number of diagnosed cases will explode… Trying to do containment when there is exponential community spread is like focusing on putting out sparks when the house is on fire. When that happens, we need to switch strategies to mitigation–taking protective measures to slow spread & reduce peak impact on healthcare. But if the cases come too quickly, then those that need hospitalization won’t get it"
"Covid-19, your community, and you — a data science perspective - KDnuggets","The US has about 2.8 hospital beds per 1000 people. With a population of 330M, this is ~1M beds. At any given time, 65% of those beds are already occupied. That leaves about 330k beds available nationwide (perhaps a bit fewer this time of year with regular flu season, etc). Let’s trust Italy’s numbers and assume that about 10% of cases are serious enough to require hospitalization"
"Covid-19, your community, and you — a data science perspective - KDnuggets","By this estimate, by about May 8th, all open hospital beds in the US will be filled. If 20% of cases require hospitalization, we run out of beds by ~May 2nd If only 5% of cases require it, we can make it until ~May 14th. 2.5% gets us to May 20th. This, of course, assumes that there is no uptick in demand for beds from"
"Covid-19, your community, and you — a data science perspective - KDnuggets","This is not just a hypothetical situation — it was clearly displayed in the 1918 flu pandemic. In the United States two cities displayed very different reactions to the pandemic: Philadelphia went ahead with a giant parade of 200,000 people to help raise money for the war. But St Louis put in place carefully designed processes to minimize social contacts so as to decrease the spread of the virus, along with cancelling all large events. Here is what the number of deaths looked like in each city, as shown in the"
"Covid-19, your community, and you — a data science perspective - KDnuggets","The elderly and disabled are at particular risk when their daily lives and support systems are disrupted. Those without easy access to health care, including rural and Native communities, might face daunting distances at times of need. People living in close quarters — whether in public housing, nursing homes, jails, shelters or even the homeless on the streets — might suffer in waves, as we have already seen in Washington state. And the vulnerabilities of the low-wage gig economy, with non-salaried workers and precarious work schedules, will be exposed for all to see during this crisis. Ask the 60 percent of the U.S"
"Covid-19, your community, and you — a data science perspective - KDnuggets","One of the big issues in the US is that very little testing is being done, and testing results aren’t being properly shared, which means we don’t know what’s actually happening. Scott Gottlieb, the previous FDA commissioner, explained that in Seattle there has been better testing, and we are seeing infection there: “The reason why we knew early about Seattle outbreak of covid-19 was because of sentinel surveillance work by independent scientists. Such surveillance never got totally underway in other cities. So other U.S"
"Covid-19, your community, and you — a data science perspective - KDnuggets","The figures we gathered suggest that the American response to the covid-19 and the disease it causes, COVID-19, has been shockingly sluggish, especially compared with that of other developed countries. The CDC confirmed eight days ago that the virus was in community transmission in the United States—that it was infecting Americans who had neither traveled abroad nor were in contact with others who had. In South Korea, more than 66,650 people were tested within a week of its first case of community transmission, and it quickly became able to test 10,000 people a day"
"Covid-19, your community, and you — a data science perspective - KDnuggets","Part of the problem is that this has become a political issue. In particular, President Donald Trump has made it clear that he wants to see “the numbers” (that as, the number of people infected in the US) kept low. This is an example of where optimizing metrics interferes with getting good results in practice"
"Covid-19, your community, and you — a data science perspective - KDnuggets","When I worked at WHO, I was part of the Global Programme on AIDS (now UNAIDS), created to help the world tackle the HIV/AIDS pandemic. The staff there were dedicated doctors and scientists intensely focused on helping address that crisis. In times of crisis, clear and accurate information is vital to helping everyone make proper and informed decisions about how to respond (country, state, and local governments, companies, NGOs, schools, families, and individuals). With the right information and policies in place for listening to the best medical and scientific experts, we will all come through challenges like the ones presented by HIV/AIDS or by COVID-19. With disinformation driven by political interests, there’s a real risk of making things way, way worse by not acting quickly and decisively in the face of a growing pandemic, and by actively encouraging behaviors that will actually spread the disease more quickly. This whole situation is incredibly painful to watch unfold"
"Covid-19, your community, and you — a data science perspective - KDnuggets","Then Trump cut Azar off. They’re there, they have the tests, and the tests are beautiful. Anybody that needs a test gets a test,” Trump said. This is untrue. Vice President Pence told reporters Thursday that the US didn’t have enough test kits to meet demand"
"Covid-19, your community, and you — a data science perspective - KDnuggets","Other countries are reacting much more quickly and significantly than the US. Many countries in SE Asia are showing great results, including Taiwan, where R0 is down to 0.3 now, and Singapore, which is being proposed as"
How do we Better Solve Analytics Problems? - KDnuggets,A lot can be achieved by starting with one or more frameworks and then customizing them. There is a high chance that your clients are aware of these and hence consuming the analytical piece is easier. Story-telling also becomes easier
How do we Better Solve Analytics Problems? - KDnuggets,"We find from the above analysis, that the ROAS of overall spends has reduced over the years. On further analysis we find that offline channel has decreasing contribution. Basis these customer behavior patterns, your organization decides to"
How do we Better Solve Analytics Problems? - KDnuggets,A few weeks later we decide to optimize spends further. Let's say that we find that different locations have differing response to offline promotions. Let's see how we fine-tune every marketing dollar to the maximum using quadrant analysis:
How do we Better Solve Analytics Problems? - KDnuggets,"One or more business frameworks can be used in conjunction and further tuned as per business drivers for effective output. From easy to the most complicated, business frameworks can be leveraged in analytics and in other walks of life. Some of my earliest interviews were cracked through a BCG as part of a self-analytics, with my 'Personal life' as a star, 'academics' as my cash cow, My 'clean' cupboard (sarcastically of-course!) as my dog, and professional career as my question mark"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","AI ethics is a very broad field, and encompasses a wide variety of seemingly very different aspects of ethical concern. Concerns over the use of AI, and of all types of technology, more generally, have existed for as long as these technologies were first conceived of. Yet given the recent explosion of AI, machine learning, and related technologies, and their increasingly quick adoption and integration into society at large, these ethical concerns have risen to the forefront of many minds both within and outside of the AI community"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","The dawn of autonomous vehicles has presented additional specific challenges related to AI ethics, as have the potential weaponization of AI systems, and a growing international AI arms race. Contrary to what some would have us believe, these aren't problems predestined for a dystopian future, yet they are problems which will require some critical thought, proper preparation, and extensive cooperation. Even with what we may believe to be adequate consideration, AI systems might still prove to be uniquely and endemically problematic, and the unintended consequences of AI systems, another aspect of AI ethics, will need to be considered"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","As AI and Machine Learning become a larger part of our lives, with smartphones, medical diagnostics, self-driving cars, intelligent search, automated credit decisions, etc. Humans can usually explain their knowledge-based decisions  (whether such explanations are accurate is a separate question) and that contributes to trust by other humans for such decisions.  Can AI and ML algorithms explain their decisions?  This is important for"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","Explainability is a multifaceted topic. It encompasses both individual models and the larger systems that incorporate them. It refers not only to whether the decisions a model outputs are interpretable, but also whether or not the whole process and intention surrounding the model can be properly accounted for. The goal is to have an efficient trade-off between accuracy and explainability along with a great human-computer interface which can help translate the model to understandable representation for the end users"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets",Geospatial is a term for any data that has a spatial/location/geographical component to it. Geospatial analysis has been gaining in popularity due to the onset of technology that tracks user movements and creates geospatial data as a by-product. The most famous technologies (Geographic Information Systems – GIS) used for spatial analysis are
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","Geospatial data can be used in applications from sales prediction modelling to assessing government funding initiative. Because the data is in reference to a specific location, there are many insights we can gather. Different countries record and measure their spatial data differently and to varying degrees. The geographic boundaries of a country are different and must be treated as unique to each country"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","GPT-2 is a generative language model, meaning that it generates text by predicting word by word which word comes next in a sequence, based on what the model has previously learned. In practice, a user-supplied prompt is presented to the model, following which subsequent words are generated.  GPT-2 was trained to predict the next word on a massive amount (40 GB) of internet text, and is built solely using transformer decoder blocks (contrast this with BERT, which uses encoder blocks). For more information on transformers see below"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","GPT-2 isn't a particularly novel undertaking; what sets it apart from similar models, however, is the number of its trainable parameters, and the storage size required for these trained parameters. While OpenAI initially released a scaled down version of the trained model — out of concerns that there could be malicious uses for it — the full model contains 1.5 billion parameters. This 1.5 billion trainable parameter model requires 6.5 GB of trained parameter (synonymous to ""trained model"") storage"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","Significant progress has been made in natural language understanding – getting a computer to interpret human input and provide a meaningful response. Many people enjoy this technology every day through personal devices, such as Amazon Alexa and Google Home. Not unexpectedly, kids really like asking for jokes"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","While business analysis and entertainment applications of computer-generated language might be appealing and culture-altering, ethical concerns are already boiling over. NLG's capability to deliver “fake news” that is autonomously generated and dispersed is causing distress, even if its intentions were not programmed to be evil. For example, OpenAI has been"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 2) - KDnuggets","The essential idea is a training algorithm that provides a reward feedback to a trial-and-error decision-making “agent” that attempts to perform some computational task. In other words, if you toss a stick across the yard for Rover to fetch, and your new puppy decides to return it to you for a treat, then it will repeat the same decision faster and more efficiently next time. The exciting feature of this approach is that labeled data is not necessary – the model can explore known and unknown data with guidance toward an optimal solution through an encoded reward"
Data Science Curriculum for self-study - KDnuggets,Most machine learning models are built with a dataset having several features or predictors. Hence familiarity with multivariable calculus is extremely important for building a machine learning model. Here are the topics you need to be familiar with:
Data Science Curriculum for self-study - KDnuggets,"Linear algebra is the most important math skill in machine learning. A dataset is represented as a matrix. Linear algebra is used in data preprocessing, data transformation, and model evaluation. Here are the topics you need to be familiar with:"
Data Science Curriculum for self-study - KDnuggets,"Python and R are considered the top programming languages for data science. You may decide to focus on just one language. Python is widely adopted by industries and academic training programs. As a beginner, it is recommended that you focus on one language only"
Data Science Curriculum for self-study - KDnuggets,"Learn how to manipulate data in various formats, for example, CSV file, pdf file, text file, etc. Learn how to clean data, impute data, scale data, import and export data, and scrap data from the internet. Some packages of interest are pandas, NumPy, pdf tools, stringr, etc. Additionally, R and Python contain several inbuilt datasets that can be used for practice. Learn data transformation and dimensionality reduction techniques such as covariance matrix plot, principal component analysis (PCA), and linear discriminant analysis (LDA)"
Data Science Curriculum for self-study - KDnuggets,Learn the fundamentals of simple and multiple linear regression analysis. Linear regression is used for supervised learning with continuous outcomes. Some tools for performing linear regression are given below:
Data Science Curriculum for self-study - KDnuggets,"Knowledge on how to use basic productivity tools such as R studio, Jupyter notebook, and GitHub, is essential. For Python, Anaconda Python is the best productivity tool to install. Advanced productivity tools such as AWS and Azure are also important tools to learn"
Data Science Curriculum for self-study - KDnuggets,"Learn basics on how to plan a project. Before building any machine learning model, it is important to sit down carefully and plan what you want your model to accomplish. Before delving into writing code, it is important that you understand the problem to be solved, the nature of the dataset, the type of model to build, how the model will be trained, tested, and evaluated. Project planning and project organization are essential for increasing productivity when working on a data science project. Some resources for project planning and organization are provided below"
Dockerize Jupyter with the Visual Debugger - KDnuggets,"Problems arise when the supporting software environment is not identical, says Docker creator Solomon Hykes.7, and then it’s going to run on Python 3 in production and something weird will happen. Or you’ll rely on the behavior of a certain version of an SSL library and another one will be installed. You’ll run your tests on Debian and production is on Red Hat and all sorts of weird things happen"
Dockerize Jupyter with the Visual Debugger - KDnuggets,"I assume that you are familiar with basic Docker commands and terminologies. Explaining how docker works is out of the scope of this article. However, if you feel you need to revisit then please refer to the Docker"
Probability Distributions in Data Science - KDnuggets,"If we are able to understand if it’s present any pattern in the data distribution, we can then tailor-made our Machine Learning models to best fit our case study. In this way, we will be able to get a better result in less time (reducing the optimization steps). In fact, some Machine Learning models are designed to work best under some distribution assumptions. Therefore, knowing with which distributions we are working with, can help us to identify which models are best to use"
Probability Distributions in Data Science - KDnuggets,The Binomial Distribution can instead be thought as the sum of outcomes of an event following a Bernoulli distribution. The Binomial Distribution is therefore used in binary outcome events and the probability of success and failure is the same in all the successive trials. This distribution takes two parameters as inputs: the number of times an event takes place and the probability assigned to one of the two classes
Probability Distributions in Data Science - KDnuggets,"When using Normal Distributions, the distribution mean and standard deviation plays a really important role. If we know their values, we can then easily find out the probability of predicting exact values by just examining the probability distribution (Figure 8). In fact, thanks to the distribution properties, 68% of the data lies within one standard deviation of the mean, 95% within two standard deviations of the mean and 99.7% within three standard deviations of the mean"
Learning from 3 big Data Science career mistakes - KDnuggets,"I wrote this blog post because I made a few mistakes while starting as a data scientist. I think a lot of people are making the same transition, and you may be one of them, or you know one (or a few) of them. I don't want you to make the same mistakes, hence this blog post. Even if you are not an industrial software developer like me, more than half of this stuff is still going to be very useful to you"
Learning from 3 big Data Science career mistakes - KDnuggets,"What days those were =:-). It was an amazing journey of being a programmer. I wrote code every day. I wrote code with every breath I took. Nothing could replace the joy of programming. When I was searching for the history of data science, I came across this very well written account by Gil Press. It reminded me of old times. I think it is true that you can't get passionate about a profession just with logic and highly trained skills, the heart has to be involved too. And in fact, the heart is the first requirement if you want to attain a high amount of skill"
Learning from 3 big Data Science career mistakes - KDnuggets,"That is a very, very important point neglected by many people starting in data science. When I shifted my career from a software developer to a data scientist, one thing I was struck by was the Mathematics involved, especially Statistics, Probability, Linear Algebra, and Calculus, almost in that order of importance. So, I spent a few months learning all four. It was good, except that it was not. While all of the mathematics I learned was quite interesting, the bigger question was:"
Learning from 3 big Data Science career mistakes - KDnuggets,"I learned all that, and I noticed it was not of much use when I dealt with real-life Big Data. When you are working on solving problems using data science in a business corporation, that much math, that many exercises from a book, theoretical problems, and deep study or research won't all become useful. You must not do it. I felt like I lived in a cave and learned all the Mathematics I needed and when I came out of the cave into the real-world of a software corporation, software as a business, all of my dreams were shattered. So I can say now with experience that Leo Breiman was correct. It was not the best use of time for an industrial software developer. It was not the best use of time for a software developer looking to shift his career within the software industry. I should have known better. I realized it rather late. I can't get those months of my life back. What I can do is to use this mistake to make better decisions this time"
Learning from 3 big Data Science career mistakes - KDnuggets,"Times change. In the last 30 years, an immense amount of software has been produced. Marc Andreessen even said that"
Learning from 3 big Data Science career mistakes - KDnuggets,"In the last few years, the focus has changed within the software industry, from creating a lot of software to using the software. With all the advancement in technology and hardware, the software is still being created but that is not where all the buzz is, now the buzz is around usability. It has shifted from C to Python. C model says hardware's time is more important than developer's because the hardware was very expensive back in those days. Python model says developer's time is more important than the hardware because the hardware is now cheap, and developers putting time where it is not needed counts as a loss for the business. With this came the shift from"
Learning from 3 big Data Science career mistakes - KDnuggets,"Compared to the last decade, the internet (or web) is now available in almost every part of the world. People are connecting to each other through social media. Social media was built on the web as its backbone and is now it has reached almost its full level of involvement in terms of users. We are connected like never before in the history of mankind and we communicate to each other with the most advanced tools available to every kind of user (in economic terms). This has changed the way software is developed and also how it's used. Usability of social media is at its peak, hence so is the size of data that is being generated. 90% of the data you see today, was generated"
Learning from 3 big Data Science career mistakes - KDnuggets,"First, buying a book on Statistics, Probability, or Linear Algebra that is being used in academia will be a complete waste of your time. The book itself may be very useful and has not much of a place in the software industry. When you are going to work in Big Data in the software industry, you need to know what tools this industry uses and where you can learn them. This is"
Learning from 3 big Data Science career mistakes - KDnuggets,"This book is where you need to spend a major amount of your time. This book feels bit mathematical of course, but you gotta get used to doing that. At least it contains less Mathematics than their"
Learning from 3 big Data Science career mistakes - KDnuggets,"ESL is more of a research-oriented book while ISL is more inclined towards real-life data analysis. Both books you can download for free from home pages I linked above. I advise you to buy the hard copies because it ain't fun reading an 800-page book on a computer. From my experience, one absorbs more content and remembers better when learning from a hard-copy"
Learning from 3 big Data Science career mistakes - KDnuggets,"Now, it is not to say that traditional Statistics is of not much use. Many traditional concepts are still used in Big Data, and they are fundamental to understanding anything related to working with data. So, you still need to spend minor time on traditional statistics and probability. Yes, it is minor time, but still, it is the time you need to invest. What I would do is free MOOCs on it from"
Learning from 3 big Data Science career mistakes - KDnuggets,"And this is the place I am standing right now. I have already done those MOOCs mentioned above, and I have read Innumeracy, have ordered and got ISL too. I will be working through ISL and will share my experience a couple of weeks later. I am also working on the obstacles I am facing while searching for, and getting, a data science job. I will write about this too once I am successful in breaking down all the obstacles"
Free Mathematics Courses for Data Science & Machine Learning - KDnuggets,"These algebra courses run the gamut from introductory algebra to linear models and matrix algebra. Algebra is helpful in computation and data science generally, and encompasses some of the main concepts in powering some machine learning algorithms, including neural networks. Descriptions come directly from the respective course websites"
Free Mathematics Courses for Data Science & Machine Learning - KDnuggets,"These calculus courses cover topics from preparatory precalculus through to differentiation, integration, to multivariate calculus and differential equations. Calculus has broad uses, generally, and contains core concepts which power neural networks work. Descriptions come directly from the respective course websites"
Free Mathematics Courses for Data Science & Machine Learning - KDnuggets,"Statistics and probability are the foundations of data science, more so than any other family of mathematical concepts. These courses will help prepare you to look at data through the statistical lens and with a critical probabilistic eye. Descriptions come directly from the respective course websites"
Free Mathematics Courses for Data Science & Machine Learning - KDnuggets,"These are mathematics topics directly related to data science and machine learning. They may include material from courses above, and may also be more elementary than some of above as well. However, they can be useful for brushing up on material you may not have studied in a while, and which is especially pertinent to the practice of data science. Descriptions come directly from the respective course websites"
Getting Started with R Programming - KDnuggets,"R is a programming language focused on statistical and graphical analysis. It is therefore commonly used in statistical inference, data analysis and Machine Learning. R is currently one of the most requested programming language in the Data Science job market (Figure 1)"
Getting Started with R Programming - KDnuggets,"Factors are a type of data object used in R to categorize and store data (eg. They can, for example, be used to one hot encode a feature or to create Bar Charts (as we will see later on). Therefore they are especially useful when working with columns with few unique values"
Getting Started with R Programming - KDnuggets,"As we can see from the resulting output (Figure 7), 50.5% of the considered mobile devices do not support Bluetooth, 50.9% is Dual Sim and 52.1% has 4G"
Getting Started with R Programming - KDnuggets,"We can now go on creating 3 different Box Plots using the same technique used before. In this case, I decided to examine how having more battery power, phone weight and RAM (Random Access Memory) can affect mobiles prices. In this Dataset, we are not given the actual phone prices but a price range indicating how high the price is (four different levels from 0 to 3)"
Getting Started with R Programming - KDnuggets,"The results are summarised in Figure 8. Increasing Battery Power and RAM consistently lead to an increase in Price. Instead, more expensive phones seem to be overall more lightweight. In the RAM vs Price Range plot have interestingly been registred some outliers values in the overall distribution"
Getting Started with R Programming - KDnuggets,"Finally, we are now going to examine the distribution of camera quality in Megapixels for both the Front and Primary cameras (Figure 9). Interestingly, the Front camera distribution seems to follow an exponentially decaying distribution while the Primary camera roughly follows a uniform distribution. If you are interested in finding out more about Probability Distributions, you can find more information"
Data Science Influencers and Keynotes Coming to ODSC East 2020 - KDnuggets,"Professor Tom Mitchell is known as the “Father of Machine Learning,'' having founded the Machine Learning Department at Carnegie Mellon University and led it as Department Head for its first 10 years, teaching many famous students including Andrew Ng. He is a world-renowned researcher in machine learning, artificial intelligence, and cognitive neuroscience. As a professor at Carnegie Mellon, some of his current research includes teaching computers to code, conversational machine learning, and personal AI agents"
Data Science Influencers and Keynotes Coming to ODSC East 2020 - KDnuggets,"Michael Kearns is a professor in the Computer and Information Science department at the University of Pennsylvania, where he holds the National Center Chair and has joint appointments in the Wharton School. He is founder of Penn’s Networked and Social Systems Engineering (NETS) program, and director of Penn’s Warren Center for Network and Data Sciences. Michael is the author of “The Ethical Algorithm: The Science of Socially Aware Algorithm Design,” which is gaining traction as a must-read for understanding the ethics behind AI and how we can better plan for the future"
Data Science Influencers and Keynotes Coming to ODSC East 2020 - KDnuggets,"Margaret Mitchell is a Senior Research Scientist at Google, where she focuses on research around vision-language and language generation. Her work is helping Google push the bounds of computer vision and NLP, even utilizing statistics and cognitive science insights. Before Google, she was a founding member of the famous Microsoft Research “Cognition” group, which is focused on developing AI at Microsoft. Margaret’s Ted Talk, “"
Data Science Influencers and Keynotes Coming to ODSC East 2020 - KDnuggets,"Lester is an avid researcher and established academic. At Microsoft, he develops new tools, models, and theory for large-scale learning tasks driven by applications from healthcare, climate, recommender systems, and the social good. He first made waves in the AI community when he co-organized the second-place team in the $1M Netflix Prize competition for collaborative filtering and won the $50K Prize4Life ALS disease progression prediction challenge"
Data Science Influencers and Keynotes Coming to ODSC East 2020 - KDnuggets,"In 2009, Matei Zaharia of Databricks started the popular machine learning framework, Apache Spark, during his PhD at UC Berkeley. Now, he leads the MLflow development effort at Databricks, helping to produce streamlined machine learning workflow processes. As a researcher, he’s interested in computer systems for emerging large-scale workloads such as machine learning, big data analytics and cloud computing"
Data Science Influencers and Keynotes Coming to ODSC East 2020 - KDnuggets,"Dr. Stonebraker has been a pioneer of database research and technology for more than forty years. He was the main architect of the INGRES relational DBMS, and the object-relational DBMS, POSTGRES. Much of his current research focuses on database technology, operating systems and the architecture of system software services"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"It is quite often that in our blog, we explore intricate connections between state-of-the-art technologies, or explore the mesmerizing depth of a new technique. However, AI or data science is not only bragging about new exciting methods that boost accuracy by 2% (which is a big gain) but about making data and technology work for you. It will help you increase sales, understand your customers, predict future faults in process lines, or make an insightful presentation, submit a term project, or have a good time with your friends working on a new idea that will change the world. And in this sense, everyone can — and to some extent should — become a data scientist"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"You have an excellent idea in your head — the one you have cherished since you were a child about having a toys-cleaning robot or the one that just came into your mind about accessing the customers in your shop by sending them fortune cookies with predictions based on their purchase preferences. However, to make your idea work, you need the attention of others. Find a compelling narrative for it; make sure that it has a hook or a captivating purpose, that it is up-to-date and relevant. Finding the narrative structure will help you decide whether you actually have a story to tell"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"You can choose to scrape data from websites or access data from social networks through public API’s. For the latter option, you need to write a small program that can download data from social networks in a programming language you feel the most comfortable with. For the cloud option, you can spin up a simple"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"As to the amount of data needed, the rule of the thumb is to get as much data as possible in a reasonable time, for example, a few days of running your program. Another important consideration is to collect as much data as the machine you are using for analytics can handle. How much data to get is not an exact science, but it rather depends on the technical limitations and the question you have"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"After getting the data, you need to select the proper tool to explore it. To make a choice, you can write down a list of analytics features you think you need and compare available tools. Sometimes you can use user-friendly graphical tools like Orange, Rapid Miner, or Knime. In other cases, you’ll have to write the analysis on your own with such languages as Python or R"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"To assess your theory at an initial step, in line with the more general and conventional content analysis, you can pinpoint trends present in the data. One way we use quite a lot is to select significant events that have been reported. Then you can try to create an analytics process that finds these trends. If analytics can find the trends you specified, then you are on the right track. Look for instances where analytics finds new trends. Confirm these trends, for instance, by searching the Internet. The results are not going to be reliable 100% of the time, so you’ll need to decide how many falsely reported trends (the error rate) you want to tolerate"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"When you have your business model and proven theory, it is time to build the first version of your product, the so-called minimum viable product (MVP). Basically, this can be the first version that you offer to customers. As a minimum viable product (MVP) is a product with just enough features to satisfy early customers and to provide feedback for future development, it should focus only on the core functionality without any fancy solutions. You should stick to simple functions that will work in the beginning and expand your system later. At this stage, the system could look something like this:"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"In principle, your focus should be on the future development of your product, not on system operation. For this, you need to automate as much as possible: uploading to S3, starting the analysis or data storing. In"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"The other face of automation is logging. When everything is automated, you can feel that you are losing control over your system and do not know how it performs. Besides, you need to know what to develop next, both in terms of new features and fixing problems. For this, you need to set up a system for logging, monitoring, and measuring all meaningful data. For instance, you should log statistics for the download of your data or upload it to S3, the time of the analytics process, and the users’ behavior"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"You probably know that AI, Machine Learning, Data Science, and other new developments are all about reiteration and fine-tuning. So, when you have your MVP running, automation, and monitoring in place, you can start enhancing your system. It is time to get rid of weaknesses, optimize the overall performance and stability, and add new functions. Implementing new features will also allow you to offer new services or products"
A Layman’s Guide to Data Science. Part 2: How to Build a Data Project - KDnuggets,"Afterward, visualize your data and incorporate trends, significance, and proportion you built your project into a narrative. Your story about the product should never end with a fixed event, but rather a set of options or questions to trigger an action from the audience. Never forget that the goal of data storytelling is to encourage and energize critical thinking for business decisions or to purchasing your product"
Python and R Courses for Data Science - KDnuggets,"Since Python and R are a must for today's data scientists, continuous learning is paramount. Online courses are arguably the best and most flexible way to upskill throughout ones career. This list contains well-known courses that can assist anyone wanting to begin a general understanding of each language and their specialized applications"
Python and R Courses for Data Science - KDnuggets,"This course is suited to those with a non-major in computer science and even those with no programming experience. According to course description, Python is one of the languages you will gain familiarity with. While this is not directly related to Data Science in terms of statistics and visualization, the basic programming concepts are still important to learn"
Python and R Courses for Data Science - KDnuggets,"This is a more intermediate class by the University of Michigan. The specialization has 5 courses and takes you through them using Python. The course touches on subjects like statistics, machine learning, information visualization, text analysis, and social network analysis"
Python and R Courses for Data Science - KDnuggets,"This course is also suited for even a beginner in data science. It takes us through the basic concepts of AI such as the algorithmic foundations of AI, graph search algorithms, classification, optimization, reinforcement learning etc. According to the course website, this is free course as well with payment only needed for a verifiable certificate"
Python and R Courses for Data Science - KDnuggets,This is more Intermediate class for someone who has used R before. It takes a more programming approach such as how to structure functions and loops in R. There is also use of statistics to be used as examples
Python and R Courses for Data Science - KDnuggets,This course will take you through using Linear Algebra for applications in the Life sciences sector. It is advised that a person taking this course to have prior knowledge of mathematics or statistics.  R will be used as the language for learning and will tackle topics such as matrix operations and statistical inference
Advice for a Successful Data Science Career - KDnuggets,"This blog is meant to show that most everyone has had to expend quite a bit of effort to get where they are. They have to work hard, sometimes experience failure, show discipline, be persistent, be dedicated to their goals, and sometimes sacrifice or take risks. With that, let’s get started!"
Advice for a Successful Data Science Career - KDnuggets,"Take note of all the interview questions you got asked, especially those questions you failed to answer. You can fail again, but don’t fail at the same spot. You should always be learning and improving"
Advice for a Successful Data Science Career - KDnuggets,One of the major themes this article is that everyone has experienced some failure. The important part is that some people go to great lengths to achieve there goals. In
Advice for a Successful Data Science Career - KDnuggets,Sacrifices and risks can come in many different forms. One risk could be ignoring commands from above. When
Advice for a Successful Data Science Career - KDnuggets,"Add a couple things, see what pops up. Add a couple more, see what changes…I hacked up a prototype. On a test site, I modified the Amazon. Looked pretty good to me. I started showing it around"
Advice for a Successful Data Science Career - KDnuggets,"While the reaction was positive, there was some concern. In particular, a marketing senior vice-president was dead set against it…At this point, I was told I was forbidden to work on this any further. I was told Amazon was not ready to launch this feature. It should have stopped there"
Advice for a Successful Data Science Career - KDnuggets,"Instead, I prepared the feature for an online test. I believed in shopping cart recommendations. I wanted to measure the sales impact"
Advice for a Successful Data Science Career - KDnuggets,"I heard the SVP was angry when he discovered I was pushing out a test. But, even for top executives, it was hard to block a test. Measurement is good. The only good argument against testing would be that the negative impact might be so severe that Amazon couldn’t afford it, a difficult claim to make. The test rolled out"
Advice for a Successful Data Science Career - KDnuggets,"The results were clear. Not only did it win, but the feature won by such a wide margin that not having it live was costing Amazon a noticeable chunk of change. With new urgency, shopping cart recommendations launched…"
Advice for a Successful Data Science Career - KDnuggets,"Hopefully you find some of the advice and examples from this blog to be useful on your data science journey. Keep in mind that a lot of advice from successful people suffers from survivorship bias. Always take advice or shared experiences with a grain of salt. If you have any questions, thoughts on the post, or just want to share your own experiences, feel free to reach out in the comments below or through"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets",The Bayesian methodology allows us to apply probability distributions to model the real world and update our beliefs as new data becomes available to us. For years statisticians have generally relied on the frequentist approach. The Bayesian approach is suitable for modelling a hypothesis that has only a small amount of data that may not be significant in the eyes of a frequentist
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","Imagine you are at the movies and a fellow moviegoer drops their ticket. You want to get their attention. This is what they look like from behind. You can’t tell their gender, only that they have long hair. Do you call out “Excuse me ma’am!” or “Excuse me sir!” Given what you know about men’s and women’s hairstyles in your area, you might assume that this is a woman. Now consider a variation of the situation where this person is standing in line for the men’s restroom. With this additional piece of information, you would probably assume that this is a man. This use of common sense and background knowledge is something that we do without thinking. Bayesian inference is a way to capture this in math so that we can make more accurate predictions"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","BERT pre-trains deep bidirectional representations of unlabeled text data on both left and right contexts, resulting in a language model which can be fine-tuned with only a single added layer. BERT has achieved state of the art performance on a number of NLP tasks, including question answering and inference. Both BERT and the Transformer were developed by Google"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","It should be intuitive that training a language model bidirectionally on text as opposed to left to right (or right to left) would result in some better sense of language ""understanding"" and word meaning. Bidirectionality allows for the learning of word meaning based on the entirety of its surroundings, as opposed to making determinations based on what can be gleaned from ""reading"" from one direction up to the point of a given word occurrence. Therefore, words with different meanings in different contexts can be treated separately and a better representation of their contextual meaning captured (think ""bank"" of a river versus a ""bank"" where you keep your money)"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","Data engineers are the ones responsible for optimizing and managing the storage and retrieval of an organization’s data. A data engineer would set out the road map on how best to acquire the data and create databases for storing it. They would typically deal with cloud services to optimize data storage and create algorithms to make sense out of the data. The role of a data engineer is highly technical and requires advanced knowledge in SQL, database design and computer science"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","In this age of machine learning, deep learning and AI, the final objective of the process is to deploy the it to put it in the hands of the final consumer. There are many services available to deploy a model through the web such as Heroku, AWS, Azure, GCP, Github etc. Different providers have different costing and provide slightly different services. Deployment and putting models into production will require some knowledge of Front-end and Back-end development as well to a certain extent as well to be able to work in teams"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","Data Scientists are swimming in data. Piles of data. Some data can be raw and unorganized as it streams in through a firehose. Other data can be neat and orderly (or curated to be so), formatted within manageable dimensions. With these “Euclidean” data sets, such as text, images, and videos, machine learning has provided much success in applications for text generation, image manipulation, and facial recognition. Pair deep learning models run on a GPU or two with a mountain of training data, and the possibilities for discovering hidden patterns and meaningful features in the data appear infinite"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","What about data that is more interrelated? Data can be connected to one other through dependent relationships. Interactions between users might impact purchasing decisions on e-commerce platforms. Chemical interactions for drug discovery are mapped out through complex interconnections of reactions. Social networks are formed and devolve through ever-changing, irregular, and unordered relationships. The human brain is built on individually communicating cells connected through an intertwined ball of spaghetti"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","Broadening this scope of leveraging artificial intelligence into the operations of an organization is AIOps that pulls in any and all machine learning technologies to extract meaningful insights from IT systems. This approach combines the intelligence of humans with that of AI algorithms to enhance IT teams in making better and faster decisions, responding to incidents in real-time, and developing optimized applications to facilitate more effective or automated business processes. With"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","Consider the following pair of issues that can arise while training machine learning models. First, often there is not enough training data available to sufficiently train a model. Second, even (and especially) if sufficient amounts of training data exists, training process is often resource- and time-consuming"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","Transfer learning involves the leveraging of existing machine learning models for use in scenarios in which the models were not originally trained. Much as humans do not discard everything they have previously learned and start fresh each time they take up a new task, transfer learning allows a machine learning model to port the ""knowledge"" it has acquired during training to new tasks, extending the reach of the combination of computation and expertise having been used as fuel for the original model. Simply put, transfer learning can save training time and extend the usefulness of existing machine learning models. It is also an invaluable technique for tasks where the large amounts of training data typically required for training a model from scratch are not available"
"20 AI, Data Science, Machine Learning Terms You Need to Know in 2020 (Part 1) - KDnuggets","Considering time- and compute-consumption, transfer learning allows us to better maximize the usefulness of a model. Concerning the insufficiency of training data, transfer learning allows us to take pretrained models trained on potentially massive amounts of data and tweak them on the smaller amounts of task-specific data available. Transfer learning is an effective approach to managing 2 distinct potential shortcomings in machine learning model training, and so it should be no surprise why it is becoming increasingly more used"
Seize Your New Career in Data Science - KDnuggets,"Becoming a data scientist isn't easy. That's why we launched our Data Science Prep course. It is a mentor-led program designed for go-getters who want to enroll in our Data Science Career Track, but who need an introduction to, or a refresher in, Python programming or core data science concepts"
Fourier Transformation for a Data Scientist - KDnuggets,"Fourier transform is widely used not only in signal (radio, acoustic, etc. One example: Fourier transform of transmission electron microscopy images helps to check the periodicity of the samples. Fourier transform of your data can expand accessible information about the analyzed sample"
Fourier Transformation for a Data Scientist - KDnuggets,Yes. That’s what a Fourier transform does. It takes up a signal and decomposes it to the frequencies that made it up
Fourier Transformation for a Data Scientist - KDnuggets,"The continuous Fourier transform converts a time-domain signal of infinite duration into a continuous spectrum composed of an infinite number of sinusoids. In practice, we deal with signals that are discretely sampled, usually at constant intervals, and of finite duration or periodic. For this purpose, the classical Fourier transform algorithm can be expressed as a Discrete Fourier transform (DFT), which converts a finite sequence of equally-spaced samples of a function into a same-length sequence of equally-spaced samples of the discrete-time Fourier transform:"
Fourier Transformation for a Data Scientist - KDnuggets,"When each sample in the dataset is a signal (time series, or images, etc. But they might actually correspond to just a few points in the Fourier domain (especially if there is some periodicity). This simplifies the problem a lot"
The Data Science Puzzle — 2020 Edition - KDnuggets,"Big Data is still important to data science. Take your pick of metaphors, but any way you look at it, Big Data is the raw material that [.] continues to fuel the data science revolution"
The Data Science Puzzle — 2020 Edition - KDnuggets,"While I don't condone the capitlization of most key terms in general, ""big data"" seemed to previously demand this treatment given its near-fabled status and brand name-like station. Notice this time around I have reneged this status, which goes hand in hand with the idea that big data is no longer top level data science terminology. As alluded to in the final sentence, moving forward big data is simply ""data,"" and we could reword part of that excerpt to read, """
The Data Science Puzzle — 2020 Edition - KDnuggets,"Look, at this point we should all be aware of how important data is to the process of data science (it's right there in the name). Whether our data is big or small or lies somewhere else on the data sizing spectrum really doesn't require distinguishing from the outset. We all want to science the data and provide value, whether the data is a lot or a little"
The Data Science Puzzle — 2020 Edition - KDnuggets,"Data is everywhere. Much of it is big. It's time we stop emphasizing so, just like it's time we stop saying ""smart"" phone. The phones are all basically smart now, and making special note of it really says more about you than it does about the phone"
The Data Science Puzzle — 2020 Edition - KDnuggets,"Machine learning is one of the primary technical drivers of data science. The goal of data science is to extract insight from data, and machine learning is the engine which allows this process to be automated. Machine learning algorithms continue to facilitate the automatic improvement of computer programs from experience, and these algorithms are becoming increasingly vital to a variety of diverse fields"
The Data Science Puzzle — 2020 Edition - KDnuggets,"Deep learning is a specific type of machine learning, which is the employment of deep neural networks for insight extraction. Neural networks still provide state of the art results in a wide variety of fields — notably computer vision and natural language processing — which is why they are often treated distinctly from machine learning. While they are just a tool, they are a tool that often seems to prove especially useful in particular data science tasks"
The Data Science Puzzle — 2020 Edition - KDnuggets,"But that doesn't mean that AI is not worthy of pursuit; AI research pays dividends in the form of inspiration and motivation. As you may have noticed, however, AI has a perception problem. Just as data mining used to be a mainstream term that struck fear into the hearts of many (mostly related to the invasion of privacy), AI frightens the masses from an entirely different viewpoint, one that evokes SkyNet-style fears. I don't know whether we thank the media, Elon Musk, the confounding of AI with deep learning and its successes, or something else entirely, but I don't think the end result is escapable: this perception issue is real, and the uninitiated are becoming terrified"
The Data Science Puzzle — 2020 Edition - KDnuggets,"There is also this: though machine learning, artificial intelligence, deep learning, computer vision and natural language processing (along with a variety of other applications of these ""intelligent"" technologies) are all separate and distinct fields and application domains, even practitioners and researchers have to admit that there is some continually evolving ""concept creep"" going on any more, beyond the regular ol' confusion and confounding that has always taken place. And that's OK; these fields all started out as niche sub-disciplines of other fields (computer science, statistics, linguistics, etc. While it is important on some level to ensure that everyone who should have a basic understanding of their differences indeed possesses this understanding, when it comes to their application in fields such as data science, I would humbly submit that getting too far into the semantic weeds doesn't provide practitioners with much benefit in the long term"
The Data Science Puzzle — 2020 Edition - KDnuggets,"As I alluded to above, getting into the semantic weeds of exactly what defines artificial intelligence is not what I'm wont to do. I have a vague idea of what I would technically classify as ""artificial intelligence,"" as do you; our vague ideas may differ. The underlying current of my perception of what I consider AI, and its main benefit, is its unattainability. AI represents a set of vaguely defined lofty goals, the closer to which we get become replaced with eve more lofty goals"
The Data Science Puzzle — 2020 Edition - KDnuggets,"I still like this definition. It's succinct, pulls things together, and doesn't really need to be elaborated on, at least in my opinion. But I did write it, so I might be biased"
The Data Science Puzzle — 2020 Edition - KDnuggets,"And those are the top-level concepts of data science, or at least how I see them. These are not the only important aspects of the landscape, obviously. Numerous other aspects such as data visualization have not been included herein, and many would argue they should have been. I stand by my selections however, and leave other concepts for another time"
Top 5 Data Science Trends for 2020 - KDnuggets,"Technology is evolving continuously, and so are we. In the upcoming years, there will be massive growth in the AI and Machine Learning field. There is already a considerable amount of data to be managed, and with new technological advancements, we can utilize big data in many ways. For that, we have to stay up to date with the latest trends in data science"
Top 5 Data Science Trends for 2020 - KDnuggets,"AI has become the mainstream technology for both small and large businesses, and it will bloom in the next few years. At present, we are at the initial stage of using artificial intelligence, but in 2020, we will see more advanced applications of AI in all fields. The reason AI is growing rapidly is that it allows enterprises to improve their overall business processes, and provides a better way of handling both customer and client’s data"
Top 5 Data Science Trends for 2020 - KDnuggets,"Though utilizing AI will still remain a challenge for many, as exploring the advancement of this technology is not that simple. In 2020, we will find more advanced apps developed with AI, Machine learning, and other technologies that can improve the way we work. Another trend that will take over the market is automated machine learning that will be helpful in transforming data science with better data management. So, you might need specialized training for executing deep learning"
Top 5 Data Science Trends for 2020 - KDnuggets,"When it comes to data science, we simply cannot ignore Big Data analysis, which helps businesses gain a competitive edge over data and achieve their objectives. Nowadays, enterprises use different tools and technologies, especially python, to analyze big data. Also, businesses are focused on identifying the reasons behind certain events that take place at present. And that’s where predictive analytics is used; it helps companies identify what can happen in the future"
Top 5 Data Science Trends for 2020 - KDnuggets,"At present, edge computing is propelled by sensors. But with the growth of IoT, Edge computing will take over mainstream cloud systems. Edge computing allows businesses to store streaming data close to the data sources so that they can be analyzed in real-time. Moreover, it offers a great alternative to Big Data analytics that require high-end storage devices and higher network bandwidth"
Top 5 Data Science Trends for 2020 - KDnuggets,"The adoption of artificial intelligence and machine learning will give rise to many new roles in the industry. One role that will be highly in demand is data science security professionals. As both AI and ML entirely depends on data, and to process this data efficiently, data scientists must have expertise in data science as well as command over computer science"
Top 5 Data Science Trends for 2020 - KDnuggets,"Though the business market already has access to many experts who are proficient in data science and computer science, there is still a need for more professional data security professionals who can process data to customers securely. For that, data security scientists must be well versed with the latest technologies of data science or big data analysis. For example, python is amongst the most used languages in data science and data analysis, so having a clear understanding of python concepts can help you tackle the problems related to data science security"
Top 5 Data Science Trends for 2020 - KDnuggets,"Data Science has become one of the growing fields in all industries, especially the IT industry. Thus, businesses adopting data science techniques and technologies must stay up-to-date with the latest trends. In this article, we covered five data science trends that will be on the top of the list in 2020. You can take help from these trends to analyze where you need to improve your business processes in order to achieve maximum growth and ROI"
Data Validation for Machine Learning - KDnuggets,"The most basic method of validating your data (i. A typical ratio for this might be 80/10/10 to make sure you still have enough training data. After training the model with the training set, the user will move onto validating the results and tuning the hyperparameters with the validation set till the user reaches a satisfactory performance metric. Once this stage is completed, the user would move on to testing the model with the test set to predict and evaluate the performance"
Data Validation for Machine Learning - KDnuggets,"Validating a dataset gives reassurance to the user about the stability of their model. With machine learning penetrating facets of society and being used in our daily lives, it becomes more imperative that the models are representative of our society. Overfitting and underfitting are the two most common pitfalls that a Data Scientist can face during a model building process. Validation is the gateway to your model being optimized for performance and being stable for a period of time before needing to be retrained"
Google Dataset Search Provides Access to 25 Million Datasets - KDnuggets,"Based on what we’ve learned from the early adopters of Dataset Search, we’ve added new features. You can now filter the results based on the types of dataset that you want (e. If a dataset is about a geographic area, you can see the map. Plus, the product is now available on mobile and we’ve significantly improved the quality of dataset descriptions. One thing hasn't changed however: anybody who publishes data can make their datasets discoverable in Dataset Search by using an open standard (schema"
Google Dataset Search Provides Access to 25 Million Datasets - KDnuggets,"Dataset Search also gives us a snapshot of the data out there on the Web. Here are a few highlights. The largest topics that the datasets cover are geosciences, biology, and agriculture. The majority of governments in the world publish their data and describe it with schema. The United States leads in the number of open government datasets available, with more than 2 million. And the most popular data formats? Tables–you can find more than 6 million of them on Dataset Search"
Data Scientist Archetypes - KDnuggets,"When you come to the field of data science for the first time, or even if you’ve been in it for a while, you can be easily overwhelmed by the number of tools and the vast range of skills required. Blog posts about what it takes to become a data scientist devolve into laundry list of skills you don’t have yet and tools you haven’t learned yet. This is discouraging. And it’s counterproductive"
Data Scientist Archetypes - KDnuggets,"These ideas are not mine alone. They are based on a series of interviews with the data scientists listed here, who are from several different companies, as well as contributors from academic data science programs. The underlying ideas are captured in"
Data Scientist Archetypes - KDnuggets,The reason data science feels so big is because it’s no longer a single field. There are actually three separate pillars. There is
Data Scientist Archetypes - KDnuggets,These three areas are all distinct. They require different skills and different sets of tools. Let’s dive a little deeper into each one
Data Scientist Archetypes - KDnuggets,"This can can be as straightforward as performing the right Google search and reading a few documents. Often, it involves instrumenting processes that are already in place, like logging code, or adding sensors to a production line, or conducting surveys. For some applications, research involves careful design of experiments. It requires using sophisticated statistical tools to plan what information will be gathered, how it will be gathered, and how it will be analyzed after the fact"
Data Scientist Archetypes - KDnuggets,"Given a large collection of data, how can we summarize it, aggregate it, visualize it, and use statistical summaries to go from a sea of bits to a nugget of knowledge. In particular, I want to call out the art of visualization. Turning numbers into a picture that is easily interpreted and conveys useful information is an art as much as a science. It involves an intuitive sense of human visual perception and understanding"
Data Scientist Archetypes - KDnuggets,"Data modeling also has several major subcategories. This is often referred to as machine learning. It is no more or less than creating a simplified description of your data that you can use to make estimates for data that you have not measured. Three main sub classes of machine learning are supervised, unsupervised, and custom model development"
Data Scientist Archetypes - KDnuggets,"These can be categorical labels, for example: Did this customer re-visit our website within the month after making a purchase, yes or no? Examples can also be labeled with numbers. For example: How many views did a video receive? Using a large collection of labeled examples, accompanied either by their category or value, supervised learning algorithms can distill the underlying patterns out. If the labels are categorical, this is called"
Data Scientist Archetypes - KDnuggets,"We don’t know the outcome or the right answer. In unsupervised learning, the goal is to discover patterns in how the data is distributed. In"
Data Scientist Archetypes - KDnuggets,"Both supervised and unsupervised learning methods are general. They begin by assuming very little about the source of the data, what it means, and what domain it resides in. General algorithms can get away with this by sheer statistical power. They are at their best when you have a large collection of data"
Data Scientist Archetypes - KDnuggets,"However, in practical applications it is often the case that we don’t have nearly as much data as we would like, and we don’t have enough to rely on naïve machine learning techniques. When this happens, we get to do custom algorithm development. We build in information that we know, or feel comfortable assuming, about the domain. For instance, one important modeling problem in the agriculture industry is to predict the yield for a field of corn, the total weight of the harvested grain. This is an unfathomably complex system, with soil texture and chemistry, genetics, temperature, and moisture interacting. And it takes a year to collect a single data point. There is no feasible way to learn a complete crop model using naïve machine learning methods. However, geologist, chemists, biologists, meteorologist, and geneticists have all investigated and described the pieces of this system. By assembling what is known and making some reasonable assumptions, a tractable crop model can be created"
Data Scientist Archetypes - KDnuggets,"Another big task of data engineers is taking code that works well in a prototype and making it ready for operation in the wider world. It is one thing to build a model in a Jupyter notebook, but to be able to use that to filter every incoming customer request, the code needs to be made compatible with the rest of the company’s code base, and it needs to be able to ingest data and publish results in the appropriate formats. Any glitches or failures need to be handled gracefully, and the system needs to be made secure against mischievous or malicious users. The gap between a smoothly running prototype and"
Data Scientist Archetypes - KDnuggets,"There’s also a surprise pillar number four, data mechanics. This is the dirty work that everyone needs to do but nobody likes to talk about. It includes things like"
Data Scientist Archetypes - KDnuggets,"Now we have our four pillars in place: data analysis, data modeling, data engineering, and data mechanics. What does this mean for you as a data scientist? These areas are all very different. I propose that a data scientist is someone who has at the very least exposure to all these skill areas. That means that you’re not necessarily an expert, or even good at them, but you know what they are, and you know the right questions to ask and right Google searches to run"
Data Scientist Archetypes - KDnuggets,A nice balance across the board gives you a broad view of problems of varying scopes. It may mean that you’ll need to collaborate with specialists for some projects. But it sets you up very well to be a technical leader or data executive
Data Scientist Archetypes - KDnuggets,"This is an anti-pattern. This is someone you do not want to be and you do not want to hire. Mechanics can get gritty. Some days it will occupy the majority of your time. But you never outgrow the need to do it. Embrace mechanics tasks, learn the skills you need to do them adequately, and get on with it"
Data Scientist Archetypes - KDnuggets,"This is the person who helps everything work fast and reliably. Their work will still require some analysis and some modeling, and definitely some mechanics, but they will be focused on engineering tasks. Makers are the ones who transform good ideas into concrete machinery"
Data Scientist Archetypes - KDnuggets,"This person is a master in every pillar. This is the data science ideal. Except of course that it doesn’t exist. Becoming a master of one pillar takes a long time. Becoming a master in all three is essentially stacking three careers on top of each other. It’s not impossible to do, but it’s a long road, and the path down it passes through one of the other archetypes as a waypoint"
Data Scientist Archetypes - KDnuggets,"Another way to represent these is in a 3-D plot with axes for analysis, modeling, and engineering. You can see the Detective, Oracle, and Maker each lying close to an axis. The Generalist is a balance of three, sitting in the middle of the space. I want to be completely clear that I’m not implying any hierarchy or value judgement between the archetypes. They are each invaluable and serve different functions. Let your preferences and interests guide you as you choose between them"
Data Scientist Archetypes - KDnuggets,"There is not a commonly accepted taxonomy for data science specializations. When looking for a job, I recommend ignoring the job titles. There’s no reliable way to translate between job title and archetype. I also recommend not worrying too much about the specific tools listed. Instead, look at this skills implied. Read between the lines to discover which archetype they want. Does the posting sound more like a Maker? A Generalist? a Detective? Based on this, you can decide whether this particular posting is a good match for you"
Data Scientist Archetypes - KDnuggets,"Also, when posting a job, ask for specific skills. Don’t ask for all of them. And don’t ask for a Unicorn. You won’t find one"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"Most ML teams don’t like to do data and infrastructure work because it is not as interesting as modelling. Mis-management of the issue can lead to a high turn-over and a toxic team atmosphere. I want to share a solution called Insight-Driven Development (IDD), a few examples of it, and five steps to adopting it. IDD aims to create a high performing, engaged, and happy Data Science teams that embrace non-ML work as much as the fun ML stuff"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"We have to roll you off the project. Sorry. I feel helpless, nervous, and sad"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"He’s a big guy, and he looks like Jason Bourne. My underfit Neural Network tells me that I have a pretty low probability of getting to the pen before he does. I should probably leave now while I can"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,Alex hated doing data work. He’s very vocal about it. He used all kinds of excuses to “offload” it to someone else. We talked about it; he still did a half-a*s job. Other people got frustrated and complained about why only Alex got to do the cool stuff
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"At the minimum, I want a system that would encourage the data science team to embrace non-ML work and have some fun doing so. Well, the obvious solution is to force people to do it (we all get paid to work, right?). But it’s not sustainable. So, the solution has to be"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"There are certain caveats to this approach. For example, some data and infrastructure work has to be standalone, especially in the early phase of the project. We will discuss more in a follow-up post"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"It seems like there are a lot more items that are analytics related. It isn’t practical to assign all of them to the Data Scientists. People have different strengths, experiences, and interests. So, this brings us to the second aspect of IDD:"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"In IDD, each person’s accountability shifts toward outputs from their immediate roles. The team “works around an insight. To keep a balance of quality, each expert sets the standard, leads the programming (if needed), and be the final quality gatekeeper for her domain expertise"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"Note that particular work still requires highly specialized skills, and it’s best to assign to the experts. You, as the project lead, need to monitor workload and provide enough guidance to people who are leading the work outside of their immediate expertise (e. Each team member needs to work closely and communicate expectations and timing"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"I have been using IDD on both business- and software-focused ML projects with teams of business analysts, data experts, data scientists, and software engineers with varying years of experience. It’s been working well, and I am going to keep using and refining IDD. Concretely, IDD works because it lets everyone to:"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"Every team at every company works differently. So, please take the principles and apply them accordingly. Do expect some confusion, tension, and uncertainty. Be patient, things will click"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,"Assemble your A-team. The core team should be you (as the project lead), a data engineer, a machine learning engineer, a software engineer, and a UI designer. You may need part-time support from a business analyst and IT"
You’re Fired: How to develop and manage a happy data science team - KDnuggets,Start the project. Resist the temptation of reverting to the old way of working. Give it some time for the team to learn (and fail). Pay attention to how individuals deliver the IDD
What Do Data Scientists in Europe Do & How Much Are They Worth? - KDnuggets,"This report will delve into insights for 2019/2020 provided from professionals of all backgrounds, ages and locations within the European region. The largest share of contributions this year were from Germany, France, UK, Netherlands and Switzerland. Because these were our greatest data points, these countries are the primary focus for us in this report"
What Do Data Scientists in Europe Do & How Much Are They Worth? - KDnuggets,"Over half of those that took part in this survey hold a Masters degree, followed by a steady increase year on year amongst PhD (+4%) and Bachelors degree (+2% participants). There is a continued decline in those that opt to study however, making up just 3% of respondents this year.45%)"
What Do Data Scientists in Europe Do & How Much Are They Worth? - KDnuggets,"[L]ogistic regression, neural networks and random forests were the top three most popular choices with roughly 56% of respondents claiming to use them. Compared to a Data Scientists tool preferences, there is a much greater variety amongst their chosen data science methods. Other options in the survey (that didn’t make it to the top seven shown above) were 34% ensemble, 31% Bayesian techniques and 28% SVMs"
What Do Data Scientists in Europe Do & How Much Are They Worth? - KDnuggets,"[A] huge 70% of respondents saying they use Python as their primary modelling coding language. This is a 10% increase on our 2019 survey. 9% use R, 4% use SQL, and 4% use Java. There were also 3% of respondents that said they don’t code"
What Do Data Scientists in Europe Do & How Much Are They Worth? - KDnuggets,"66% of respondents said their primary production coding language is also Python. A further 9% said Java, as well as 7% either using Scala or not coding at all. 6% use C++"
What Do Data Scientists in Europe Do & How Much Are They Worth? - KDnuggets,"29% of respondents spend 11-20 hours a week coding which is the same as shown in our 2019 survey. Only 14% of respondents said they spend 31+ hours coding weekly, compared to 7% not coding at all. Overall the response was pretty varied, which could be indicative of the diversity in seniority of participants"
What Do Data Scientists in Europe Do & How Much Are They Worth? - KDnuggets,"Europe is certainly not a monolith, and this is evident in the survey responses. There are salary discrepancies between countries, to be sure, but this does not, itself, tell a complete story without consideration given to cost of living differences. While should be obvious and is beyond the scope of a salary survey, it is worth noting explicitly"
The Data Science Interview Study Guide - KDnuggets,Some data science interviews are very product and metric driven. These interviews focus more on asking product questions like what kind of metrics would you use to show what you should improve in a product. These are often paired with SQL and some Python questions
The Data Science Interview Study Guide - KDnuggets,"We recommend asking the recruiter if you aren’t sure which type of interview you will be facing. Some companies are very good at keeping interviews consistent, but even then, teams can deviate depending on what they are looking for. Here are some examples of what we have noticed about some companies' data science interviews"
The Data Science Interview Study Guide - KDnuggets,"Product sense is an important skill for data scientists. Knowing what to measure on new products and why it can help determine whether a product is doing well or not. The funny thing is, sometimes certain metrics going the way you want them to might not always be good. The reason people are spending more time on your website might be because webpages are taking longer to load or other similar user-facing problems. This is why metrics are tricky and what you measure is important"
The Data Science Interview Study Guide - KDnuggets,"Just because data science doesn’t always require heavy programming, it doesn’t mean that interviewers won’t ask you traverse a binary tree. So make sure you ask your interviewer what to expect. Don’t be daunted by these questions. Pick a few to do just so you’re not surprised in an interview"
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,"With a rapid rise in the usage of mobiles and mobile users, mobile apps have taken the place of desktop versions. Mobile apps have changed the way a particular task is performed. Now you don’t need to visit a restaurant to order and take away your food, you can do this by sitting at your home by using food ordering apps in just one click"
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,Users love how everything is so convenient with mobile apps and they can carry it with them everywhere. People should also use mobile apps to increase their knowledge in any field they are interested in. There are many apps that offer users to learn about the latest technology and how to use that technology so that it can be beneficial for them in the future
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,A mobile app that is designed in such a way that trains your brain to enhance your skills. Your session will consist of 3 sessions per day. The activities that are selected are chosen on the basis of your performance in the previous day’s activity. You will be asked questions in the areas you lack
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,This app helps you boost your productivity and uplift your skills. You will only get some limited activities in the free version of the app. If you want to access more personalized training programs you need to take the premium version of the app
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,"A customized game and brain training program to challenge your reasoning skills. The app helps you improve in areas such as English reading and writing, logical, and mathematical abilities. These training sessions are very addictive as they challenge you to improve and do more. In the basic version, you get only 3 exercises but if you want more you can have it by paying some amount as subscription fee"
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,Now you can learn this language from the comfort of your mobile phones. The application is available for Android users. It is a very high rated app in the play store and is best compatible with python 2.7. The app can also execute python codes and documents from QR codes
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,Everyone has studies basic statistics in their school life. This app helps users to revise the basic statistics for beginners in Data science field. Data Science is a mixture of both statistics and coding so the developers should know the basic statistics to derive insights from large data sets. This app can help developers learn and improve their statistics basics
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,It is compulsory for a data scientist to know how to use excel. This app lets you learn beginner and advanced versions of excel. Users can learn through video tutorials available on the app. You can even practice what all you have learned through video tutorials in the application only. Learn excel anytime and anywhere using Excel tutorial
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,Udemy helps people to learn with over access to 3200 courses and tutorials for different topics. You can download tutorials and learn. You need to take a premium version of the app in order to access those videos
Top 9 Mobile Apps for Learning and Practicing Data Science - KDnuggets,"With a boom in the mobile app development industry, there is a launch of a new app every day. Users can’t download all the apps on their mobile phones so they should carefully choose the ones that are beneficial for them and which improve their productivity. Try and spend more time using these applications rather than using social media because these apps will help you to learn and practice data science from the comfort of your home without bearing any cost"
Handling Trees in Data Science Algorithmic Interview - KDnuggets,They are necessarily the same thing. Don't be surprised. Below is how data scientists and software engineer's look at trees
Handling Trees in Data Science Algorithmic Interview - KDnuggets,There are variations to this definition when it comes to equalities. Sometimes the equalities are on the right-hand side or either side. Sometimes only distinct values are allowed in the tree
Handling Trees in Data Science Algorithmic Interview - KDnuggets,"Trees are inherently recursive, and so we use recursion here. We take the mid element of the array and assign it as the node. We then apply the"
Handling Trees in Data Science Algorithmic Interview - KDnuggets,"Trees form the basis of some of the most asked questions in Data Science algorithmic interviews. I used to despair such tree-based questions in the past, but now I have grown to like the mental exercise involved in them. And I love the recursive structure involved in such problems"
Handling Trees in Data Science Algorithmic Interview - KDnuggets,Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"When machine learning tools are developed by technology first, they risk failing to deliver on what users actually need. It can also be difficult for development teams to establish meaningful direction. This article explores the challenges of designing an interface that enables users to visualise and interact with insights from graph machine learning, and explores the very new, uncharted relationship between machine learning and UX"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"I am a User Experience (UX) designer. Folks like me look into all aspects of the end-user’s interaction with a company, its services, its product. What drives us is designing meaningful products that are intuitive, meet user needs, and delight"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"My job as a UX designer is to question what problems our users can solve with graph machine learning. Graphs have proven to be well suited to crime investigation, and the potential of using machine learning on graphs for investigative analytics in law enforcement is immense. The trouble is, understandable security restrictions in this domain make access to data impossible"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"In the absence of data, it’s difficult to develop a tool that can analyse or visualise it. When products are developed by technology first, they risk failing to deliver on what the user actually needs. It can also be difficult for teams to establish a meaningful direction, ending up with something like this:"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"Manual labelling is labour-intensive. Say it takes just five minutes to manually check whether a user is hateful or not hateful, then labelling 100K users (equivalent to our Twitter dataset) would take just shy of 12 months for one person working 24/7. Additionally, new"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"So, I paired Ian with Dan; a data scientist specialised in machine learning on graphs. In my scenario, Dan plans to use an algorithm that uses the retweet network to infer whether a user is likely to be hateful or not across the remaining portion of data that is unlabelled. This will produce a shortlist of suspects for Ian to further investigate which he can filter by characteristics of the data or the confidence of the label, for example"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"When it came to user testing our tools, this storyboard acted as a context snapshot which was effective especially for participants not familiar with machine learning process and terminology. I also return frequently to the storyboard as a basis for my design to explore ideas, concepts and validation of a fleshed-out feature. What would Ian need to do at that step? What are the subsequent steps Dan might take?"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"Here’s where I confess that usually when I’m designing an interaction, I explore the solutions of the designers before me. To defend myself and perhaps other UX designers, it’s not out of laziness or pure plagiarism, nor an unwillingness to be creative. We have this heuristic in interaction design:"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"For now, let’s return to our scenario. We have a subset of 5k Twitter users labelled by humans as either hateful, or normal (not hateful). These human-labelled users represent our ‘ground truth’ — what we know for certain — while our model will"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"Knowing whether a user is hateful or not is of great value to Ian the intelligence analyst. Accordingly, we applied this as the attribute to define the style of the node from a visualisation point of view. For example, you can see differentiated below both human-labelled nodes, and predicted ones:"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"To find out if the result was as intuitive as we hoped, we needed to do more user testing. From 11 participants, 10 found the visual styling intuitive. The eleventh participant saw something like the screen below right:"
Graph Machine Learning Meets UX: An uncharted love affair - KDnuggets,"Et, voila. We have a visualisation system that is flexible enough to accommodate different user requirements, but also different organisational needs. I can thank Ian and Dan for that"
Improving the partnership between Data Science and IT - KDnuggets,"On the other hand, IT provides and manages the technology landscape that makes it all possible. They are responsible for ensuring platforms are safe, governed, scalable, compliant, and cost-effective. And they serve many internal customers, not just Data Science"
Improving the partnership between Data Science and IT - KDnuggets,"What keeps your IT teams up at night? According to Cornett, understanding “where IT may be coming from” helps his team get ahead of potential issues and build a more integrated effort between data science and IT. For example, it’s easy for IT teams to want to lock down data to protect customer information and privacy. Addressing data privacy and security concerns at the outset of any project can help clear the path a bit, especially when data scientists seek access to new data types"
Improving the partnership between Data Science and IT - KDnuggets,"As models transition from innovative lab experiments to real-world products, data science and IT teams must coordinate on a host of activities, from how to integrate models into downstream systems to monitoring for model drift. Lee Davidson, who leads Morningstar’s Quant Research team and the company’s Head of Technology and Product Analytics, Jeff Hirsch, work closely to ensure their teams “integrate early and often” to productionalize new models. Here are a few steps they take:"
7 Steps to a Job-winning Data Science Resume - KDnuggets,"Only do that if it’ll add value to your profile. If you’re linking to an empty LinkedIn profile or a GitHub account that was last updated months ago, there’s no point. If there are significant achievements on GitHub, Kaggle or any other public platform, feel free to mention the same in a separate section in your resume"
7 Steps to a Job-winning Data Science Resume - KDnuggets,It is nothing but an identification marker right below your name that communicates your professional identity. The recruiters shouldn’t have to scan your work-ex to gauge whether you’re relevant to them. You’re adding a job title to make their job easier
7 Steps to a Job-winning Data Science Resume - KDnuggets,Let’s say you are working as a Data Analyst and your target profile is that of a Data Engineer. Don’t try to fool the recruiter and write “Data Engineer” as your job title. Sticking to the truth is the only sustainable way to get what you want
7 Steps to a Job-winning Data Science Resume - KDnuggets,"If you have any certifications to help your cause, we already covered that in the first point. If you don’t have that, you can mention your projects or competitions in the relevant section. In that case, the job title can turn into something along the lines of “<current designation> & Data Science Enthusiast”"
7 Steps to a Job-winning Data Science Resume - KDnuggets,"A summary would be a brief 3-4-line statement describing the impact you can deliver in the next organization. Don’t go heavy with your data science skills here - that’s what the rest of the resume is for. In summary, simply mention the value you can deliver using your specific skills"
Fast Track Your Data Science Career - KDnuggets,"Student can also gain relevant, hands-on experience using real-world data sets. Online students can earn their degree on a part-time basis while they continue to work. As a result, they are better able to apply these new skills to their current job for a faster return-on-investment"
7 Resources to Becoming a Data Engineer - KDnuggets,"Azure Data Engineers design and implement the management, monitoring, security, and privacy of data using the full stack of Azure data services to satisfy business needs. This certification is the final stage after a number of training modules have been successfully completed. Each module trains the user to become skilled in using Azure's suite of products to successfully become a data engineer on the platform. Each learning module takes less than one day and should not take more than 10 hours depending on the commitment of each person in any given time"
7 Resources to Becoming a Data Engineer - KDnuggets,The course begins with an introduction to Python and moves onto SQL which develops further into learning how to use PostgresSQL and Data Structures and Algorithms. It seems to have a more breadth view of the topics and centers around using Python and SQL. This is a good course for someone beginning their journey into the data engineering landscape but because of the course structure it seems to be useful to have some basic Python knowledge at the least
7 Resources to Becoming a Data Engineer - KDnuggets,The course by the University of California San Diego's course on Coursera centers around using the Hadoop framework and Spark and applying these big data handling techniques in a machine learning instance at the end. There is no programming experience required according to the course description. The course has been made in partnership with Splunk
How HR Is Using Data Science and Analytics to Close the Gender Gap - KDnuggets,"But the gender gap can also extend to the lack of equal representation in certain industries or career paths. There's an extraordinarily long way to go before people will be on equal footing in the labor market. Fortunately, human resources (HR) professionals can rely on data analytics to make progress"
How HR Is Using Data Science and Analytics to Close the Gender Gap - KDnuggets,"For example, a company's hiring history may make it biased. U.S"
How HR Is Using Data Science and Analytics to Close the Gender Gap - KDnuggets,"In other words, the tool might look at the characteristics that made a person succeed in a role, then cause an HR professional to repeatedly choose professionals who fit into those parameters. Problems can result, for example, if it just so happens that the majority of people who excelled in their roles were white men. Did that happen because they genuinely were better than others, or because the system never gave minorities a chance?"
How HR Is Using Data Science and Analytics to Close the Gender Gap - KDnuggets,"AI would not be a perfect solution. It, too, can show bias, mainly because human programming is at the heart of what makes AI work, and people aren't wholly non-judgmental. But with experimentation and hard work dedicated to improving AI algorithms, we may someday eliminate many of the types of unconscious bias in the workplace and hiring process"
How HR Is Using Data Science and Analytics to Close the Gender Gap - KDnuggets,"A data analytics platform can show useful statistics, such as the gender breakdown in particular parts of the company or the percentage of females in leadership roles at a company. Then, HR teams can understand if gender problems exist. If data analytics platforms uncover such issues, people can see where the problems lie, then get to work addressing them"
How HR Is Using Data Science and Analytics to Close the Gender Gap - KDnuggets,"As mentioned in the introduction, people often discuss the gender gap by citing wage discrepancies. For example, statistics revealed the earnings of corporate compliance officers. They showed that female compliance professionals"
How HR Is Using Data Science and Analytics to Close the Gender Gap - KDnuggets,"In another case, researchers built a big data tool to check the gender ratio of sources quoted online by Canadian news outlets. It keeps track of each instance in near real-time. Unfortunately, considering that the results showed"
How HR Is Using Data Science and Analytics to Close the Gender Gap - KDnuggets,When the financial brand U.S. Bank started
How HR Is Using Data Science and Analytics to Close the Gender Gap - KDnuggets,"Due to a tweaked process that applied predictive analytics early in and throughout the process, U.S. Bank found more of those desirable candidates in five weeks than it had during a year of using the old method. Concerning diversity, there was a 17% overall increase in diverse new hires and a 74% uptick in diverse top managers"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","Ultralearning data science begins with utilizing metalearning strategies. The first step is to create a map that suits your life. Don’t go ahead and plan out an impossible pathway that requires you to learn for 12 hours a day every week. That’s just unrealistic, and you’ll experience burn out. Be more pragmatic when you plan your journey: Take into account your personal responsibilities, hobbies, friends, and family, etc"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","Another key thing is to find your idiosyncratic way of learning. Everyone has their own learning methodology. Don’t try to be someone else and copy their way of learning. If you learn better through visualizations and videos, take MOOCs and go on YouTube. If you prefer the old-fashioned textbook-learning style, go ahead and do so. Nothing is stopping you from learning at your own pace"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","Distractions are everywhere. Without focus, it's almost impossible to learn. Having the discipline to control yourself and the self-awareness that distractions come from within, not externally, is the first step. Everything depends on you and you alone"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","Our brain has its limits. Don’t go pushing yourself too hard. When you just can’t get anything done, go take a walk, play the piano, talk to your friends (in person). Then try to focus again"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","With focus, learning becomes a bit easier. Now comes the hard part. Learning just by reading and memorizing is pointless. That’s the college methodology. It’s time to leave that behind and start learning effectively"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","A few key takeaways from this part is to practice application. Doing is substantially better than absorbing passively. We’ve all learned things by doing, ever since we were infants. We study the world and formulate our very own framework"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","Want to learn machine learning? Don’t start studying mathematics and stuff heaps of concepts and facts into your brain. Install an IDE (PyCharm/VS Code), search up YouTube videos/websites with tutorials on how to implement machine learning, and choose a language (preferably Python). That’s it — that’s all you need to get the essence of machine learning and the flow of it"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","By knowing the essence of something, novel solutions to complex problems can be engendered. To achieve deep understanding, start by getting the basics right, and understand it inside out. Only with this fundamental knowledge of the groundwork can a person start branching out into more in-depth concepts in the semantic tree"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","Another vital thing is to borrow from experts. Experts have a copious amount of experience in the field and are adept in their workflow. By replicating how an expert solves a problem from top to bottom, you’ll eventually acquire the skills and expertise through repeated replication"
"How To “Ultralearn” Data Science: summary, for those in a hurry - KDnuggets","Experimenting with data and pipelines is the underlying ingredient of data science. Models can be biased and filled with errors — only with perpetual experimentation with different features (feature engineering) and with algorithms can one improve a model. By putting constraints on which features to choose and hybridizing your ML tools, you can create a good model"
"How To “Ultralearn” Data Science: deep understanding and experimentation, Part 4 - KDnuggets","This will prepare you to deal with any kind of data thrown at you in the future. In your next data science project, try to use raw data, and clean it yourself. You’ll learn a lot"
"How To “Ultralearn” Data Science: deep understanding and experimentation, Part 4 - KDnuggets",You don’t know the answer to the question asked of you — or even if an answer exists. You don’t know how long it will take to produce a result or how much data you need. The easiest approach is to just come up with an idea and work on it until you have something
The Augmented Scientist Part 1: Practical Application Machine Learning in Classification of SEM Images - KDnuggets,"Welcome to the first blog in our Augmented Scientist series, where we will be looking at how machine learning can be used to improve the way that Scientists work on a day-to-day basis. As this series grows we’ll be looking at more applications for ML to work as a research aid for scientists, removing a lot of repetitive analysis work and allowing them to explore their fields deeper. Both my wife and I come from science backgrounds, her chemistry and me physics. What struck me when I started working on machine learning is the enormous opportunity to speed up much of the tedious analysis that consumes so much time in research labs"
The Augmented Scientist Part 1: Practical Application Machine Learning in Classification of SEM Images - KDnuggets,"The first area that we’re going to look at is SEM image analysis, a technique my wife, as an electrochemist, has worked with extensively throughout her career. SEM is used extensively in chemistry and biology to create images of surfaces at a nanometer scale. It works by scanning a surface with a focused electron beam. The reflection of electrons off the surface creates an image of the topography and composition of the surface"
The Augmented Scientist Part 1: Practical Application Machine Learning in Classification of SEM Images - KDnuggets,"Using the pretrained Inception-v3 model, implemented in Tensorflow, Modarres et al achieved an accuracy of ~90%, with a precision of ~80% and a recall@1 of ~90%. Modarres et al also reported that an imbalance in the dataset had no effect on the classifier’s performance in accurately predicting the under represented categories. Their confusion matrix, shown below, shows that the least populated categories, Porous Sponge, Films Coated Surface and Fibers, all performed quite well. Modarres et al credited this to the distinct patterns of these categories"
The Augmented Scientist Part 1: Practical Application Machine Learning in Classification of SEM Images - KDnuggets,"Wanting to see if we could achieve a higher accuracy than Modarres et al, we recreated their study with some of our own changes. First off we used the Resnet 50 implementation on the Fastai v1 framework and executed on a Colab GPU. To avoid overfitting, we adopted two approaches introduced by Fast"
The Augmented Scientist Part 1: Practical Application Machine Learning in Classification of SEM Images - KDnuggets,"Data augmentation is essentially altering/distorting each image, effectively creating a new image. Fastai v1 has a great tool called get_transforms that handles this process for us. The"
The Augmented Scientist Part 1: Practical Application Machine Learning in Classification of SEM Images - KDnuggets,"The concept is to crop the images in the dataset to a much smaller size, then after we’ve trained the model on the cropped images we increase their size and train the model again. We repeat this process a number of times, each time increasing the size of the images in the dataset. In this case we started with images of size 64x64 pixels. We trained the Resnet 50 on this dataset for a total of 15 epochs and then increased it iteratively to 128 and finally 224. Progressive resizing has two advantages. First off a lot of the early training is carried out on smaller images, which means the computational cost is lower and training takes less time. Secondly, repeatedly changing the size of the images and retraining the model can make it very difficult for the model to overfit. This means that your final model is more stable on new data it’s never seen before"
The Augmented Scientist Part 1: Practical Application Machine Learning in Classification of SEM Images - KDnuggets,"We achieved an overall accuracy of 94.5%, more than 4.5% of an increase on the previous state-of-the-art by Modarres et al, although our training process did take nearly 400 mins. Given that there is an imbalance in the data set, it is useful to consider metrics other than accuracy, such as"
The Augmented Scientist Part 1: Practical Application Machine Learning in Classification of SEM Images - KDnuggets,"Precision is essentially what proportion of the positive identification is actually correct, while recall is the proportion of actual positives which were identified correctly. Our model, while slightly over-fitting, achieved a precision of 94.2% and a recall of 91.8%. Comparing these to the values that Modarres et al reported of ~80% and ~90% respectively indicates that our model also performs better on the underrepresented categories in the dataset"
The Augmented Scientist Part 1: Practical Application Machine Learning in Classification of SEM Images - KDnuggets,"I hope you enjoyed the first blog post in our Augmented Scientist series. This series will explore more avenues where machine learning can be used to assist the work of Scientists across many disciplines. If you’re interested in getting involved, or have examples of where you’ve used machine learning to augment your own work please let us know, we’d love to hear from you"
What is a Data Scientist Worth? - KDnuggets,"2019 has been an eventful year in analytics, data science, and machine learning. New trends, new tools, new outlooks. We recently put together a trilogy of articles with the insights of a few dozen experts in order to map out the key events of 2019, and to lay out predictions on where things are headed in 2020 (and likely beyond). These article approach the subject matter from the differing points of view of research, technology, and industry. If interested, you can find these here:"
What is a Data Scientist Worth? - KDnuggets,"These IT professionals use their knowledge of statistics and modeling to make sense of complicated data from a variety of sources. To earn a midpoint salary of $125,250, data scientists need business savvy and communication skills in addition to statistics, mathematics and computer science expertise. Knowledge of programming languages such as Python or Java is often necessary for the job as well"
What is a Data Scientist Worth? - KDnuggets,"While thinking of data scientists as ""developers"" is somewhat problematic, there is undoubtedly a lot of crossover in the types of skills that a contemporary coding data scientist possesses and those which a developer does, on a technical level. That said, the comments on higher education acting as a primary (they allude to it being a sole) contributor to higher salaries for data scientists is not outlandish. However, these may be apples to oranges comparisons; we would need to have access to the raw data of the salaries of similarly highly educated data scientists as well as non-data scientist developers, otherwise normalized, in order to draw such conclusions"
What is a Data Scientist Worth? - KDnuggets,"So, what is a data scientist salary in 2019? As it turns out, that's a really good question. Of course, you probably knew coming into this that there would not be a single, magical number once we were finished. You do, however, now have some data to help make some reasonable forecasts as to what fair salaries would be under a variety of different circumstances. Let's see what 2020 brings to the data scientist salary discussion"
Alternative Cloud Hosted Data Science Environments - KDnuggets,"Major cloud providers such as AWS, GCP and Azure all offer a Data Science environment using a jupyter environment. For a while they were the only options for Data Scientists that needed the strong compute and storage capacity. Over the years new alternative providers have risen to provided a solitary data science environment hosted on the cloud for data scientist to analyze, host and share their work"
Alternative Cloud Hosted Data Science Environments - KDnuggets,"MatrixDS provides a data science environment with a social network type interface to share their work and receive reviews of their work as well. Users can easily add someone else to join their project to collaborate with their peers. The platform also allows you to fork another persons project like GitHub and has a private and public mode as well. We are able upload our files directly on to the platform or pull from GitHub, Amazon S3, Dropbox, or Google Cloud"
Fidelity on How to Find a Tailor-Fit Unicorn Data Scientist - KDnuggets,"Victor S.Y. Lo is a seasoned Big Data, Marketing, Risk, and Finance leader with over 25 years of extensive consulting and corporate experience employing data-driven solutions in a wide variety of business areas. He is actively engaged with causal inference and is a pioneer of Uplift/True-lift modeling, a key subfield of data science"
Fidelity on How to Find a Tailor-Fit Unicorn Data Scientist - KDnuggets,"For academic services, Victor has been a visiting research fellow and corporate executive-in-residence at Bentley University. He has also been serving on the steering committee of the Boston Chapter of the Institute for Operations Research and the Management Sciences (INFORMS) and on the editorial board for two academic journals. He is also an elected board member of the National Institute of Statistical Sciences (NISS). Victor earned a master’s degree in Operational Research and a PhD in Statistics, and was a Postdoctoral Fellow in Management Science. He has co-authored a graduate level econometrics book and published numerous articles in Data Mining, Marketing, Statistics, Analytics, and Management Science literature, and is completing a graduate level book on causal inference in business"
"How To “Ultralearn” Data Science: removing distractions and finding focus, Part 2 - KDnuggets","One example of this is setting a timer for five minutes. First, you promise yourself that you can stop working after those five minutes. This impetus provided by the promise that you stop in a short amount of time will usually keep a person going and continue working"
"How To “Ultralearn” Data Science, Part 1 - KDnuggets","But in reality, learning is tough. I say this because schools and colleges have turned into a business, and the act of learning has become a lost art. Students are merely studying to pass a test and no longer learning for the sake of learning. Ask a student “why” instead of “what” and they’ll likely struggle"
"How To “Ultralearn” Data Science, Part 1 - KDnuggets","To grasp the concept of big-picture understanding, let’s apply this concept to data science. Data Science is the sexiest job of the 21st century. But most data scientists jump into the field without thinking clearly about what data science is all about"
"How To “Ultralearn” Data Science, Part 1 - KDnuggets",Data science is not an easy field. One has to have business acumen and great communication to be able to translate data into something that people can understand and comprehend well. To be a data scientist is almost like a translator of the messy data in the world
"How To “Ultralearn” Data Science, Part 1 - KDnuggets","Now that you have brief exposure to ultralearning, metalearning, and big-picture understanding. Spend 5–10 minutes reflecting on whether you have been learning with these concepts in mind. If not, there no better time to change than now. Formulate your metalearning strategy according to by following the 4 steps below and start learning the right way today!"
Top 25 Session Highlights at ODSC East 2020 - KDnuggets,"ODSC East is back in Boston, Apr 13-17, 2020. Preliminary schedule is a unique collection of the leading experts and rising stars of data science. Register soon, as our 50% discount ends this Friday, Jan 31!"
Plotnine: Python Alternative to ggplot2 - KDnuggets,The style I would say is 99% similar to ggplot2 in R. The major difference would be the use of parentheses as you will see in a few short examples below. One of the best takeaways of using plotnine is that the output is basically the same as you would get in R. There is visually no striking difference
Plotnine: Python Alternative to ggplot2 - KDnuggets,As we go deeper we see that Plotnine gives us that simple API and stunning visuals we get from using ggplot2 in R. The ability to format plot with a single line of code is available in Seaborn but not in Matplotlib. Seaborn itself does have its similarities to Plotnine and ggplot2 in a way but the easily deciphered syntax is what gives it a unique selling point to make the switch
7 Data Trends for 2020 (and one non-trend) - KDnuggets,"We love data at Pipedrive. From the boldest business decision to the smallest UI tweak, we want to have just the right mix of data and intuition helping us. Pipedrive has been investing for years into data processing - here are the 2020 predictions from our data team"
7 Data Trends for 2020 (and one non-trend) - KDnuggets,"OK, you did it. You hired a bunch of statistics Ph.D. You should be well on track churning out great machine learning solutions. Except, there is a tiny problem with . You have petabytes of data. But it looks like it's not the right data. You need labeled data. Cleaned, profiled, and analyzed data. Shaped into a form suitable for machine learning algorithms. It slowly dawns that machine learning is 80% of data engineering. So besides a bunch of data scientists, you now need a four times bigger group of data engineers"
7 Data Trends for 2020 (and one non-trend) - KDnuggets,Aggregating company data into a central Data Warehouse has been serving mostly analysis functions. But data is useful everywhere; you want your customer service rep to have data on full customer history and the last ten activities. Data Warehouse has that data. You want your microservices to know where in the onboarding flow the customer is. Data Warehouse has that data. You want to know which product to recommend. Well. You know who has the answer. Providing operational support is a growing role for Data Warehouse
7 Data Trends for 2020 (and one non-trend) - KDnuggets,"During the gold rush, sell shovels. In 2019, not a week passed without contact from the company offering AI services. They come in all shapes and sizes and provide everything from consulting to ""5-minute integration, complete solution"" services. While there are companies who are providing valuable machine learning services, there is also plenty of snake oil out there, trying to cash in the hype. Be sure to kick the tires before you open your wallet"
7 Data Trends for 2020 (and one non-trend) - KDnuggets,There is a thorny issue on the location of the data team in the org chart. Where does it belong? Is it finance? Marketing? Engineering? Business Operations? No one knows for sure. It looks like we are coming to an agreement that data should be under data. There should be a Chief Data Officer who has a seat in the executive team and who takes care of a precious asset called data
7 Data Trends for 2020 (and one non-trend) - KDnuggets,"We are restless. The pulse of the time is an all-time high. If something happens, we need to know now! Yes, it's cheaper and more comfortable to load new data during your overnight loading window, but the business never sleeps. Realtime data is here to stay"
7 Data Trends for 2020 (and one non-trend) - KDnuggets,"There are capable visualization tools that still look like they stepped out from Excel 3.0. I'm looking at you, matplotlib! While capable is good, and correctness is mandatory. Time and again, flashy beats correct and useful. Let's make our visualizations beautiful. People like beautiful"
7 Data Trends for 2020 (and one non-trend) - KDnuggets,"We need our profit charts on mobile too. Right next to our LinkedIn or TikTok app. It has been taking way too long for technology providers to get there. Probably because it's very hard. But we are getting there. Both Tableau and Power BI have capable mobile client and development tools available. Everything is ready. Hopefully, you can read your daily business performance report instead of the back of the shampoo bottle while away from your desk"
7 Data Trends for 2020 (and one non-trend) - KDnuggets,"Nope. It has not happened in the past 20 years. Not going to happen next. Every year, it's just around the corner. A little bit more and everyone can ask all the questions in their mother tongue, and the system spits up answers. Look, there is a reason why a good analyst is worth its weight in gold. Answering questions like ""What was the profit last quarter in Germany?"" is not self-service BI. It's a bar chart with extra steps. Understanding the domain, finding hidden patterns in from a large amount of data, and presenting complex results in an easy to understand way is still far away from machine capabilities"
The Decade of Data Science - KDnuggets,"After I completed my data science course, I covered market trend analysis, time series, recommender systems, A/B testing, CNNs, computer vision, natural language processing, categorical and continuous modeling, unstructured clustering, and more. I utilized mostly python coding and SQL to complete these projects. These skills can be applied to virtually any topic which is why companies across all industries are hiring data scientists who need domain knowledge"
The Decade of Data Science - KDnuggets,"I decided to join the data science field for several reasons. I have a business degree and a background in finance and real estate. Since I graduated from college, I noticed that the preferred qualification sections of my target jobs were more and more populated by knowledge of python, SQL, and machine learning. I was also working with large data sets that excel could no longer handle. I wanted to become as competitive and relevant as possible so I began to look for programs that would cover these skills. After a significant amount of research, I decided to explore data science"
The Decade of Data Science - KDnuggets,"Data Science is statistics merged with computer science, among other things. The Python coding language has a growing number of packages that enable data scientists to wrangle data and create models. The libraries I’ve used most often are (in no particular order):"
The Decade of Data Science - KDnuggets,"In numerous projects, I would create a column of arrays in a pandas data frame. I would then need to save the data frame and retrieve the data later on. No matter what file type I used, whenever I read in the saved file the arrays column would load as a string type with literal spaces ("
The Decade of Data Science - KDnuggets,I then added brackets to the string to make it look like an array. Once the string reads like an array I applied the ast method to the string and obtained an actual array. The statement which returned the array is as follows:
The Decade of Data Science - KDnuggets,"Speed matters, especially during live demonstrations. I completed a facial similarity program that compared a user’s face embeddings to a database of embeddings of celebrities and politicians. I used the cosine similarity metric to compare the users to the database in a live demonstration. I originally utilized"
The Decade of Data Science - KDnuggets,I searched for the bottleneck by putting text print statements after each function. I soon discovered that the calculation of cosine similarity was taking far too long. I read about the speed of NumPy calculations and decided to switch to a pure NumPy implementation of the calculation after reading Danushka Bollegala’s
The Decade of Data Science - KDnuggets,"The TQDM status bar tells you which iteration the function is completing, the total number of expected iterations (helpful for locating problems if the function breaks), the total time spent, the expected remaining time, and the number of iterations per second. TQDM works well with Jupyter Notebooks. The NumPy implementation of cosine similarity was 75% faster than the Scikit-learn implementation on my data set"
The Decade of Data Science - KDnuggets,Applying containerization to data science projects can be tricky because of how abstract and customizable a container is. The most common container I’ve interacted with is one that holds a SQL database with python scripts that run queries. Docker’s
The Decade of Data Science - KDnuggets,"The project was presented to me in an interesting way. We agreed that the majority of a data scientist’s time is spent obtaining and cleaning data, and then asking the right questions. Building the actual model takes comparatively less time and is not as hard as the previous parts of a project. If you are working on a categorical project, you will build several models and use a tool like"
The Decade of Data Science - KDnuggets,You will then select a metric to compare the models and select the most effective one for your purposes. This type of model building depends on your ability to select the best range for each parameter and patience for waiting for the GridSearch to locate the optimal hyperparameter values. OpenScale takes this process to the next level by essentially working as a cloud-based GridSearch of categorical models
The Decade of Data Science - KDnuggets,"The outlook for the data science field is strong. There are always newer tools being released that I look forward to using. This field covers a growing amount of topics, which means there is always something new to learn. Data science can be applied to any industry for multiple purposes and can create value for any organization if you ask the right questions"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"The field of Data Science is growing with new capabilities and reach into every industry. With digital transformations occurring in organizations around the world, 2019 included trends of more companies leveraging more data to make better decisions. Check out these next trends in Data Science expected to take off in 2020"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"Data Science has become an integral part of those transformations. With Data Science, organizations no longer have to make their important decisions based on hunches, best-guesses, or small surveys. Instead, they’re analyzing large amounts of real data to base their decisions on real, data-driven facts. That’s really what Data Science is all about — creating value through data"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"Data is giving companies a sharp advantage over their competitors. With more data and better Data Scientists to use it, companies can acquire information about the market that their competitors might not even know existed. It’s become a game of Data or perish"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"Even in today’s digital age, Data Science still requires a lot of manual work. Storing data, cleaning data, visualizing and exploring data, and finally, modeling data to get some actual results. That manual work is just begging for automation and thus has been the rise of automated Data Science and"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"In general, companies are investing heavily in building and buying tools and services for automated Data Science. Anything to make the process cheaper and easier. At the same time, this automation also caters to smaller and less technical organizations that can leverage these tools and services to have access to Data Science without building out their own team"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"Privacy and security are always sensitive topics in technology. All companies want to move fast and innovate, but losing the trust of their customers over privacy or security issues can be fatal. So, they’re forced to make it a priority, at least to a bare minimum of not leaking private data"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"The entire Data Science process is fueled by data, but most of it isn’t anonymous. In the wrong hands, that data could be used to fuel global catastrophes and upset everyday people’s privacy and livelihood. Data isn’t just raw numbers as it represents and describes real people and real things"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"As we see Data Science evolve, we’ll also see the transformation of the privacy and security protocols surrounding data. That includes processes, laws, and different methods of establishing and maintaining the safety, security, and integrity of data. It won’t be a surprise if cybersecurity becomes the new buzzword of the year"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"The volume of data that a typical Fortune 500 company might need to analyze has gone far past what a personal computer can handle. A decent PC might have something like 64GB of RAM with an 8 core CPU and 4TB of storage. That works just fine for personal projects, but not so well when you work for a global company such as a bank or retailer who has data covering millions of customers"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"Huge advancements in NLP through Deep Learning are fueling the full-on integration of NLP into our regular Data Analysis. Neural Networks can now extract information from large bodies of text incredibly quickly. They’re able to classify text into different categories, determine sentiment about a text, and perform analysis on the similarity of text data. In the end, all of that information can be stored in a single feature vector of numbers"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"As a result, NLP becomes a powerful tool in Data Science. Huge datastores of text, not just one-word answers but full-on paragraphs, can be transformed into numerical data for standard analysis. We’re now able to explore datasets that are far more complex"
The 4 Hottest Trends in Data Science for 2020 - KDnuggets,"For example, imagine a news website that wants to see which topics are gaining more views. Without advanced NLP, all one could go off of would be the keywords, or maybe just a hunch as to why a particular title worked well versus another. With today’s NLP, we’d be able to"
Data Science Curriculum Roadmap - KDnuggets,"We venture to suggest a curriculum roadmap after receiving multiple requests for one from academic partners. As a group, we have spent the vast majority of our time in industry, although many of us have had spent time in one academic capacity or another. What follows is a set of broad recommendations, and it will inevitably require a lot of adjustments in each implementation. Given that caveat, here are our curriculum recommendations"
Data Science Curriculum Roadmap - KDnuggets,"We want to lead by emphasizing that the single most important factor in preparing students to apply their knowledge in an industry setting is application-centric learning. Working with realistic data to answer realistic questions is their best preparation. It grounds abstract concepts in hands-on experience, and it teaches data mechanics and data intuition at the same time, something that is impossible to do in isolation"
Data Science Curriculum Roadmap - KDnuggets,"The curriculum recommendations for each of these program archetypes will be different. However, all of them will share some core topics. Then analytics, engineering, and modeling-centric programs will have additional topic areas of their own. A general curriculum will include some aspects of the analytics, engineering, and modeling curricula, although perhaps not to the same depth. It is common for students to self-select courses from any combination of the three areas"
Data Science Curriculum Roadmap - KDnuggets,"Curricula for domain specific programs look similar to a general program, except that topics, and even entire courses, will be focused on specific skills common to the area. For instance, an actuarial-focused data analytics program would likely include software tools most commonly used in insurance companies, time series and rare-event prediction algorithms, and visualization methods that are accepted throughout the insurance industry. The student can best practice their skills through a project based on real domain-specific data. Hands-on projects or internships are highly recommended. When designing the programs, institutions may also consider offering interdisciplinary degrees and programs. Domain specific programs often combine courses from multiple departments or colleges"
Data Science Curriculum Roadmap - KDnuggets,"Note that for each topic and subtopic, there are many effective ways to split it into courses. The best way for your institution will depend on many factors, including length of term, hours per class, existing departmental boundaries, instructor availability, and the rate at which your students are expected to absorb information. These recommendations assume a two-year masters program with the primary goal of preparing students for employment and continued career growth, although they can certainly be scaled up or down to fit the scope of other programs"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","In particular, continued fears of AI were mentioned more than once, and this prediction certainly seems to have panned out. Talk of advances in automated machine learning was prevalent, though opinions were split as to whether it would be useful or would falter. I think the jury is still out on this to some degree, but when expectations of the technology are tempered it becomes easier to see it as a useful addition as opposed to a looming replacement. Increased AI for good was also singled out, for good reason, and there are myriad examples to point to the accuracy of this prediction. The idea that practical machine learning would have a reckoning was put out there, signalling that fun and games is coming to an end and it's now time for machine learning to put up. This rings true, with anecdotal evidence of practitioners seeking out these opportunities mounting. Finally, mention of the increased concern surrounding dystopian AI developments regarding surveillance, fear, and manipulation can confidently be added to the successful predictions category by a simple spot-check of the past year's news"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets",In 2020 Data Science teams and commercial teams will be more integrated. 5G will act as a catalyst for the growth of an intelligent IoT with AI inferencing on the edge meaning that AI will increasingly enter the physical world. Deep Learning combined with Augmented Reality will transform the customer experience
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","I think it is hard to argue against the fact that this has been the year of Deep Learning and NLP. Or more concretely, the year of language models. Or even more concretely the year of Transformers and GPT-2. Yes, it might be hard to believe, but it has been less than a year since OpenAI first"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","Researchers aimed to develop a better understanding of deep learning, its generalization properties, and its failure cases. Reducing dependence on labeled data was a key focus, and methods like self-training gained ground. Simulations became more relevant for AI training and more realistic in visual domains such as autonomous driving and robot learning, including on NVIDIA platforms such as DriveSIM and Isaac. Language models went big, e. NVIDIA’s 8 billion Megatron model trained on 512 GPUs, and started producing coherent paragraphs. However, researchers showed spurious correlations and undesirable societal biases in these models. AI regulation went mainstream with many prominent politicians voicing their support for ban of face recognition by Governmental agencies. AI conferences started enforcing a code of conduct and increased their efforts to improve diversity and inclusion, starting with the NeurIPS name change last year. In the coming year, I predict that there will be new algorithmic developments and not just superficial application of deep learning. This will especially impact “AI for science” in many areas such as physics, chemistry, material sciences and biology"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","In 2019, we have valued the impressive capabilities of Deep Learning models, such as YOLOv3, for various complex computer vision tasks, particularly for real-time object detection. We have also seen Generative Adversarial Networks continue to attract interest in the Deep Learning community for image synthesis with the BigGAN model on ImageNet generation, and the StyleGAN for human image synthesis. This year we have also realised how easy it is to fool Deep Learning models, some studies also show how deep neural networks are vulnerable to adversarial examples. In 2019 we have also seen biased AI decision-making models being deployed for facial recognition, hiring, and legal applications. In 2020, I expect to see development in multi-tasking AI models which are designed to be generic and multi-purposed, and I also expect to see increased interest in developing ethical AI models, since AI is changing decision making in the health, financial services, automotive and many other sectors"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","This also reflects my personal view i. 2019 was the year of  cloud maturity. It was a year when the various technologies we speak of (Big Data, AI, IoT etc) came together within the framework of the cloud. This trend will continue – especially for the enterprise. Companies will undertake ‘digital transformation’ initiatives – where they will use the cloud as a unifying paradigm to transform processes driven by AI (kind of like reengineering the corporation 2.0)"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","In 2020, I also see NLP maturing (BERT, Megatron). 5G will continue to be deployed. We will see wider applications of IoT when 5G is fully deployed (ex: self-driving cars) beyond 2020. Finally, on the IoT front, I follow a technology called MCU (Microcontroller units) – specifically the deployment of machine learning models o MCUs"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","BERT, ELMO, GPT2, and all that! AI in 2019 saw huge advances in NLP. OpenAI released their big GPT2 model--i. DeepFakes for text. Google announced using BERT for Search -- the biggest change since Panda. Even my collaborators at UC Berkeley released (quantized) QBERT for low footprint hardware. Everyone is making their own document embeddings now"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","What does this mean for 2020. According to Search experts, 2020 will be the year of Relevance *(uh, what have they have been doing?). Expect to see vector space search finally gaining traction, with BERT-style fine-tuned embeddings"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","Under the hood, in 2019, PyTorch overtook Tensorflow as the choice for AI research. And with the release of TensorFlow 2. AI coding in 2020 will be all about eager-execution"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","Are big companies making progress with AI ? Reports indicate a 1 in 10 success rate. Not great. So AutoML will be in demand in 2020, although I personally think, like making great search results, successful AI requires custom solutions specific to the business"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","Everyone is opting for ""DIY AI"" instead of cloud solutions. One factor driving this trend is the success of transfer learning, which has made it easier for anyone to train their own models with good accuracy, fine-tuned to their very specific use case. With one user per model, there's no real economy of scale for a service provider to exploit. Another advantage of transfer learning is that datasets don't need to be as large anymore, so annotation is moving in-house as well. The in-housing trend is a positive development: commercial AI is a lot less centralized than many people thought it would be. A few years ago, people worried that everyone would get ""their AI"" from just one provider. Instead people aren't getting their AI from any provider – they're doing it themselves"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","The major advancements in the world of Artificial Intelligence in 2019 have been in the areas of Auto-ML, Explainable AI and Deep Learning. Democratization of Data Science remains a key aspect since the last couple of years and various tools and frameworks pertaining to Auto-ML are trying to make this easier. The caveat still remains that we need to be careful when using these tools to make sure we don’t end up with biased or overfit models. Fairness, accountability and transparency still remain key factors for customers, businesses and enterprises to accept decisions made by AI. Hence Explainable AI is no longer a topic just restricted to research papers. A lot of excellent tools and techniques have started making machine learning model decisions more interpretable. Last but not the least, we have seen a lot of progress in the world of Deep Learning and Transfer Learning especially for Natural Language Processing. I expect to see more research and models coming up in 2020 around areas of Deep Transfer Learning for NLP and Computer Vision and hopefully something which looks at taking the best of Deep Learning and Neuroscience which can lead us towards true AGI"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","I expect that the main ML development trends of 2020 will continue within NLP and computer vision. Industries adopting ML and DS have realised that they are overdue defining shared standards for best practices in hiring and retaining data scientists, managing the complexity of projects that involve DS and ML, and ensuring the community remains open and collaborative. Thus we should see more focus placed on such standards in the near future"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2019 and Key Trends for 2020 - KDnuggets","What does 2020 have in store for AI? We'll see further advances in conversational AI, as well as better generation of images and video. Those advances will raise even greater concerns around malicious applications, and we'll probably see a scandal or two, especially in an election year. The tension between good and evil AI isn't going away, and we'll have to learn better ways to deal with it"
A Non-Technical Reading List for Data Science - KDnuggets,"The world still cannot be reduced to numbers on a page because human beings are still the ones making all the decisions. So, the best data scientists understand the numbers and the people. Check out these great data science books that will make you a better data scientist without delving into the technical details"
A Non-Technical Reading List for Data Science - KDnuggets,"O’Neil’s book, released in early 2016, is needed now more than ever before. The end of 2016 saw the devastation wreaked on the American democratic process by Russian actors who took advantage of Facebook’s algorithms to spread propaganda. Far from being an academic exercise, these actions had real-world consequences, raising into question the legitimacy of elections in the United States"
A Non-Technical Reading List for Data Science - KDnuggets,"Algorithms are only going to play a larger role in our daily lives moving forward. Already, where we go to school, what we read, whether we are approved for a loan, if we get a job, and what we buy are all decided to a significant extent by algorithms we have no control over and cannot query for an explanation. O’Neil’s book may seem pessimistic about machine learning models, but I like to think of it more as a necessary criticism: with so much unbridled enthusiasm surrounding machine learning, we need people who are willing to take a step back and ask: are these tools really improving peoples’ lives and how should we as a society adopt them?"
A Non-Technical Reading List for Data Science - KDnuggets,"Machine learning algorithms are just tools, and as with many tools, they can be used for good and bad. Fortunately, we are still at an early stage which means we can shape the use of models to ensure they work towards making objective decisions and creating the best outcomes for the greatest number of people. The choices we make now in this regard will shape the future of data science in the decades to come, and it’s best to go into these debates well-informed"
A Non-Technical Reading List for Data Science - KDnuggets,"Computer science and statistics (and every other field of study) suffer from one problem when they are taught in school: they are boring in the abstract. It’s only when they are applied to real-world problems that they become interesting enough to make us want to understand. Both of these books do an incredible job of transforming dry subjects into entertaining and informative narratives about how to use algorithms, stats, and maths in our daily lives"
A Non-Technical Reading List for Data Science - KDnuggets,"If you haven’t realized it yet, then here’s a useful lesson: humans are irrational, and we routinely make terrible decisions in all aspects of life. However, there is a reason for hope: once we understand why we don’t act optimally, we can start to alter our behavior for better outcomes. This is the central premise of Kahneman’s masterwork documenting decades of experimental findings"
A Non-Technical Reading List for Data Science - KDnuggets,"Using System 1 is natural, and we have to overcome millions of years of evolution to employ System 2. Even though it’s difficult, in our data-rich world, we need to spend time honing our System 2 thinking. Sure, we may sometimes run into problems with overthinking, but underthinking — using System 1 instead of System 2 — is a far more serious problem"
A Non-Technical Reading List for Data Science - KDnuggets,"The experiencing self is the moment-to-moment feelings we have during an event but is much less important than the remembering self which is our perception of the event afterward. The remembering self rates an experience according to the Peak-End rule which has profound implications for medicine, life satisfaction, and forcing ourselves to do unpleasant tasks. We will remember events for far longer than we experience them, so it’s crucial that during an experience, we try to maximize the future satisfaction of our remembering self"
A Non-Technical Reading List for Data Science - KDnuggets,"We should not only expect world-changing events to happen with high frequency, but we should not listen to experts who are constrained by what has occurred in the past. As anyone who invests in the stock market should know, past performance is no predictor of future performance, a lesson we’d be wise to consider in our data science models (which use past data). Also, our world is not normally distributed, but instead fat-tailed, with a few extreme events — the Great Recession — or a few wealthy individuals — Bill Gates — overshadowing all the others. When extreme events occur, no one is prepared because they far exceed the magnitude of any previous ones"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"Everyone, starting from e-commerce websites to social networks need to prevent fraud and provide a high level of security for the visitors. The platforms do their best to get the trust of their visitors. Safe and trusted platforms are expected to be actively visited by a broad spectrum of people eager to communicate, buy, learn, etc. There is nothing strange that the words trust and safety are used together so often. These terms are closely interrelated by their nature. Trust is a multidimensional concept. Trusting to a brand, source, etc"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"Consumers' expectations are growing extremely fast. All the customers are willing to get excellent experience regularly. Thus, the establishment of digital trust and safety has become essential for numerous service and product providers. C-suite has to make its best to organize an effective fraud management system, as nowadays staying ahead of fraud is as tricky as never before"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,Fraud and cybercrime are using advanced technologies and techniques which are difficult to detect. Sophisticated mechanisms improving the ways to perform illegal activity online are under constant development. Years dedicated to the establishment of customers' trust may become just a waste of time after a trust violation incident
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"The ever-growing demand in personal data requires development of strategies to improve trust and safety. Just think about it. Companies want their customers to make payments to someone they do not know, and share with them personal details including credit card numbers, etc. Trust and safety teams have become constituent - an inevitable part for many companies and organizations. These teams usually think about customers’ data protection and prevention of fraudulent activity"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"Companies can no longer expect to reach high sales quickly. Customers get even more sophisticated in their purchasing habits and decisions every day. Wide variety of goods and services on offer, high quality of these goods and services and various price offers have a considerable influence on the decision making process"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"Price is a significant management decision. Three types of factors should be taken into consideration. These are a willingness to pay, incremental profit, objectives and constraints.  Intelligent price optimization includes big data analytics solutions helping to predict customers’ response and reaction to price changes with the objective to maximize sales and profitability"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,Security is of great importance in price optimization. The reliability of the calculations is one of the most critical issues. Protection from the price spirals is presented here as well. Various smart predictions and warning mechanisms are applied within the scope of price optimization to predict and detect negative effects
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,Trust is fundamental for interaction. Trust in AI may considerably improve the quality of customers` experience. The AI engines and algorithms may be trained to perform various tasks and facilitate the process of Trust and Safety improvement
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"Introduction of AI into trust and safety improvement process has opened new opportunities for companies and organizations. Thanks to AI-powered software they now can detect and ban users for suspicious activity even before they make a fraud. Besides, there appears an option to ban the IPs coming from the same source, in case this source is regarded as unreliable"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"AI models may be tuned to detect various factors using both the internal and external data. Thus, the companies can prevent payment fraud, account takeover or account abuse. Moreover, the introduction of AI has resulted in the reduction of bias"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"In recent years the chatbots have significantly improved and reached the stage when it is difficult to distinguish between a chatbot and a human in the course of a dialogue. Chatbots are widespread. Therefore, vast amounts of personal data are transferred back and forth by the chatbots every minute"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"Collection of the personally identifiable information is crucial for the chatbots operation. As a result, they may become subjects for attacks or fraud. In such a case, the limitations and privacy boundaries must be precisely defined. Trust is critical in situations when the trustor largely depends on a trustee. Reliability of the chatbots may be determined according to the trust score. Trust Score is a mechanism that is a transaction registry containing successfully verified transactions"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"Cybersecurity is a matter of a somewhat higher level. Cybersecurity covers a wider scope of issues and problems. Its crucial task is to provide security to all valuable data. Therefore, trust and safety and cybersecurity are inter-dependable. Cybersecurity disposes of a vast number of techniques and measures to ensure integrity, confidentiality, and availability of data. The matter of trust in all these techniques and massive data management and storage systems as a whole has never been as topical as it is now. The stakes get higher every day when it comes to matters of personal and financial data security. Companies and organizations face the need to find new ways to provide a sufficient security level and encourage customers to trust"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"Besides, the analytics helps to define where the trust is lost. Big data analytics offers an opportunity to monitor and analyze the processes that previously were hidden from the companies. Big data security tools cover the scope of security information and event management technologies, and performance and availability monitoring technologies. Due to this fact, a big data analytics tool can quickly discover devices on the network and support incident response workflow"
Top 7 Data Science Use Cases in Trust and Security - KDnuggets,"We live in the age when personal data gets publicly available due to active usage of various websites, platforms, media and networks. The data gained via processes of registration, entering different systems, online payments, etc. Moreover, this information presents interest to the fraudsters and your company competitors"
"Open Source Projects by Google, Uber and Facebook for Data Science and AI - KDnuggets","Open source is becoming the standard for sharing and improving technology. Some of the largest organizations in the world namely: Google, Facebook and Uber are open sourcing their own technologies that they use in their workflow to the public. This has allowed the common person to utilize technologies that are used in the biggest companies in the world. Probably the most well-known open source projects are PyTorch and Tensorflow (both coincidentally being the de-facto standard for Deep Learning)"
The Future of Careers in Data Science & Analysis - KDnuggets,"The need for new data scientists and data analysts is growing by the year, and bright minds are needed to fill the gaps. These fields are complex and sophisticated, but any individual who can harness the necessary skills will have many career options from which to choose. To better understand the possibilities, let’s look at the nuances of these positions and where you could work in the future"
The Future of Careers in Data Science & Analysis - KDnuggets,"The scientist is responsible for understanding how data could help any particular field. Once they understand which data they need to find, models are created, and systems are programmed so that this data can be unearthed. Finally, they must know how to separate and interpret the data to verify that what they found is as useful as they hope"
The Future of Careers in Data Science & Analysis - KDnuggets,"They work in a variety of industries to either solve problems or predict trends and then provide their statistical analysis to management and stakeholders in an easily digestible format. Data analysts need to understand numbers. They need to be able to use critical thinking and see through the details to come up with logical conclusions, and if they need more info, they have to communicate their needs clearly to the data scientist"
The Future of Careers in Data Science & Analysis - KDnuggets,"Every single thing that consumers do creates data. Whether it is going to the doctor, buying groceries, or watching videos online, and all of that data is crucial for the creation and advancement of a company's products. While this is a known fact to many business professionals, a manager or CEO may not comprehend how to obtain and present this information, and that is why the careers of data analysts and data scientists are so highly sought after"
The Future of Careers in Data Science & Analysis - KDnuggets,"Both professions are highly requested by any company that has a marketing division. The skills can be used to determine what customers are buying and what they are interested in purchasing in the future so that better marketing campaigns can be devised. After the campaign is created, data can then measure how the customers are taking to the marketing, and if they are buying what the company is trying to sell. One example is Netflix, which often sends out emails recommending a particular movie or TV show. They create these suggestions by going through the data and finding out what interests subscribers, with the intended end result of getting you back on their platform"
The Future of Careers in Data Science & Analysis - KDnuggets,"Every business needs to have a risk management plan in place that categorizes potential damage to products or customers, along with a plan of action to solve the issues. Data analysts and scientists help with this too. For example,"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"The argument was that this would enable a “360-degree view of the customer” or “data-driven” decisions. After millions of dollars and multi-year implementations, many companies are hard-pressed to quantify the business benefits of these initiatives. The telling term here is these efforts were supposed to ‘enable’ analytics, rather than deliver value"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"The ‘data first’ enterprise is misguided from a data science perspective. First, there is an infinite amount of useful data out there. Second, rigorous data science works from the top down — from objective to the dependent data. Accordingly, the first thing you have to do is define a business objective and metric"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"We are now a few years into an AI-hype bubble. Every consulting firm, system integrator, IT firm, software provider, even tax and audit firm — is now claiming expertise. They all profess to occupy prime real estate in a magic quadrant. When focused on delivering value, a well-conceived data science project should be able to break even within 6 months. How do you sift through the mathematics, visualizations, and magic demos to choose an analytic partner who can deliver on this promise?"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"In this article, we argue analytics is no different from any other business endeavor and can be assessed and managed accordingly. This article offers suggestions for both phases. The first part highlights key questions to discriminate and evaluate vendors at the proposal phase. The second part highlights some best practices in managing an engagement for success"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"Most large organizations run on inertia. They also tend to value only what they know, leading them to conflate their legacy practices with Data Science. If their legacy practices were selling mainframes, they are now pushing the cloud. System integrators and consultancies continue to sell complex integrations and consulting. The Big 4 tend to sell BI and reporting tools. Others are selling ‘platforms. There may be a need for many of these things, but their costs should be justified by empirical analysis, in other words,"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"Many firms have simply rebranded their services and staff as ‘Data Science. Try looking up their profiles on Linkedin or Google Scholar. It is surprising how many organizations have literally no scientists on staff, or claim to have “a bunch of data scientists off-shore, somewhere"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"In an illuminating interview, a Big 4 Senior Partner of Artificial Intelligence admitted that his team has no data assets, no analytics assets, no data scientists, and one successful consulting project in two years. He went on to boast he has never hired a data scientist that was over 26 years old. His practice lead did not even have a college degree. Meeting the full team over dinner, the Global Head of Artificial Intelligence regaled us with stories about how socially awkward scientists are. When asked how can his firm compete with other providers, he explained his strategy: “[w]e are a trusted professional services partner. We are already embedded in their business. We can do their data science as well"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"Contrary to the current zeitgeist, the industry is not suffering from a shortage of skills or junior resources. But there is certainly a shortage of leaders with a deep understanding of the underlying mathematics and track record of successful data science solutions. Most engagements also require a field engineering lead — someone to work directly with the Business and Operations lead, to capture the process flows and business constraints, IT, decision points, ultimate source of both input and outcome data. No one wants to run a project through intermediaries, across time zones, etc. It adds a great deal of confusion, delay, and overhead and puts delivery at risk"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"A sound, technical approach is necessary, but of course not a sufficient condition for success. For example, commercially successful fraud detection solutions have employed a wide variety of advanced algorithms, including anomaly detection, network analysis, graph theory, cluster analysis, number theory, decision trees, neural networks, linear programming, and Kalman filtering. Figure 1 compares the performance of two real-time fraud solutions. The incumbent solution (blue) combines expert rules with optimized decision trees. The challenger solution (yellow) combines temporal signal processing, NLP, and neural networks. All three approaches have credible analytical and theoretical foundations; the only way to resolve which approach is superior is an empirical test"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"There really is no excuse for not being able to quote performance. Data science implies a disciplined, empirical approach to business problems. Performance and business benefit over the BAU practice can be directly calculated on the data, or if necessary, tested in a champion/ challenger live rollout"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"This question should rightly raise alarm, but a surprising number of solutions on the market have never actually been tested on live data or were developed on ancillary or unrelated datasets. To a data scientist, this is literally inconceivable, but solutions built on “synthetic data” are common in legacy software companies, as the focus historically has been on establishing a standardized API, rather than extracting value from the data. Many firms do not even secure access their client data, so literally cannot validate whether their solution can deliver value. Such systems are often essentially rules engines, and can severely limit the sophistication and value of downstream decisioning technologies"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"A frustrating fact of life is that customers are not always willing to serve as a reference site. Given the sensitivity of some projects, this is understandable. However, if a promising vendor does not have an “Alpha” deployment and its technical approach and team seem credible, you have a unique opportunity to negotiate price. Being a public reference site and data research rights are all assets that are traded for services. Entering into a co-development agreement allows you to build out bespoke new functionality at a discount. At many of the large consultancies, the Data Science teams have been running as a loss for many years, and they will be eager to publicly prove their"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"From the onset, the project should be overseen by a standing committee of the key stakeholders (typically the P&L owner, line of business or product owner, the operations lead, and an analytics lead). Internal IT team leaders can be used to conduct due diligence, But IT departments typically do not have data science skills and can ‘cost’ a project out of existence (inflating the implementation cost estimates) no matter how trivial, if they don’t understand the mission or the technologies being used. Analytics teams can actively or passive sabotage an objective test, by non-cooperation. Another route is to engage a third-party advisor to conduct vendor due diligence"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"As much as practicable, the engineering objective should be defined by the business metrics (profit, revenue, costs, losses, incidence rates, conversion rates, and so on. Clear metrics also simplify due diligence, establishing concrete client expectations and a ‘success criteria’ for the vendor. Two examples of poor choices for proof-of-concept goals are predicting customer attrition or creating a customer segmentation. Neither of these efforts has any direct business benefit"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"Any business outcome or KPI can be used as a target, and if it can be measured, it can be predicted. In a full data diagnostic, the information value of current and potential data sources can be measured against these metrics. Even the value of an “Art of the Possible” POC can be simply and clearly stated in terms of cost reduction or revenue opportunities identified"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"Within 2–3 weeks of providing access to the data, an interim review should be scheduled to review the preliminary results. By this time, the vendor should have been able to verify if the objective is supported by the data and provide a guarantee of minimal performance. On the other hand, in the course of analysis, the vendor may have discovered and recommended alternative objectives and priorities"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"This initial report, sometimes called a ‘Diagnostic,’ or ‘Sizing and Opportunity Analysis’ — in and of itself — should be viewed as a deliverable. Often, deep empirical analyses of efficiencies, performance drivers, and root causes produce value-added recommendations of policies and processes that do not require a predictive analytic solution. In this sense, such ‘actionable insights’ are a bonus, collateral benefits of a data science engagement. While there is no guarantee that such ‘quick fixes’ exist, typically the benefits of implementing these recommendations can exceed the entire project cost"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"Some firms will try to recoup their costs at this stage, by overselling platforms and infrastructure. Several industries heavily rely on decades-old decision engines. Replacing these systems is an expensive proposition and often not necessary for data science delivery. An infrastructure agnostic scoring engine can be used to create customer decisions, which in turn, can be ‘pushed’ into legacy decision engines, loaded as a table into a database, or fed into existing BI tools. This minimally invasive approach, working in parallel with production data flows or systems of record, is both the fastest course to value and the lowest cost. Enhancements and added functionality are relatively painless, as the Data Science delivery team has ongoing ownership of the engine"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"The potential of data science continues to be diluted by ill-conceived initiatives and pretender practitioners. The keys to success are to conduct rigorous due diligence, define the business problem, establish clear metrics, and run a proof of value. There is gold in these hills, but be careful of whom you choose to prospect with"
Would you buy insights from this guy? (How to assess and manage a Data Science vendor) - KDnuggets,"He has served as Scientific Advisor for several prominent analytics firms, including IBM, KPMG, Opera Solutions, NICE/Actimize, HCL, HNC Software, Mastercard Europe, JP Morgan Chase, and Halifax Bank of Scotland. He has a Ph.D"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"Data science is mostly applied in marketing areas of profiling, search engine optimization, customer engagement, responsiveness, real-time marketing campaigns. Moreover, new ways to apply data science and analytics in marketing emerge every day. Among these, the new use cases include digital advertising, micro-targeting, micro-segmentation, and many others"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"All customers are individuals. Therefore, a one-size-fits-all approach is not efficient at all.  Customer segmentation comes to the rescue of the marketers in this case. Application of the statistical analysis allows marketers to slice the data and group customers"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"Application of micro-segmentation appears to be a rising trend in marketing. Micro-segmentation is far more advanced. It helps to segment people into more precise categories especially concerning behavioral intentions. Thus, marketing actions may be tailored to the preferences even of the least numerous customer groups"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"Customer data provides insights into customers’ wants, preferences, and needs. Operational data reflect various transactions, actions, and decisions made by the customers. Application of real-time data analysis brings efficiency, speed and high-performance rates to marketing campaigns"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"Predictive analytics is the application of statistical and machine learning algorithms to predict future with high probability. There are a lot of opportunities to apply predictive analytics in marketing. Let's consider those, which proved to be the most efficient"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"Here belong predictive scoring, identification models and automated segmentation. These are related to qualifying and prioritizing leads to make your marketing efforts more effective. Applying these models, you can make sure that the most effective ready to purchase leads will get your call to action correctly"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"Optimization of marketing campaign involves the application of smart algorithms and models allowing to increase the efficiency. Modern technologies bring automation to the data collection and analysis process, reduce time spent on them, provide real-time results and spot the slightest changes in patterns. Smart data algorithms treat each customer individually. Thus, the high personalization level becomes more achievable"
Top 8 Data Science Use Cases in Marketing - KDnuggets,Invest money in those tools that will efficiently gather and analyze data. Make sure the tools you choose can work together for the benefit of your campaign. Integrate the tools with existing systems and data
Top 8 Data Science Use Cases in Marketing - KDnuggets,"Customers' path through the sales funnel is staffed with various opportunities, options, and choices. Lead scoring is applied to identify those prospective customers who will go through the funnel and make their choice to the benefit of your product or service. What is the trick?"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"The essence of all the marketing efforts is to reach the right customer. However, the marketing landscape has been changed and moved to the online world. Thus, the main task for the companies is to assure a strong online presence for the brand"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"The leading part here is given to the selection of optimal digital marketing channels: email marketing, pay-per-click advertisement, search engine optimization, display advertising, Social Media Marketing, content marketing, affiliate marketing, online public relations. The choice is vast. To make this choice more comfortable, take the following steps:"
Top 8 Data Science Use Cases in Marketing - KDnuggets,"In its turn, a digital marketing challenge determines the type of content the brand can use. Blog posts, articles, videos, stories etc. All these types prove to be more or less effective depending on the channel used to distribute them"
Data Science for Managers: Programming Languages - KDnuggets,"Programming languages are a tool for the realization of many powerful data science applications. But, there are so many of them and it has become confusing to choose the optimal one for your specific project. In this article, we are going to talk about popular languages for Data Science and briefly describe each of them"
Data Science for Managers: Programming Languages - KDnuggets,"That’s why it is cutting-edge in data science. We can extend the functionality of the base R language by software libraries called packages. The most popular package repository is the Comprehensive R Archive Network (CRAN). Now, it contains over 10,000 packages that are published"
Data Science for Managers: Programming Languages - KDnuggets,It helps to solve linear and nonlinear problems numerically and to perform other tasks by using language that is practically similar to MATLAB. Octave is one of the major free alternatives to MATLAB. Octave uses an interpreter to execute the Octave scripting language
Data Science for Managers: Programming Languages - KDnuggets,"All in all. Our advice is to think about the purpose of your application, whether you plan future integration, etc. After that, you can choose the most suitable option"
What is Data Science? - KDnuggets,Never forget this. No sane manager in the world would follow an unknown algorithm for managing their company’s money only because its AUROC is greater than 95%. Managers
What is Data Science? - KDnuggets,"Maybe you have magic formulas in your mind, graphs, and so on. Forget about them. Data Science is told by"
What is Data Science? - KDnuggets,There they are. We are slaves in a world of deadlines and expectations. When you were a software engineer you had
What is Data Science? - KDnuggets,"And finally, the fun part. Python, R, Knime, reading scientific papers, optimization algorithms, cross-validation, and so on. The technical and"
What is Data Science? - KDnuggets,"It’s very easy to understand. You only need paper, a pencil and a Cartesian plane with some points drawn on it. That’s it. If it produces very nice results, everybody will finally see you like the great business partner you think you are"
What is Data Science? - KDnuggets,"Data Science is an exciting job, but it can be very difficult to perform if you speak to a non-technical audience. Data and business are intimately related to each other, and you must remember this point when you work with business-oriented people. The only way to survive is to"
Set Operations Applied to Pandas DataFrames - KDnuggets,"The question then becomes: Why would it be useful? Here’s the answer. As we know, data science problems typically require the analysis of data obtained from multiple sources. At some point in the analysis of data from a study, you may face the problem of having to compare the contents of two or more DataFrames to determine if they have elements (rows) in common. In this tutorial you will learn that set operations are one of the best and most natural techniques you can choose to perform such a task"
Set Operations Applied to Pandas DataFrames - KDnuggets,"Here's the complete explanation of the code. Initially, we created two DataFrames, P (Python students) and S (SQL students). Once created, they were submitted the three set operations in the second part of the program"
Set Operations Applied to Pandas DataFrames - KDnuggets,"The difference operation has a slightly more complicated code. As we know, the difference between two sets P and S is the operation that aims to determine the elements of P that are not part of S. In pandas, we can implement this operation using the"
Data Sources 101 - KDnuggets,"Data collection is one of the first steps of the data lifecycle — you need to get all the data you require in the first place. To collect the right data, you need to know where to find it and determine the effort involved in collecting it. This article answers the most basic question: where does all the data you need (or might need) come from?"
Data Sources 101 - KDnuggets,"Getting started with the data universe can be overwhelming. Big data, alternative data, primary data, internal data — the list goes on. A common confusion is the difference between these terms and understanding the difference is quite significant"
Data Sources 101 - KDnuggets,"When you collect data from sources that someone else owns, it's called secondary data. If you use data from Google Analytics to understand how many people visit your website, you're using secondary data. It's still data on your organization, but it's something that a secondary organization (in our example, Google) collected for you"
Data Sources 101 - KDnuggets,"So, alternative data is considered to be big data. It all began with hedge funds using non-financial information such as rental payments and utility bills to estimate the lending risk of an individual. This data transformed the financial industry (See"
Data Sources 101 - KDnuggets,So there you have it. This should give you a rough idea of where all the world's data comes from. Here's an illustration that quickly summarizes the various sources of data
Data Sources 101 - KDnuggets,"P.S. For the past few months, I've been working on a community project called The Atlan Data Wiki — a fun, helpful, jargon-free encyclopedia for navigating through the data universe. If you like my article,"
"Bye Data Scientists, Hello AI? Not Likely! - KDnuggets","We have all been working hard to become such people known as Data Scientists (or whatever anyone wants to call it). AI is becoming more mainstream, especially in the last 2 years. The fact that computers/robots will learn after being built and will surpass a human's intelligence is terrifying. Computers can definitely think faster than us and compute faster than us. But really, isn't that mostly the case. Being able to compute larger volumes of data does not necessarily mean smarter. This is not always the case but AI is nothing without humans. AI has already surpassed humans in computational speed without a doubt. But it cannot learn our intuition and adaptability so easily. A Data Scientist must think  on their feet and adapt to situations that AI cannot actually replicate at this time. It can win in chess but it does not mean it is unbeatable"
"Bye Data Scientists, Hello AI? Not Likely! - KDnuggets","These tasks have to a great extent been automated thanks to advancements in machine intelligence. AI complements our profession. Helping us automate routine tasks or the repetitive nature of machine learning where we need to compute predictions, detect anomalies in data etc. AI is not always a robot. AI can be the manifestation of parts of human intelligence on a machine, computer program etc. AI is mostly used in the very extreme case such as self-driving cars and robot helpers"
"Bye Data Scientists, Hello AI? Not Likely! - KDnuggets","All of the above was done by teams of software developers, data scientists and engineers. The teams that bring AI to fruition are something that even if can be replaced, should not be replaced. Scientists are working hard to engineer AI for a dynamic environment. Indeed it will be a monumental achievement. However as for the us who need to continuously learn about the domain of our analytics problem, adapt to a new process and communicate with stakeholders to showcase the fruits of our labour, we need not worry now (Unless someone built an AI program to use stackoverflow)"
"Reproducibility, Replicability, and Data Science - KDnuggets","If something is replicable, it means that the same conclusions or outcomes can be found using slightly different data or processes. Without reproducibility, process and findings can’t be verified. Without replicability, it is difficult to trust the findings of a single study"
"Reproducibility, Replicability, and Data Science - KDnuggets",The Scientific Method was designed and implemented to encourage reproducibility and replicability by standardizing the process of scientific inquiry. By following a shared process of how to ask and explore questions – we can ensure consistency and rigor in how we come to conclusions. It also makes it easier for other researchers to converge on our results. The
"Reproducibility, Replicability, and Data Science - KDnuggets","As a researcher or data scientist, there are a lot of things that you do not have control over. You might not be able to collect your data in the most ideal way or ensure you are even capturing what you’re trying to measure with your data. You can’t really guarantee that your research or project will replicate. The only thing you can guarantee is that your work is reproducible"
"Reproducibility, Replicability, and Data Science - KDnuggets","Additionally, encouraging and standardizing a paradigm of reproducibility in your work promotes efficiency and accuracy. Often in scientific research and data science projects, we want to build upon preexisting work – work either done by ourselves or by other researchers. Including reproducible methods – or even better, reproducible code – prevents the duplication of efforts, allowing more focus on new, challenging problems. It also makes it easier for other researchers (including yourself in the future) to check your work, making sure your process is correct and"
"Reproducibility, Replicability, and Data Science - KDnuggets","Reproducibility is a best practice in data science as well as in scientific research, and in a lot of ways, comes down to having a software engineering mentality. It is about setting up all your processes in a way that is repeatable (preferably by a computer) and well documented. Here are some (hopefully helpful) hints on how to make your work reproducible"
"Reproducibility, Replicability, and Data Science - KDnuggets","The first, and probably the easiest thing you can do is use a repeatable method for everything – no more editing your data in excel ad-hoc and maybe making a note in a notepad file about what you did. Leverage code or software that can be saved, annotated and shared so another person can run your workflow and accomplish the same thing. Even better, if you find you are using the same process repeatedly (more than a few times) or for different projects, convert your code or workflows into"
"Reproducibility, Replicability, and Data Science - KDnuggets","Documentation of your processes is also critical. Write comments in your code (or your workflows) so that other people (or you six months down the road) can quickly understand what you were trying to do. Code and workflows are usually the best or most elegant when they are simple and can be easily interpreted, but there is never a guarantee that the person looking at your work thinks the same way you do; don’t take the risk here, just spend the extra time to write about what you’re doing"
"Reproducibility, Replicability, and Data Science - KDnuggets","Another best practice is to keep every version of everything; workflows and data alike, so you can track changes. Being able to back-version your data and your processes allows you to have awareness into any changes in your process, and track down where a potential error may have been introduced. You can use a version control system like"
"Reproducibility, Replicability, and Data Science - KDnuggets","One of these obstacles is computer environments. When you share a script, you can’t necessarily guarantee that the person receiving the script has all the same environmental components that you do – the same version of Python or R, for example. This can result in the outcomes of your documented and scripted process turning out differently on a different machine"
"Reproducibility, Replicability, and Data Science - KDnuggets","Replicability is often the goal of scientific research. We turn to science for shared, empirical facts, and truth. When our findings can be supported or confirmed by other labs, with different data or slightly different processes, we know we’ve found something potentially meaningful or real"
"Reproducibility, Replicability, and Data Science - KDnuggets","Often, p-hacking isn’t done out of malice. There are a variety of incentives, particularly in academic research, that drive researchers to manipulate their data until they find an interesting outcome. It’s also natural to try to find data that supports your hypothesis. As a scientist or analyst, you have to make a large number of decisions on how to handle different aspects of your analysis – ranging from removing (or keeping) outliers, to which predictor variables to include, transform, or remove. P-hacking is often a result of specific researcher bias - you believe something works a certain way, so you torture your data until it confesses what you “know” to be the truth. It is not uncommon for researchers to fall in love with their hypothesis and (consciously or unconsciously) manipulate their data until they are proven right"
"Reproducibility, Replicability, and Data Science - KDnuggets","One relatively easy and concrete thing you can do in data science projects is to make sure you don't overfit your model; verify this by using a holdout data set for evaluation or leveraging cross-validation. Overfitting is when your model picks up on random variation in the training dataset instead of finding a ""real"" relationship between variables. This random variation will not exist outside of the sampled training data, so evaluating your model with a different data set can help you catch this"
"Reproducibility, Replicability, and Data Science - KDnuggets","Above all, it is important to acknowledge uncertainty, and that a successful outcome can be finding that the data you have can't answer the question you're asking, or that the thing you suspected isn't being supported by the data. Most scientific experiments end in ""failure,"" and in many ways, this failure can be considered a successful outcome if you did a robust analysis. Even when you do find ""significant"" relationships or results, it can be difficult to make guarantees about how the model will perform in the future or on data that is sampled from different populations. It is important to acknowledge the limitations or possible shortcomings of your analysis. Acknowledging the inherent uncertainty in the scientific method and data science and statistics will help you communicate your findings realistically and correctly"
"Reproducibility, Replicability, and Data Science - KDnuggets","Data science can be seen as a field of scientific inquiry in its own right. The work we do as data scientists should be held to the same levels of rigor as any other field of inquiry and research. It is our responsibility as data scientists to hold ourselves to these standards. Data science has also, in a lot of ways, been set up for success in these areas. Our work is computer-driven (and therefore reproducible) by nature, as well as interdisciplinary – meaning we should be working in teams with people that have different skills and backgrounds than ourselves"
How to Get the Most out of ODSC West 2019 - KDnuggets,"ODSC West comes to San Francisco on Oct 29 - Nov 1. With over 300 hours of content, 200+ speakers, and thousands of attendees, there is certainly a lot to see, learn, and do at the conference. Register by Friday for 10% off your pass"
How to Get the Most out of ODSC West 2019 - KDnuggets,"ODSC has been hailed by Kirk Borne as “the best community data science event on the planet…. [C]omprehensive and totally community-focused: it's the conference to engage, to build, to develop, and to learn from the whole data science community,” and we can’t wait to host our West event in San Francisco on October 29 - November 1. With over 300 hours of content, 200+ speakers, and thousands of attendees, there is certainly a lot to see, learn, and do at the conference. To make sure you enjoy our conference to the fullest, we’ve put together this handy guide full of tips for optimizing your experience"
How to Get the Most out of ODSC West 2019 - KDnuggets,"As we mentioned above, ODSC West has a lot of excellent data science and AI content to pick from. With any situation in which there is an abundance of choice, it’s important to set your strategy before you leave for the conference. Decide on what your goals for this conference are. Do you want to develop new skills in a specific tool or platform? Explore several unfamiliar topics? Hear from only the most in-demand, hard-to-see speakers? Whatever your goal might be, make sure to study the"
How to Get the Most out of ODSC West 2019 - KDnuggets,"ODSC West’s speakers (Pieter Abbeel, Dawn Song, Rachel Thomas, and Michael I. Jordan to name just a few) are leaders in their fields, who have spent many years working and researching to build their expertise. Understanding their past and current work can help you get the most out of their talk, workshop, tutorial, or training session. This knowledge can help you to ask insightful questions that will further your understanding of the topic after their presentation"
How to Get the Most out of ODSC West 2019 - KDnuggets,"As you know, ODSC West offers workshops and training sessions for data scientists of all skill levels: beginner, intermediate, and advanced. To ensure that you will get the most out of the sessions you attend, it’s important that you come with not only the necessary software, but also have the required foundational knowledge. Many of our speakers list any experience, knowledge, or software requirements in their abstracts, so be sure to check out our"
How to Get the Most out of ODSC West 2019 - KDnuggets,"With so much knowledge to gather and so many new skills to learn, it can be easy to forget about ODSC West’s community-focused and networking events. However, to do so would be a mistake. Events, such as dinner or drinks with data scientists and the networking reception, give you the opportunity to relax, unwind, explore San Francisco, and make new friends. Remember, decompressing and relaxing your brain can help you absorb and retain what you learn during the day, and the new connections and friends you make during the events could last for a lifetime. So check out some of our event highlights below (and a full list of events"
How to Get the Most out of ODSC West 2019 - KDnuggets,"Although we’ve been instructing you to plan, plan, plan throughout this article, we’re going to countermand that now and suggest that you leave room for spontaneity while you are at the conference. One of the great advantages of a conference that attracts thousands of data scientists and hundreds of expert speakers is the opportunity to encounter people you’d never otherwise have a chance to meet and engage in conversations you’d never have had a chance to have. So be sure to give yourself that opportunity. Take advantage of the coffee breaks and the hallway track in between talks and connect with fellow attendees, stop for conversations that seem interesting, and don’t be so set on your schedule that you miss the forest for the trees"
How to Get the Most out of ODSC West 2019 - KDnuggets,"We hope that this guide to ODSC West 2019 helps you have the best possible conference experience. We can’t wait to see you in San Francisco in just a few weeks. And, for those of you who have been wanting to go, but haven’t bought your passes yet, there’s still time"
How to Write Web Apps Using Simple Python for Data Scientists - KDnuggets,That was easy. But it seems pretty basic right now. Can we add some charts?
How to Write Web Apps Using Simple Python for Data Scientists - KDnuggets,Here is the code for our simple app. We just used four calls to streamlit. Rest is all simple python
How to Write Web Apps Using Simple Python for Data Scientists - KDnuggets,"In the start we said that each time we change any widget, the whole app runs from start to end. This is not feasible when we create apps that will serve deep learning models or complicated machine learning models. Streamlit covers us in this aspect by introducing"
How to Write Web Apps Using Simple Python for Data Scientists - KDnuggets,"In our simple app. We read the pandas dataframe again and again whenever a value changes. While it works for the small data we have, it will not work for big data or when we have to do a lot of processing on the data. Let us use caching using the"
How to Write Web Apps Using Simple Python for Data Scientists - KDnuggets,"I love writing in Markdown. I find it less verbose than HTML and much more suited for data science work. So, can we use Markdown with the streamlit app?"
How to Write Web Apps Using Simple Python for Data Scientists - KDnuggets,"Yes, we can. There are a couple of ways to do this. In my view, the best one is to use"
How to Write Web Apps Using Simple Python for Data Scientists - KDnuggets,"In this post, we created a simple web app. But the possibilities are endless. To give an example here is"
How to Write Web Apps Using Simple Python for Data Scientists - KDnuggets,Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
What is the most important question for Data Science (and Digital Transformation) - KDnuggets,"Consider for a moment a different perspective, that of someone far up your leadership chain, the corporate executive. You may feel that they don’t understand what you do. You’re probably right. Because for most of them, these"
What is the most important question for Data Science (and Digital Transformation) - KDnuggets,"Because as leaders, their challenge is to decide which new buzzwords bring business value and which are best left in the lab to mature. Feeling competitive pressure, many leap ahead and adopt corporate initiatives around data science or artificial intelligence/machine learning (AI/ML), even as they are still trying to figure out their meaning. To keep it simple, I often bundle these buzzwords and call them “fancy math,” and as passionate as I am about their power to make a positive impact on business, I also believe that starting with math misses the point"
What is the most important question for Data Science (and Digital Transformation) - KDnuggets,"Math can indeed move the world, but it is imperative to give it a chance to succeed. You, newly-minted data scientist, are in hot demand, but it will take a lot more than just hiring you to actually impact the business. Because as these McKinsey consultants write in the"
What is the most important question for Data Science (and Digital Transformation) - KDnuggets,"Why does business problem definition matter so much? Too many expensive corporate AI initiatives fail because a team of data scientists is hired in a vacuum, with no vision for why they exist or strategy to deploy them. You scribble equations, sling code, and build a complex model that solves the wrong business problem. It is too easy to trip along common paths to failure: ignoring the input of end users, forgetting to engage IT on deployment, or building a black box model neither executives nor regulators understand. Getting clear on what problem you’re charged with solving, how to define a better decision through the benefits you want to achieve, and what assumptions and criteria impact that solution are all part of framing the problem and preventing large data science investments from failing to deliver their promised value"
What is the most important question for Data Science (and Digital Transformation) - KDnuggets,"Another pitfall involves data, the bane of your existence. You know it is a critical and very time-consuming step, but sometimes leadership may need a deeper understanding of the importance of access to the data you need and the time needed to prepare it for analysis. Paying attention to data is not very sexy, but because AI/ML models are predicated on consuming large amounts of data, they will need to understand that they will fail without being properly fed"
What is the most important question for Data Science (and Digital Transformation) - KDnuggets,"While vendors are criticized for falling into the trap of being a hammer in search of a nail (“we build great optimization software”), buyers can fall into that same trap. If your company has a corporate AI/ML initiative and compares vendors based on a list of algorithms, then they run the risk of letting a tactical choice of a math method drive a strategic decision of what problems are being solved. Naturally, as a data scientist, you want access to the latest, greatest toys, but many other factors should drive what software is the best choice"
What is the most important question for Data Science (and Digital Transformation) - KDnuggets,"I’m drawn not only to more accurate models, but also pattern-finding and decision-making far beyond the cognitive capacity of the human mind. While we must hold our models accountable to the human biases they can reflect if properly built they also have great potential to reduce human bias and yield more consistent, objective decisions. And math can automate many decisions, taking off our plate routine decisions and saving time for the exceptions that most need human skill and judgment"
What is the most important question for Data Science (and Digital Transformation) - KDnuggets,"Occam’s Razor holds that the simplest solution to a problem is likely the best. It is important to stay accountable to both the criteria and metrics and this principle, because fancier math may seem more fun. Depending on the business problem, the savings from a more accurate answer may justify greater complexity, in spite of longer compute times or a black box method. But for other problems, a good-enough answer reached quickly and widely and easily explainable may be the best choice"
What is the most important question for Data Science (and Digital Transformation) - KDnuggets,"I believe in the power of fancy math to deliver true business value and enable smarter organizations, but the best way to deliver this value is for leadership to start with the strategic decision of setting a vision and defining the business problems and criteria, and then (and only then) choose the math. Human intelligence and vision, combined with fancy math (and data scientists like you), can truly transform a company. When you’ve all taken this hard journey together and delivered results, then you can truly call “bingo"
The Most In Demand Tech Skills for Data Scientists - KDnuggets,"I looked at demand for general skills such as statistics and communication. I also looked at demand for technologies such as Python and R. Software technologies change must faster than demand for general skills, so I include only technologies in this updated analysis"
The Most In Demand Tech Skills for Data Scientists - KDnuggets,"This time I decided to write the code to scrape the job listings instead of searching by hand. This endeavor proved fruitful for SimplyHired, Indeed, and Monster. I was able to use the"
The Most In Demand Tech Skills for Data Scientists - KDnuggets,"Scraping LinkedIn proved far more arduous. Authentication is required to see an exact count of job listings. I decided to use Selenium for headless browsing. In September 2019, a"
The Most In Demand Tech Skills for Data Scientists - KDnuggets,"LinkedIn’s data might not have provided an apples-to-apples comparison from last year to this year, anyway. This summer I noticed that LinkedIn started having huge fluctuations from week to week for some tech job search terms. I hypothesize that they might have been experimenting with their search results algorithm by using natural language processing to gauge intent. In contrast, relatively similar numbers of job listings for ‘Data Scientist’ appeared for the three other search sites over both years"
The Most In Demand Tech Skills for Data Scientists - KDnuggets,"Here’s the chart from number 2 above, showing the gains and losses in terms of the average percentage of listings between 2018 and 2019. AWS show an increase of 5% points. It appeared in an average of 19.4% of listings in 2019 and an average of 14.6% of listings in 2018"
The Most In Demand Tech Skills for Data Scientists - KDnuggets,"Here’s the chart for number 3 above, showing the percentage change year over year. PyTorch had 108.1% growth compared to the average percentage of listings it appeared in for 2018"
The Most In Demand Tech Skills for Data Scientists - KDnuggets,It’s by far the most frequent keyword. It’s in nearly three out of four listings. Python saw a decent increase from 2018
The Most In Demand Tech Skills for Data Scientists - KDnuggets,"Python has pretty clearly overtaken R as the language of choice for data science. Nonetheless, R remains very popular, showing up in about 55% of listings. If you know R, don’t despair, but think about learning Python too, if you want a more in-demand skill"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","I hope by now, you have a solid foundation on ultralearning and learning how to learn, as well as hacks for deep work and focused learning. With that, now you’re ready to learn. But just as one can"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","Data science is a broad field with many subfields. To learn all the necessary skills and prowess solely with formal education is implausible. Imagine once you finish a bachelor’s degree in data science, you look for an internship or even a job, and you have no experience in the market at all. Imagine everything you’ve studied in college for the past four years, did not prepare you for the real world. This is why some data scientists have a Ph.D. They had a few more years before diving into the real world. This is a common situation, and it applies to almost every other major as well, and it’s called failure the transfer"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","Transfer is the process of learning something in one context (statistics and programming) and then transferring it to another (predicting the temperature rise of Earth in the next 20 years). Despite its importance, formal education often fails to optimize transfer. Ironically, this transfer learning concept also exists in the field of deep learning"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","The problem with formal education is that it sets up an indirect path between the learning context and the target environment — the context in which learned skills and knowledge are applied. For example, you learn about linear algebra in college. You spend hours on practice questions and past year papers. However, when it comes to applying it in data science, you fail to transfer it into an application, as you don’t have an underlying cognizance of what concepts (such as determinants, invertible matrices, Eigenvectors, Gram-Schmidt processes, and so on) actually mean. In other words, you didn’t grasp the essence of linear algebra"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets",The most direct way to learn something is to do it. The most effective way to learn to code is to write code. The most effective way to learn data science is to engage in data science projects and solve real-world problems
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","Of course, not everyone has time for immersive learning. Moreover, some skills don’t lend themselves to this approach. There’s a reason that trainee pilots don’t immerse themselves by flying Boeings on their first day of training. Instead, they learn in flight simulators"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","Crucially, you should never begin your project by drilling. Instead, use the direct-then-drill approach. To do this, start with direct practice, whether you’re writing code or solving business problems. Use this direct practice to identify the areas where you wish to drill. After drilling, go back to direct practice until it becomes necessary to drill again"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","How should you design your drills? That depends on the area you want to drill. Can it be easily isolated from the rest of your project? If so, try time-slicing, where you isolate one step in a more involved process and repeat the step until you’ve perfected it. If you want to perfect your data wrangling, for example, you could time-slice by drilling your code-cleaning capabilities. Or, separate your desired skill into different cognitive components and drill each separately. For example, in Python programming, you could drill Pandas, scikit-learn, or PyTorch"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","If you’re working on a more creative or complex project, you might find it challenging to drill in isolation — it’s hard to drill problem-solving, for example. In that case, try the copycat method instead. Choose a successful person you admire, whether it’s billionaire investor Warren Buffet or genius entrepreneur Bill Gates, and emulate the way they solve problems as closely as you can"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","Learning statistics is a great way to improve your problem-solving skills — but only if your hard-won knowledge doesn’t desert you when you’re at your easel. It’s pointless learning new skills, concepts, and procedures if you’re unable to retrieve them quickly and efficiently. As a data scientist, you must have the prowess to understand data — down to the fundamental level — and clean, model and present the data in the right form. After that, you have to tell a story about your data, converting your ingenious analysis into layman terms. That said, to ensure you’re always ready to mess with data and articulate it, there are two methods you can use to improve your retrieval rate"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","The reason we prefer review over recall all comes down to a concept called the judgment of learning. Essentially, we humans believe that we have learned a concept when we can process it without any difficulty. In college, students read back their notes over and over again, fabricating an impression that they have grasped the information. That’s why we gravitate toward passive review strategies: They confirm our perception that we’re learning successfully"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","But perception isn’t everything. Struggling to recall something in the short term means you’re far more likely to remember it in the long term. Experts call this desirable difficulty — the difficulty posed by the recall is ultimately desirable, as it maximizes our chances of retaining what we’ve learned"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","This feedback validates that you’ve reached the desired outcome. Imagine you’re giving a presentation of your data, and your clients fully comprehend the result and applaud you for your work. That’s outcome feedback. It can be encouraging, but it’s hard to glean any more information from this type of feedback"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","This feedback gives you more to work with, as it alerts you about your mistakes. Imagine if you made a mistake, and your data is entirely wrong. The lead data scientist then pulls you out of the project and hands it over to someone else. This kind of feedback is useful for highlighting problem areas and isolating your mistakes"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","This is the best feedback, as it tells you what you’re doing wrong and how to fix it. This is where the lead data scientist gives you notes on what went well, what didn’t land, and how you can improve. In this scenario, you are given corrective feedback that’s constructive and helps you develop and grow"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","How do you ensure you’re receiving enough feedback in the first place? Start by remembering to fail for feedback. If you’re not extending yourself to the point where you fail, you stop yourself from getting useful informational or corrective feedback. Pushing beyond your limits will elicit helpful feedback. Acting on that feedback will, in turn, extend your limits"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","Don’t neglect to seek meta-feedback, either. It’s important to seek feedback on how well your learning methods are working. A simple way to test your learning methods is to track your learning rate — try timing how long it takes you to clean your data, for example. If your learning rate isn’t tracking upward, act on this negative feedback by revisiting your learning methods"
"How To “Ultralearn” Data Science: optimization learning, Part 3 - KDnuggets","So, to learn things so that they stick, the most productive strategy you can employ is to settle on a memorization system and incorporate it at regular, closely spaced stages throughout your journey. The key is to use a memorization system that’s both easy to integrate into your project and well-suited to the type of project. For example, it’s better to have a semantic network type of memorization system for data science as it is a very diverse field with many concepts to remember from different subjects. And it establishes the underpinnings of each convoluted topic and allows you to append new information to it as you learn. This substantiates your ability of retention, as the foundation is there"
Data Science is Boring (Part 2) - KDnuggets,"Boring problems are good because they represent steady-state operational issues. These operations drive the core of businesses. The core of the business creates consistent and substantive value. Therefore, businesses prioritize investment to solve boring problems that hinder their cores. Boring problems get real and constant attention"
Data Science is Boring (Part 2) - KDnuggets,"I love thinking about and solving them too. But, statistically speaking, I don’t have the privilege to work on such projects every day. Message me if you got a Zero-to-One idea and need help. You can find me on"
Data Science is Boring (Part 2) - KDnuggets,"To illustrate, here is an example of my favorite iPhone feature — the “For You” personalized photo album. I break down the feature into the archetypes and more granular implementation details. It is over-simplifying (UX, data, and systems are all important), but I hope you can see"
Data Science is Boring (Part 2) - KDnuggets,"This is not a bad thing. It just shows an intrinsic drive to push the boundary. However, as I mentioned in my"
Data Science is Boring (Part 2) - KDnuggets,"I write down all the viable options that are directly relevant to objective(s). I estimate the effort for each option. The effort varies case by case, so apply your best judgment or do it with a team if possible (it helps to de-bias and de-risk)"
Data Science is Boring (Part 2) - KDnuggets,"Boring problems are great. They make up the core of businesses and signify real operational needs. We should solve more boring problems because your work will translate into immediate, tangible, and sustainable values. From a tactical standpoint, it is important to 1) recognize what fundamental problems there are and corresponding ML solution(s), 2) how ML archetypes can make up powerful ML applications, 3) identify the next best opportunities to deliver immediate value and gain more traction. I hope the"
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,"By 2020, it’s estimated that 1.7 MB of data will be created every second for every person on earth. Absurd! Data is going to be the new oil of our digital era. The growth of data creates a need to make meaning out of it. This has spawned many professions which manage and analyze data to make smarter business decisions. Many of these professions require you to be proficient in managing data in databases"
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,A common way to manage data is with a relational database management system. A relational database stores data in a tabular form consisting of rows and columns. These databases usually consist of both data and metadata. Data is the information stored in the tables while metadata is the data that describes the data’s structure or data types within the database
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,"SQL is a declarative and domain-specific language mostly used by business analysts, software engineers, data analysts, and many other professions that make use of data. You don’t need to be a programmer or know programming languages such as Python to master SQL. SQL’s syntax is similar to English. With a little bit of memorization of simple syntax, you’re ready to work comfortably with database systems"
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,If we’re only interested in part of the data in the table we can filter the table. We have multiple statements that allow us to filter our tables. Filters basically select rows that match certain criteria and return the results back as a filtered data set. Filtering tables does not mutate the original table
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,"Imagine you’d like to know which customer ordered what products. If a database follows a proper database normalization technique, then products, customers, and orders would be in separate tables. If we want to see which customer ordered what products, we would then have to look at the customer ID inside the order table, then go to the customer table and see the products purchases, then use the product ID to look up the product table. As you can see, this is a huge headache if were to repeat it multiple times. In order to do this more easily, SQL has a statement called"
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,"The statement above returns the column’s order ID and customer names. We join the table orders (left) and customers (right), but only rows that have matching customer IDs. Re-read this sentence while looking at the inner join Venn diagram and hopefully this will be easier to grasp"
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,"So far, all the queries that we’ve looked at are basic queries. Practically speaking, the queries that we execute in our day to day life usually consist of a combination of multiple SQL statements or functions. When operations are complex, this will lower the execution time of the queries"
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,This will create an index to look up data from the column quickly. It is to be noted that indexes are not stored in the table and are invisible to the naked eye. We most often use indexes when we have a lot of data retrieval on tables
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,"A common example of the use of transactions is transferring money from one account to another in a bank. In order for a transfer to be successful money must be removed from account A and added to account B. Otherwise, we would roll back the transaction to start fresh. When the transaction is complete we say that the transaction is"
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,Not all SQL queries are individual and isolated. Sometimes we would like to perform an action on table A when a different event happens to another table B. This is where we get to use a
The Last SQL Guide for Data Analysis You’ll Ever Need - KDnuggets,"You can generate powerful queries from endless permutations of the SQL statements we saw in this article. Remember, the best way to cement the concepts and get better at SQL is by practicing and solving SQL problems. Some of the examples above were inspired by"
The 4 Quadrants of Data Science Skills and 7 Principles for Creating a Viral Data Visualization - KDnuggets,"I teach CS and Design Thinking. But today, I am on a mission to show how to do great charts because I dislike confusing charts. You may think of me as the Marie Kondo of charts or the Cole Knaflic of Dubai, and you might not be entirely wrong. In this post, I will share"
The 4 Quadrants of Data Science Skills and 7 Principles for Creating a Viral Data Visualization - KDnuggets,Let’s pretend you are asked to visualize the poll. What would you do? The first ML instinct is to do a scatter plot to identify interesting clusters. The X-axis can be the percentage of respondents that
The 4 Quadrants of Data Science Skills and 7 Principles for Creating a Viral Data Visualization - KDnuggets,"There are unspoken rules about charts. One is about gravity. Y-axis is aligned with the gravity metaphor (highly wanted, high Y). Another unspoken rule (this one by Guy Kawasaki) is that “you want (desired goals) to be high and to the right"
The 4 Quadrants of Data Science Skills and 7 Principles for Creating a Viral Data Visualization - KDnuggets,"If you make a chart and no one remembers it. Did it still happen? Earlier we made meaning with categories but what good are they if no one remembers them? One way to help your audience to remember is personas (memes, in Gen-Z speak). Let’s apply some personas and clustering principles. Four quadrants could mean:"
The 4 Quadrants of Data Science Skills and 7 Principles for Creating a Viral Data Visualization - KDnuggets,"Quadrants create meaning and reduce complexity, but Marie Kondo is not done yet. Let’s further tidy by highlighting one skill per quadrant. Here we use a red box, but we could have used a larger font type or bold letters if you feel so stylish"
The 4 Quadrants of Data Science Skills and 7 Principles for Creating a Viral Data Visualization - KDnuggets,Now we can think about the gap between present and future. This chart was done in ggplot2. Luckily the way ggplot2 plots differently negative and positive stack bars comes handy in this case. Notice also how we avoid the temptation of ordering the bars by ascending order because it would create a spurious pattern in the crest
The 4 Quadrants of Data Science Skills and 7 Principles for Creating a Viral Data Visualization - KDnuggets,"Data scientists, more than other professionals, are in prominent positions to disseminate knowledge. Data visualization is one of the ways to do so. Unfortunately, there is a lot of good work out there that goes unnoticed, like"
The 4 Quadrants of Data Science Skills and 7 Principles for Creating a Viral Data Visualization - KDnuggets,"He is currently with the Computer Science Department at UAE where he teaches Agile, Lean UX and Design Thinking. He also coaches entrepreneurs in Dubai, teaching at the university's incubator program and is the founder of PyData Dubai. He has taught Design Thinking, Big Data Analytics and Business Models at Apple, Bielefeld University, Mexico's CEDIM, and Hult Business School in Dubai. He consults on UX Design and Data Science for various companies such as Awok, Etihad and Healint in Singapore. In 2007- 2008, he developed two startups (a visual Twitter and a photo sharing site). He is the author of Introduction to data visualization & Storytelling (2019), Brown Book of Design Thinking,  and Sketch Thinking. His research focuses on creativity, HCI, UX and data science. Jose is a Kaggle master"
Data Preparation for Machine learning 101: Why it’s important and how to do it - KDnuggets,"Businesses use data for various purposes. On a broad level, it is used to make informed business decisions, execute successful sales and marketing campaigns, etc. But, these cannot be implemented with just raw data"
Data Preparation for Machine learning 101: Why it’s important and how to do it - KDnuggets,"Data becomes a precious resource only if it is cleansed, well-labeled, annotated, and prepared. Once the data goes through various stages of fitness tests it then finally becomes qualified for further processing. The processing could be of several methods - data ingested to BI tools, CRM database, developing algorithms for analytical models, data management tools and so on"
Data Preparation for Machine learning 101: Why it’s important and how to do it - KDnuggets,"Now, it is important that the insights you gather from the analysis of this information is accurate and trustworthy. The foundation of achieving this output lies in the health of the data. Additionally, whether you build your own models or get them from third parties, you must ensure that the data behind this entire process is labeled, augmented, clean, structured - to summarize, that is data preparation"
Data Preparation for Machine learning 101: Why it’s important and how to do it - KDnuggets,"More than 80% of a data scientist’s time is spent on preparing the data. While this is a good sign, considering that as good data goes into building the analytical model, the accurate gets the output. But, data scientists should ideally be spending more of their time interacting with data, advanced analytics, training and evaluating the model, and deploy to production"
Data Preparation for Machine learning 101: Why it’s important and how to do it - KDnuggets,"Let’s say there are two columns, one is income and the other is the output classification (A, B, C). The output A, B, C are dependent on the income range $2k - $3K, $4k - $5K, and $6K - $7K. The new feature would be income-range assigning numerical values 1,2, and 3. Now, these numerical values are mapped to the 3 datasets we created initially"
The Rise of User-Generated Data Labeling - KDnuggets,"Let’s say your project is humongous and needs data labeling to be done continuously - while you’re on-the-go, sleeping, or eating. I’m sure you’d appreciate User-generated Data Labeling. I’ve got 6 interesting examples to help you understand this, let’s dive right in!"
The Rise of User-Generated Data Labeling - KDnuggets,"Cheetah uses supervised learning techniques to catch its prey. That’s a bizarre, random out-of-the-blue statement you may say. But, think about it. A cheetah has adapted a very refined approach to hunting by honing its skills through practice, observation, experience, and computation"
The Rise of User-Generated Data Labeling - KDnuggets,Much like training datasets to create a spectacular AI model. They’re trained and taught continuously until they’re able to operate on their own. The marvelous cheetah species too goes through a similar process until it can anticipate the escape tactics of various prey and modulate its speed for rapid turns - and not just rely on its agility and speed. Cognition is achieved through immense training and the core of this process is
The Rise of User-Generated Data Labeling - KDnuggets,"But, let’s say your project is humongous and needs data labeling to be done continuously - while you’re on-the-go, sleeping, or eating. That’s when you need to get it done for free. Of course, it can be outsourced, but if you consider the cost, probabilities covered, and accuracy achieved, I’m sure you’d appreciate"
The Rise of User-Generated Data Labeling - KDnuggets,"A simple application of data science on platforms like Netflix would, of course, be how their recommendation engines work with implicit data. Let’s say a user “A” binge-watched a show, say, “Jane the Virgin” (all seasons in 4 days), the implicit data is that you liked the show because you obviously sacrificed a lot of sleep to watch it. Behavioral data combined with thousands of other data points is the basis on which the machine learning algorithm at Netflix actually works"
The Rise of User-Generated Data Labeling - KDnuggets,"So where is user-generated data labeling here? Exactly where Netflix collects different thumbnail images and annotates them based on the user’s past behavior, preference towards a particular genre, filters and lighting, favorite stars, and more. This recommendation is unique to every user which is based on thousands of similar interests that have helped improve the click-through rates. It’s brilliant how Netflix sneaks into collecting data from users and effectively utilizing it to improve the experience"
The Rise of User-Generated Data Labeling - KDnuggets,"Well, the image clearly says the neural network didn’t recognize it (I’m not a Picasso, right?) Anyway, the game gets interesting only when the user feels victorious when the neural network recognizes how they conceive and represent what’s been asked to draw. That precisely is the success of this concept. But, owing to millions of different possibilities and deal with the skills of people like myself who’s fit to play Pictionary (drawing in air), the model utilizes user-generated data"
The Rise of User-Generated Data Labeling - KDnuggets,"All writers are familiar with Grammarly. It is a tool that helps you keep your writing free of grammar errors including spelling, punctuation, and more. Grammarly is not all about grammatical errors, it also helps you identify plagiarism, poor word choices, slang, tone detection, etc. Grammarly uses a sophisticated artificial intelligence system with a team of computation linguists and deep learning engineers behind the scenes. The algorithms learn the rules of writing and make suggestions by analyzing from research corpora which are a huge collection of labeled text for research and development"
The Rise of User-Generated Data Labeling - KDnuggets,"So, we all know that the fundamental of any AI model is continuous learning. Like how humans get smarter with enriching their knowledge, so do machines! But, machines work based on rules. If there is a negation, they report an error. Likewise, Grammarly gets smarter with your help (user). Let me take an example of the previous line you just read. I thought I had made a mistake seeing a red underline for “is”(see below)"
The Rise of User-Generated Data Labeling - KDnuggets,"But, it might logically and grammatically be right to use “are”. But, beyond grammar, writing needs to connect, most importantly your sentences need to read well. In this example, I have used “is” because the process goes on and hence the present continuous tense. However, Grammarly had other ideas. So, as a user, I helped Grammarly label this to better fit the context and ignored the suggestion to make it a little smarter"
The Rise of User-Generated Data Labeling - KDnuggets,"Let’s say a Non-English speaker uses Google map (voice) to track the location and the app returns incorrect information as a result. When you quickly resort to pressing the back button, the model learns that there has been an error that becomes learning. The reason as we know the pronunciation and accents vary across the region.  These datasets would help the ML model perform the application of voice to text conversion accurately"
The Rise of User-Generated Data Labeling - KDnuggets,"We all have experienced a gazillion set of images that Google requires you to confirm it to a requirement in order to tell humans and robots apart. Due to the increasing bypass of this captcha through bots, they’ve now gotten a little complex. In order to improve the accuracy of the image classification, the user participation goes in as a training dataset to the machine learning model to improve its accuracy over time"
The Rise of User-Generated Data Labeling - KDnuggets,"NLP helps a machine understand a language, the way a human does. As for Instagram, a sentence filled with only neutral words could still be offensive while a sentence filled with swear words could be a popular song. It’s complicated. But, not for Instagram that uses DeepText to automatically identify and remove offensive comments"
The Rise of User-Generated Data Labeling - KDnuggets,"AI is used in multiple cases at Bigbasket, one for instance is how they analyze current traffic data and map it with time-to-deliver commitment. The other is the Smartbasket - this  is super-interesting. Smartbasket was introduced with an intention to create personalized shopping basket for customers"
The Rise of User-Generated Data Labeling - KDnuggets,"There are three types of recommender systems - collaborative filtering, content-based, and hybrid recommendation. Companies like Bigbasket and Netflix leverage hybrid recommendation i. In a nutshell, here again, users play a huge role in making the recommendation engines smarter!"
The Rise of User-Generated Data Labeling - KDnuggets,"But for the one like Quick draw, the labeling is outright simple. If the neural network fails, the data gets immediately fed as learning. These instances are common for huge companies that deal with gazillion information in real-time. But, for other AI projects, where data labeling is still important, you can resort to some good data annotation tools available in the market like -"
Why data analysts should choose stories over statistics - KDnuggets,"Chip and Dan Heath’s book, “Made to Stick: Why Some Ideas Survive and Others Die” has had a profound impact on my career. Its principles transformed the way I think about communication. It was also one of the first books that highlighted to me the power that stories have over statistics. As a data geek, I often thought logic and reason (facts!) were all you needed to inform decision making. However, this book opened my eyes to why storytelling is such an essential form of persuasion"
Why data analysts should choose stories over statistics - KDnuggets,"My advice is to never forget “the last mile” in the analysis process. After all the hard work it takes to find a key insight, you must then communicate it effectively so that other people (decision makers) can understand it and be compelled to act on it. Too often, data professionals excel in the exploratory phase but then fail in the explanatory phase. As a result, their insights go nowhere. Mastering the art of data storytelling can ensure you succeed in this critical last mile"
Why data analysts should choose stories over statistics - KDnuggets,I think we can all safely say “big data” was always just “data. I’ve also never been a fan of the term “citizen data scientist”. This term mischaracterizes the capacity for business users to become “pseudo” data scientists
5 Famous Deep Learning Courses/Schools of 2019 - KDnuggets,"Deep Learning is/has become the hottest skill in Data Science at the moment. There is a plethora of articles, courses, technologies, influencers and resources that we can leverage to gain the Deep Learning skills. Deep Learning is not just one thing though! There are so many applications that one cannot simply learn all in a short span of time (maybe some can but"
5 Famous Deep Learning Courses/Schools of 2019 - KDnuggets,"He is by far one of the most prolific figures in Machine Learning / Deep Learning / AI in today's world. His courses and teaching style have convinced millions to look up to him. They have 3 different specializations in Deep Learning, Tensorflow and AI for Everyone. AI for Everyone is a non-technical course for the people in the organization without a technical background so that they may also identify opportunities within their organization where AI can be applied"
5 Famous Deep Learning Courses/Schools of 2019 - KDnuggets,Everyone in technology knows IBM. They are one of the original innovators in technology. There discoveries have gone on to change the lives of consumers. Arguably their greatest invention was SQL. That is right! One of the most widely used technologies in the world for data science was initially developed at IBM. They now make their name by being a major cloud provider
5 Famous Deep Learning Courses/Schools of 2019 - KDnuggets,"Udacity offers a range of courses and pathways to develop a range of skills in Machine Learning / Deep Learning / AI. Some courses are free as well. Udacity is known for a great focus on a range of technology related jobs and skills. In addition to Data Science and AI, they also have courses for Web Development, Mobile Development, Software Development etc. This seems to be a good resource to gain complementary skills in other areas of technology as well"
5 Famous Deep Learning Courses/Schools of 2019 - KDnuggets,"The Massachusetts Institute of Technology (MIT) is one of the most famous schools of technology for people interested in the STEM. So many great innovators (including Nobel Laureates) have graced the halls of MIT. The site gives you video tutorials, code etc for you to continue learning and hopefully develop your own solutions as well"
5 Beginner Friendly Steps to Learn Machine Learning and Data Science with Python - KDnuggets,"Two years ago, I started learning machine learning online on my own. I shared my journey through YouTube and my blog. I had no idea what I was doing. I’d never coded before but decided I wanted to learn machine learning"
5 Beginner Friendly Steps to Learn Machine Learning and Data Science with Python - KDnuggets,"When people find my work, they sometimes reach out and ask questions. I don’t have all the answers but I reply to as many as I can. The most common question I get is “where do I start?” The next most common question is"
5 Beginner Friendly Steps to Learn Machine Learning and Data Science with Python - KDnuggets,"Remember, if you’re starting to learn machine learning, it can be daunting. There’s a lot. Take your time. Bookmark this article so you can refer to it as you go"
5 Beginner Friendly Steps to Learn Machine Learning and Data Science with Python - KDnuggets,The email said they’d already done some Python. But this step is for someone who’s completely new as well. Spend a few months learning Python code at the same time as different machine learning concepts. You’ll need them both
5 Beginner Friendly Steps to Learn Machine Learning and Data Science with Python - KDnuggets,Matplotlib will help you make graphs and visualizations of your data. Understanding a pile of numbers in a table can be hard for humans. We much prefer seeing a graph with a line going through it. Making visualizations is a big part of communicating your findings
5 Beginner Friendly Steps to Learn Machine Learning and Data Science with Python - KDnuggets,"Along the way, it would be ideal if you practised what you were learning with small projects of your own. These don’t have to be elaborate world-changing things but something you can say “I’ve done this with X”. And then share your work via Github or a blog post. Github is used to showcase your code, a blog post is used to show how you can communicate your work. You should aim to release one of each for every project"
5 Beginner Friendly Steps to Learn Machine Learning and Data Science with Python - KDnuggets,You could spend 6-months or more on each. Don’t rush. Learning new things takes time. The main skill you are building as a data scientist or machine learning engineer is how to ask good questions of data then using your tools to try and find answers
5 Beginner Friendly Steps to Learn Machine Learning and Data Science with Python - KDnuggets,Some days you’ll feel like you’re learning nothing. Even going backwards. Ignore it. Don’t compare your progress day to day. Compare your progress year on year
5 Beginner Friendly Steps to Learn Machine Learning and Data Science with Python - KDnuggets,"You will learn these things along the way. Start with code first. Get things running. Trying to learn all of the statistics, all of the math, all of the probability before running your code is like trying to boil the ocean. It will hold you back"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"The cyber world is such a vast concept to comprehend. At the time, I decided I wanted to get into cybersecurity during my undergrad in college. What intrigued me was understanding the concepts of malware, network security, penetration testing & the encryption aspect that really plays a role in what cybersecurity really is"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,Being able to protect the infrastructure is important but interesting nonetheless. Sure there is coding but I never really learned how we can implement code into cybersecurity principles. This is what I really want to know next that could stretch my knowledge in information technology & computer science. I learned more about coding especially in Python. I dabbled a little in Scala & I already had a good foundation of Sequel & Java applications during my undergrad that learning it during my boot camp allowed me to feel a little more comfortable with it
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"The data science immersive program taught me how to gather data through Sequel, JSON, HTML or web-scrapping applications where I went into cleaning the data & then applying Python related code for statistical analysis. I then as able to model the data to either find trends, make predictions, or provide suggestions/recommendations. I wanted to apply this to my background in Cisco NetaCad, cybersecurity principles & software development"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"I then wanted to relate this to my cybersecurity background. I decided to gather data from Data. In this blog post, I decided to write about how I was able to connect my data science knowledge to my cybersecurity background in real world industries. I will provide some background of the project, a code along & some insight into what the FCC could do to better understand the data as I did. This could be useful for future situations or other government related cybersecurity projects"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,There are different approaches I had taken into account when attempting this project. The 1st is diving straight in in order to better understand the data itself where I focused on just the priority of the event & applying a ton of machine learning classification models to it. The 2nd approach was being able to apply natural language processing techniques on the description of the event & see how that correlates to the priority of the event
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"Now that we have our data we can explore, clean & understand the data. Below, I have provided a function for basic data exploratory analysis. We want to do this to fully understand the data we’re dealing with & what the overall goal needs to be met"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"Based off of the data we say that this was a unbalanced classification problem! What we need to do next is eliminate any “NaN” or null values & somewhat balance our class. Based off of our exploratory data analysis, we can see that most of the data has “NaN” values in the object type columns. We can fix this with the following function below!"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"Looking at the priority column(s), we have a object related column that ranks the severity of the event to important, highly important & critical. Another column that corresponds to that priority column ranks them 1–3, with 1 being important, 2 as highly important & 3 being critical. Based off of the variety of priorities, we see that the data is unbalanced. We fix this by renaming our column for better understand & then balancing it where we focus on the highly important & critical events"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"My next step was understanding which columns correlated best for patterns & trends with my priorities column. It seemed that all columns that worked well were binary! The description column were text related & machines don’t like handling text objects. With the below code, we can see which columns are the most positively correlated & most negatively correlated with our predictor column"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"It is basically a machine learning algorithm that is used as a classifier. Whenever you have a large amount of data and you want divide it into different categories, we need a good classification algorithm to do it. Hence the word ‘boosting’, as in it boosts other algorithms!"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"My 2nd approach was focusing on the description column. After my 1st approach, I wanted to see how the priority of the attack correlated with what was given in the description. The description column gave us a short explanation of what happened & a suggested FCC compliant resolution in what they might do in stopping that similar event"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,But what if we wanted more information? We can group the descriptions based on the urgency or severity of the importance of the event. Maybe it’s nothing serious so it’s ranked a 0 (Not Important) or really bad ranked as 1 (Really Important). We can do this with the below code based on our pre-processed columns. We can then visualize what the most common really important words are
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"Finally, we can start modeling regression & classification metrics on the tokenized data. Let’s start by applying a logistic regression model through a pipeline where can apply a grid search tool in order to tune our BEST features or BEST parameters. Let’s establish our X variable, our features! We will use the words or features within the tokenized column within the processed data frame we created above. The processed data frame is an entirely NEW data frame that contains our tokenized, stemmed & lemmatized columns"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"Once the grid search has fit (this can take awhile!) we can pull out a variety of information and useful objects from the grid search object. Often, we’ll want to apply several transformers to a data set & then finally build a model. If you do all of these steps independently, your code when predicting on test data can be messy. It’ll also be prone to errors. Luckily, we’ll have"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,Now we can apply the pipeline on our logistic regression model within a grid search object. Notice the countvectorize model being instantiated. We did this because we want see how that factors into our accuracy & importance of the words we associated with our network attacks
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"From our best hyper parameters, our models favors a Ridge Regression technique. Now we want to make predictions off of our logistic regression & be able to make suggestions. How do we go upon doing that? Let’s look at the coefficients associated with our features that will predict the outcomes of the best y variable"
Applying Data Science to Cybersecurity Network Attacks & Events - KDnuggets,"For the FCC’s CSRIC’s best practices, my best recommendation would be to fix the simple issues first so they don’t happen to often & soak up your resources. This can allow them to focus on the more important & complex events or attacks. Based off what I could predict & analyzing the given data"
"DataTech20 Seeking Speaker Submissions (16 March 2020, Glasgow) - KDnuggets","DataTech is a one-day conference on 16 Mar 2020, at the Technology and Innovation Centre in Glasgow, focusing on key topics in data science, and welcoming members of industry, academia, and the public sector alike. DataTech provides a forum for these different communities to meet, share knowledge and expertise, and forge new collaborations. We are currently welcoming workshop, talk and poster proposals for the DataTech20 conference"
"DataTech20 Seeking Speaker Submissions (16 March 2020, Glasgow) - KDnuggets","Some of Janelle's most popular experiments have included algorithms that try to generate recipes, paint colors, cat names, and candy heart messages - all meant to highlight the reasons to be skeptical of, and look more closely at AI. Janelle has written for the New York Times, the New Yorker, Popular Science, and Slate. Her book 'You Look Like a Thing and I Love You: How AI Works and Why It's Making the World a Weirder Place' is an accessible, hilarious exploration of the present and future of artificial intelligence"
Top 7 Things I Learned in my Data Science Masters - KDnuggets,"This was one of the first things learned. We were introduced to this guy which is like a rock star when it comes to the world of data, churn modeling to be more precise. And hearing that sentence was probably when the first myth I had had about data science got destroyed"
Top 7 Things I Learned in my Data Science Masters - KDnuggets,"Down below is a simple example of feature engineering. Given a list of strings, you need to create a variable that will equal to 1 if the given string contains a question mark (?) and 0 otherwise. You can see how you could achieve this with and without list comprehensions ("
Top 7 Things I Learned in my Data Science Masters - KDnuggets,"Statistics is used to process complex problems in the real world so that Data Scientists and Analysts can look for meaningful trends and changes in Data. In simple words, Statistics can be used to derive meaningful insights from data by performing mathematical computations on it.[1]"
There is No Free Lunch in Data Science - KDnuggets,"Extending this logic (again, Wolpert does it with math equations), he demonstrates that for any two algorithms, A and B, there are as many scenarios where A will perform worse than B as there are where A will outperform B. This even holds true when one of the given algorithms is random guessing. Wolpert proved that for all possible domains (all possible problem instances drawn from a uniform probability distribution), the average performance for algorithms A and B is the same"
There is No Free Lunch in Data Science - KDnuggets,The assumptions made by machine learning algorithms mean that some algorithms will fit certain data sets better than others. It also (by definition) means that there will be as many data sets that a given algorithm will not be able to model effectively. How effective a model will be is directly dependent on how well the assumptions made by the model fit the true nature of the data
There is No Free Lunch in Data Science - KDnuggets,"What this all boils down to: if no prior assumptions about the optimization program can be made, no optimization strategy can be expected to perform better than any other strategy (including random searching). A general purpose universal optimization strategy is theoretically impossible, and the only way one strategy can outperform another is for it to be specialized for the specific problem at hand. Sound familiar? There is no free efficient search optimization either"
There is No Free Lunch in Data Science - KDnuggets,"The “all possible problems” component of the no free lunch theorems is the first sticking point. All possible problems don’t reflect the conditions of the real world. In fact, many would argue that problems taken on by machine learning and optimization, “real world” problems, are nothing like many of the domains included in all problems. The real world tends to exhibit patterns and structures, where the “all possible problems” space includes scenarios that are entirely random or chaotic"
There is No Free Lunch in Data Science - KDnuggets,"A priori knowledge is knowledge that exists independently of experience. This is the type of knowledge that is based on the relation of ideas, for example, 2 + 2 = 4, or an octagon has eight sides. This type of knowledge will be true regardless of what the world is like outside"
There is No Free Lunch in Data Science - KDnuggets,A posteriori knowledge or “matters of fact” are types of knowledge that require experience or empirical evidence. This is the type of knowledge that makes up most personal and scientific knowledge. A posteriori knowledge requires observation
There is No Free Lunch in Data Science - KDnuggets,"There is no reasoning behind assuming uniformity. According to Hume, induction is instinctive for humans, like how dogs seem to instinctively chase rabbits and squirrels. Hume doesn’t really suggest a solution or justification for induction -- he kind of just shrugs and says, we can’t really help the way we are, so let’s not worry about it"
There is No Free Lunch in Data Science - KDnuggets,"Science (and machine learning models) inherently believe that under the same conditions everything always happens the same way. There is no reason to assume that because a model worked well on one data set, it will work well on other data sets. Additionally, science, statistics, and machine learning models cannot give guarantees about future experiments based on the results of previous experiments. No system can give guarantees about prediction, control, or observation in any environment"
There is No Free Lunch in Data Science - KDnuggets,"Bias-free learning is futile because a learner that makes no a priori assumptions will have no rational basis for creating estimates when provided new, unseen input data. The assumptions of an algorithm will work for some data sets but fail for others. This phenomenon is important to understand the concepts of"
There is No Free Lunch in Data Science - KDnuggets,"The combination of your data and a randomly selected machine learning model are not enough to make accurate or meaningful predictions about the future or unknown outcomes. You, the human, will need to make assumptions about the nature of your data and the world we live in. Playing an active role in making assumptions will only strengthen your models and make them more useful,"
Using DC/OS to Accelerate Data Science in the Enterprise - KDnuggets,"While this made the book much more valuable for beginners who would otherwise have trouble running the code, it has been extremely difficult to maintain and keep running. Older software falls off the internet and the platform rots. There have been"
Using DC/OS to Accelerate Data Science in the Enterprise - KDnuggets,"It has become fairly easy to setup a Jupyter Notebook in any given cloud environment like Amazon Web Services (AWS), Google Cloud Platform (GCP) and Microsoft Azure for an individual data scientist to work. For startups and small data science teams, this is a good solution. Nothing stays up to be maintained and notebooks can be saved in Github for persistence and sharing"
Using DC/OS to Accelerate Data Science in the Enterprise - KDnuggets,"For large enterprises, things are not so simple. At this scale, temporary environments on transitory assets across multiple clouds can create chaos rather than order, as environments and modeling become irreproducible. Enterprises work across multiple clouds and on premises, have particular access control and authentication requirements, and need to provide access to internal resources for data, source control, streaming and other services"
Using DC/OS to Accelerate Data Science in the Enterprise - KDnuggets,"From the DC/OS web console, the Data Science Engine is available along with many other services like Kafka, Spark and Cassandra from the Catalog menu. We need only select the 'data-science-engine'package and configure the resources to give the service: CPUs, RAM and GPUs. There are many other options if you need them, but they aren’t required"
Using DC/OS to Accelerate Data Science in the Enterprise - KDnuggets,"The tutorial creates a Stack Overflow tagger for the 786 most frequent tags based upon a convolutional neural network document classifier model called Kim-CNN. The notebook is typical for deep networks and NLP. We first verify that GPU support works in Tensorflow and we follow the best practice of defining variables for all model parameters to facilitate a search for hyper parameters. Then we tokenize, pad and convert the labels to a matrix before performing a test/train split to enable us to independently verify the model’s performance once it is trained"
Using DC/OS to Accelerate Data Science in the Enterprise - KDnuggets,"Finally, it is not enough to know theoretical performance. We need to see the actual output of the tagger at different confidence thresholds. We create a DataFrame of Stack Overflow questions, their actual tags and the tags we predict to give us a direct demonstration of the model and it’s real world performance"
Using DC/OS to Accelerate Data Science in the Enterprise - KDnuggets,"All in all I was impressed with the DC/OS Data Science Engine. The setup was fairly easy manually, the environment was suitable for real use and automation proved easy. I will definitely consider this platform as an option for running the examples in my book. If you’d like to learn more, check out the full post"
Classification vs Prediction - KDnuggets,"The field of machine learning arose somewhat independently of the field of statistics. As a result, machine learning experts tend not to emphasize probabilistic thinking. Probabilistic thinking and understanding uncertainty and variation are hallmarks of statistics. By the way, one of the best books about probabilistic thinking is Nate Silver’s"
Classification vs Prediction - KDnuggets,"By not thinking probabilistically, machine learning advocates frequently utilize classifiers instead of using risk prediction models. The situation has gotten acute: many machine learning experts actually label logistic regression as a classification method (it is not). It is important to think about what classification really implies. Classification is in effect a decision. Optimum decisions require making full use of available data, developing predictions, and applying a loss/utility/cost function to make a decision that, for example, minimizes expected loss or maximizes expected utility. Different end users have different utility functions. In risk assessment this leads to their having different risk thresholds for action. Classification assumes that every user has the same utility function and that the utility function implied by the classification system is"
Classification vs Prediction - KDnuggets,"A frequent argument from data users, e. This is simply not true. First of all, it is often the case that the best decision is “no decision; get more data” when the probability of disease is in the middle. In many other cases, the decision is revocable, e. In surgical therapy the decision to operate is irrevocable, but the choice of"
Classification vs Prediction - KDnuggets,"When are forced choices appropriate? I think that one needs to consider whether the problem is mechanistic or stochastic/probabilistic. Machine learning advocates often want to apply methods made for the former to problems where biologic variation, sampling variability, and measurement errors exist. It may be best to apply classification techniques instead just to high signal:noise ratio situations such as those in which there there is a known gold standard and one can replicate the experiment and get almost the same result each time. An example is pattern recognition - visual, sound, chemical composition, etc. If one creates an optical character recognition algorithm, the algorithm can be trained by exposing it to any number of replicates of attempts to classify an image as the letters A, B, … The user of such a classifier may not have time to consider whether any of the classifications were “close calls. In addition, there is a single “right” answer for each character. This situation is primarily mechanistic or non-stochastic. Contrast that with forecasting death or disease where two patients with identical known characteristics can easily have different outcomes"
Classification vs Prediction - KDnuggets,"When close calls are possible, or when there is inherent randomness to the outcomes, probability estimates are called for. One beauty of probabilities is that they are their own error measures. If the probability of disease is 0.1 and the current decision is not to treat the patient, the probability of this being an error is by definition 0.1. A probability of 0.4 may lead the physician to run another lab test or do a biopsy. When the signal:noise ratio is small, classification is usually not a good goal; there one must model"
Classification vs Prediction - KDnuggets,The U.S. Weather Service has always phrased rain forecasts as probabilities. I do not want a classification of “it will rain today
Classification vs Prediction - KDnuggets,"Whether engaging in credit risk scoring, weather forecasting, climate forecasting, marketing, diagnosis a patient’s disease, or estimating a patient’s prognosis, I do not want to use a classification method. I want risk estimates with credible intervals or confidence intervals. My opinion is that machine learning classifiers are best used in mechanistic high signal:noise ratio situations, and that probability models should be used in most other situations"
Classification vs Prediction - KDnuggets,"A special problem with classifiers illustrates an important issue. Users of machine classifiers know that a highly imbalanced sample with regard to a binary outcome variable Y results in a strange classifier. For example, if the sample has 1000 diseased patients and 1,000,000 non-diseased patients, the best classifier may classify everyone as non-diseased; you will be correct 0.999 of the time. For this reason the odd practice of subsampling the controls is used in an attempt to balance the frequencies and get some variation that will lead to sensible looking classifiers (users of regression models would never exclude good data to get an answer). Then they have to, in some ill-defined way, construct the classifier to make up for biasing the sample. It is simply the case that a classifier trained to a"
Classification vs Prediction - KDnuggets,"The classifier would have to be re-trained on the new sample, and the patterns detected may change greatly. Logistic regression on the other hand elegantly handles this situation by either (1) having as predictors the variables that made the prevalence so low, or (2) recalibrating the intercept (only) for another dataset with much higher prevalence. Classifiers’ extreme dependence on prevalence may be enough to make some researchers always use probability estimators like logistic regression instead. One could go so far as to say that classifiers should not be used at all when there is little variation in the outcome variable, and that only tendencies (probabilities) should be modeled"
Classification vs Prediction - KDnuggets,"One of the key elements in choosing a method is having a sensitive accuracy scoring rule with the correct statistical properties. Experts in machine classification seldom have the background to understand this enormously important issue, and choosing an improper accuracy score such as proportion classified correctly will result in a bogus model. This is discussed in detail"
The 5 Graph Algorithms That Data Scientists Should Know - KDnuggets,As you can see we are able to find distinct components in our data. Just by using Edges and Vertices. This algorithm could be run on different data to satisfy any use case that I presented above
The 5 Graph Algorithms That Data Scientists Should Know - KDnuggets,"As I said, it was a twenty-minute invention. In fact, it was published in ’59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually that algorithm became, to my great amazement, one of the cornerstones of my fame"
The 5 Graph Algorithms That Data Scientists Should Know - KDnuggets,"For this exercise, we are going to be using Facebook data. We have a file of edges/links between facebook users. We first create the FB graph using:"
The 5 Graph Algorithms That Data Scientists Should Know - KDnuggets,There are a lot of centrality measures which you can use as features to your machine learning models. I will talk about two of them. You can look at other measures
The 5 Graph Algorithms That Data Scientists Should Know - KDnuggets,You can see the nodes sized by their betweenness centrality values here. They can be thought of as information passers. Breaking any of the nodes with a high betweenness Centrality will break the graph into many parts
The 5 Graph Algorithms That Data Scientists Should Know - KDnuggets,"There are a lot of graph algorithms out there, but these are the ones I like the most. Do look into the algorithms in more detail if you like. In this post, I just wanted to get the required breadth into the area"
The 5 Graph Algorithms That Data Scientists Should Know - KDnuggets,Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
An ODSC West Guide to the Most Important Topics in Data Science Right Now - KDnuggets,"Originally only known as a side effect of GANs, deepfakes can create images, videos, and voice files capable of deceiving, at least initially, the general public. However, as the threat has escalated, so has the response from data scientists and tech organizations. Many are"
An ODSC West Guide to the Most Important Topics in Data Science Right Now - KDnuggets,"Or, of greater concern, an AI model can be tricked into thinking that an image of machine guns is, in fact, an image of a helicopter. As AI models are incorporated into our everyday lives, it becomes increasingly important to understand how to keep them secure. If this is a subject that interests you, don’t miss"
An ODSC West Guide to the Most Important Topics in Data Science Right Now - KDnuggets,"The last, but definitely not the least, major topic that we’ll address in today’s article is homelessness. Although this is another complicated, intricate, and difficult to address societal issue, data scientists have been making progress in recent years. In 2018,"
An ODSC West Guide to the Most Important Topics in Data Science Right Now - KDnuggets,"Every year ODSC West strives to bring you content on the tools, platforms, and topics that influence and affect you every day. This year, we believe our talks, workshops, and trainings will cover content that is not only interesting and relevant, but also important for the creation of a better society and data science community. To learn more, and see how you might contribute to these efforts, we hope you’ll join us at"
"Will Machine Learning End Retail? Data Science Seattle Oct 17, 2019 - KDnuggets","We’re definitely seeing the needle move on the business analytics side according to Dhivya Rajprasad, Data Scientist at Levi Strauss & Co.  Also, the lack of ability to interact with clothing means that shoppers are more likely to return items that they buy online. We use ML to recommend items we think shoppers will like, and we use ML to help shoppers get the right size, which decreases the need to return items"
"Will Machine Learning End Retail? Data Science Seattle Oct 17, 2019 - KDnuggets","It’s also clear that when it comes to privacy, retailers have taken notice. Rajprasad agrees, “Privacy and ethics are and should be the cornerstone of data science vision for a company. One challenge we face is ensuring the highest level of privacy by not encoding real time PII data which will be still be connected but completely anonymized to ensure a degree of personalization without compromising the privacy of the person.  We have a responsibility to protect that data, to ensure that the raw data is not exposed with any personally identifying information, and to provide value to the user in the form of good recommendations. We are also working to provide experiences for shoppers who elect to remain anonymous by providing general information about products that do not require information specific to shoppers"
"Will Machine Learning End Retail? Data Science Seattle Oct 17, 2019 - KDnuggets","Textor disagrees, “I think it's too early to think about a convergence of tools and methods.  There have been many exciting advances in recommendation and computer vision technologies that are powering new experiences such as visual search in shopping.  It's too early to know where those experiences are succeeding and where they fall short. As the academic and research communities continue to rapidly advance the basic science in this area, we will continue to try out new tools"
I wasn’t getting hired as a Data Scientist. So I sought data on who is. - KDnuggets,"As a dedicated Medium platform for “sharing concepts, ideas, and codes” in data science, it is not surprising that such learning resources attain high popularity amongst Towards Data Science followers, who are probably navigating data-centric projects and professions. But to a novice looking to prioritize what is essential, it can quickly become daunting. Should one train to become a master Kaggler? Apply neural networks in image recognition or Natural Language Processing?"
I wasn’t getting hired as a Data Scientist. So I sought data on who is. - KDnuggets,"Based on my job history, the first half of that pair is probably more accurate, as I’ve only ever secured short-term contract gigs in data science. Having voluntarily moved out of an earlier career as a healthcare statistician, I was becoming vexed with my attempts to land a full-time data scientist position where I’m based, which is Singapore. I’ve seen some acquaintances, with only a Bachelor’s degree, readily secure positions while my Master’s in Medical Statistics and General Assembly certificate in Web Development did not seem to land the knockout blow that I hoped they would (two out of three in the"
I wasn’t getting hired as a Data Scientist. So I sought data on who is. - KDnuggets,"A plurality (44%) hold a Master’s degree, while Ph.D. Only 6% of data scientists reported some form of MOOC, bootcamp or non-traditional certification as their primary qualification. This suggests that prospective employers trust the signaling provided by an advanced degree to fulfill the complex requirements of the data scientist position"
I wasn’t getting hired as a Data Scientist. So I sought data on who is. - KDnuggets,"2%). The picture that emerges is that while computing- and engineering-related fields have demonstrated continuing relevance for becoming a data scientist, mathematics and statistics are somewhat being eclipsed by the newer business-oriented field of Analytics (and its variants). Nevertheless, a very long tail of other fields represents the broad diversity of disciplines that have been pursued by current data scientists"
I wasn’t getting hired as a Data Scientist. So I sought data on who is. - KDnuggets,"LinkedIn only displays profiles that have at least a 3rd-degree connection to you, and the profiles might have been sorted by a non-random algorithm (my scraper extracted the top profile results in order). There is a case to be made that I am not optimally connected to obtain a truly random sample of data scientists from my target market (e. Getting more profiles from other LinkedIn accounts and doing a sensitivity analysis would throw more light on this question"
10 Great Python Resources for Aspiring Data Scientists - KDnuggets,"Many prospective data scientists are first faced with the issue of which programming language might be their choice when diving into data science. This is further complicated if you don't already bring a set of existing programming skills on which to rely. Even better would be a thorough understanding of Python as you shift to data science (substitute another language if it is to be your preferred data science programming tool), but many newcomers to the field find themselves either starting from relative scratch when it comes to either programming in general, or Python more specifically"
10 Great Python Resources for Aspiring Data Scientists - KDnuggets,Most of these ‘tricks’ are things I’ve used or stumbled upon during my day-to-day work. Some I found while browsing the Python Standard Library docs. A few others I found searching through PyPi
10 Great Python Resources for Aspiring Data Scientists - KDnuggets,"However, Python ecosystem co-exists in Python 2 and Python 3, and Python 2 is still used among data scientists. By the end of 2019 the scientific stack will stop supporting Python2. As for numpy, after 2018 any new feature releases will only support Python3"
10 Great Python Resources for Aspiring Data Scientists - KDnuggets,"Before asyncio (sometimes written as async IO), which is a concurrent programming design in Python, there were generator-based co-routines; Python 3.10 removes those. The asyncio module was added in Python 3.4, followed by async/await in 3.5"
10 Great Python Resources for Aspiring Data Scientists - KDnuggets,"Django is a fully featured Python web framework that can be used to build complex web applications. In this tutorial, you’ll jump in and learn Django by example. You’ll follow the steps to create a fully functioning web application and, along the way, learn some of the most important features of the framework and how they work together"
10 Great Python Resources for Aspiring Data Scientists - KDnuggets,"Some programming languages live in the heart of data science. Python is one of those languages. It is an integral ingredient for Data Science and vice versa. And actually, it would take prominently long to explain why"
10 Great Python Resources for Aspiring Data Scientists - KDnuggets,"Let’s start with the fact that Python provides great functionality to deal with mathematics, statistics and scientific function. When it comes to data science application, it provides extensive libraries to deal with. Not to mention it is open-source, interpreted, high-level tool!"
10 Great Python Resources for Aspiring Data Scientists - KDnuggets,"Python is a general purpose programming language, meaning it can be used in the development of both web and desktop applications. It’s also useful in the development of complex numeric and scientific applications. With this sort of versatility, it comes as no surprise that Python is one of the fastest growing programming languages in the world"
Automate your Python Scripts with Task Scheduler: Windows Task Scheduler to Scrape Alternative Data - KDnuggets,"Imagine your manager asks you to wake up in the middle of night to run a script. This will be your biggest nightmare. You wake up prematurely, expose with the horrendous blue light, and avoid decent sleeps every midnight"
Automate your Python Scripts with Task Scheduler: Windows Task Scheduler to Scrape Alternative Data - KDnuggets,"In a larger picture, this task would contain the script and metadata to define what and how the action will be executed. You can add certain security context in the argument and control where the scheduler will run the program in. Windows will serialize all of these tasks as a"
Automate your Python Scripts with Task Scheduler: Windows Task Scheduler to Scrape Alternative Data - KDnuggets,"You will have an option to pick the time trigger in daily weekly, and even monthly. Logically, this choice depends largely on how often you want to refresh the values from your data source. For example, if your task is to scrape MarketWatch Stocks balance sheet, you should run the scripts every financial quarter"
Automate your Python Scripts with Task Scheduler: Windows Task Scheduler to Scrape Alternative Data - KDnuggets,"Here you will be able to start the Python Scripts, send an e-mail, and even display a message. Feel free to choose ones which you are most comfortable with. However, you should watch out as there are deprecated tasks which will be removed in the subsequent patches"
Automate your Python Scripts with Task Scheduler: Windows Task Scheduler to Scrape Alternative Data - KDnuggets,"Here is the gif animation for your references. Notice how the scheduler runs the Python Scripts by itself. Once the scripts finish running, it will dump the extracted value inside SQLite database. In the future, this application will run every time the trigger condition is met and append the updated values into SQLite"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"The energy sector is under constant development, and more of significant inventions and innovations are yet to come. The energy use has always been involved in other industries like agriculture, manufacturing, transportation, and many others. Thus these industries tend to enlarge the amount of energy they consume every day. Energy seems to be very demanding in terms of new technologies application and development of new energy sources"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"The rapid development of the energy sector and utilities directly influences social development. People are now facing challenges of smart energy management and consuming, application of renewable energy sources and environmental protection. Smart technologies play a crucial role in the resolution on these matters. In this article, we will consider the most vivid data science use cases in the industry of energy and utilities"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Active application of probability modeling helps to increase performance, predict occasional failures in the functioning and as a result to reduce maintenance costs. The energy companies invest vast amounts of money into maintenance and proper functioning of their machines and devices. Unexpected failures in their operations result in considerable financial losses. Moreover, for people who rely on these companies as their energy source the situation gets critical. As a result, general reliability and image of the energy provider may suffer"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Despite the efforts made by the companies belonging to the energy industry, the power outage still takes place, leaving a considerable number of people without power. In this respect, people tend to regard the blackouts as a failure of the electric grids. However, the blackout is a preventive measure, a result of the automatic protection system operation"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"In previous years, the energy systems engineers used static algorithms and models rather than real-time solutions. Nowadays, numerous companies dealing with energy and utilities are actively upgrading their systems to improve outage detection and prediction. Modern smart power outage communication systems are capable of:"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Outage detection and prediction starts with the identification of the right metrics and the threshold value for it. Every single outage event should be carefully analyzed to identify the root cause. Only after that, predictive algorithms may be applied to model the future likelihood of an outage. The application of the smart energy outage ecosystems allows providing accurate real-time outage statuses to improve general customer experience and satisfaction"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Dynamic energy management systems belong to the innovative approach to managing the load. This type of management covers all the conventional energy management principles concerning demand, distributed energy sources, and demand-side management along with modern energy challenges like energy saving, temporary load, and demand reduction. Therefore, smart energy management systems have developed abilities to combine smart end-use devices, distributed energy resources, and advanced control and communication"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Big data analytics plays a leading part here as it empowers dynamic management systems in Smart Grids. This largely contributes to the optimization of the energy flows between the providers and consumers. The efficiency of the energy management system, in its turn, depends on the load forecasting and renewable energy sources"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Energy theft may be regarded as one of the most expensive types of theft. Therefore, energy companies make great efforts to prevent it. Energy theft with smart grids often happens via a direct tap into the distribution cable"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"To get the maximum return on investments and to use complex machines and equipment at the peak of their efficiency, the companies dealing with energy distribution and utilities have been applying preventive equipment maintenance for decades. Smart data solutions, sensors, and trackers are used to collect the defined metrics, process and analyze the data. On the basis of the output, the smart systems alert the energy outage, the poor functioning of the mechanisms and urge people to take right and immediate decisions"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Under conditions of a constant search for renewable energy sources and the need to use energy efficiently, smart energy management is at the peak of its popularity. A key to successful energy management lays in the balance between demand and supply. Both high and low demand rates cause a lot of problems and costs for both energy providers and consumers"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Therefore, demand response is a strategy that has proved its efficiency over time. Specific real-time management applications and solutions allow monitoring metrics of energy use, define the activity pick and adjust the energy flow to the current demand rate. Moreover, there exist response management programs encouraging consumers to use energy at a specific time and save money. Thus, consumers get a chance to shift to a better pricing program and providers get an opportunity to achieve the desired balance in energy provision"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"There is nothing strange in the companies desire to improve their customer service and increase the customers’ satisfaction rates. Energy and utility companies do not lack behind the others. They strive to bring visibility into the service provision process, billing and payment operations, improve quality and eliminate delays, misunderstanding or disputable issues. Companies use a whole bunch of applications and software to manage numerous customers, billing, payment, invoicing. Customers, in their turn, have an opportunity to monitor the transaction as well"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Energy and utility companies use smart data application and software to detect the matters, operations and functions worth of optimization. Real-time monitoring provides data concerning time, activity rate, state of some operations. The data is processed in combination with the external factors to define the average efficiency. Data science here is used for modeling of various situations and prediction of possible efficiency rates under various circumstances"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"Real-time data concerning assets health, supply and demand analysis helps to improve asset performance. Data-driven and business analytics tools and software are used to monitor conditions, costs, and performance, as well as to define scoring methods and the areas of critical priority. Data-driven and business analytics tools and software are used to enhance the reliability, capacity, and availability of the assets and minimize costs. The more data you have, the more you can do to manage the assets better"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"There exist two prioritized dimensions of work for energy and utility companies directly related to general brand reputation. These are operational excellence and customer experience, which by nature are interdependent. The rapid development of smart technologies and the growing popularity of smart home provide new opportunities to users. Due to this fact, customers become more sophisticated in their choice of company or service. Thus, the demand for high-quality services increases"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"All the companies are doing their best to adhere to the customers’ needs and desires. First of all, the application of multiple communication channels should be applied for this purpose. Omni channels provide the company with valuable insights for further processing. With the help of accurate analysis, the companies can effectively reveal the information about the customers’ demographics, behavior, and sentiment. As a result, they can tailor personalized recommendations, suggestions, and services"
Top 10 Data Science Use Cases in Energy and Utilities - KDnuggets,"The energy and utility companies are under constant pressure to provide high-quality services without delays and failures at an affordable price 24/7. People rely on energy sources in their daily dealings and work. Due to the fast development and improvement of technologies, the industry stands before new opportunities and new challenges every day"
Types of Bias in Machine Learning - KDnuggets,We all have to consider sampling bias on our training data as a result of human input. Machine learning models are predictive engines that train on a large mass of data based on the past. They are made to predict
Types of Bias in Machine Learning - KDnuggets,"This again is a cause of human input. Prejudice occurs as a result of cultural stereotypes in the people involved in the process. Social class, race, nationality, gender can creep into a model that can completely and"
Types of Bias in Machine Learning - KDnuggets,"This type of bias results from when you train a model with data that contains an asymmetric view of a certain group. For example, in a certain sample dataset if the majority of a certain gender would be more successful than the other or if the majority of a certain race makes more than another, your model will be inclined to believe these falsehoods. There is label bias in these cases. In actuality, these sorts of labels should not make it into a model in the first place. The sample used to understand and analyse the current situation cannot just be used as training data without the appropriate pre-processing to account for any potential unjust bias. Machine learning models are becoming more ingrained in society without the ordinary person even knowing which makes group attribution bias just as likely to punish a person unjustly because the necessary steps were not taken to account for the bias in the training data"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"Despite the increasing demand and appetite for experienced data scientists, the job is ambiguously described most of the times. Also, the delineation between data science and data analytics or engineering is still loosely defined by a lot of hiring managers. This lack of real industry standard confuses a lot of professionals who desire to switch to a data science role. Speaking to many analysts and software developers I realise how overwhelming the available information about AI and machine learning (ML) can be. I also know from experience how hard it is to know where to start without any guidance. Currently, I am a data scientist at"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"Highly skilled data scientists are able to change the computer program at the level of mathematics and thus drive real improvement in model performance. It is important to have the mathematical skills, especially statistics and linear algebra. Having the"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"A data analyst reports, summarises and interprets both historical and current information to make it useable for the business. That is very different from a data scientist, whose role is to summarise data in a way that allows to make a prediction about the future or a prescriptive decision. The core task of data scientists is to"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"Theory and heavy equations can be overwhelming sometimes and should not keep someone out of the field. An approach that worked for me was doing my reading in parallel to coding. For example, try to build a single-layer perceptron (the simplest kind of neural network) from scratch to fully understand what you’ve read in the books"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,The real skill of a data scientist is knowing what technology and machine learning methodologies are needed to answer business questions at hand. The field is booming during the last decade and continuous thirst for knowledge is required to shine as a data science professional. I would strongly advise to read both published academic papers and ML/AI blogs of different technology companies and key personas in the field. This can turn out helpful when you are asked to deliver solutions for abstract problem statements which do not provide an immediate solution. Finding the right solution via researching what’s out there is 80% of the job done. Andrej Karpathy very well said in the Stanford class
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"In my team, we don’t underestimate the effort and time others have put in finding the architecture that currently works best. Instead of rolling our own architecture for a common problem, we import libraries, download pre-trained models and fine-tune them on our data. The business world expects you to"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"Data analysts use data in a way to help businesses make informed decisions. They are masters of SQL, Excel and visualisation tools such as Tableau or Power BI. On the other hand, data scientists need to build robust models to extrapolate and solve business problems at scale. Hence, they are required to develop their programming skills. I wasn’t coding from the age of 10 with a hoodie on, but it was never too late for me to start learning how to code. At university I learnt Machine Learning in Matlab and I coded in JavaScript for different work projects, but it was important to practise the pythonic ways"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"Entering hackathons, participating in kaggle competitions, working on a personal coding projects are all different ways of improving your programming skills. Identifying or getting involved in data science opportunities that come out of the results of your analysis can be a way to gain experience in your current role. Algorithms for forecasting and anomaly detection can be other projects you can ask to work on even as part of your development as an analyst. I remember my first data science project in the industry was an algorithm to autocomplete search queries on an"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"Software engineering skills become necessary when you want your models to see the production light. Cultivating a coding attitude that aims for reproducibility of projects and results via automation is critical for both methodological and legal reasons. In a company with a mature data science culture, someone might create the prototype, someone else might write the production code and someone else might deploy it. In reality though and irregardless of the company’s size, it’s unlikely that you will have all the support required and knowing just the statistics will not be enough to deliver a data science project"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"As a data person and a subject matter expert, you can overcome blockers such as a missing business or KPI definition by finding proxies in data or make it outright a latent factor which you learn with ML. Data science usually brings disruption to a business and as a result, you will need to pitch your ideas to senior leadership in order to get the appropriate support and resources. Someone might say that making an algorithm understandable for all stakeholders in the business is a form of art. Learning how to translate what I’ve built in order to show its importance to others is something I am constantly having to learn and relearn. As"
The secret sauce for growing from a data analyst to a data scientist - KDnuggets,"Ideally, your manager understands your day to day job and where you want to get to. Otherwise, find the extra guidance you might need outside your team or company, for example from an alumni or professor from your university or from a friendly data scientist from your network. Meet-ups and conferences could also be inspiring and help you with this task"
7 Super Simple Steps From Idea To Successful Data Science Project - KDnuggets,Twitter has some strict API rules for how much data you can query in a certain time. You have to throttle your software to not go all out. It will get timed out otherwise
7 Super Simple Steps From Idea To Successful Data Science Project - KDnuggets,You know how fast you can ingest. You know the performance of the storage and the analytics. You have a clear indication what customers are doing
"Data Science Symposium 2019, Oct 10-11, Cincinnati - KDnuggets","The UC Center for Business Analytics will present the Data Science Symposium 2019 on Oct 10 & 11. This event will be held at the new Lindner College of Business and feature 3 keynote speakers and 16 tech talks/tutorials on a wide range of data science topics and tools. Capacity is limited for this event and early registration is recommended. For event details and registration, please go to"
Turbo-Charging Data Science with AutoML - KDnuggets,"Although there are an increasing number of commercial AutoML products, the open-source ecosystem has been innovating here as well. In the early days of the AutoML movement, the focus was on those looking to leverage the power of ML models without a background in data science - citizen data scientists. Today, however, AutoML tools have a lot to offer experts too"
Turbo-Charging Data Science with AutoML - KDnuggets,"Join this technical webinar, where Domino Chief Data Scientist Josh Poduska will dive into popular open source AutoML tools such as auto-sklearn, TPOT, MLBox, and AutoKeras. We will also walk through hands-on examples of how to install and use these tools, and highlight special features of each while providing Jupyter notebooks so you can start using these technologies in your work right away. Those who wish to follow along interactively during the webinar and download the notebooks and slides can do so by"
Process Mining with R: Introduction - KDnuggets,"Hierarchical as an event log is composed out of multiple trails recording the execution of one process instance, which in itself is composed out of multiple events. These events are ordered based on their order of execution. A prime use case in process mining is hence to abstract this complex data set to a visual representation, a process model, highlighting frequent behavior, bottlenecks, or any other data dimension that might be of interest to the end user"
Process Mining with R: Introduction - KDnuggets,"Traditional data mining tooling like R, SAS, or Python are powerful to filter, query, and analyze flat tables, but are not yet widely used by the process mining community to achieve the aforementioned tasks, due to the atypical nature of event logs. Instead, an ecosystem of separate tools has appeared, including, among others: Disco, Minit, ProcessGold, Celonis Discovery, ProM, and others. In this article, we want to provide a quick whirlwind tour on how to create rich process maps using a more traditional data science programming language: R. This can come in helpful in cases where the tools listed above might not be available, but also provides hints on how typical process mining tasks can be enhanced by or more easily combined with other more data mining related tasks that are performed in this setting. In addition, the direct hands-on approach of working with data will show that getting started with process discovery is very approachable. We pick R here as our “working language” as the fluidity of modern R (i"
Process Mining with R: Introduction - KDnuggets,"This latter requires some considerate thinking. Even although a distinct list of (ActivityA-ActivityB)-pairs is sufficient to construct the process map, we do need to take into account the correct number of times an arc appears in order to calculate frequency and other metrics. We hence first merge the (Case Identifier, RowNum, NextNum) listing with (Case Identifier, PrevNum, RowNum) and filter out all entries where an endpoint of the arc equals NA. We then apply the “distinct” operator to make sure we get a unique listing, albeit at the row number level, and not at the activity name level. This makes sure our frequency counts remain correct. We can then figure out the endpoints per arc at the activity level by joining the event log on itself on both endpoints. We then select, for each arc, the case identifier, endpoint row numbers (“a” and “b”), the activity names of the endpoints, the activity start and completion times of the endpoints, and the duration of the arc based on the endpoints, i"
Process Mining with R: Introduction - KDnuggets,"This is all the information we need to start constructing process maps. We’ll use the excellent “igraph” package to visualize the process maps. Plotting it directly in R, however, gives non-appealing results by default. Sadly, igraph, like most graphing tools, is built with the assumption that graphs can be very unstructured (think social network graphs), so that the layout for process oriented graphs does not look very appealing. Luckily, igraph comes with powerful export capabilities that we can use to our benefit. Here, we’ll export our graphs to the dot file format and use “"
Data Science is Boring (Part 1) - KDnuggets,"Feeling bored creates tension; it, ultimately, leads to a high turnover rate in data science. I want to share what I actually do and how I cope with the “boringness in data science”. I hope to help you, the aspiring Data Scientists, to set the right expectations. So, once you decide to pursue a Data Science career, you are in it for the long game. Enjoy"
Data Science is Boring (Part 1) - KDnuggets,"We will launch a free Community Edition on September 20, 2019. We are looking for 1000 early users who can share their feedback and help us improve. See more details and sign up"
Data Science is Boring (Part 1) - KDnuggets,"My young and handsome cousin, Shawn, came to Canada recently. He’s here to pursue a master degree in Computer Science. Like many students, Shawn is very passionate about Machine Learning. He wants to become a Data Scientist (or anything that has to do with ML) when he graduates in 2 years"
Data Science is Boring (Part 1) - KDnuggets,"This is when we collectively get “high” intellectually to problem-solve and propose brilliant ideas. These ideas can include new model architecture, data features, and system design, etc. Very soon, we would hit a low because we need to go with the simplest (and often the most boring) solution due to time constraints and other priorities"
Data Science is Boring (Part 1) - KDnuggets,"Our codes generally fit into five categories (% of total lines of code): data pipeline (50-70%), system and integration things (10–20%), ML model (5–10%), analysis to support debugging and presentations (5–10%). It’s roughly in line with other people’s observations. Here is a bigger picture"
Data Science is Boring (Part 1) - KDnuggets,"Although the ML component is very critical, modern frameworks and coding languages (e. Keras, XGBoost, Python’s sklearn, etc) have abstracted lots of the complexity away. This means that achieving the results we need does not require a heavy codebase; the workflow is already well-standardized and optimized (doing low-level optimization is different, but it’s probably 1% of cases)"
Data Science is Boring (Part 1) - KDnuggets,"This is a nightmare for any delivery team managers and not specific to data science. It does not matter how thought out the timeline is. Things always come up and throw you off track. To be concrete, surprises can be grouped into three categories:"
Data Science is Boring (Part 1) - KDnuggets,"All of these is to say that real-world data science is difficult. People with aspirations to pursue a career in ML should recognize the fact that there are a lot more to just building models. You will eventually get bored and frustrated, just like you would with any career"
Data Science is Boring (Part 1) - KDnuggets,"We will launch a free Community Edition on September 20, 2019. We are looking for 1000 early users who can share their feedback and help us improve. See more details and sign up"
6 Books Every Data Scientist Should Keep Nearby - KDnuggets,"Apache Hadoop is the primary framework used to process and manage large quantities of data. Anyone working in programming or data science will be familiar with the platform, because it’s necessary. In fact, it’s one of the most efficient ways to develop a scalable system"
6 Books Every Data Scientist Should Keep Nearby - KDnuggets,"It is the job of a data scientist to look at raw, unfiltered data and identify usable trends and patterns. This book will not only help you do that, but come up with the necessary predictive algorithms to improve future operations and processes. Consider it the Bible of predictive analytics"
6 Books Every Data Scientist Should Keep Nearby - KDnuggets,"Statistical learning and related methods are necessary to work in data science. This textbook is designed to help anyone and everyone, from an undergraduate to a Ph.D"
The thin line between data science and data engineering - KDnuggets,"That’s why we sat down with Akshay Singh, who among other things has worked in and managed data science teams at Amazon, League and the Chan-Zuckerberg Initiative (formerly Meta. Akshay works at the intersection of data science and data engineering, and walked us through the fine line between data analytics and data science, the future of the field, and his thoughts on best practices that aren’t getting enough love. Here were our key take-homes:"
Top Handy SQL Features for Data Scientists - KDnuggets,"SQL is to data science as nature is to life. Without preserving nature, there is no possibility of life. Without preserving the knowledge of SQL, there would be no data science"
Top Handy SQL Features for Data Scientists - KDnuggets,"SQL (Structured Query Language) has been around for a long time and is a very comfortable programming language for many developers. SQL comes with easy and quick to learn features to organize and retrieve data, as well as perform actions on it in order to gain useful insights. You can read some of the commonly asked"
Top Handy SQL Features for Data Scientists - KDnuggets,"If you are into web development, you know that data is fetched from the backend (database) and presented to the front-end (UI). In the same way, data is entered by the user into the database. Visit Squareboat to know how we can help you with"
Top Handy SQL Features for Data Scientists - KDnuggets,"Getting such limited number of records from a huge data set is called top-n analysis. Some real-life examples could be – top 2 students who scored the highest marks, top 10 Facebook users who spend most time browsing, top 3 products that were sold by a company, top 5 employees of the month, least 3 performers of the month and so on. For example –"
Top Handy SQL Features for Data Scientists - KDnuggets,"There are many useful string functions in SQL just like we have in programming languages. You can avoid writing lines of code when such features are available in the DBMS itself, because it is faster. Some common string functions are –"
Top Handy SQL Features for Data Scientists - KDnuggets,"To convert the entire string to upper or lower case, we can use this function. This can come in handy when we want something to be printed in a respective case. For example, if we want a student's first name to be prints in all caps, we can use upper function –"
Top Handy SQL Features for Data Scientists - KDnuggets,"If we have to replace one or more characters with another, we can use this function. For example, let us say we need to get the list of mobile numbers without dashes in between. The value database is 1-832-234-1098 and we want to replace all the hyphens with spaces"
Top Handy SQL Features for Data Scientists - KDnuggets,"Sometimes database entries are with a lot of spaces at the beginning or end – it may be that the user entered it, or a program padded the spaces; we don’t know. If you wish to trim those and present the data, use these functions. Example –"
Top Handy SQL Features for Data Scientists - KDnuggets,"Aggregate functions allow us to find the sum (SUM), average(AVG), minimum(MIN), maximum(MAX) and count(COUNT) values from a set of data. We use these functions with group by and having clauses. For example, if we want to know what is the average percentage of marks that the students of each department have collectively, we can use the AVG function"
Top Handy SQL Features for Data Scientists - KDnuggets,"You can do the same using any programming language like JavaScript, but it is simpler and less time consuming than executing code. Regular expressions can also be used to find a specific pattern in any columns of a table. For example, if you want to get the name of students that contains ‘ya’, you can do so by using the LIKE clause"
Top Handy SQL Features for Data Scientists - KDnuggets,"As we saw earlier, using group by helps us to get sets of data so that we can find trends and deduce business opportunities by analyzing it. Bucketing is the term used to find those groupings (most of the time with timestamps and numbers) and to generate histograms. This reduces human observation errors. For example, using the truncate function, we can get the nearest round off value for a decimal"
Top Handy SQL Features for Data Scientists - KDnuggets,"In the same way, date_trunc is used to group dates together. This could be useful in tracking user activities over a period of time. For example, the activities of the students in an online learning portal over a specific date interval, for example, weekdays and weekends"
Top Handy SQL Features for Data Scientists - KDnuggets,"Using SQL sequences, we can generate a numeric sequence on demand in ascending or descending order. Sequences can be created using create sequence <sequence_name> by giving necessary details. Sequences are not associated with any tables, hence it is useful to retrieve table values using a sequence directly, if there is one. Calling ‘next value’ function retrieves the next row instead of selecting from the table"
Top Handy SQL Features for Data Scientists - KDnuggets,"As a data scientist, you may not be using all the powerful features of SQL. However, once you start playing with the data, you would definitely enjoy digging more. The above list should help you get through the features you will be expected to use as a data scientist. Do visit"
Top Handy SQL Features for Data Scientists - KDnuggets,"After working for a decade in Infosys and Sapient, he started his first startup, Leno, to solve a hyperlocal book-sharing problem. He is interested in product marketing, and analytics. His latest venture"
It Only Takes One Line of Code to Run Regression - KDnuggets,"I am a usual IT graduate from an Indian tier-2 engineering college. And like a usual student, I used to read just enough course material so as to not flunk the semester-end exams. I was the kind of girl who believed “"
It Only Takes One Line of Code to Run Regression - KDnuggets,"Although I had read theory behind almost every machine learning technique in college, I had never implemented any of them. I thought this is the coolest thing there is. I always assumed it takes a lot of learning, coding and understanding. But I had to start somewhere"
It Only Takes One Line of Code to Run Regression - KDnuggets,"I came back to taking online courses. I started with, yes you guessed it right, Machine Learning by Andrew Ng on Coursera. It got a little too much for me and I went astray for a few months. I had to pull myself back and I started with another"
It Only Takes One Line of Code to Run Regression - KDnuggets,"This course broke a lot of myths that I had been carrying for years. This is precisely what got me started on my journey to discovering the power of data and analytics. From there on,"
Advice For New and Junior Data Scientists - KDnuggets,"[for career advancement or fulfillment], then they would fight hard to make sure I got the help that I needed to succeed. Conversely, if I wasn’t on my mentor’s critical path, then I was usually left to fend for myself. […] If you get on someone’s critical path, then you force them to tie your success to theirs, which will motivate them to lift you up as hard as they can"
Advice For New and Junior Data Scientists - KDnuggets,"This turns out to be a very common question among data scientists, since many struggled to decide which language to choose. For me, there is clearly a switching cost once committed to one or the other. I went through the pros and cons to understand the tradeoffs, but the more I thought about it, the more I fell into the trap of decision paralysis"
Advice For New and Junior Data Scientists - KDnuggets,"The appropriateness of a tool is always context dependent and problem specific. It’s not about whether I should learn Python, it’s whether Python is the right tool for the job. To elaborate more on this point, here are a few examples:"
Advice For New and Junior Data Scientists - KDnuggets,"Given that I was my own teacher, insights from Dr. Ericsson were very helpful. For example, I kicked off my “learning project” by curating a set of materials that were most relevant for doing ML in Python. This process took me a few weeks until I settled on a personalized"
Advice For New and Junior Data Scientists - KDnuggets,"Slowly and gradually, I got better each week. It certainly wasn’t easy though: there were times when I had to look up basic syntax in both R and Python because I was switching back and forth between the two languages. That said, I kept in mind that this is a long term investment, and dividends will be paid as I dived into the ML project"
Stanford online Data Science and Data Mining courses and certificates - KDnuggets,"Learn from and interact with Stanford faculty. Get feedback from the teaching team. Attend virtual office hours, problem sessions, and study groups"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"It’s a great question. I am also frequently asked What’s the difference between a computer scientist and a data scientist? The fact that both disciplines question if there is effectively anything new here is telling. While both domains are contributing in important and meaningful ways to this nascent discipline, neither is independently sufficient"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"Data is not only growing in size, but the definition of what we even consider to be data is expanding. For example, text and image are increasingly common forms of data to be integrated into analytical methodologies like classification and risk modelling. This expanding definition of data is pushing both statistics and computer science out of their traditional cores and into their respective fringes – and it’s at those fringes where the new thinking is taking place – and the fusion of the fringes is forming the basis of Data Science. Much of the traditional core of statistics does not readily accommodate problems defined by billions of records and/or by unstructured data. Similarly, while the core of computer science enables the efficient capture and storage of massive amounts of structured and unstructured data, the discipline is ill equipped to accommodate to the translation of that data into information through modelling, classification and then visualization"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"I do agree that in Data Science circles, statisticians are more likely to get the short end of the stick. I think this is unfortunate. A few years ago, there was an article on the Simply Statistics blog, “"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"The article highlighted the issue of how a rush to the excitement of machine learning, text mining, and neural networks missed the importance of basic statistical concepts related to the behavior of data—including variation, confidence, and distributions. Which lead to bad decisions. While Data Science is NOT statistics, statistics contributes in a foundational way to the discipline"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"The term has been traced back to computer scientist Peter Naur in 1960, but “Data Science” also has evolutionary seeds in Statistics. In 1962 John W. Tukey (one of the best known and respected Statisticians of our time) wrote: “For a long time I thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have … come to feel that my central interest is in data analysis… data analysis is intrinsically an empirical science"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"A reference to the term “Data Science” was made in the Proceedings of the Fifth Conference of the International Federation of Classification Societies in 1996. The article was titled “Data Science, Classification, and Related Methods”. In 1997, during his inaugural lecture as the H. C. Carver Chair in Statistics at the University of Michigan, Professor C. F"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"5 million managers” who have deep analytical skills. I think everyone needs to have some level of analytical skills. I think that basic data literacy should be as foundational in our education system as reading and math. It’s encouraging to see basic coding skills increasingly being taught in elementary schools. At the university level, my opinion is that Data Science should be part of the General Education curriculum (right now I can hear our Academic Affairs office gasping)"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"So, while the current talent gap is very real, it’s a function of an education system that has been misaligned with the demands of the market. Education at all levels is still pivoting and likely will continue to do so for the foreseeable future. I would expect that within a generation, the demand for these skills will not diminish, but the supply will be more closely aligned"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"I see a lot of these people in my office. I have had more than one conversation that goes like this “I just paid $10,000 to XX university to complete a certificate in Data Science…and I still can’t get a job”. While some of these “certificates” are well developed and are good value, sadly, many are not"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"Third, I think people need to take an inventory of where their skills are now and where they want to go. The answer to that question of course will dictate how to get there. Those that fall into the lure of the easy online certificate programs should be mindful of the Cheshire Cat from Alice in Wonderland – “if you don’t know where you are going, it really does not matter what path you take”"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"Full time. Most graduate programs in Data Science are less than two years and most offer some form of graduate research assistantship. You should be looking for programs that include programming, statistics, modeling. But also ample opportunity to work on REAL world projects with local companies, nonprofits, local governments…etc. I can’t emphasize strongly enough how critical applied, hands on, real experience is to any Data Science program. This is why online/short term certificate programs don’t work for people who are starting from scratch in this area. It’s through hands on experience that will help people understand the more latent aspects of data science – like the role of story telling, creativity (which is woefully underappreciated) and project management"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"I get the argument that not everyone wants to become a computer programmer – I do not particularly enjoy programming. I had to learn to program to get answers to the research questions that were posed to me. If I could have found the answers using my trusty HP-12C and a mechanical pencil I would have. You have to know basic math, you have to be able to read and write and, increasingly, you have to be competent in some basic programming in the 21st century"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"Examples of the “natives” are the companies that dominate the headlines – as well as the stock market – Amazon, Google, Facebook. These companies could not have existed 30 years ago. Not only did the data that is so foundational to who they are and what they do did not exist, but even if it did, we did not have the computing power to capture it or to execute the deep analytical methodologies related to AI, machine learning, deep learning…that enable them to do what they do"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"However, another dimension to these companies that is often overlooked is that because they are native to data, this has HUGE cultural implications. These are data-driven companies from the top to the bottom of the org chart. They have data running through their DNA. Most everyone who comes into these companies has a data centric orientation – and likely studied a computational discipline – increasingly Data Science. The median age of an employee at Facebook is 29. At Google, it’s also 29 and at Amazon its 30 (not including warehouse employees)"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"Companies that are non-native to data are the companies that were successful long before we heard terms like “Data Science” and “Big Data. These are very successful companies that did not initially have data running through their DNA. And although these companies now lean heavily into data to inform their decision making and delivery of their products and services, there is a great deal of variation across the org chart in terms of computational literacy. But their leadership has been very forward thinking in terms of making these companies leaders in their markets because of the cultural shifts in becoming fact-based, data-driven organizations. Others in their respective markets (Sears, Macys…McDonald’s, Wendy’s) have not"
Want to Become a Data Scientist? Read This Interview First - KDnuggets,"I’m not really an expert in this area, but I would say that any forecasting of the death of Statistics, Computer Science or Data Science because of automation is premature. Calculators “automated” mathematics…but mathematics is broader and more complex now than it was before calculators. I expect that same will be true in Data Science"
"An opinionated Data Science Toolbox in R from Hadley Wickham, tidyverse - KDnuggets","With this package loaded into your project, you can easily perform the fundamental data science tasks like importing, plotting, wrangling and modeling data as well as functional programming for new developments. The bone marrow of this super package is comprised of a robust array of R packages as ggplot2, dplyr, tidyr, readr, purr, and tibble among others. We will go into the details of each of these packages further along as we will show some basic examples to get you started with this amazing toolbox. This package is intended to be a harmonious and compatible set of tools and commands that bring to life the by-the-book definition of an effective data science workflow"
"An opinionated Data Science Toolbox in R from Hadley Wickham, tidyverse - KDnuggets","First, the history of tidyverse, how it came to be and who is the mastermind behind it. This ‘package of packages’ was developed by Hadley Wickham, Chief Scientist at RStudio and co-author of the amazing O’Reilly series book “R for Data Science”; he is also in charge of maintaining it to its best. You may already be familiar with Hadley as he developed previous R packages like reshape, reshape2, and plyr; which in turn, were building blocks of tidyverse as a product of several experiments and versions. It was created for statisticians and data scientists with the sole purpose of boosting their productivity and as an attempt to reproduce and abstract the Canonical Data Science Workflow (figure) into an actual product. This is an extremely versatile and consistent package as its powerful features range from productivity and workflow enhancement to new data science software development and data science education"
"An opinionated Data Science Toolbox in R from Hadley Wickham, tidyverse - KDnuggets","It is important to expand a bit more on the general features of this package, so we can see it in action later in our practical example. Its consistency is deeply rooted in the fact that variables, functions, and operators follow regular patterns and syntax. For example, the first argument of every function will be a tidy data frame (one row per observation, one column per variable, one entry by cell). To perform operations, one can intuitively connect a sequence of commands, base functions, and operators to create a tidy pipeline. The way in which the core packages are organized, the coding style and testing procedures comprise a second, lower-level degree of consistency, so when we say the word “consistent”, it should not be taken lightly. Finally, because there is a one-to-one relationship between the analysis workflow processes and the different tidyverse sub-packages, it is extremely easy to establish effective end-to-end workflows that respond to specific analytic purposes and utilize several types of data"
"An opinionated Data Science Toolbox in R from Hadley Wickham, tidyverse - KDnuggets",The purpose of our example is to run you through some common operations you can perform in tidyverse and show a bit more the syntax and the power of the consistency of this super R package. I hope you enjoy reproducing this code yourself. I encourage you to go through the documentation so you can come up with your own tidyverse recipes for data analysis
"An opinionated Data Science Toolbox in R from Hadley Wickham, tidyverse - KDnuggets",Note: You will see a particular this symbol (‘%>%’) very often throughout the example. This is called the pipe operator and it is very useful when scripting as it allows you to keep track of the logic of your analysis. Think of it in the following way: The function f(x) is now expressed as x %>% f
Data Science –The need for a Systems Engineering approach - KDnuggets,"As Data Science matures as a discipline, I see the need to address more complex and holistic problems. To achieve these goals, we can learn from existing disciplines like Systems Engineering. I discuss these issues in the"
Applied Data Science: Solving a Predictive Maintenance Business Problem - KDnuggets,"Over the past few months, many people have been asking me to write on what it entails to do a data science project end to end i. When I pondered on that request, I thought it made sense. The data science literature is replete with articles on specific algorithms or definitive methods with code on how to deal with a problem. However an end to end view of what it takes to do a data science project for a specific business use case is little hard to find. From this week onward, we would be starting a new series  called the Applied Data Science Series. In this series I would be giving an end to end perspective on tackling business use cases or societal problems within the framework of Data Science. In this first article of the applied data science series we will deal with a predictive maintenance business use case. The use case involved is to predict the end life of large industrial batteries, which falls under the genre of use cases called preventive maintenance use cases"
Applied Data Science: Solving a Predictive Maintenance Business Problem - KDnuggets,"The start of any data science project is with a business problem. The problem we have at hand is to try to predict the end life of large industrial batteries. When we are encountered with such a business problem, the first thing which should come to our mind is on the key variables which will come into play . For this specific example of batteries some of the key variables which determine the state of health of batteries are conductance, discharge , voltage, current and temperature"
Applied Data Science: Solving a Predictive Maintenance Business Problem - KDnuggets,"The next questions which we need to ask is on the lead indicators or trends within these variables, which will help in solving the business problem. This is where we also have to take inputs from the domain team. For the case of batteries, it turns out that a key trend which can indicate propensity for failure  is drop in conductance values. The conductance of batteries will drop over time, however the rate at which the conductance values drop will be accelerated before points of failure. This is a vital clue which we will have to be cognizant about when we go for detailed exploratory analysis of the variables"
Applied Data Science: Solving a Predictive Maintenance Business Problem - KDnuggets,"The other key variable which can come into play is the discharge. When a battery is allowed to discharge the voltage will initially drop to a minimum level and then it will regain the voltage. This is called the “Coup de Fouet” effect. Every manufacturer of batteries will prescribes standards and control charts as to how much, voltage can drop and how the regaining process should be. Any deviation from these standards and control charts would mean anomalous behaviors. This is another set of indicator which will have to look out for when we explore data"
Applied Data Science: Solving a Predictive Maintenance Business Problem - KDnuggets,In addition to the above two indicators there are many other factors which one would have to be aware of which will indicate failure. During the business exploration phase we have to identify all such factors which are related to the business problem which we are to solve and formulate hypothesis about them. Once we formulate our hypothesis we have to look out for evidences / trends within the data about these hypothesis. With respect to the two variables which we have discussed above some hypothesis we can formulate are the following
Applied Data Science: Solving a Predictive Maintenance Business Problem - KDnuggets,"When we go about in exploring data, hypothesis like the above will be point of reference in terms of trends which we will have to look out on the variables involved. The more hypothesis we formulate based on domain expertise the better it would be at the exploratory stage. Now that we have seen what it entails within the business discovery phase, let us encapsulate our discussions on key considerations within the business discovery phase"
Applied Data Science: Solving a Predictive Maintenance Business Problem - KDnuggets,"In the case of the battery prediction problem, there are three different data sets . These data sets pertained to different set of variables. The frequency of data collection and the volume of data captured also varies. Some of the key data sets involved are the following"
Applied Data Science: Solving a Predictive Maintenance Business Problem - KDnuggets,"As seen, we have to play around with three very distinct data sets with different sets of variables, different frequency of time when the data points arrive and different volume of data for each of the variables involved. One of the key challenges, one would encounter is in connecting all these variables together into a coherent data set, which will help in the predictive task. It would be easier to get this done if we can formulate the predictive problem by connecting the data sets available to the business problem we are trying to solve. Let us first attempt to formulate the predictive problem"
Business intuition in data science - KDnuggets,"This is an extremely crucial stage given that a wrong interpretation of the business problem/objective at hand can lead to a faulty solution and undesirable results. The role of data-science, if you really think about it, is to use data and insights from it to solve real-world problems. From that perspective, accurately identifying the problem and defining the objective is crucial for a successful outcome. In this example, the marketer wants to send customized emails to each of its customer showing a list of product offers curated according to the customer’s preferences and tastes:"
Business intuition in data science - KDnuggets,"Sometimes, data processing and storage costs could also be a consideration. Processing 100 MM customers and their characteristics, running machine-learning algorithms can be quite time and resource intensive. While infrastructure could be available to handle that, but along with the first two considerations, it may make sense to exclude some customers, especially to speed up time to market"
Business intuition in data science - KDnuggets,"For example, we could see some very high values of “last 12 month spend” or “annual income”. In case of spend, it could be because of some one-off high dollar spending by certain customers which may not persist and which can bias the entire data, hence capping the spend values at some threshold (e. 99 or 95 percentile value of “last 12 month spend”) can help reduce such bias"
Business intuition in data science - KDnuggets,"Sometimes, we may see that there are distinct segments of customers within the data that behave very differently. For example, if we look at recent customers (became members of the online retailer in the last 6 months), these customers are likely to behave in a very different manner than the rest of the customers (they may be very inquisitive, so email open rates could be very high, but purchase rates could be low). Hence mixing these customers with the rest of the customers can either bias the data on certain parameters or these customer’s features may get overshadowed by rest of the customers, reducing their representation on any prediction algorithm that is built. In such cases, it may make sense to build separate algorithms for these two"
Business intuition in data science - KDnuggets,"Features or variables are really what gives predictive power to the algorithms. So, having the right set of features is key to building a robust algorithm – hence the focus on feature engineering. Types of feature engineering:"
"XGBoost, a Top Machine Learning Method on Kaggle, Explained - KDnuggets","XGBoost has become a widely used and really popular tool among Kaggle competitors and Data Scientists in industry, as it has been battle tested for production on large-scale problems. It is a highly flexible and versatile tool that can work through most regression, classification and ranking problems as well as user-built objective functions. As an open-source software, it is easily accessible and it may be used through different platforms and interfaces. The amazing portability and compatibility of the system permits its usage on all three Windows, Linux and OS X. It also supports training on distributed cloud platforms like AWS, Azure, GCE among others and it is easily connected to large-scale cloud dataflow systems such as Flink and Spark. Although it was built and initially used in the Command Line Interface (CLI) by its creator (Tianqi Chen), it can also be loaded and used in various languages and interfaces such as Python, C++, R, Julia, Scala and Java"
"XGBoost, a Top Machine Learning Method on Kaggle, Explained - KDnuggets","The implementation of XGBoost offers several advanced features for model tuning, computing environments and algorithm enhancement. It is capable of performing the three main forms of gradient boosting (Gradient Boosting (GB), Stochastic GB and Regularized GB) and it is robust enough to support fine tuning and addition of regularization parameters. According to Tianqi Chen, the latter is what makes it superior and different to other libraries"
"XGBoost, a Top Machine Learning Method on Kaggle, Explained - KDnuggets","First, let’s clarify the concept of boosting. This is an ensemble method that seeks to create a strong classifier (model) based on “weak” classifiers. In this context, weak and strong refer to a measure of how correlated are the learners to the actual target variable. By adding models on top of each other iteratively, the errors of the previous model are corrected by the next predictor, until the training data is accurately predicted or reproduced by the model. If you want to dig into boosting a bit more, check out information about a popular implementation called AdaBoost (Adaptive Boosting)"
"XGBoost, a Top Machine Learning Method on Kaggle, Explained - KDnuggets","Now, gradient boosting also comprises an ensemble method that sequentially adds predictors and corrects previous models. However, instead of assigning different weights to the classifiers after every iteration, this method fits the new model to new residuals of the previous prediction and then minimizes the loss when adding the latest prediction. So, in the end, you are updating your model using gradient descent and hence the name, gradient boosting. This is supported for both regression and classification problems. XGBoost specifically, implements this algorithm for decision tree boosting with an additional custom regularization term in the objective function"
"XGBoost, a Top Machine Learning Method on Kaggle, Explained - KDnuggets",You may download and install XGBoost regardless of which interface you are using. To learn more on how to use on each specific platform please follow the instructions on this link. You will also find official documentation and tutorials
KDD Impact Program to support Data Science projects with positive impact on society - KDnuggets,"The goal of the program is to support projects that promote data science, increase its impact on society, and help the data science community. Project duration is one year with the possibility of being extended. Funded projects will be required to present mid-project results of their work and outcomes at the KDD 2018 conference in London in August 2018"
KDD Impact Program to support Data Science projects with positive impact on society - KDnuggets,"The KDD Impact Program is NOT a research program. It focuses on projects that benefit the community and the society. Pure research projects will not be funded. The call is open to every country, participant must be affiliated to an institution with reputable standing. Funds are to be used for not-for-profit activities, and this program will not fund activities for which alternative funding is readily available"
4 Major Trends Influencing the 2017 Predictive Analytics Hiring Market - KDnuggets,"The proliferation of analytics initiatives has happened broadly across many industries. This diversification necessitates considering applicants from other industry verticals when hiring, in order to ensure access to the biggest talent pool. Because analytics skills are the key consideration and tend to be transferable across domains, it’s best to look outside a specific industry when searching for talent"
4 Major Trends Influencing the 2017 Predictive Analytics Hiring Market - KDnuggets,"As new tools emerge, companies should invest in their teams by providing training and other opportunities to learn. Predictive analytics professionals see value in being able to use the latest tools and technologies, and many enjoy tackling new challenges. Candidates will often prioritize job opportunities that are committed to training and teaching them new skills"
4 Major Trends Influencing the 2017 Predictive Analytics Hiring Market - KDnuggets,"Simply providing or executing analysis is not sufficient when establishing the analytics team’s value within an organization. Data evangelism is a key attribute for analytics leaders, but analytics professionals at all levels should be prepared to serve as advocates for the use of analytics, by, for example, providing insight into specific ways the team can contribute additional value. Data evangelism at all career levels will help develop a strong data culture throughout the company"
4 Major Trends Influencing the 2017 Predictive Analytics Hiring Market - KDnuggets,"When hiring senior leaders, look for those who can be an influencer as well as a strategic leader. Regardless of whether it is a new team or an established one, it is important to have a leader in place that can forge partnerships with other members of senior leadership. This skillset can ensure that the analytics team has the proper tools and support, and so is able to provide value enterprise-wide, as well as build confidence in the analytics team"
Putting Machine Learning in Production - KDnuggets,"If you try to have your training and server code in the same repository you would probably end up with a big mess that is hard to maintain. Training models and serving real-time prediction are extremely different tasks and hence should be handled by separate components. Last but not least, there is a proverb that says"
Putting Machine Learning in Production - KDnuggets,"Once we have our coefficients in a safe place, we can reproduce our model in any language or framework we like. Concretely we can write these coefficients in the server configuration files. This way, when the server starts, it will initialize the logreg model with the proper weights from the config"
Putting Machine Learning in Production - KDnuggets,I will try to write a clean version of the above scripts. We will use Sklearn and Pandas for the training part and Flask for the server part. We will also use a parallelised GridSearchCV for our pipeline
"Data Science Bootcamp in Zurich, Switzerland, January 15 – April 6, 2018 - KDnuggets",Come to the land of chocolate and Data Science where the local tech scene is booming and the jobs are a plenty. Learn the most important concepts from top instructors by doing and through projects. Use code KDNUGGETS to save
"Data Science Bootcamp in Zurich, Switzerland, January 15 – April 6, 2018 - KDnuggets","Located at the heart of Europe, Switzerland’s strategic location and quality of life makes it an easy sell to those in Europe and the rest of the world. The program’s world-class instructors have experience at Facebook, Airbnb, Hyperloop technologies, and MIT. The focus is on learning by doing and students work on real company data for their final projects. For US students, this is a perfect opportunity to get away for a while and be part of a killer program"
"Data Science Bootcamp in Zurich, Switzerland, January 15 – April 6, 2018 - KDnuggets","To be accepted into the program, candidates must pass a personal and technical interview. Anyone has the potential to get in, but accepted candidates usually have a background in mathematics, science, and finance. Ready for the challenge?"
How to Choose a Data Science Job - KDnuggets,"In these positions, workers must trawl through endless rows of data manually, cleaning and structuring wherever required. This makes it a cumbersome and difficult process, requiring some technical know-how and, more importantly, considerable attention to detail. Despite these responsibilities, these roles are typically billed as an entry-level position"
The 5 Best Industries to Find a Job in Data Science - KDnuggets,"Data powers everything we do these days. And that means how we accumulate, disseminate, study, store and act upon data is one of the most important jobs out there. That must be why data science is such a booming business — one that’s"
The 5 Best Industries to Find a Job in Data Science - KDnuggets,"Some energy sources are more modern than others. But all the industries related to powering our world rely on data — and lots of it. Whether we’re exploring new ways to extract energy and mineral wealth from the earth, dreaming up newer and safer ways to store and transport crude oil or doing the responsible thing and building better and cheaper solar panels, data scientists are in high demand in the energy sector"
The 5 Best Industries to Find a Job in Data Science - KDnuggets,"When companies get sloppy, everybody suffers. But building a better mousetrap — or any other product, really — means you need to stay connected with all kinds of meaningful data. Ever have your favorite app ask you, pretty please, for a review? That’s a kind of crowd-sourced quality control. The numerical rating you assign and the positivity or negativity of your review are the data being collated"
The 5 Best Industries to Find a Job in Data Science - KDnuggets,"Beyond self-driving automobiles and the obvious shipping applications, the transportation industry is also eyeing ever-more-efficient ways to store and transport energy. These breakthroughs entwine closely with a turbulent regulatory environment and the development of better battery technology. In short, every sector in the very large and very important transportation industry looks to benefit from skilled data scientists"
The 5 Best Industries to Find a Job in Data Science - KDnuggets,"Actually, it’d be hard to overstate just how important data architecture and data science will be as we continue to develop the Internet into what it was always meant to become: a unifying, civilizing and democratizing communication medium. When we need to know how to tell people about a new product, we call on user data. When we need to figure out where to bury new fiber-optic cables, we need environmental data. We rely on data scientists to design and roll out more energy-efficient server farms"
The 5 Best Industries to Find a Job in Data Science - KDnuggets,"The point is, everything about how we talk these days comes back in some way to the Internet. And the Internet isn’t possible without the free exchange of information and meaningful data. And though some folks might"
Data Science and the Imposter Syndrome - KDnuggets,"If this sounds familiar, know that you are not alone. You are not the only one who wonders how much longer they can get away with pretending to be a data scientist. You are not the only one who has nightmares about being laughed out of your next interview"
Data Science and the Imposter Syndrome - KDnuggets,"Imposter syndrome is feeling like everyone else in your field is more qualified than you are, that you will never get hired or, if you already have been, that you are a mistake of the hiring process. Despite its statistical implausibility, most of us feel below average. Based on my conversations with colleagues, I estimate that 9 out of 10 of us suffer from imposter syndrome at one time or another"
Data Science and the Imposter Syndrome - KDnuggets,"The field we call data science is still relatively young, yet already too broad for an individual to be an expert in every corner of it. In my experience, the master-of-all-trades data science unicorn is a mythical beast. None of us can cover all the bases. So how are we to proceed?"
Data Science and the Imposter Syndrome - KDnuggets,"A generalist does not necessarily know the details of how an algorithm works and the tricks of using a tool. They will tell you that data cleaning is critical, but may not be able to enumerate the trade-offs between methods for replacing missing values. They will tell you that Spark is a good way to speed up your computations, but may not be able to advise you on the best settings to use"
Data Science and the Imposter Syndrome - KDnuggets,"A specialist does not necessarily know much about something that is outside their area. They will know the best architecture for running a linear regression on 500 million data points, but may not be able to explain a naive Bayes classifier. They will keenly grasp the trade-offs between square loss, hinge loss and logistic loss, but may be unable to query data from a Hive table"
Data Science and the Imposter Syndrome - KDnuggets,"Another way to describe generalists and specialists is “broad” versus “deep”. They are both technically savvy, but their expertise is distrubted differently. We are all part generalist and part specialist. As you evolve through your career, you get to find the mixture that works best for you"
Data Science and the Imposter Syndrome - KDnuggets,"Traditionally we establish our qualification in a field with advanced degrees. Unfortunately for most of us, there are few such degrees available in data science. We have no piece of paper to use as a shield when someone questions our qualifications. So what do we do instead? How can we answer our critics, or interviewers, our colleagues, and harshest of all, the voices in our head?"
Data Science and the Imposter Syndrome - KDnuggets,"Consider woodworking. Imagine that you want to install a custom cabinet in your kitchen. Three carpenters show up inquiring after the job. The first one presents you with a certificate. She says, “I apprenticed with the premier cabinet maker in the city for seven years. When you pull the handle with a fingertip, a drawer slides out soundlessly. She says, “I made this"
Data Science and the Imposter Syndrome - KDnuggets,"Certifications, tools and portfolio are all popular ways for establishing credentials. I won’t argue that one is superior to another, but portfolios are particularly effective for data scientists. Certifications are few and not yet standardized. Listing algorithms and computer languages we have used doesn’t convey our depth of familiarity with them or what we can do with them"
Data Science and the Imposter Syndrome - KDnuggets,"Note that both generalists and specialists have lots of things they don’t know. This means that even real data scientists will spend most of their days feeling lost. Our project lead will ask us questions that we don't know the answer to. Colleagues will talk comfortably about algorithms we've never heard of. Teammates will write code that we can't begin to decipher. Articles will cite ""hot"" subfields that we didn't know existed. Archiv papers will throw around equations that may as well be hieroglyphic gibberish. Interns will point out fundamental flaws in our reasoning. This is OK. You're not doing it wrong. This is OK"
Data Science and the Imposter Syndrome - KDnuggets,"Our goal isn’t to accumulate answers, but to ask better questions. If you are asking questions and using data to find answers, YOU ARE A DATA SCIENTIST. Period"
"Top 10 Active Big Data, Data Science, Machine Learning Influencers on LinkedIn, Updated - KDnuggets","We collected a list of the top influencers on LinkedIn, ranked by their number of followers, and limited to those who were active for the past three months. Those who did not publish anything, or anything relevant to Big Data, Data Science or Machine Learning on LinkedIn since June 2017, were not included. We note that one can create a totally different ranking by looking at the number of followers on other social media, e. I hope you enjoy reading through each of the profiles. All percent changes in the number of followers is relative to a similar post"
"Top 10 Active Big Data, Data Science, Machine Learning Influencers on LinkedIn, Updated - KDnuggets","As a best-selling author, strategy consultant and CEO he has become one of the most recognized and respected leaders in the world of data. He’s published numerous reports and books on Big Data and has advised several well-recognized firms. He has figured among the top five LinkedIn business influencers. Also"
"Top 10 Active Big Data, Data Science, Machine Learning Influencers on LinkedIn, Updated - KDnuggets","Former US Chief Data Scientist under President Barack Obama, responsible for creating and establishing major data-driven initiatives in healthcare, criminal justice and national security among others. He was also Head of Data Products and Chief Scientist at LinkedIn. Also"
"Top 10 Active Big Data, Data Science, Machine Learning Influencers on LinkedIn, Updated - KDnuggets","For the last 18 years, Carla has worked with Fortune 100 and 500 companies and is highly experienced tackling complicated databases and deciphering complex business needs to provide insight into key performance metrics. Her areas of expertise are Customer Satisfaction and Retention Analysis, Brand Research & Competitive Analysis, Employee Retention, Survey Creation & Analysis, Database creation and mining, Incentive Promotions and Project Management. Also"
"Top 10 Active Big Data, Data Science, Machine Learning Influencers on LinkedIn, Updated - KDnuggets","Data Science, Business Intelligence and Data Mining expert; President of KDnuggets, voted Best Twitter and Top Influencer in Big Data and Data Science. Co-founder of Knowledge Discovery and Data Mining (KDD) Conferences and its professional organization SIGKDD. He has also contributed to over 60 publications and edited several books on Data Mining and Knowledge Discovery. Also"
"Top 10 Active Big Data, Data Science, Machine Learning Influencers on LinkedIn, Updated - KDnuggets","7k). Recognized in the field of digital transformation by publications and organizations as Onalytica, Dataconomy, and Klout. In addition to these recognitions, Ronald is also an author for a number of leading big data websites, including The Guardian, The Datafloq, and Data Science Central. Also"
"Top 10 Active Big Data, Data Science, Machine Learning Influencers on LinkedIn, Updated - KDnuggets","7k), 149% increase. Kirk is a major and well-recognized Big Data and Data Science advisor, TedX speaker, consultant, researcher, blogger, Data Literacy advocate. Kirk is also a Public Speaker, Consultant, Astrophysicist and a Space Scientist. Worldwide top influencer since 2013"
"Top 10 Active Big Data, Data Science, Machine Learning Influencers on LinkedIn, Updated - KDnuggets","Vin has 8 years of experience using modern data science/machine learning tools and methodologies in both startups and Fortune 10 companies. He is a published business strategy expert and is followed by Walmart, Accenture, Microsoft, IBM, and recognized by Agilience, Klout, Dataconomy, and Onalytica as a thought leader in data science and machine learning. Influencer for IBM, Intel, and many other amazing brands"
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,"Imagine walking into your office on a Monday morning, just a couple years from now. You pour yourself a cup of coffee, check the news, and then put on a pair of AR glasses. You find yourself surrounded by a sea of gently glowing, colored orbs. The orbs represent all of the data that drives your business. You know this data well. The patterns and colors of these orbs are like a fingerprint. But there's something atypical about the data floating over the coffee maker. You reach out and select that data. A summary of all the relevant details appears on a nearby computer screen"
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,My team at IBM is working to make the experience described above real. Immersive Insights is an augmented reality data visualization app. Check out our progress in the video below:
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,"Data can have a lot of attributes. Take, for example, Instacart's open source data set on grocery purchases. Each person in this data set can be thought of as a data-point. Each of those data-points can be described by a list of purchased products. That's over 50,000 possible products for each user"
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,"Oftentimes, when Data Scientists first get a data set, they'll use a matrix of 2D scatter plots to quickly overview the contents. 2D scatter plots show the relationships between pairs of attributes. But for data with lots of attributes, that type of analysis just doesn't scale"
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,"The data sets that are suitable for this technique are often used to train machine learning models. If Data Scientists used this process to understand the embedded relationships in their data, it could help them improve their ML features and models. The technique could also be useful for gaining intuition on what black box predictive models might be doing under-the-hood"
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,"Instacart's users were modeled with a vector which describes their relationship to every product that they could potentially purchase. This vector took the form of a sparse array of 0s and 1s. Each 1 corresponds to a product that had been purchased at least once by that user. Each 0 corresponds to an unpurchased product. This technique is called """
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,"Each principal component summarizes an aspect of variance in the data. In practice, I could only analyze 120,000 users due to memory constraints while performing PCA. After performing PCA, the prepped data is exported to CSV"
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,"The R notebook color-codes users according to various criteria that could affect the user distribution in latent space. Users were color-coded according to the department from which they ordered stuff most frequently (e. An alternate color-coding scheme indicated whether users had purchased any organic food. I also referenced the IDs of various users in Immersive Insights, and then looked up their purchase history in the R notebook. This allowed me to gain an intuition for what types of product purchases resulted in users being placed in different areas of the latent space"
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,"Users who didn't purchase any organic foods were tightly clustered within the latent space. This finding was a compelling piece of evidence which supported a qualitative observation drawn from the visualization: much of the variance in Instacart purchasing patterns appears to be between users who purchase premium items, and users who prefer lower cost versions of similar items. This difference between cost conscientious and premium purchasers has meaningful implications for Instacart's marketing, promotional, and recommendation strategies"
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,We also found that Instacart users buy far more produce than any other category of item. There are many different types of users who love produce. Almost everyone buys produce!
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,"Big Data, AR, and ML are emerging as three disruptive technologies which will shape the future of business and society. As demonstrated, these disruptive technologies can build on each other in creative, useful ways. You can read more about the potential impact of combined disruptive technologies here:"
Visualizing High Dimensional Data In Augmented Reality - KDnuggets,The technique described in this article has limited scope. But the scope of Immersive Insight's vision is huge. The Immersive Insights team is excited to continue envisioning the future of data visualization and analysis. Our goal: to make data simple
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"Both types are important not only to data scientists but also to managers and executives, who must evaluate project proposals and results. To managers I would say: It’s not necessary to understand the inner workings of a machine learning project, but you should understand whether the right things have been measured and whether the results are suited to the business problem. You need to know whether to believe what data scientists are telling you"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"The write-up was posted in early 2017, along with a related video presenting the results. Some aspects are confusing, as you’ll see, but I haven’t sought clarification from the authors because I wanted to critique it just as reported. This makes for a realistic case study: you often have to evaluate projects with missing or confusing details"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"Their approach was fairly straightforward. They had a historical data sample of previous drivers’ records on which to train and test. They represented each driver’s record using 70 features, encompassing both categorical and numerical features, although only a few of these are shown"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,They state that their client had previously used a Random Forest to solve this problem. A Random Forest is a well known and popular technique that builds an ensemble of decision trees to classify instances. They hope to do better using a deep learning neural network. Their network design looks like this:
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"I have no reason to doubt their representation choices or network design, but one thing looks odd. Their output is two ReLU (rectifier) units, each emitting the network’s accuracy (technically: recall) on that class. I would’ve chosen a single"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,This changes things—they’re not using composite classification accuracy (the common meaning of “accuracy”) but the recall on each class. We can calculate some of the information to evaluate their system. Is it enough?
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"78. By convention, the rare class is usually positive, so this means the True Positive (TP) rate is 0.78, and the False Negative rate (1 – True Positive rate) is 0.22. The"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"79, so the True Negative rate is 0.79 and the False Positive (FP) rate is 0.21"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,What they had previously reported as Random Forest Accuracy is 0.39. Now we realize that this value is really the single-class recognition rate on the
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"We should want multiple runs using cross-validation or bootstrap to provide an indication of variation. As it is, we have no confidence that these numbers are representative. Most Machine Learning courses introduce the student to basic model evaluation and emphasize the need for multiple evaluations to establish confidence regions"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,Bottom line: We don’t know. They haven’t given us enough information. Probably not as good as they think
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"Again, all we have for the Random Forest is a single True Positive rate value (0.39) but not the accompanying False Positive rate. This isn’t enough; by simply classifying"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"39. The RF performance is a single point somewhere on that line. If its False Positive rate is less than about 0.10, Random Forest is actually better than the neural network. So we can’t even answer which model is better"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"Here is the reasoning. In order to judge how well the Neural network is doing, we really need to know the costs of the errors. They aren’t provided. This is not uncommon: these numbers would have to be provided by the client (the insurance company), and in my experience most clients can’t assign exact costs to the errors. So we need some other way of understanding the performance. There are several ways to do this"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,So they have a 99/1 proportion of negatives to positives. Their performance on false positives to true positives is 0.22/0.78. We can answer the original question by multiplying these together:
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"Is this acceptable to the insurance company? I’m not an expert, but I would guess not. 4% precision is low, and 28 false alarms per real alarm is a high cost to tolerate. Unless the company has devoted a large workforce to this task, they probably can’t afford it"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"Stepping back from the technical details, we can critique this as a solution to a business problem. The “case” they’ve given us here doesn’t say anything really about what business problem they’re trying to solve. We can’t judge whether the solution is appropriate for solving it. The write-up simply states, “"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"Ignoring their model’s poor performance, is this even a good approach? “Understanding” often includes understanding the predictors, not just the names and account numbers of the high-scoring drivers. Nothing is said about how this understanding is achieved. Though neural networks are extremely popular right now, deep learned neural networks are notorious for not being comprehensible. They may not have been the ideal choice for this problem"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"An important part about solving applications with machine learning is understanding how the model is going to be used in the context of the application (the larger process). This project simply showed a quick translation of a business problem into a two-class problem, then showed the technical details of the model and the results attained. Because the initial analysis of the insurance problem was superficial, we don’t even know what performance would be acceptable"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"I’ve been pretty merciless in critiquing this project simply because it illustrates a lot of mistakes data scientists make when they work on business problems. In other words, it was convenient, though hardly unique. In case you think only amateurs or beginners make these kinds of mistakes,"
Evaluating Data Science Projects: A Case Study Critique - KDnuggets,"From what I’ve seen, it’s rare for a machine learning or data science curriculum to cover the application issues I’ve mentioned here. Most courses concentrate on teaching algorithms, so they tend to simplify the data and evaluation complexities that come up in real-world applications. It took me many years of experience to understand these subtleties and know which problems to anticipate"
Data Science Study of “Unite The Right” and social media - KDnuggets,Former Pres. Barack Obama’s tweet paraphrasing Nelson Mandela in response to the violence of the Aug. 12 ‘Unite the Right’ rally in Charlottesville is the most liked tweet in Twitter history – with more than four and a half million likes and more than one million retweets
Data Science Study of “Unite The Right” and social media - KDnuggets,"A capstone project involving four graduate students and three faculty advisors, including DSI Director Dr. Philip Bourne, Founding Director Don Brown, and Dr. Abby Flower, will use the data to ask two key questions:"
Data Science Study of “Unite The Right” and social media - KDnuggets,"It has now been confirmed that in at least one case, social media was used to mislead/misdirect valuable first responders to events that were not actually taking place. This resulted in less resources at the scene of major activity. Using events that were promoted on social media and that actually occurred as a control, researchers will mine the data to determine if there are language differences that could be indicative of whether an event will actually transpire"
Data Science Study of “Unite The Right” and social media - KDnuggets,"The projects pair Universities with agencies and capabilities to help answer grand challenges in the Commonwealth of Virginia. The Data Science Institute has previously partnered with the Department of Motor Vehicles (DMV), the Department of Aging and Rehabilitative Services (DARS) and the Department of Consolidated Laboratory Services (DCLS), among others. We are also working on a project this year with the Virginia Department of Health on the opioid epidemic as a part of the GDIP program"
Data Science Study of “Unite The Right” and social media - KDnuggets,"The UVA Data Science Institute’s innovative project will aid in understanding the predictability of the level of violence via the dissemination of information through social media and other channels. Through this partnership between academia, industry, and government, DSI researchers can use the latest techniques and experts with unique data access. There are opportunities to bring in additional data sets to gain even greater insight with the possibility of resulting platforms, algorithms, and analyses that can be disseminated to government officials and Universities to be used in the case of future incidents"
Data Science: (not) the preferred nomenclature - KDnuggets,"This is not to say that one is the “real thing” and the other is intellectually inferior; or that the two do not overlap and interact. On the contrary: I believe that a real synergy arises from the Science OF Data being contemporaneously being developed and advanced with advanced applications arising from doing Science WITH Data. It creates a similar “buzz” to the one in early twentieth-century quantum physics, when discoveries in experimental physics went hand-in-hand with advances in theoretical physics"
Search Millions of Documents for Thousands of Keywords in a Flash - KDnuggets,"You want to check if the document contains the word python or not. So you open the document, press ctr+f and search for ‘python’. And you find it :)"
Search Millions of Documents for Thousands of Keywords in a Flash - KDnuggets,Now say you have a 100 documents. Well you can open each document in a loop. Per document you search for each term in the document
Search Millions of Documents for Thousands of Keywords in a Flash - KDnuggets,But soon we expanded to multi million documents with 10K+ keywords. And the same code was now going to take 10+ days to run. So we set out to find a better way
Search Millions of Documents for Thousands of Keywords in a Flash - KDnuggets,I wrote a custom implementation based on Trie data structure to suit our use case. It worked quite well. The keyword extraction process takes 15 mins with this algorithm. Down from 10+ days with the
"277 Data Science Key Terms, Explained - KDnuggets","This post presents a collection of data science related key terms with concise, no-nonsense definitions, organized into 12 distinct topics. Starting with Big Data and progressing through to natural language processing, this definition train has stops at machine learning, databases, Apache Hadoop, and several more. It may take come time, but once you get through the terminology presented herein, you should have a good idea of the key terms of importance in data science. And don't worry if the definitions are too slim for you; links abound for expanded related reading opportunities where appropriate"
"277 Data Science Key Terms, Explained - KDnuggets","But just because one has heard the term, or has taken part in (or opposed) its flippant usage, that really doesn't mean one knows what it actually means, or what it fully encompasses. Indeed, trying to exhaustively describe what Big Data is in a single post would be nonsensical, not the least of which reason being that there is no agreed-upon exhaustive description, nor should there be. Collecting some key terms associated with Big Data is not a bad idea, however, as it lays a common foundation from which to work forward"
"277 Data Science Key Terms, Explained - KDnuggets","Deep learning is a relatively new term, although it has existed prior to the dramatic uptick in online searches of late. Enjoying a surge in research and industry, due mainly to its incredible successes in a number of different areas, deep learning is the process of applying deep neural network technologies - that is, neural network architectures with multiple hidden layers - to solve problems. Deep learning is a process, like data mining, which employs deep neural network architectures, which are particular types of machine learning algorithms"
"277 Data Science Key Terms, Explained - KDnuggets","Data needs to be curated, coddled, and cared for. It needs to be stored and processed, so that it may be transformed into information, and further refined into knowledge. The mechanism for storing data, subsequently facilitating these transformations, is, clearly, the database"
"277 Data Science Key Terms, Explained - KDnuggets",Hadoop is a very powerful open source platform managed by Apache Foundation. Hadoop platform is built on Java technologies and capable of processing huge volume of heterogeneous data in a distributed clustered environment. Its scaling capability makes it a perfect fit for distributed computing
"277 Data Science Key Terms, Explained - KDnuggets","Hadoop ecosystem consists of Hadoop core components and other associated tools. In the core components, Hadoop Distributed File System (HDFS) and the MapReduce programming model are the two most important concepts. Among the associated tools, Hive for SQL, Pig for dataflow, Zookeeper for managing services etc are important. We will explain these terms in details"
"Python vs R for Artificial Intelligence, Machine Learning, and Data Science - KDnuggets","This includes a discussion of the many key concepts associated with programming languages and other aspects of computer science, with an emphasis of those applicable to data science and advanced analytics. There is also a discussion of commonly used integrated development environments (IDE) for both Python and R. The article ends with a brief overview of which language to use, when, and why"
"Python vs R for Artificial Intelligence, Machine Learning, and Data Science - KDnuggets","A very interesting and final thing to note: recently KDnuggets published an article with results from a survey they conduct annually on the leading languages and tools used for analytics, data science, and machine learning. The results amazingly show that the python ecosystem overtook R as the leading platform in 2017. You can read more about the results and breakdown of the data"
Putting the “Science” Back in Data Science - KDnuggets,"Lately I’ve seen a lot of hype surrounding -- and lots of newcomers to -- the Data Science field. But what exactly is SCIENCE in Data Science? The scientific method to approach a problem, in my point of view, is the best way to tackle a problem and offer the best solution. If you start your data analysis by simply stating hypotheses and applying Machine Learning algorithms, this is the wrong way"
Putting the “Science” Back in Data Science - KDnuggets,"The picture below shows the steps necessary for scientific research, corresponding data analysis and simulation. In fact, it is a sketch of what I did in my PhD thesis. In a few words, I studied the past 27 years of Business Management literature and I tried to develop an epistemologically disruptive approach to measure and predict service quality, mixing Business Administration with Electrical Engineering concepts. Over the course of 4 years I performed quali-quantitative longitudinal research and developed a simulation using Agent-Based Modeling to try to find a 5 State Cellular Automata rule that could mimic human behavior. I approached Complexity concepts, self-organizing systems, emergence of order, and social networks"
Putting the “Science” Back in Data Science - KDnuggets,"Let’s say customers are leaving because they don’t see VALUE in the business. Value is something unique, usually brought by human resources, that cannot be copied and no competitor can offer. This leads to competitive advantage, more profits, loyalty, word-of-mouth advertising and repurchase"
Putting the “Science” Back in Data Science - KDnuggets,"Note that so far we didn’t even thought about hypotheses and algorithms. Only after knowing exactly which are the variables involved in the problem, we develop hypotheses. Let’s say you suppose that profit is leveraged by a positive customer perception of product quality and high word-of-mouth advertising about your firm. This is the nomological network, where you draw correlations and causalities. In Data Science, you need to know what the customer perception is and also if there is word-of-mouth advertising. Then you realize you are working with different datasets, one Market Research and other Social Media referrals. You also have another dataset with the financial data of your company (contains profit data)"
Putting the “Science” Back in Data Science - KDnuggets,"Now it’s time to choose: do you opt for a quantitative approach, structured data in Market Research dataset and in Financial Data? But Social Media is unstructured, so you have to make a qualitative approach using Natural Language Processing. Even worse, you want to make a longitudinal analysis, converting data into time series to analyze with ARIMA. Ah, profit call be predicted using Deep Neural Networks using data from Market Research, Financial Data and word embeddings from Social Media as features!"
Putting the “Science” Back in Data Science - KDnuggets,"Yes, the fun has begun, but note that in this specific case it was a long journey before we reached the algorithms. There was an entire aspect of planning on the research problem. One cannot simply “apply the algorithm“ and check measures of fit and overfit. Another big issue is that when you have a complete picture of what is going on, you usually will find out there is data you need, and it simply does not exist"
Putting the “Science” Back in Data Science - KDnuggets,"After validating your data analysis result, you will confirm or reject the hypotheses and suggest strategic moves to upper management. Note that Data Scientists need the complete involvement of Business Managers to succeed. Findings from data analysis and modeling must generate insights for strategic decisions, market positioning, product launch, brand image and so many other areas"
Putting the “Science” Back in Data Science - KDnuggets,"So, the SCIENCE in Data Science is not only about Machine Learning, Deep Learning, Natural Language Processing, A.I. It’s not only STEM. It’s about an interdisciplinary and rigorous approach we borrow from academia to bring profits above average for businesses, frequently involving Psychology, Game Theory, Business Administration, Complexity, Non linear effects and complex causality"
How to Become a Data Scientist: The Definitive Guide - KDnuggets,"The main topics concerning mathematics that you should familiarize yourself with if you want to go into data science are probability, statistics, and linear algebra. As you learn more about other topics such as statistical learning (machine learning) these core mathematical foundations will serve as a base for you to continue learning from. Let’s briefly describe each and give you a few resources to learn from!"
How to Become a Data Scientist: The Definitive Guide - KDnuggets,"T. Jaynes. Since these are textbooks they can be quite expensive if you buy new directly from amazon, so I suggest looking at used copies online or at pdf versions to save yourself some money!"
How to Become a Data Scientist: The Definitive Guide - KDnuggets,"If you prefer learning through a video format, you can also check out Khan Academy’s video series on probability. You can also check out MIT’s OpenCourseWare lectures on Probability and Statistics. Both can be found easily for free on Youtube with a simple search"
How to Become a Data Scientist: The Definitive Guide - KDnuggets,"Python is by far the best supported out of all of these, although the other languages improve all the time! Jupyter notebooks are extremely popular in the field of data science and machine learning. I use this for all my Python courses and most students have really enjoyed it. While probably not the best solution for larger projects that need to be deployed, its fantastic for a learning environment"
How to Become a Data Scientist: The Definitive Guide - KDnuggets,"It has a great community behind it, its basic full version is completely free. It displays visualizations well, gives you lots of options for customizing experience and a lot more. It is pretty much my go to for anything with R! Jupyter Notebooks also support R kernels, and while I have used them, I have found the experience lacking compared to Jupyter Notebook’s capabilities with Python"
How to Become a Data Scientist: The Definitive Guide - KDnuggets,"NumPy is a numerical scientific computing package that serves as the base for almost all the other Python packages in the Python Data Science ecosystem. Pandas is a data analysis library that is built directly off of NumPy that is designed to mimic many of the built-in features or R, such as DataFrames! You can think of it as a super version of Excel that allows you to quickly clean and analyze data. If you become a data scientist that uses Python, pandas will quickly become one of your main tools! It is personally my favorite Python library! I would also recommend checking out"
How to Become a Data Scientist: The Definitive Guide - KDnuggets,"Matplotlib was created to provide a visualization API for Python reminiscent of the style used in MatLab. If you have used MatLab for visualization before, the transition will feel very natural. However, due to its huge library of capabilities, a lot of other visualization libraries have been created off of matplotlib in an attempt to simplify things or provide more specific functionality!"
How to Become a Data Scientist: The Definitive Guide - KDnuggets,"One of the issues with R for beginner data scientists is that it has a huge variety of options for packages when it comes to machine learning. Each major algorithm can have its own separate packages, each with different focuses. When you are starting out I recommend first checking out the"
42 Steps to Mastering Data Science - KDnuggets,"After a quick review -- and a few options for a fresh perspective -- this post will focus more categorically on several sets of related machine learning tasks. Since we can safely skip the foundational modules this time around -- Python basics, machine learning basics, etc. We can also categorize our tutorials better along functional lines this time"
42 Steps to Mastering Data Science - KDnuggets,"This collection of reading materials and tutorials aims to provide a path for a deep neural networks newcomer to gain some understanding of this vast and complex topic. Though I do not assume any real understanding of neural networks or deep learning, I will assume your familiarity with general machine learning theory and practice to some degree. To overcome any deficiency you may have in the general areas of machine learning theory or practice you can consult the recent KDnuggets post"
42 Steps to Mastering Data Science - KDnuggets,"Clearly, SQL is important in data science. As such, this post aims to take a reader from SQL newbie to competent practitioner in a short time, using freely-available online resources. Lots of such resources exist on the internet, but mapping out a path from start to finish, using items which complement each other, is not always as straightforward as it may seem. Hopefully this post can be of assistance in this manner"
42 Steps to Mastering Data Science - KDnuggets,"The term NoSQL has come to be synonymous with schema-less, non-relational data storage schemes. NoSQL is an umbrella term, one which encompasses a number of different technologies. These different technologies aren't even necessarily related in any way beyond the single defining characteristic of NoSQL: they are not relational in nature; for right or wrong, Structured Query Language (SQL) has become conflated with relational database management systems over the years"
What data has to teach us about deep learning? - KDnuggets,"Budapest is calling Data Scientists and Data engineers to CRUNCH Conference, Oct 18-20. CRUNCH will feature talks from Google, Airbnb, Tesla, LinkedIn, Netflix, Uber, and more. Use code KDnuggetsAtCrunch to save"
Data Science Primer: Basic Concepts for Beginners - KDnuggets,"Data mining functionality can be broken down into 4 main ""problems,"" namely: classification and regression (together: predictive analysis); cluster analysis; frequent pattern mining; and outlier analysis. There are all sorts of other ways you could break down data mining functionality as well, I suppose, e. However, this is a reasonable and accepted approach to identifying what data mining is able to accomplish, and as such these problems are each covered below, with a focus on what can be solved with each ""problem"
Full Stack Data Science at ODSC - KDnuggets,"Data Science is built on a rapidly expanding stack. Let me explain by using a software analogy. To build functioning web apps you need a data store, model (or business) layer, some kind of message layer, and a front-end, all running on the cloud. Think of it as a somewhat narrow vertical stack"
Full Stack Data Science at ODSC - KDnuggets,"For most professional data scientists, those days are long gone. The vertical stack is now well established in data science. The kicker is that in many respects the stack for data science is also more horizontal or fatter than traditional software development stacks. Take the data store layer. Structured data stores may sit alongside unstructured data stores, or even real-time streaming data feeds. Thus you may end up with a layer composed of PostgreSQL, MongoDB, and Kafka with plenty to tools still to choose from. The transformation layer may be a combo of Spark and a language like Python, R or Julia"
Full Stack Data Science at ODSC - KDnuggets,"However, it’s the model layer that data scientists start to detect a noticeable bulge. Chances are if you are working with various formats and sources of data you will be using a host of models and tools to suit. DL on TensorFlow for images, NLP on Spark for text, Prophet for time series, and so on. It’s also not uncommon to stack (aka meta ensemble) the models themselves to generate a new model"
Full Stack Data Science at ODSC - KDnuggets,"Nobody can expect to be an expert in every layer of the stack. Quite the opposite. With today's plethora of tool releases, it’s extremely difficult to stay current. ODSC West 2017 is the perfect environment to test drive many of these tools such as:"
Full Stack Data Science at ODSC - KDnuggets,"With AI, the abstraction layer goes one step further up the stack. Whatever your flavor of AI is such as conversational AI, autonomous machines,, etc. ODSC is on it also with talks and workshops on:"
Top Influencers for Data Science - KDnuggets,"How the K-core influencer score is calculated? Relationships between people can be presented as graphs where vertices are entities and edges are connections among them. Finding communities in the graph can be done as finding subgraphs. In large-scale network analysis,"
Top Influencers for Data Science - KDnuggets,"According to Kcore Analytics reports, six influencers in the top ten of 2016 maintained their influence in the top ten of 2017. Among of these, @KirkDBorne, @KDnuggets, and @Ronald_vanLoon have more than new 30 thousand followers from 2016 to 2017. We use the last year number of followers to see the growth rate (given the released period is 412 days).  Some top handles increased less than 50% but still made the list. It might be explained by the"
Top Influencers for Data Science - KDnuggets,"For example, KDnuggets maintained the second ranked with just 41.5% growth rate. For handles not in the list of 2016, the growth rates are marked as NA. Below is table of the list with details of the scores and number of followers in 2017. The original rank for 2017 can be seen in the screenshot of this post. The 2016 report can be found in other sources"
Data Version Control in Analytics DevOps Paradigm - KDnuggets,"The eternal dream of almost every Data Scientist today is to spend all (well, almost all) the time in the office exploring new datasets, engineering decisive new features, inventing and validating cool new algorithms and strategies. However, reality is often different. One of the unfortunate daily routines of a Data Scientist work is to do raw data pre-processing. It usually translates to the challenges to"
Data Version Control in Analytics DevOps Paradigm - KDnuggets,"DVC is an open source tool for data science projects. It makes your data science projects reproducible by automatically building data dependency graph (DAG). Your code and the dependencies could be easily shared by Git, and data — through cloud storage (AWS S3, GCP) in a single DVC environment"
Data Version Control in Analytics DevOps Paradigm - KDnuggets,"ETL is going to be easy and repeatable once you configure it with DVC scripting. It will become a solid pipeline to operate without major supportive effort. Moreover, it will track all changes and trigger an alert for updates in the pipeline steps via DAG"
Data Version Control in Analytics DevOps Paradigm - KDnuggets,"Machine Learning modeling is an iterative process and it is extremely important to keep track of your steps, dependencies between the steps, dependencies between your code and data files and all code running arguments. This becomes even more important and complicated in a team environment where data scientists’ collaboration takes a serious amount of the team’s effort. DVC will be the arm to help you with it"
Data Version Control in Analytics DevOps Paradigm - KDnuggets,"One of the ‘juicy’ features of DVC is ability to support multiple technology stacks. Whether you prefer R or use promising Python-based implementations for your industrial data products, DVC will be able to support your pipeline properly. You can see it in action for both"
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,"Concerning a definition for statistics, it is a field that is a science unto itself and that benefits all other fields and everyday life. What is unique about statistics is its proven tools for decision making in the face of uncertainty, understanding sources of variation and bias, and most importantly, statistical thinking. Statistical thinking is a different way of thinking that is part detective, skeptical, and involves alternate takes on a problem. Statistics involves measurement refinement, experimental design, data analysis, inference, and interpretation of trends and evidence"
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,When there is no design (as in casual data collection) or the design used is not consistent with the project goals (prospective vs. A famous quote by one of the founders of modern statistics R. A. Fisher nicely summarizes this issue:
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,"Regarding measurements, I see a lot of statisticians forgetting the adage “question everything” and trusting the client’s selection or computation of measurements. For example, nice continuous measurements are often categorized, resulting in great loss of information, power, precision, and generalizability. Or an investigator may derive a response variable using a normalization procedure that would best be modeled than used to create a ratio. Given a good design and appropriate measurements, the approach to statistical analysis needs to be based on good statistical principles as I attempted to overview at"
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,Then the result needs to be actionable by estimating things on scales that are useful to the client (e. A very common problem with the rise of machine learning (see below) is improper use of classification as opposed to prediction; classification makes too many assumptions for the client and does not provide a gray zone. I discuss this in detail at
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,"It was not used very much until powerful computers became available to statisticians. The Bayesian approach requires one to specify an anchor/starting point (“prior distribution”) which can require much thought but can also just specify a degree of skepticism to apply to the data. The benefits of going through this step are great – no need to create one-off solutions to complex design/sampling schemes, and Bayes provides directly actionable probabilities – probabilities that effects are positive, for example, as opposed the frequentist p-values which are probabilities of making assertions about effects being positive when in fact they are zero"
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,"Yes. To oversimplify things, I would say that data science is applied statistics + computer science, with less attention to statistical theory and hypothesis testing, in favor of estimation and prediction. Machine learning is an extremely empirical way of doing statistical modeling, without caring very much about being able to separate effects of variables. Many machine learning practitioners are well grounded in statistics but many are not. The latter group seems to be constantly reinventing the wheel and using approaches that statistics has shown decades ago don’t work"
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,"A hallmark of a good statistician is knowing how to quantify accuracy of estimates and predictions. The latter group of machine learning practitioners have never learned the principles and theory behind measures of predictive accuracy (including proper probability accuracy scores) and are constantly developing “classifiers” when predictions or optimal Bayes decisions were needed for the problem. These classifiers have a host of problems including failing to generalize to new samples with much different outcome frequencies, as discussed in more detail in"
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,"First, I start with arithmetic. It is amazing how many people don’t know that you don’t add ratios unless they represent proportions for mutually exclusive events. In general ratios multiply. I see papers all that time that either analyzed ratios without taking the log or that analyzed percent change from baseline, failing to note that the math doesn’t work. Take for example a subject who starts at a value of 1.0 and increases to 2.0. This is a 100% increase. Then consider a subject starting at 2.0 who decreases to 1.0. This is a 50% decrease. The average of 100% and -50% is +25% whereas the two should cancel, arriving at an average of 0%. Percent change is an asymmetric measure and can’t be used in statistical analysis except under special restrictions"
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,"Regarding the improper addition of ratios, many medical papers add odds ratios or hazard ratios when they should have added the logs of these. A simple example shows why. When developing a risk score suppose that two risk factors have regression coefficients of 1 and -1 in a logistic regression model. The two odds ratios are 2.72 and 0.37. Adding these pretends that both risk factors are harmful when in fact the second risk factor is protective. Change from baseline has a host of other problems as described in my blog. Instead we should be analyzing the raw response variable as a dependent variable, covariate-adjusting for the raw baseline variable. Statisticians and other data analysts need to carefully critique the math being used by their collaborators!"
Vital Statistics You Never Learned… Because They’re Never Taught - KDnuggets,"He also works as an expert statistical advisor in the Office of Biostatistics, Center for Drug Evaluation and Research, FDA. He is the author of numerous publications, the influential book Regression Modeling Strategies and the R packages rms and Hmisc. He can be followed on his blog"
From Notebooks to JupyterLab – The Evolution of Data Science IDEs - KDnuggets,"As notebooks grew in popularity, the data science community asked for more features to enable notebooks to play a more central role in data science projects. With this in mind, members of the community joined forces to create JupyterLab, an open source project that takes the classic notebooks to the next level. The new platform adapts easily to multiple workflows, enabling data scientists to seamlessly move between notebooks, scripts, data, and applications. And this is just the beginning"
"Top Quora Data Science Writers and Their Best Advice, Updated - KDnuggets","Data scientists build predictive models. That’s the core of what they do. In addition, they need to know a little bit of:"
"Top Quora Data Science Writers and Their Best Advice, Updated - KDnuggets","A full-stack data scientist would be able to seamlessly perform the role of a data engineer, software engineer, business analyst and data scientist. If you needed someone to develop an app, the FSDS could step in and do it. If you needed someone to set up a data warehouse, or to analyze the strategic management processes of a business, the FSDS could do it"
"Top Quora Data Science Writers and Their Best Advice, Updated - KDnuggets",This brings me to the future. Over the next five years I expect to see lots of companies that are currently claiming to be involved actually trying using it on serious projects. I expect a good chunk of those projects to fail and the whole industry to have generally matured with far more understanding of what works and what doesn’t
"Top Quora Data Science Writers and Their Best Advice, Updated - KDnuggets",Look at the number of GUI tools that support machine learning now. Things like Excel add-ons that automatically cluster data. Give it five years and I expect most people to think only of them when they think about data science
"Top Quora Data Science Writers and Their Best Advice, Updated - KDnuggets","In ten years I think fashion will have well and truly moved on. Data Science will be a skill that is common and expected in other disciplines and specialist data scientists will be looked at a little strangely. You will also have a situation where is is common and normal for the data that is captured by systems to be amenable to data science, as opposed to what is happening now where most data is structured in a way that requires significant manipulation"
"Top Quora Data Science Writers and Their Best Advice, Updated - KDnuggets",Product analytics is a hidden gem. It is fun but doesn’t get talked about nearly as much. This includes:
"Top Quora Data Science Writers and Their Best Advice, Updated - KDnuggets","In my own area of object recognition, we went from ~35% accuracy (mean average precision on Pascal VOC) to above 65% in just 3–4 years. Previously, we were advancing by 1–2% per year, despite object recognition being the hottest area of computer vision with the largest fraction of papers appearing in top conferences every year. Deep learning also made major breakthroughs in reinforcement learning, which is what yielded successes in general Atari game playing, and beat world grand master in Go decades ahead of expectations! It has finally enabled speech recognition to achieve useable levels of accuracy"
"Top Quora Data Science Writers and Their Best Advice, Updated - KDnuggets","Keeping code in version control system just like any other code is the only logical way, because if you, as a DS, perform some heavy ETL or if your code makes decisions that can bring/cost a lot of money, there’s no way it’s going around code review. No. Way"
"Top Quora Data Science Writers and Their Best Advice, Updated - KDnuggets","For some things that are more typical for data scientists, though, I don’t think that storing Jupyter notebooks in version control is a good practice. You can’t see a decent diff on them, they are not “production code” and in general, when you are finished with something, you want to push at least a “camera-ready” python script. Jupyter notebooks are great for experiments and demonstrations, but outside of these cases there’s always something better"
What is hardcore data science – in practice? - KDnuggets,"Naturally, there are many ways to compute data-driven recommendations. For so-called collaborative filtering, user actions like product views, actions on a wish-list, and purchases, are collected over the whole user base and then crunched to determine which items have similar user patterns. The beauty of this approach lies in the fact that the computer does not have to understand the items at all; the downside is that one has to have a lot of traffic to accumulate enough information about the items. Another approach only looks at the attributes of the items, for example, recommending other items from the same brand, or with similar colors. And of course, there are many ways to extend or combine these approaches"
What is hardcore data science – in practice? - KDnuggets,"The above figure shows the cost function to optimize here, mostly to illustrate the level of complexity data science sometimes brings with it. The function itself uses a pairwise weighted ranking metric, with regularization terms. While being very mathematically precise, it is also very abstract. This approach can be used not only for recommendations in an e-commerce setting, but for all kinds of ranking problems, provided one has reasonable features"
What is hardcore data science – in practice? - KDnuggets,"The typical data science workflow looks like this: the first step is always identifying the problem and then gathering some data, which might come from a database or production logs. Depending on the data-readiness of your organization, this might already prove very difficult because you might have to first figure out who can give you access to the data, and then figure out who can give you the green light to actually get the data. Once the data is available, it’s preprocessed to extract features, which are hopefully informative for the task to be solved. These features are fed to the learning algorithm, and the resulting model is evaluated on test data to get an estimate of how well it will work on future data"
What is hardcore data science – in practice? - KDnuggets,"Data science projects are intrinsically exploratory, and to some amount, open ended. The goal might be clear, but what data is available, or whether the available data is fit for the task at hand, is often unclear from the beginning. After all, choosing machine learning as an approach already means that one cannot simply write a program to solve the problem. Instead, one resorts to a data-driven approach"
What is hardcore data science – in practice? - KDnuggets,"The whole process is inherently iterative, and often highly explorative. Once the performance looks good, one is ready to try the method on real data. This brings us to production systems"
What is hardcore data science – in practice? - KDnuggets,"Probably the main difference between production systems and data science systems is that production systems are real-time systems that are continuously running. Data must be processed and models must be updated. The incoming events are also usually used for computing of key performance indicators like click-through rates. The models are often retrained on available data every few hours and then loaded into the production system that serve the data via a REST interface, for example"
What is hardcore data science – in practice? - KDnuggets,"If we put these two systems side-by-side, we get a picture like the Figure above. On the top right, there is the data science side, characterized by using languages like Python, or systems like Spark, but often with one-shot, manually-triggered computations, and iterations to optimize the system. The outcome of that is a model, which is essentially a bunch of numbers that describe the learned model. This model is then loaded by the production system. The production system is a more classical enterprise system, written in a language like Java, which is continually running"
What is hardcore data science – in practice? - KDnuggets,"Finally, it’s important to note that this whole system is not “done” once it is set up. Just as one first needs to iterate and refine the data analysis pipeline on the data science side, the whole live system also needs to be iterated as data distributions change, and new possibilities for data analysis open up. To me, this ""outer iteration"" is the biggest challenge to get right—and also the most important one, because it will determine whether you can continually improve the system and secure your initial investment in data science"
What is hardcore data science – in practice? - KDnuggets,"So far, we have focused on how systems typically look in production. There are variations in how far you want to go to make the production system really robust and efficient. Sometimes, it may suffice to directly deploy a model in Python, but the separation between the exploratory part and production part is usually there"
What is hardcore data science – in practice? - KDnuggets,"The work of data scientists is usually highly exploratory. Data science projects often start with a vague goal and some ideas of what kind of data is available and methods that could be used, but very often, you have to try out ideas and get insights into your data. Data scientists write a lot of code, but much of this code is there to test out ideas and is not expected to be part of the final solution"
What is hardcore data science – in practice? - KDnuggets,"Developers, on the other hand, naturally have a much higher focus on coding. It is their goal to write a system, to build a program that has the required functionality. Developers sometimes also work in an exploratory fashion, building prototypes, proof of concepts, or performing benchmarks, but the main goal of their work is to write code"
What is hardcore data science – in practice? - KDnuggets,"In practice, developers and data scientists often have problems working together. Standard software engineering practices don’t really apply to a data scientist’s exploratory work mode because the goals are different. For example, introducing code reviews and an orderly branch, review, and merge back workflow would just not work for data scientists and may slow them down. Likewise, applying this exploratory mode to production systems also won’t work"
What is hardcore data science – in practice? - KDnuggets,"For example, data science and developer codebases could still be separate, but there is a part of the production system that has a clearly identified interface into which the data scientists can hook their methods. The code that communicates with the production system obviously needs to follow stricter software development practices, but would still be in the responsibility of the data scientists. That way, they can quickly iterate both internally and also with the production system"
What is hardcore data science – in practice? - KDnuggets,"One concrete realization of that architecture pattern is to take a microservice approach and have the ability in the production system to query a microservice owned by the data scientists for recommendations. That way, the whole pipeline used in the data scientist’s offline analysis can be repurposed to also perform A/B tests or even go into production without developers having to re-implement everything. This also puts more emphasis on the software engineering skills of the data scientists, but we are increasingly seeing more people with that skill set. In fact, we have lately changed the title of data scientists at Zalando to “research engineer (data science)” to reflect the fact"
What is hardcore data science – in practice? - KDnuggets,"So, I’ve outlined the typical anatomy of an architecture to bring data science into production. The key concept to understand is that such a system needs to constantly adapt and improve (as do almost all data-driven projects working with live data). Being able to iterate quickly, trying out new methods, and testing the results on live data in A/B-tests is most important"
What is hardcore data science – in practice? - KDnuggets,"In my experience, this cannot be achieved by keeping data scientists and developers separate. At the same time, it’s important to acknowledge that their working modes are different because they follow different goals—data scientists are more exploratory and developers are more focused on building software and systems. By allowing both sides to work in a fashion that best suits these goals and defining a clear interface between them, it is possible to integrate the two sides so that new methods can be quickly tried out. This requires more software engineering skills from data scientists, or at least support by engineers who are able to bridge between both worlds"
Road Lane Line Detection using Computer Vision models - KDnuggets,Detecting lane lines is a fundamental task for autonomous vehicles while driving on the road. It is the building block to other path planning and control actions like breaking and steering. Lets get started implementing them!
Road Lane Line Detection using Computer Vision models - KDnuggets,"The Hough transformation converts a “x vs. Points in the image will correspond to lines in hough space. An intersection of lines in hough space will thus correspond to a line in Cartesian space. Using this technique, we can find lines from the pixel outputs of the canny edge detection output. A detailed explanation of the Hough transformation can be found here"
How GDPR Affects Data Science - KDnuggets,"If your organization collects data about citizens of the European Union (EU), you probably already know about the General Data Protection Regulation (GDPR). GDPR defines and strengthens data protection for consumers and harmonizes data security rules within the EU. The European Parliament"
How GDPR Affects Data Science - KDnuggets,"One caveat before we begin. GDPR is complicated. In some areas, GDPR defines high-level outcomes, but delegates detailed compliance rules to a new entity, the European Data Protection Board. GDPR regulations intersect with many national laws and regulations; organizations that conduct business in the United Kingdom must also assess the unknown impacts of Brexit. Organizations subject to GDPR should engage expert management and legal counsel to assist in developing a compliance plan"
How GDPR Affects Data Science - KDnuggets,"GDPR affects data science practice in three areas. First, GDPR imposes limits on data processing and consumer profiling. Second, for organizations that use automated decision-making, GDPR creates a “right to an explanation” for consumers. Third, GDPR holds firms accountable for bias and discrimination in automated decisions"
How GDPR Affects Data Science - KDnuggets,"The new rules allow organizations to process personal data for specific business purposes, fulfill contractual commitments, and comply with national laws. A credit card issuer may process personal data to determine a cardholder’s available credit; a bank may screen transactions for money laundering as directed by regulators. Consumers may not opt out of processing and profiling performed under these “safe harbors"
How GDPR Affects Data Science - KDnuggets,"As noted above, one regulator interprets the law to cover credit applications, recruitment, and insurance decisions. Other regulators or law courts may interpret the rules differently, but it’s clear that the right applies in specific settings. It does not apply to every automated decision"
How GDPR Affects Data Science - KDnuggets,"GDPR expressly prohibits the use of personal characteristics (such as age, race, ethnicity, and other enumerated classes) in automated decisions. However, it is not sufficient to just avoid using this data. The mandate against discriminatory outcomes means data scientists must also take steps to prevent indirect bias from proxy variables, multicollinearity or other causes. For example, an automated decision that uses a seemingly neutral characteristic, such as a consumer’s residential neighborhood, may inadvertently discriminate against ethnic minorities"
How GDPR Affects Data Science - KDnuggets,"Data scientists must also take affirmative steps to confirm that the data they use when they develop predictive models is accurate; “garbage in/garbage out,” or GIGO, is not a defense. They must also consider whether biased training data on past outcomes can bias models. As a result, data scientists will need to concern themselves with"
How GDPR Affects Data Science - KDnuggets,"If you do business in the European Union, now is the time to start planning for GDPR. There is much to be done: evaluating the data you collect, implementing compliance procedures, assessing your processing operations and so forth. If you are currently using machine learning for profiling and automated decisions, there are four things you need to do now"
How to Turn your Data Science Projects into a Success - KDnuggets,"The 125,000 products in 600 stores lead to 75 million predictions on a monthly basis. The predictive model optimized the spare part assortment, ultimately leading to an increased customer satisfaction by ensuring the right products in the right stores always being available while keeping the in-stock assortment to a minimum. The project was very interesting because it was a mix between the analytical modeling and deployment of the predictive models in the business. The latter is a common challenge for companies"
How to Turn your Data Science Projects into a Success - KDnuggets,"It typically requires rationalization and need a lot of governance around it before it’s acceptable for the business. The rationalization revolves around understanding which predictions are used and which predictions are ignored. For example, if the retailer sold more than 10 items of a product in a store the year prior, and the model indicated that the demand for next year would be very low, the business decided to override the prediction and ensure the item was not taken out of the assortment. This is a very simple example, but it shows that you can’t just take every prediction and apply that in the business context without further thought"
How to Turn your Data Science Projects into a Success - KDnuggets,"The governance is concerned with setting up processes for model scoring on new data, auditing those scores, doing continuous model validations against new observations, setting up procedures to handle escalation if models do not work as expected. All those processes need to be carefully craftedfor an analytic project to be successful. A pure data scientist might be proud with a high AUC or F1 score for a predictive model, but that’s only the start of the analytical deployment process. I always say:"
How to Turn your Data Science Projects into a Success - KDnuggets,"It takes a company quite some effort to become analytic-minded. Maybe there’s a data scientist who knows what to do with data, but it’s the business who needs to learn to appreciate working with analytical outcomes. For example, I once worked with a B2B company who were trying to create an analytical retention program. They had a ‘save desk’ that handled angry customers calling in because of service issues. The model that I built was able to identify customers who were likely to leave – with quite a high success rate. However, when the save desk started reaching out to those customers, they didn’t feel the model was working, because they expected angry customers on the phone. Instead, the model identified issues prior to the customer being at its boiling point. The save desk had to make a turn to become a client relationship desk and once they managed to do so, the analytics started working for them"
How to Turn your Data Science Projects into a Success - KDnuggets,"This is not simply stating: “I want to predict X, please apply some deep learning method. I see many opportunities here for data scientists as well. The market will move towards auto-predictions. Today you already see multiple software offerings available that evaluate a large number of predictive models, tune some hyper-parameters and choose the best model. Does this mean you don’t need data scientists anymore? I think the contrary is true: we need many more data scientists. People who have an analytics-savvy view on the business and have the skills to carefully prepare data to answer the exact right business question"
How to Turn your Data Science Projects into a Success - KDnuggets,"I frequently explain data science as cooking: the better cooking equipment you have, the more sophisticated your meals become. I’ve not seen a kitchen that cooks itself. It’s the data scientist that rather than grilling over a bonfire now needs to learn to operate a modern oven with temperature and time sensors. So, the question now is: does the quality of the roast improve because of the oven or because of the cook who knows how to operate the oven? For the answer is the latter one: although the oven is a joy to work with, it’s the artistry of the chef who makes the meal an unforgettable one"
How to Turn your Data Science Projects into a Success - KDnuggets,"You need to have that really clear. It’s not enough to say: “we will do marketing with it,” you have to spell out how exactly the prediction is being acted upon. This also forces you to think through the timing of the data (when will what data be available) and the deployment option (automation, data pipelines, real-time or batch scoring). With this comes the question:"
How to Turn your Data Science Projects into a Success - KDnuggets,"Many companies have barely properly implemented predictive analytics. I see companies dream on AI – specially on executive level – as the golden solution to not have to deal with data anymore. You ‘just’ collect all internal data in one data lake, add external data such as Facebook, Twitter, blog posts, and the new AI will automatically make sense of it all. The truth couldn’t be further away. Although the most advanced models today do surprising things, they all have one thing in common: they rely on"
How to Turn your Data Science Projects into a Success - KDnuggets,"My best recommendation is: don’t dream that your models will solve your problems, but instead ensure what you are doing makes commercial sense. Dig deep into the data and let your findings guide your next exploratory steps. The curious mind will suddenly hit the “Aha!” moment and then use your data science model knowledge to capture your findings into an actionable structure. This is true data-based creativity which is for me the ultimate data science"
How to Turn your Data Science Projects into a Success - KDnuggets,"In his current role, he helps IBM clients identify and quantify analytic opportunities. He enjoys articulating complex analytical concepts in layman’s terms, contextualized to the business, and he is known for his ability to accelerate analytic deployments. In his prior position as Worldwide Predictive Analytics Solutions leader, he stood at the birth of great many Analytics projects across all geographies and industries such as Telco, Banking, Automotive, Retail, and Insurances. He is passionate about analytics-based data monetization and deep-learning"
The Key to Data Monetization - KDnuggets,"Many organizations are associating data monetization with selling their data. But selling data is not a trivial task, especially for organizations whose primary business relies on its data. Organizations new to selling data need to be concerned with privacy and Personally Identifiable Information (PII), data quality and accuracy, data transmission reliability, pricing, packaging, marketing, sales, support, etc. Companies such as Nielsen, Experian and Acxiom are experts at selling data because that’s their business; they have built a business around gathering, aggregating, cleansing, aligning, packaging, selling and supporting data"
The Key to Data Monetization - KDnuggets,"For organizations seeking to monetize their customer, product and operational insights, the Analytic Profile is indispensible. While I have talked frequently about the concept of Analytic Profiles, I’ve never written a blog that details how Analytic Profiles work.  So let’s create a “Day in the Life” of an Analytic Profile to explain how an Analytic Profile works to capture and “monetize” your analytic assets"
The Key to Data Monetization - KDnuggets,"To support the “Improve Campaign Effectiveness” use case, the data science team worked with the business stakeholders to brainstorm, test and confirm that they needed to build Demographic and Behavioral Segments for each individual customer. The Demographic segments are based upon customer variables such as age, gender, marital status, employment status, employer, income level, education level, college degrees, number of dependents, ages of dependents, home location, home value, work location and job title.  The Behavioral segments are based upon purchase and engagement transactions such as frequency of purchase, recency of purchase, items purchased, amount of money spent, coupons or rebates used, discounts applied, returns and consumer comments"
Data Science Governance - Why does it matter? Why now? - KDnuggets,"GDPR is just around the corner (May 2018) and carries significant financial fines for non-compliance. Without a doubt, the advent of ML, AI and Data Science has had a massive impact on our lives over the last couple of years and will continue to do so in the foreseeable future. In this post I’ll talk about the emergence of Data Science Governance"
Top 15 Python Libraries for Data Science in 2017 - KDnuggets,"When starting to deal with the scientific task in Python, one inevitably comes for help to Python’s SciPy Stack, which is a collection of software specifically designed for scientific computing in Python (do not confuse with SciPy library, which is part of this stack, and the community around this stack). This way we want to start with a look at it. However, the stack is pretty vast, there is more than a dozen of libraries in it, and we want to put a focal point on the core packages (particularly the most essential ones)"
Top 15 Python Libraries for Data Science in 2017 - KDnuggets,"The most fundamental package, around which the scientific computation stack is built, is NumPy (stands for Numerical Python). It provides an abundance of useful features for operations on n-arrays and matrices in Python. The library provides vectorization of mathematical operations on the NumPy array type, which ameliorates performance and accordingly speeds up the execution"
Top 15 Python Libraries for Data Science in 2017 - KDnuggets,"SciPy is a library of software for engineering and science. Again you need to understand the difference between SciPy Stack and SciPy Library. SciPy contains modules for linear algebra, optimization, integration, and statistics. The main functionality of SciPy library is built upon NumPy, and its arrays thus make substantial use of NumPy. It provides efficient numerical routines as numerical integration, optimization, and many others via its specific submodules. The functions in all submodules of SciPy are well documented — another coin in its pot"
Top 15 Python Libraries for Data Science in 2017 - KDnuggets,"Pandas is a Python package designed to do work with “labeled” and “relational” data simple and intuitive. Pandas is a perfect tool for data wrangling. It designed for quick and easy data manipulation, aggregation, and visualization"
Top 15 Python Libraries for Data Science in 2017 - KDnuggets,"Another great visualization library is Bokeh, which is aimed at interactive visualizations. In contrast to the previous library, this one is independent of Matplotlib. The main focus of Bokeh, as we already mentioned, is interactivity and it makes its presentation via modern browsers in the style of Data-Driven Documents (d3"
How HR Managers Use Data Science to Manage Talent for Their Companies - KDnuggets,"Talent acquisition may sound a minuscule term to most of us who have never been on the other side of the table. But in reality, the entire process of hiring is way too time-consuming, expensive and employs a number of resources. Hiring managers are responsible for all the hiring processes taking place in an organization. They have to devise such plans that attract a large number of eligible candidates while creating an environment where candidates would prefer to stay"
Your Checklist to Get Data Science Implemented in Production - KDnuggets,"A data project is a messy thing. It’s lots of data in loads of different formats stored in different places, and lines and lines (and lines!) of code, and scripts in different languages turning that raw data into predictions. Packaging all that together can be tricky if you do not support the proper packaging of code or data during production, especially when you’re working with predictions"
Your Checklist to Get Data Science Implemented in Production - KDnuggets,"This is critical during the development of the project to ensure that the end product is understandable and usable by business users. Once the data product is in production, it remains an important success factor for business users to assess the performance of the model, since they base their work on it. There are several ways to do this; the most popular is setting up live dashboards to monitor and drill down into model performance. Automatic emails with key metrics can be a safer option to make sure business teams have the information at hand"
Your Checklist to Get Data Science Implemented in Production - KDnuggets,"Modern data science relies on the use of several technologies such as Python, R, Scala, Spark, and Hadoop, along with open-source frameworks and libraries. This can cause an issue when production environments rely on technologies like JAVA, .NET, and SQL databases, which could require complete recoding of the project"
Your Checklist to Get Data Science Implemented in Production - KDnuggets,"Real-time scoring and online learning are increasingly trendy for a lot of use cases including scoring fraud prediction or pricing. Many companies who do scoring use a combination of batch and real-time, or even just real-time scoring. And more and more companies report using online machine learning"
7 Techniques to Handle Imbalanced Data - KDnuggets,"Data used in these areas often have less than 1% of rare, but “interesting” events (e. However, most machine learning algorithms do not work very well with imbalanced datasets. The following seven techniques can help you, to train a classifier to detect the abnormal class"
7 Techniques to Handle Imbalanced Data - KDnuggets,"Applying inappropriate evaluation metrics for model generated using imbalanced data can be dangerous. Imagine our training data is the one illustrated in graph above. If accuracy is used to measure the goodness of a model, a model which classifies all testing samples into “0” will have an excellent accuracy (99.8%), but obviously, this model won’t provide any valuable information for us"
7 Techniques to Handle Imbalanced Data - KDnuggets,"Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modelling"
7 Techniques to Handle Imbalanced Data - KDnuggets,"On the contrary, oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing the size of rare samples. Rather than getting rid of abundant samples, new rare samples are generated by using e"
7 Techniques to Handle Imbalanced Data - KDnuggets,Note that there is no absolute advantage of one resampling method over another. Application of these two methods depends on the use case it applies to and the dataset itself. A combination of over- and under-sampling is often successful as well
7 Techniques to Handle Imbalanced Data - KDnuggets,"Keep in mind that over-sampling takes observed rare samples and applies bootstrapping to generate new random data based on a distribution function. If cross-validation is applied after over-sampling, basically what we are doing is overfitting our model to a specific artificial bootstrapping result. That is why cross-validation should always be done before over-sampling the data, just as how feature selection should be implemented. Only by resampling the data repeatedly, randomness can be introduced into the dataset to make sure that there won’t be an overfitting problem"
7 Techniques to Handle Imbalanced Data - KDnuggets,"The easiest way to successfully generalize a model is by using more data. The problem is that out-of-the-box classifiers like logistic regression or random forest tend to generalize by discarding the rare class. One easy best practice is building n models that use all the samples of the rare class and n-differing samples of the abundant class. Given that you want to ensemble 10 models, you would keep e.000 cases of the rare class and randomly sample 10.000 cases of the abundant class. Then you just split the 10.000 cases in 10 chunks and train 10 different models"
7 Techniques to Handle Imbalanced Data - KDnuggets,"The previous approach can be fine-tuned by playing with the ratio between the rare and the abundant class. The best ratio  heavily depends on the data and the models that are used. But instead of training all models with the same ratio in the ensemble, it is worth trying to ensemble different ratios.  So if 10 models are trained, it might make sense to have a model that has a ratio of 1:1 (rare:abundant) and another one with 1:3, or even 2:1. Depending on the model used this can influence the weight that one class gets"
7 Techniques to Handle Imbalanced Data - KDnuggets,"An elegant approach was proposed by Sergey on Quora [2]. Instead of relying on random samples to cover the variety of the training samples, he suggests clustering the abundant class in r groups, with r being the number of cases in r. For each group, only the medoid (centre of cluster) is kept. The model is then trained with the rare class and the medoids only"
7 Techniques to Handle Imbalanced Data - KDnuggets,"All the previous methods focus on the data and keep the models as a fixed component. But in fact, there is no need to resample the data if the model is suited for imbalanced data. The famous XGBoost is already a good starting point if the classes are not skewed too much, because it internally takes care that the bags it trains on are not imbalanced. But then again, the data is resampled, it is just happening secretly"
7 Techniques to Handle Imbalanced Data - KDnuggets,"This is not an exclusive list of techniques, but rather a starting point to handle imbalanced data. There is no best approach or model suited for all problems and it is strongly recommended to try different techniques and models to evaluate what works best. Try to be creative and combine different approaches. It is also important, to be aware that in many domains (e. So, check if past data might have become obsolete"
Data Science for Newbies: An Introductory Tutorial Series for Software Engineers - KDnuggets,"To do some serious statistics with Python one should use a proper distribution like the one provided by Continuum Analytics. Of course, a manual installation of all the needed packages (Pandas, NumPy, Matplotlib etc. In this article we’ll use the Anaconda Distribution. The installation under Windows is straightforward but avoid the usage of multiple Python installations (for example, Python3 and Python2 in parallel). It’s best to let Anaconda’s Python binary be your standard Python interpreter"
Data Science for Newbies: An Introductory Tutorial Series for Software Engineers - KDnuggets,"Patterns are everywhere but many of them can’t be immediately recognized. This is one of the reasons why we’re digging deep holes in our databases, data warehouses, and other silos. In this article we’ll use a few more methods from Pandas’ DataFrames and generate plots. We’ll also create pivot tables and query an MS SQL database via ODBC. SqlAlchemy will be our helper in this case and we’ll see that even Losers like us can easily merge and filter SQL tables without touching the SQL syntax. No matter the task you always need a powerful tool-set in the first place. Like the Anaconda Distribution which we’ll be using here. Our data sources will be things like JSON files containing reddit comments or SQL-databases like Northwind. Many 90’es kids used Northwind to learn SQL"
Data Science for Newbies: An Introductory Tutorial Series for Software Engineers - KDnuggets,"By its own definition Spark is a fast, general engine for large-scale data processing. Well, someone would say: but we already have Hadoop, so why should we use Spark? Such a question I’d answer with a remark that Hadoop is EJB reinvented and that we need something more flexible, more general, more expandable and…much faster than MapReduce. Spark handles both batch and streaming processing at a very fast rate"
Data Science for Newbies: An Introductory Tutorial Series for Software Engineers - KDnuggets,"From my non-scientist perspective I’d define ML as a subset of the Artificial Intelligence research which develops self-learning (or self-improving?) algorithms that try to gain knowledge from data and make predictions based on it. However, ML is not only reserved for academia or some “enlightened circles”. We use ML every day without being aware of its existence and usefulness. A few examples of ML in the wild would be: spam filters, speech-recognition software, automatic text-analysis, “intelligent game characters”, or the upcoming self-driving cars. All these entities make decisions based on some ML algorithms"
Data Science for Newbies: An Introductory Tutorial Series for Software Engineers - KDnuggets,"Before we start using DataFrames we first have to prepare our environment which will run in Jupyter (formerly known as “IPython”). After you’ve downloaded and unpacked the Spark Package you’ll find some important Python libraries and scripts inside the python/pyspark directory. These files are used, for example, when you start the PySpark REPL in the console. As you may know, Spark supports Java, Scala, Python and R. Python-based REPL called PySpark offers a nice option to control Spark via Python scripts"
Data Science for Newbies: An Introductory Tutorial Series for Software Engineers - KDnuggets,"In this article we’ll explore Microsoft’s Azure Machine Learning environment and how to combine Cloud technologies with Python and Jupyter. As you may know I’ve been extensively using them throughout this article series so I have a strong opinion on how a Data Science-friendly environment should look like. Of course, there’s nothing against other coding environments or languages, for example R, so your opinion may greatly differ from mine and this is fine. Also AzureML offers a very good R-support! So, feel free to adapt everything from this article to your needs. And before we begin, a few words about how I came to the idea to write about Azure and Data Science"
Data Science for Newbies: An Introductory Tutorial Series for Software Engineers - KDnuggets,"While finishing my Data Science and ML Essentials course, I discovered that Azure ML has a built-in support for Jupyter and Python which, of course, made it very interesting to me because it makes Azure ML an ideal ground for experimentation. They even call one of their working areas “Experiments” so one can expect good Python (and R) support and many cool off-the-shelf modules. Being no different than other tech-enthusiasts I quickly decided to write an article describing some of the key parts of Azure ML"
Qualitative Research Methods for Data Science? - KDnuggets,"Any kind of research has its pros and cons and qualitative is no exception. One of my main criticisms is that the level of expertise among practitioners is unpredictable. A few folks doing qualitative are amateurs, to be blunt. In some cases, lack of training or inexperience is the culprit. Others just don't seem to have it as researchers but are good at client handling, so they get the work"
Qualitative Research Methods for Data Science? - KDnuggets,"Unexamined assumptions is another pet peeve of mine, and qualitative researchers seem more prone to this than quantitative researchers. A third criticism often leveled by ""quants"" is that analysts sometimes draw quite strong conclusions based on their own intuition rather than evidence. Treating speculation as fact is a cognitive error we all make, but I encounter this more among qualitative researchers"
What Advice Would You Give Your Younger Data Scientist Self? - KDnuggets,"I get asked data science career advice on LinkedIn regularly. As much as I would like to use my limited knowledge to answer peoples' specific questions, it genuinely becomes too time-consuming. I have created posts in the past to try to shed light on topics of interest to wider numbers of people,"
What Advice Would You Give Your Younger Data Scientist Self? - KDnuggets,"I hesitated to even write the above paragraph, since I think it may paint me as a bit of a jackass, quite honestly. I don't posses any special insight beyond what my limited experience has allotted me; however, my association with KDnuggets and my activity on LinkedIn perhaps makes people think that the individual behind that activity might be worth reaching out to. I'm really the furthest thing from conceited that there is, and would be happy to share what I do know with everyone, time permitting"
What Advice Would You Give Your Younger Data Scientist Self? - KDnuggets,"Realistic expectations are a must, too. Do you have a PhD? A Master's? An undergraduate degree? Are you self-taught, with MOOCs, textbooks, and the like? Don't let anyone tell you that the world of ""data science"" is unreachable for someone who is self-taught. Know where it is that you might fit in"
What Advice Would You Give Your Younger Data Scientist Self? - KDnuggets,"Insofar as R and Hadoop, they’re just part of the data science toolkit. They don’t constitute “data science” any more than a scalpel constitutes “surgery. But the mathematics of physics is not a substitute for scientific thinking, analysis, approach or method—and neither are Hadoop and R substitutes for understanding behaviour in data"
How to Build a Data Science Pipeline - KDnuggets,"Start with y. Concentrate on formalizing the predictive problem, building the workflow, and turning it into production rather than optimizing your predictive model. Once the former is done, the latter is easy"
How to Build a Data Science Pipeline - KDnuggets,"This sounds simple, yet examples of working and well-monetized predictive workflows are rare. Companies struggle with the building process. The questions they need to ask are:"
How to Build a Data Science Pipeline - KDnuggets,"While they are not wrong, they are mostly irrelevant. Building and optimizing the predictor is easy. What is hard is finding the business problem and the KPI that it will improve, hunting and transforming the data into digestible instances, defining the steps of the workflow, putting it into production, and organizing the model maintenance and regular update"
How to Build a Data Science Pipeline - KDnuggets,"Companies usually start by what seems to be a no brainer: asking their IT departments to put in place the big data infrastructure and build a data lake. They then hire their first data scientists. These experts, fresh off school and a handful of Kaggle challenges, armed with the data science toolkit, are eager to put their hands on the data. They can predict anything, and they do! They talk the business unit, they find reasonable prediction targets for which labels exist already, they try a dozen models, hyperopt them, and choose the best. They build POCs and send reports to the business unit. And then start again"
How to Build a Data Science Pipeline - KDnuggets,"The business unit doesn’t know what to do with them. They can’t interpret the scores. The prediction target seems reasonable, yet they have no clue how they will make a money with their"
How to Build a Data Science Pipeline - KDnuggets,"If they do, putting the POC into production seems insurmountable. Code has to be rewritten. Real time data has to be channeled into the workflow. Operational constraints need to be satisfied. The decision support system needs to be integrated with the existing work tools of the users. Model maintenance, user feedback, rollback needs to be put in place. These operations usually cost more and create more risk than the safe POCs the data scientist worked on, and the POC simply cannot drive the process"
Why Data Science Argues against a Muslim Ban - KDnuggets,"Let’s not lose our scientific minds. As the White House’s travel ban maneuvers toward the U.S. Supreme Court for possible approval, let’s take a scientific look at whether it makes sense for security. Does data support the ban’s underlying supposition that disallowing Muslims—at least those from certain countries—would decrease the risk of terrorism?"
Why Data Science Argues against a Muslim Ban - KDnuggets,"It is the discipline of improving operational decisions—often made by hunches and intuition—with empirical, fact-based insights. Data science has assumed the expansive role of running our increasingly data-driven world. It drives security screening for both government and commerce, including predictive policing, assessing convicts for parole decisions and detecting fraudulent transactions"
Why Data Science Argues against a Muslim Ban - KDnuggets,"Unfortunately, the tricky pitfalls of quantitative analysis lead some to the opposite conclusion. As a result, for all the value that today’s growing use of data contributes to organizational decision-making, if data are interpreted as supporting a ban, it would only contribute uncertainty when it comes to the status of religious equality. Here are three analytical traps that cause people to misinterpret data as supporting a Muslim ban:"
Why Data Science Argues against a Muslim Ban - KDnuggets,"First, people excessively “slice and dice” data. Depending on how data are selected, certain portions of a terrorism database can wrongly appear to justify religion-based security screening. If a cherry-picked data sample designates Muslims as being most likely to commit an act of terror, then another sample may designate them as least likely. It depends on which records you include for analysis"
Why Data Science Argues against a Muslim Ban - KDnuggets,"For example, a sample of terror incidents could be selected for analysis based on the country attacked, suspect’s country of origin, basis for entry (refugee, student, fiancé, etcetera) and era (for example, before or after 9/11). Different samples will suggest different conclusions and, indeed, published reports on the proportion of terrorism enacted by Muslims have disagreed with one another for this very reason. By poignant coincidence, data scientists affectionately describe such manipulation as “torturing the data until it confesses"
Why Data Science Argues against a Muslim Ban - KDnuggets,"Second, people misjudge the risk individual immigrants present. Even if the odds of violence differed greatly between religions, the odds of any one individual being a terrorist would remain tiny. This is because across any major religion the vast majority of people would never engage in terrorism. Imagine hypothetically that, after slicing and dicing, data show immigrants of a certain religion from a certain country were five times more likely to engage in terrorism than average over the past 10 years.01 percent, in most cases. For no major religion are individual members especially dangerous"
Why Data Science Argues against a Muslim Ban - KDnuggets,"Third, screening by religion only impairs the ability to predict who the terrorists are. It’s more than just philosophical to say people are defined by their behavior—by what they do rather than in which category they fit. In practical application, data science repeatedly shows that people’s prior actions predict future behavior more accurately than demographic profile data do"
Why Data Science Argues against a Muslim Ban - KDnuggets,"The civil rights perspective further bolsters the position that behavior is more predictive than religion. A Muslim ban epitomizes prejudice in the most literal sense: It would be the very act of prejudging individuals based on a protected class, religion. Religion carries the status of a protected class because we respect it as a defining attribute, intrinsic to one's identity. As with other defining attributes like race, gender and country of origin, an individual’s religion holds at most an indirect relationship with whether the person would engage in terrorism. Although terrorism can be shaped by religion, religious scholars argue that it doesn’t stem from religion per se. Terrorism’s causes include socioeconomic and geopolitical factors; an act of terror “in the name of” a religion does not mean it was “because of” that religion. Consequently, it’s an individual's past behavior—not his or her religion—that’s pertinent to detecting any possible malicious intent"
Why Data Science Argues against a Muslim Ban - KDnuggets,"Get the target right. Rather than screening by religion we must focus squarely on the threat we’re compelled to predict: terrorism. The very act of treating an immigrant as an individual, evaluating by way of the person’s unique backstory of behavior, will better predict risk and thereby improve security. In contrast, a ban that targets Muslims would be an epic national mistake—not only in terms of social justice but also in terms of security"
Teaching the Data Science Process - KDnuggets,"The main design feature we needed to change was complete openness. To be able to grade students based on individual performance, we needed to close the leaderboard. In the closed phase students see each other’s scores but not each other’s codes. We grade them using a capped linear function of their score. This typically 1–2 week long closed phase is followed by a “classical” open RAMP in which we grade students based on their activities and their ability of generating diversity and improving their own closed phase score"
Teaching the Data Science Process - KDnuggets,"Since students were free to choose any available data set, data collection was mostly a non-issue. Workflows were relatively simple, so almost all teams delivered working starting kits. On the other hand, many times students fell into the trap of trying to find a business case for a “nice” data set. About half of the teams at least attempted to design a meaningful business case. The top 3 teams (out of 22) delivered top notch products:"
Golden State Warriors Analytics Exercise - KDnuggets,"It’s difficult to not spend too much time gathering and cleansing data. On average, the teams spent 50% to 80% of their time gathering and preparing the data. That only left 10% to 20% of their time for the actual analysis. It’s really hard to know when “good enough” is really “good enough” when it comes to gathering and preparing the data"
Golden State Warriors Analytics Exercise - KDnuggets,"Different teams came up with different sets of predictive variables. Team #1 came up with Total Rebounds, Three-Point Shooting %, Fast Break Points and Technical Fouls as the best predictors of performance. They tested a hypothesis that the more “aggressive” the Warriors played (as indicated by rebounding, fast break points and technical fouls), the more likely they were to win (see"
Golden State Warriors Analytics Exercise - KDnuggets,"Team #2 then tested their analytic models against two upcoming games: New Orleans and Houston. Their model accurately predicted not only the wins, but the margin of victory fell within their predicted ranges. For example in the game against New Orleans, their model predicted a win by 21 to 30 points, in which the Warriors actually won by 22 (see"
Golden State Warriors Analytics Exercise - KDnuggets,"One of the many reasons why I love teaching is the ability to work with students who don’t yet know what they can’t accomplish. In their eyes, everything is possible. Their fresh perspectives can yield all sorts of learnings, and not just for them. And yes, you can teach an old dog like me new tricks!"
Data science through the lens of research design - KDnuggets,"You are a data scientist or engaged in a data science project in your organization. Congratulations! You have one of the most interesting, influential, and intellectually stimulating jobs on the market.  You’ve mastered stats, machine learning, become a programming wizard, an expert in visualization, a big data evangelist, and a math god"
Data science through the lens of research design - KDnuggets,"These last three years, our group has lead numerous data science projects across diverse verticals, including ad tech, fin tech, health tech, cloud computing, security, and the telecom industry. Surprisingly, many of our projects share similar attributes despite originating from different domains. Trivial commonalities are evident in the employed algorithms, platforms, and tools. But more important similarities lie in the life cycles of data science projects – from inception to production. I would claim that a successful data science project likely stems not from the technical skills mentioned above, but from something far more fundamental to classic scientific methods and research design"
Data science through the lens of research design - KDnuggets,"This seems trivial but is a point often neglected. Never agree to do a project in which you are asked to “tell us something about the data”. This would most certainly lead to project failure, dissatisfaction of your clients/bosses, and to overall frustration. Try to lead your team to identify the exact definition of the theoretical research/business question. For example, in a classification task of bad vs"
Data science through the lens of research design - KDnuggets,"For example, if the theoretical research problem is defined by classifying good vs. One way of establishing this would be to count number of clicks a user made in a session relative to other users in the same geographical location, and during the same day and hour. This definition must be known and acceptable to all people involved in the project"
Data science through the lens of research design - KDnuggets,"Whatever approach you choose is important, whether you implement XGBoost, random forests, HMMs, CNNs or RNNs. Moreover, you can engineer clever features, produce amazing visualization, and optimize scalable code. Nevertheless, if you are not addressing the concise business problem and its operationalization, you are essentially pushing water uphill with a rake"
42 Essential Quotes by Data Science Thought Leaders - KDnuggets,"Skeptical detective. Signal-extractor. Possessed. If you’re a data scientist, it’s entirely legitimate for you to claim one (or all) of these identities as your own. At least, so say some of the leading innovators in data science today. In these quotes, they share their advice and insights drawn from years of experience to address the field’s most pressing questions. The list is designed to help both aspiring and established data scientists advance their careers, and also to spark some intelligent conversation at your next data science meetup"
What Do Frameworks Offer Data Scientists that Programming Languages Lack? - KDnuggets,"It used to be that computer programming was all about using the right language. Between systems like C, Lisp, and Pascal, programmers had their choice of speciality and format. However, the differences between programming languages have largely been fixed with increased computing power that allows systems to understand and easily move between all languages. Today, the focus is on frameworks, which tend to be more modern and forward-thinking and which can overcome many of the outdated practices of programming languages"
What Do Frameworks Offer Data Scientists that Programming Languages Lack? - KDnuggets,"Frameworks are cohesive sets of library code that simplify programming in any given language, whereas language is the actual syntax and grammar of writing a code. Frameworks come with a number of advantages. While programming languages will never be completely obsolete, a growing number of programmers prefer working with frameworks and view them as the more modern and cutting-edge option for a number of reasons. The move towards frameworks is part of the"
What Do Frameworks Offer Data Scientists that Programming Languages Lack? - KDnuggets,"Programming used to be all about getting the most you could out of code, but that practice has largely been done away with by automated code-writing systems. Today, instead of focusing on how to write an API, programmers care more about what that API can do. It’s like the difference between trying to get the correct spelling of a word versus actually understanding what the word means and how to use it in a sentence. Since most coding is"
What Do Frameworks Offer Data Scientists that Programming Languages Lack? - KDnuggets,"Thanks to automated systems and tutorials, having a deep knowledge of various programming languages simply isn’t as important as it once was. Mistakes can be automatically corrected through various programs that are constantly looking for coding errors. Instead of spending time going over the minute details of a code, frameworks allow programmers to think in the bigger picture. With a better understanding of what the systems and APIs are capable of and automated systems to take care of the more tedious details, programmers can put more effort into turning their programs into something bigger with more high-level features and potential"
What Do Frameworks Offer Data Scientists that Programming Languages Lack? - KDnuggets,"One of the most important parts of using a programming language is understanding the algorithms and making sure the code fits into those algorithms. However, algorithms can be limited by language because they are actually defined by the frameworks. Changing and establishing algorithms as part of the framework is much safer and more effective than trying to tinker with them as part of the language. Frameworks have"
What Do Frameworks Offer Data Scientists that Programming Languages Lack? - KDnuggets,"No matter what language they focus on, many programmers agree that frameworks are the future of coding. If code is law, programming language is the enforcer that makes sure the law is put into action, but frameworks are the systems that actually create the laws. By focusing on frameworks, programmers can have a bigger say in the future and can actually set the rules for the codes instead of simply implementing them. Once the rules are set, everyone must work inside of them, so understanding framework gives users a chance to set the rules that everyone else must follow. In a fast-moving and forward-thinking programming world, being able to set some of the rules provides power and opportunity to change the game"
What Do Frameworks Offer Data Scientists that Programming Languages Lack? - KDnuggets,"This brings us to how frameworks can greatly benefit data scientists in their work. All of the above points can prove advantageous to data scientists. For one, a focus on frameworks means data scientists don’t always need to have extensive experience in coding and programming languages. They can instead bring their experience in their respective industries to the table. Frameworks also help data scientists in the process of data mining and data analysis, while also provided them with the time to focus on the bigger picture"
What Do Frameworks Offer Data Scientists that Programming Languages Lack? - KDnuggets,"Hadoop, for example, was one of the first big data processing frameworks to be adopted by a wide variety of businesses. Hadoop also led to to the growth of an entire ecosystem of technologies like Hive and Pig dedicated to big data processing. Other frameworks like Spark, Samza, and Flink all have their place and have helped data scientists better achieve positive results, derive insights from large data sets, and efficiently manage big data projects"
Getting Into Data Science: What You Need to Know - KDnuggets,"There are many different roles that involve working with big data analytics. The term “data scientist” is sometimes used as a general term instead of to describe a specific role in the field. Data scientists often fulfill a number of responsibilities, and many companies have different names for the same role. In essence, however, a data scientist looks at large and complex"
Getting Into Data Science: What You Need to Know - KDnuggets,"Though a few data scientists get by with a bachelor’s degree, you should focus on your education if you are interested in entering the field. Necessary skills for data scientists include coding proficiency, machine learning/data mining knowledge, mathematics and statistics, big data platforms, structured and unstructured data, business knowledge, curiosity, and communication skills. Many data scientists wear many hats and need a wide range of skills to succeed"
Getting Into Data Science: What You Need to Know - KDnuggets,"From 2014 to 2024, the data scientist career path is expected to grow by 11%–4% faster than for all occupations. If you’re interested in following this career path, start looking into educational programs and consider moving to the coasts—65% of data scientist openings are concentrated in these areas. What do current data scientists recommend? Interact with others in your field, and start building your connections and knowledge"
Take The Next Step in Your Data Science Career - KDnuggets,"Increase your marketability with a broad-based background that will allow you to convert data into actionable intelligence. The Saint Mary's College Master of Science in Data Science program will prepare you to enter into the data analysis process at any stage, from the initial formulation of the question, to visualizing data, to interpreting the results and drawing conclusions. You'll gain the range and depth of expertise to be a leader in data-driven industries"
The Best Python Packages for Data Science - KDnuggets,"In other words, it depends. However, there is no doubt Python is a language of choice for a large percentage of data scientists who want to understand data, especially those looking to leverage its great data science packages. Python is also boasts being open source which is great for anyone looking to get started with data science in their spare time"
Do We Need Balanced Sampling? - KDnuggets,"In many real-world classification tasks such as churn prediction and fraud detection, we often encounter the class imbalance problem, which means one class is significantly outnumbered by the other class. The class imbalance problem brings great challenges to standard classification learning algorithms. Most of them tend to misclassify the minority instances more often than the majority instances on imbalanced data sets.  For example, when a model is trained on a data set with 1% of instances from the minority class, a 99% accuracy rate can be achieved simply by classifying all instances as belonging to the majority class. Indeed, the problem of learning on imbalanced data sets is considered to be one of the ten challenging problems in data mining research"
Do We Need Balanced Sampling? - KDnuggets,"In order to solve the problem of learning from imbalanced data sets, many solutions have been proposed in the past few years. Resampling approaches try to solve the problem by resampling the data and act as a preprocessing phase. Their usage is assumed to be independent of the classifier and can be applied to any learning algorithm. Hence, resampling solutions are very popular in practice. One important question when we use resampling is whether we actually need a perfectly balanced data set. Our research on churn prediction shows that a balanced sampling is unnecessary"
Do We Need Balanced Sampling? - KDnuggets,"We used 11 real-world data sets from the telecommunication industry in our experiments. Seven sampling methods were considered, which include random over-sampling, random under-sampling SMOTE sampling and so on. We consider three different settings for the class ratios: 1:3, 2:3 and 1:1 (minority vs majority). Four benchmark classifiers are used in the experiments: logistic regression, C4.5 decision tree, support vector machine (SVM) and random forests (RF), which are widely used in churn prediction. The following table shows part of the results by using a 5 × 2 cross-validation experimental setup, where each entry represents the mean performance of each sampling rate across different classifiers and sampling methods. Beside the AUC measure, we also consider the maximum profit measure, which measures the profit produced by a retention campaign (Verbraken et al"
Do We Need Balanced Sampling? - KDnuggets,"As the table shows, the ratio of 1:3 is the best on two data sets and the ratio of 2:3 ranks the first on two data sets. The balanced class ratio never reaches the top position. The results definitely show there is no need to produce balanced data sets after sampling and the less balanced strategy (1:3) would be our recommendation due to its relatively good performance. The complete results and more discussions can be found in our recent paper “Benchmarking sampling techniques for imbalance learning in churn prediction” published in JORS"
Models: From the Lab to the Factory - KDnuggets,3% on NYSE and 16.9% on NASDAQ. Due to a
Models: From the Lab to the Factory - KDnuggets,"The data lab is an environment for exploration for data scientists, divorced from application’s production concerns. We may have an eventual end goal of being able to use data to drive decision making within the organization, but before we can get there, we need to understand which hypotheses make sense for our organization and prove out their value. Thus we are mainly focused on creating an environment—“the lab”—where data scientists can ask questions, build models, and experiment with data"
Models: From the Lab to the Factory - KDnuggets,"To provide a structure to the model, we define it based on its components—data dependencies, scripts, configurations, and documentation. Additionally, we capture metadata on the model and its versions to provide additional business context and model-specific information. By providing a structure to the model, we can then keep inventory of our models in the model registry, including different model versions and associated results which are fed by the execution process. The diagram below illustrates this concept"
Models: From the Lab to the Factory - KDnuggets,"Once a model has been approved for deployment, we need to go through steps to ensure the model can be deployed. There should be tests in place to verify correctness, the pipeline of extracting raw data, feature generation, and model scoring should be analyzed to make sure that model execution can run automatically, will expose results in the way needed by consumers, and meets the performance requirements defined by the business. Also ensure that model execution is monitored in case of errors or if a model has gone stale and is no longer producing accurate results"
Models: From the Lab to the Factory - KDnuggets,"Once deployed in production, we want to expose the predictions of the model to consumers. How many users will be consuming this model prediction? How quickly must the feature data be available when scoring the model? For example, in the case of fraud detection, if features are generated every 24 hours, there may be that much lag between when the event happened and when the fraud detection model detected the event. These are some of the scalability and performance questions that need to be answered"
Models: From the Lab to the Factory - KDnuggets,"In the case of an application, ideally, we want to expose the results of the model via a web service, either via real-time scoring of the model or by exposing scores that were produced offline via a batch process. Alternatively, the model may need to support a business process and we need to place the results of the model in a location where a report can be created for decision makers to act on these results. In either case, without a model registry, it can be challenging to understand where to find and consume the results of a current model running in production"
Models: From the Lab to the Factory - KDnuggets,"Another use case is wanting to understand how the model is performing against live data in order to see whether the model has gone stale or whether a newly developed model outperforms the old one. An easy example of this is a regression model where we can compare the predicted vs actual values. If we do not monitor the results of a model over time, we may be making decisions based on historical data that is no longer applicable to the current situation"
Models: From the Lab to the Factory - KDnuggets,"In this post, we walked through the model life cycle, and discussed the needs of the lab and the factory, with the intent to reduce the risk of deploying “bad” models that could impact business decisions (and potentially incur a large cost). In addition, the registry provides transparency and discoverability of models in the organization. This facilitates new model development by exposing existing techniques used in the organization to solve similar problems and facilitates existing model maintenance or enhancements by making it clear which model version is currently in production, what are its associated assets, and a process to publish a new model version to consume"
Data Science for the Layman (No Math Added) - KDnuggets,"You’ve seen how data science has revolutionized the way we live and work. Google uses it to generate relevant results to your search queries and Amazon uses it to recommend products you might like. Recently, data science algorithms have also been used to drive the development of digital personal assistants and autonomous vehicles. Understanding how these algorithms work is crucial to appreciating its potential impact on our future"
Automate Stacking In Python: How to Boost Your Performance While Saving Time - KDnuggets,"Utilizing stacking (stacked generalizations) is a very hot topic when it comes to pushing your machine learning algorithm to new heights. For instance, most if not all winning Kaggle submissions nowadays make use of some form of stacking or a variation of it. First introduced in the 1992 paper"
Automate Stacking In Python: How to Boost Your Performance While Saving Time - KDnuggets,"Wolpert himself noted at the time that large parts of stacked generalizations are “black art”, it seems that building larger and larger stacked generalizations win over smaller stacked generalizations. However, as these models keep increasing in size, they also increase in complexity. Automating the process of building different architectures would significantly simplify this process. The remainder of this article will deal with the package"
Automate Stacking In Python: How to Boost Your Performance While Saving Time - KDnuggets,"The main idea behind the structure of a stacked generalization is to use one or more first level models, make predictions using these models and then use these predictions as features to fit one or more second level models on top. To avoid overfitting, cross-validation is usually used to predict the OOF (out-of-fold) part of the training set. There are two different variants available in this package but I’m going to describe ‘Variant A’ in this paragraph. To get the final predictions in this variant, we take the mean or mode of all of our predictions. The whole process can be visualized using this GIF from vecstacks’ documentation:"
Automate Stacking In Python: How to Boost Your Performance While Saving Time - KDnuggets,"After having taken a look at the documentation, it was time to try using the package myself and see how it works. To do so, I decided to use the wine data set available on the UCI Machine Learning Repository. The problem statement for this data set is to use the 13 features, which all represent different aspects of the wine, to predict from which of three cultivars in Italy the wine was derived"
Automate Stacking In Python: How to Boost Your Performance While Saving Time - KDnuggets,"We are getting closer to the interesting part. Remember the GIF from earlier? It is time now to define a few first level models for our stacked generalization. This step definitely deserves its own article but for purposes of simplicity, we are going to use three models: A KNN-Classifier, a Random Forest Classifier and an XGBoost Classifier"
Automate Stacking In Python: How to Boost Your Performance While Saving Time - KDnuggets,"Again, referring to the GIF, all that’s left to do now is fit the second level model(s) of our choice on our predictions to make our final predictions. In our case, we are going to use an XGBoost Classifier. This step is not significantly different from a regular fit-and-predict in sklearn except for the fact that instead of using X_train to train our model, we are using our predictions S_train"
Is Kaggle Learn a “Faster Data Science Education?” - KDnuggets,"First, we focus more on practical applications instead of academic theory. When I was in data science consulting, I worked on projects solving business problems for dozens of companies. It's striking how conventional courses focus on so many skills that don't matter in practice, while missing many of the things that matter most. For example, students might learn to code an algorithm from scratch (which no one will ever need you to do again), but you won't learn how to manipulate data to get it into mainstream libraries that implement the algorithm"
Is Kaggle Learn a “Faster Data Science Education?” - KDnuggets,"I've also been involved in hiring many times, and I know how well interesting, hands-on projects can get a hiring manager's attention. So, we make our courses really short, so you can get to making projects sooner. Personal projects also help you focus on the skills involved in real work. As an another benefit, most people enjoy their personal projects, so they find more time to do it"
Is Kaggle Learn a “Faster Data Science Education?” - KDnuggets,"Strategically, Kaggle is in a unique position. Most business charge for their courses. The prices for online courses are extremely fair (a small fraction of what you'll earn when you get a data science job), and the people creating them are putting in a lot of work to make something. But when someone needs to charge for their courses, they feel pressure to create longer courses. It would be hard to charge hundreds or thousands of dollars for a 4 hour course and then tell the user ""your better off working on a personal project at this point"
Is Kaggle Learn a “Faster Data Science Education?” - KDnuggets,"We're about to release a feature engineering course, natural language processing course, geospatial analytics course, and a reinforcement learning course. Each course will only be about 4 hours long. You won't become an expert in any of these fields in that amount of time. But you'll learn enough to work independently, find new answers as you need them, and start doing interesting work"
An Overview of Python’s Datatable package - KDnuggets,"Modern machine learning applications need to process a humongous amount of data and generate multiple features. Python’s datatable module was created to address this issue. It is a toolkit for performing big data (up to 100GB) operations on a single-node machine, at the maximum possible speed"
An Overview of Python’s Datatable package - KDnuggets,Modern machine learning applications need to process a humongous amount of data and generate multiple features. This is necessary in order to build models with greater accuracy. Python’s
An Overview of Python’s Datatable package - KDnuggets,"The datatable module definitely speeds up the execution as compared to the default pandas and this definitely is a boon when working on large datasets. However, datatable lags behind pandas in terms of the functionalities. But since datatable is still undergoing active development, we might see some major additions to the library in the future"
Crafting an Elevator Pitch for your Data Science Startup - KDnuggets,"The term “elevator pitch” originally came from the idea of entrepreneurs and small business owners developing a 30-second pitch when they were asked the question, “What do you do?” This might be at social occasions and events, etc. It was an opportunity for the business owner/entrepreneur to provide a very short (ergo, 30-second) explanation of the value that they provide to their customers or clients, followed by presenting a business card. This activity has always been seen as a tool to “spread the word,” certainly not to seek funding"
Crafting an Elevator Pitch for your Data Science Startup - KDnuggets,"Over time, the elevator pitch definition has expanded significantly and now refers to not just the short little answer to that simple question. It has expanded into pitches that founders of startups use as they seek funding. Obviously, this type of pitch is longer and more detailed, and it occurs when such founders seek seed money to begin or to seek additional funding, once they have launched and need investment dollars to expand"
Crafting an Elevator Pitch for your Data Science Startup - KDnuggets,"As a data science founder, you are in a pretty good position. The field is new enough that investors are looking put dollars into startups that can provide these services to small and mid-sized businesses that cannot afford to employ their own data scientist. You don’t’ yet have a huge competition in this market, and that puts you in a favorable position if you can get your pitch right"
Crafting an Elevator Pitch for your Data Science Startup - KDnuggets,This is important. Investors want to see evidence that the product/service is viable. Here is how you do that:
Crafting an Elevator Pitch for your Data Science Startup - KDnuggets,"An elevator pitch requires a script – and your enthusiasm for your company will not be sufficient. If you intend to create a video presentation (and you must), then understand that it has to be both professional and personal, depending upon your audiences. You have to research those audiences and perhaps develop unique scripts for each of them. And then, of course, the script must be written and practiced before delivery. Again, professional help may be warranted. Joe Lesinski, a scriptwriter for clients of"
Crafting an Elevator Pitch for your Data Science Startup - KDnuggets,"The bottom line is this: investors and funders get pitches every day – hundreds of them. You have to somehow stand out among all of this competition, and your amazing data science skills will not be enough to convince. Communication is the key element that may be the most challenging. Follow these guidelines, and your chances will be much greater"
Crafting an Elevator Pitch for your Data Science Startup - KDnuggets,"Estelle Liotard is, first and foremost, a marketing specialist who has consulted with businesses from their founding forward. From her experience, she has also turned to writing about how companies can launch, get funding, and grow. She is a frequent contributor to a number of blogs and an editorial staff member of"
Command Line Basics Every Data Scientist Should Know - KDnuggets,"If you are a data scientist or learning data science and want to move beyond using jupyter notebooks to writing production-ready code, the chances are that you will need to use the command line for some tasks. I find that documentation for production data science tools and processes often assume that you already know these basics. However, if you don’t come from a computer science background then it is possible that you won’t know how to complete some of the simpler tasks from the terminal"
Manual Coding or Automated Data Integration – What’s the Best Way to Integrate Your Enterprise Data? - KDnuggets,"To design a good data integration strategy, it’s essential to evaluate the scope of your enterprise data and its impact on your plans beyond the initial integration projects. The crux of the matter is that as your company grows, so will your data. Designing an integration strategy that accounts for this growth will help you establish your reputation as a data-driven organization"
Manual Coding or Automated Data Integration – What’s the Best Way to Integrate Your Enterprise Data? - KDnuggets,"In the majority of organizations, data integration is the first step towards something bigger, such as migration or data warehousing. For instance, an organization initially looking to integrate their marketing and sales data may have the end goal of creating a master data management system with detailed customer records. Thus the goal of the project should be taken in regard when making the selection"
Manual Coding or Automated Data Integration – What’s the Best Way to Integrate Your Enterprise Data? - KDnuggets,"Your integration initiatives, whether manual or automatic, will incur costs. This includes the resources for designing, maintaining, and scaling a constant flow of projects if you want to establish your reputation as a data-driven organization. Simultaneously, it may also require a company-wide change in regards to organizational functions, in addition to the technical ones. If you want to modify the data landscape of your company, make sure to work with stakeholders that understand how those changes impact the business and IT users working with data"
Manual Coding or Automated Data Integration – What’s the Best Way to Integrate Your Enterprise Data? - KDnuggets,"The ability to integrate new technologies with your existing systems promises tangible improvements for your organization. However, if you lean towards the manual approach, you won’t be able to incorporate these technologies in your data ecosystem without devoting a considerable amount of development time and developer resources. An integration strategy that is scalable enough to accommodate new technological advancements, like cloud-based apps and infrastructure, can bring significant improvements to the organization"
Manual Coding or Automated Data Integration – What’s the Best Way to Integrate Your Enterprise Data? - KDnuggets,"If you are undertaking an integration job using Big Data or analytics, chances are it will be a recurring one. In that case, it may be time-consuming to modify the code or find experts to maintain the tasks to sustain the integration flow. Whereas with one-time tasks, you’d be better off with manual coding"
Manual Coding or Automated Data Integration – What’s the Best Way to Integrate Your Enterprise Data? - KDnuggets,"In today’s world, data integration is one of the fastest ways to acquire business-critical insights and gain a competitive edge. Therefore, it becomes crucial for an organization to select the right strategy and tools to achieve the desired business goals. The simplicity of manual coding makes it an appealing choice, but the automated, straightforward experience of ETL works in the long run. Which integration strategy works for your organization?"
Statistical Modelling vs Machine Learning - KDnuggets,"Statistical modelling is a method of mathematically approximating the world. Statistical models contain variables that can be used to explain relationships between other variables. We use hypothesis testing, confidence intervals etc to make inferences and validate our hypothesis"
Statistical Modelling vs Machine Learning - KDnuggets,Inherently all statistical models are wrong or not perfect. They are used to approximate reality. Sometimes the underlying assumptions of the model are far too strict and not representative of reality
Statistical Modelling vs Machine Learning - KDnuggets,"One cannot be done without the other. A true ""Data Scientist"" needs to have both in their arsenal. Machine learning foundations are from statistical theory and learning. At times it may seem Machine Learning can be done these days without a sound statistical background but those people are not really understanding the different nuances. Code written to make it easier does not negate the need for an in-depth understanding of the problem"
Data Science: Scientific Discipline or Business Process? - KDnuggets,"Simply put, data science is an attempt to understand given data using the scientific method. That's why data science is a scientific discipline. You are free (and encouraged!) to apply data science to business use cases, just as you are encouraged to apply it to many other domains"
Data Science: Scientific Discipline or Business Process? - KDnuggets,But let's say you are hired by BASF. Or DuPont. Or ExxonMobil. They would then want you to use your chemistry knowledge in order to add value to their business. They would be interested in extracting the relevant know-how you posses in order to enhance business processes
Data Science: Scientific Discipline or Business Process? - KDnuggets,"Simply put, data science is an attempt to understand given data using the scientific method. That's why data science is a scientific discipline. Plain and simple. The fact that the word ""science"" is included in the term, while the word ""business"" is not, should be a dead giveaway"
Data Science: Scientific Discipline or Business Process? - KDnuggets,"You are free (and encouraged!) to apply data science to business use cases, just as you are encouraged to apply it to many other domains. But this application does not permit the wholesale co-opting of data science as an exclusively business term. And some of the biggest problems that we can find insights into using data science — weather prediction, climate change, earthquake prediction, to modestly name but a few — have nothing at all to do with business"
Getting Started With Data Science - KDnuggets,"Hello World! It’s been a very very long time since my last post. I’ve been trying to find time, but somehow haven’t been able to. Anyway, now that I’m back, let me jump right into the topic!"
Getting Started With Data Science - KDnuggets,"Over the past many months, I’ve received hundreds of messages from people asking me how they could get started with Data Science. Now, I’ve been trying to reply to many of these messages, but increasingly I’ve been finding it difficult to keep up. Therefore, I thought it would be useful to write down a framework for those wanting to get started with Data Science"
Getting Started With Data Science - KDnuggets,"Data Science is in general an umbrella term. A lot of different fields have come together to contribute to the term ‘Data Science’. As a result, there is a lot of confusion out there. In fact, there are many different ways to actually get started with Data Science"
Getting Started With Data Science - KDnuggets,"One thing that struck me when answering the messages that I get was the background of the people asking me questions. From students to engineers to accounting executives, there’s a lot of people from diverse backgrounds wanting to get into Data Science. That’s great for the field! But it’s difficult for me to be able to offer a generic advice to someone who has never programmed to someone who’s been programming all their life! That’s why, I’ve come up with this framework of getting started with Data Science based on the background of the person"
Getting Started With Data Science - KDnuggets,"It’s my hope that this framework will help you identify the path you can possibly take. With this said, please go ahead and read the sections of the blog post that are most relevant to you. Happy learning!"
Getting Started With Data Science - KDnuggets,"Alright! Hope you’ve been able to identify exactly where you belong and have started working on it! Note that this is only a framework. You don’t have to follow it exactly to the T. Feel free to pick and choose parts of the framework as it suits you. But from what I’ve seen, if you can do most of what’s mentioned in the framework, it will help you perform well in your Data Science interviews and get a job in Data Science! Let me know if this framework has helped you in any way"
What 70% of Data Science Learners Do Wrong - KDnuggets,"I actively searched for hard and useful classes for most of my time in college. But, I got tired by my final year, and I wanted a break. So I took a “fun” class from the engineering department called “The Physics of Sailing"
What 70% of Data Science Learners Do Wrong - KDnuggets,"We diagrammed the forces allowing a sailboat to go faster than the wind. We learned how the shape of a boat can make it stable or unstable. I’d already taken more physics than most of my classmates. So, I did well on the homework and assumed I’d be a natural if I ever went sailing"
What 70% of Data Science Learners Do Wrong - KDnuggets,"The boats felt tippy, and my knowledge of buoyancy and “righting arms” didn’t keep me in the boat. Turning required coordinating multiple movements. And when I got the timing wrong, a two-meter long metal pole (called the boom) swung around and clocked me in the head. The cracking sound of the boom on my head made my ears ring for minutes each time"
What 70% of Data Science Learners Do Wrong - KDnuggets,"Corporate data science is still a new field. Many academics haven’t worked on real problems for real businesses yet. So they teach textbook algorithms in a way that’s separated from data and business context. This can be intellectually fun. But, students are mistaken if they assume these courses prepare them well to work as data scientists"
What 70% of Data Science Learners Do Wrong - KDnuggets,"There’s a lot to learn about manipulating data, interpreting it, and connecting your tools to reality. I’ve intentionally reduced the amount of abstract theory I teach, to help learners focus on practical skills. I think this approach will keep you from smacking themselves in the back of the head when you start your serious projects"
Learn how to use PySpark in under 5 minutes (Installation + Tutorial) - KDnuggets,"Apache Spark is one of the hottest and largest open source project in data processing framework with rich high-level APIs for the programming languages like Scala, Python, Java and R. It realizes the potential of bringing together both Big Data and machine learning. This is because:"
Learn how to use PySpark in under 5 minutes (Installation + Tutorial) - KDnuggets,"Typically when you think of a computer you think about one machine sitting on your desk at home or at work. This machine works perfectly well for applying machine learning on small dataset . However, when you have huge dataset(in tera bytes or giga bytes), there are some things that your computer is not powerful enough to perform. One particularly challenging area is data processing. Single machines do not have enough power and resources to perform computations on huge amounts of information (or you may have to wait for the computation to finish)"
Learn how to use PySpark in under 5 minutes (Installation + Tutorial) - KDnuggets,3. Hit Return and the script will run. It will output to your terminal a log of what is going to install. Hit Return to continue or any other key to abort
Learn how to use PySpark in under 5 minutes (Installation + Tutorial) - KDnuggets,"0 which minimize the number of concepts to remember or construct.0.0, the three main connection objects were SparkContext, SqlContext and HiveContext)"
Learn how to use PySpark in under 5 minutes (Installation + Tutorial) - KDnuggets,Spark has seen immense growth over the past several years. Hundreds of contributors working collectively have made Spark an amazing piece of technology powering the de facto standard for big data processing and data sciences across all industries. But please remember to use it for manipulations of
The 2017 Data Scientist Report is now available - KDnuggets,"For the third year in a row, we surveyed data scientists (nearly 200 this year) from all manner of organizations. We asked questions from years past like gauging happiness levels and demand for data scientists and new questions about characteristics of the data used by our respondents. New this year -- lots of insights into the word of AI with a special emphasis on the important components of AI success like algorithms and training data"
The 2017 Data Scientist Report is now available - KDnuggets,P.S. 28% of data scientists would rather break a leg than accidentally delete their training data. Learn more about the value of training data at
Five Command Line Tools for Data Science - KDnuggets,This function will automatically fix common CSV errors and remove any bad rows. A useful aspect of this function is that it automatically outputs a new cleaned version of the CSV file so that the raw data is preserved. The new file always has the following naming convention
Five Command Line Tools for Data Science - KDnuggets,"At times you may also want to obtain a data set via a SQL query on a database. The tool csvsql, which is also part of the csvkit tool, supports querying, writing and creating tables directly on a database. It also supports SQL statements for querying a CSV file. Let’s run an example query on the cleaned dataset"
Ten more random useful things in R you may not know about - KDnuggets,"I had a feeling that R has developed as a language to such a degree that many of us are using it now in completely different ways. This means that there are likely to be numerous tricks, packages, functions, etc that each of us use, but that others are completely unaware of, and would find useful if they knew about them. As"
Ten more random useful things in R you may not know about - KDnuggets,"People say Python is much better for web scraping. That may be true. But for those of us who like working in the tidyverse, the"
Ten more random useful things in R you may not know about - KDnuggets,"It’s really easy to use. Simple edits to the YAML header of your document can invoke a specific style theme throughout the document, with numerous themes available. For example, this will invoke a lovely clean blue coloring and style across titles, tables, embedded code and graphics:"
P-values Explained By Data Scientist - KDnuggets,"We’ll disregard the possibility in the other direction since the consequences of having a mean delivery time lower or equal to 30 minutes are even more preferable. What we want to test here is to see if there is a chance that the mean delivery time is greater than 30 minutes. In other words, we want to see if the pizza place lied to us somehow"
P-values Explained By Data Scientist - KDnuggets,"Recall that we randomly sampled some pizza delivery times and the goal is to check if the mean delivery time is greater than 30 minutes. If the final evidence supports the claim by the pizza place (mean delivery time is 30 minutes or less), then we will not reject the null hypothesis. Otherwise, we’ll reject the null hypothesis"
P-values Explained By Data Scientist - KDnuggets,"The rule of thumb is to set alpha to be either 0.05 or 0.01 (again, the value depends on your problems at hand)"
P-values Explained By Data Scientist - KDnuggets,"As mentioned before, assume that we set the alpha to be 0.05 before we began the experiment, the result obtained is statistically significant since the p-value of 0.03 is lower than the alpha"
P-values Explained By Data Scientist - KDnuggets,"At the end of the day, the calculation of p-values is simple. The hard part comes when we want to interpret the p-values in our hypothesis testings. Hopefully the hard part now becomes at least slightly easier for you"
"Top 10 Best Podcasts on AI, Analytics, Data Science, Machine Learning - KDnuggets","We reviewed many more podcasts, so this list drills down to only those published on iTunes with the highest ratings (4.5+), the most reviews, and at least one recent episode within the current month. All show descriptions are adapted from the podcast listing"
"Top 10 Best Podcasts on AI, Analytics, Data Science, Machine Learning - KDnuggets",Bringing you the most inspiring Data Scientists and Analysts from around the world to help you build your successful career in Data Science. Data is growing exponentially and so are salaries of those who work in analytics. This podcast can help you learn how to skyrocket your analytics career
"Top 10 Best Podcasts on AI, Analytics, Data Science, Machine Learning - KDnuggets","You will learn what digital assistants are and where they're going with the help of people like Ray Kurzweil at Google. You will hear Kevin's philosophy on 'what gets measured gets managed' and what it means for marketing and data science. You will also learn why websites are less and less important, how segmentation is slowly transitioning to personalization, creating amazing customer experiences, disk profiles, natural language processing, and computer vision and their role in the future of marketing"
"Top 10 Best Podcasts on AI, Analytics, Data Science, Machine Learning - KDnuggets","Attend any conference, and you will hear people say that the most informative discussions happened in the bar after the show. Read any business magazine, and you will find an article saying something along the lines of ""Business Analytics is the hottest job category out there, and there is a significant lack of people, process and best practice. After a few pints and a few hours of discussion about the cutting edge of digital analytics, they realized they might have something to contribute back to the community. This podcast is one of those contributions. Each episode is a closed topic and an open forum - the goal is for listeners to enjoy listening to Michael, Tim, and Moe share their thoughts and experiences and hopefully take away something to try at work the next day"
"Top 10 Best Podcasts on AI, Analytics, Data Science, Machine Learning - KDnuggets","That kind of broad definition gets to the heart of the confusion surrounding the differences between business intelligence and artificial intelligence. The line is starting to get blurry. Our guest this week is Elif Tutuk, Senior Director at Qlik. Tutuk talks about how business intelligence is evolving and how we might define it now that a lot of BI is becoming AI. Tutuk discusses where AI is making its way into business intelligence and what that might enable for businesses"
"Top 10 Best Podcasts on AI, Analytics, Data Science, Machine Learning - KDnuggets","Discover the hidden side of everything with Stephen J. Dubner, co-author of the Freakonomics books. Each week, Freakonomics Radio tells you things you always thought you knew (but didn’t) and things you never thought you wanted to know (but do) — from the economics of sleep to how to become great at just about anything. Dubner speaks with Nobel laureates and provocateurs, intellectuals and entrepreneurs, and various other underachievers. Special features include series like “The Secret Life of a C.E.O"
Did you know cavemen were already dealing with “Big Data” issues? - KDnuggets,"For millennia our ancestors have been living as foragers hunting for animals and looking for sweet berries. At this point in history data was already crucial. If you always forgot that snakes are dangerous and red mushrooms are poisonous your chances of survival were slim. Luckily, the amount of important data was small so people could store everything they needed to know in their brains. However, when humans started living in bigger settlements and communities the limits of our brainpower became painfully obvious"
Did you know cavemen were already dealing with “Big Data” issues? - KDnuggets,"Living in these larger societies required an entirely different mental skillset. Many problems that arose were data related and could not be solved with the conventional technique of the day: memorization. For example, local rulers had to know how much grain they had in their inventories and whether it was enough to survive the winter. There were taxes to be collected from thousands of inhabitants and thus payment data that had to be stored. There were legal systems that had to handle property data and ever growing families that had to keep track of their family tree. There was just so much more data around than before that just learning everything by heart, like we had been doing for millennia, was never going to work out"
Did you know cavemen were already dealing with “Big Data” issues? - KDnuggets,"What we needed was a quantum leap. Thus our ancestors conceived a genius system roughly 5000 years ago that encoded their thoughts into abstract symbols, which in turn could be interpreted by others who were familiar with the rules of this system. This made storing and processing information outside early humans’ brains possible for the first time ever. Writing was born"
Did you know cavemen were already dealing with “Big Data” issues? - KDnuggets,"Passionate about data analytics, art and traveling the world. Currently writing his master’s thesis on data maturity in organizations and the role of the CDO. Blogging to fuel interesting discussions!"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Data scientists are highly educated — 88% have at least a Master’s degree and 46% have PhDs — and while there are notable exceptions, a very strong educational background is usually required to develop the depth of knowledge necessary to be a data scientist. To become a data scientist, you could earn a Bachelor’s degree in Computer science, Social sciences, Physical sciences, and Statistics. The most common fields of study are Mathematics and Statistics (32%), followed by Computer Science (19%) and Engineering (16%). A degree in any of these courses will give you the skills you need to process and analyze big data"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"After your degree programme, you are not done yet. The truth is, most data scientists have a Master’s degree or Ph.D and they also undertake online training to learn a special skill like how to use Hadoop or Big Data querying. Therefore, you can enroll for a master’s degree program in the field of Data science, Mathematics, Astrophysics or any other related field. The skills you have learned during your degree programme will enable you to easily transition to data science"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"In-depth knowledge of at least one of these analytical tools, for data science R is generally preferred. R is specifically designed for data science needs. You can use R to solve any problem you encounter in data science. In fact, 43 percent of data scientists are using R to solve statistical problems. However, R has a steep learning curve"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"It is difficult to learn especially if you already mastered a programming language. Nonetheless, there are great resources on the internet to get you started in R such as Simplilearn’s Data Science Training with R Programming Language. It is a great resource for aspiring data scientists"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Python is the most common coding language I typically see required in data science roles, along with Java, Perl, or C/C++. Python is a great programming language for data scientists. This is why 40 percent of respondents surveyed by O’Reilly use Python as their major programming language"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Because of its versatility, you can use Python for almost all the steps involved in data science processes. It can take various formats of data and you can easily import SQL tables into your code. It allows you to create datasets and you can literally find any type of dataset you need on Google"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Although this isn’t always a requirement, it is heavily preferred in many cases. Having experience with Hive or Pig is also a strong selling point. Familiarity with cloud tools such as Amazon S3 can also be beneficial. A study carried out by CrowdFlower on 3490 LinkedIn data science jobs ranked Apache Hadoop as the second most important skill for a data scientist with 49% rating"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"As a data scientist, you may encounter a situation where the volume of data you have exceeds the memory of your system or you need to send data to different servers, this is where Hadoop comes in. You can use Hadoop to quickly convey data to various points on a system. That’s not all. You can use Hadoop for data exploration, data filtration, data sampling and summarization"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Even though NoSQL and Hadoop have become a large component of data science, it is still expected that a candidate will be able to write and execute complex queries in SQL. SQL (structured query language) is a programming language that can help you to carry out operations like add, delete and extract data from a database. It can also help you to carry out analytical functions and transform database structures"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"You need to be proficient in SQL as a data scientist. This is because SQL is specifically designed to help you access, communicate and work on data. It gives you insights when you use it to query a database. It has concise commands that can help you to save time and lessen the amount of programming you need to perform difficult queries. Learning SQL will help you to better understand relational databases and boost your profile as a data scientist"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Apache Spark is becoming the most popular big data technology worldwide. It is a big data computation framework just like Hadoop. The only difference is that Spark is faster than Hadoop. This is because Hadoop reads and writes to disk, which makes it slower, but Spark caches its computations in memory"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Apache Spark is specifically designed for data science to help run its complicated algorithm faster. It helps in disseminating data processing when you are dealing with a big sea of data thereby, saving time. It also helps data scientist to handle complex unstructured data sets. You can use it on one machine or cluster of machines"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Apache spark makes it possible for data scientists to prevent loss of data in data science. The strength of Apache Spark lies in its speed and platform which makes it easy to carry out data science projects. With Apache spark, you can carry out analytics from data intake to distributing computing"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"A large number of data scientists are not proficient in machine learning areas and techniques. This includes neural networks, reinforcement learning, adversarial learning, etc. If you want to stand out from other data scientists, you need to know Machine learning techniques such as supervised machine learning, decision trees, logistic regression etc. These skills will help you to solve different data science problems that are based on predictions of major organizational outcomes"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,The business world produces a vast amount of data frequently. This data needs to be translated into a format that will be easy to comprehend. People naturally understand pictures in forms of charts and graphs more than raw data. An idiom says “A picture is worth a thousand words”
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"As a data scientist, you must be able to visualize data with the aid of data visualization tools such as ggplot, d3. These tools will help you to convert complex results from your projects to a format that will be easy to comprehend. The thing is, a lot of people do not understand serial correlation or p values. You need to show them visually what those terms represent in your results"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"It is critical that a data scientist be able to work with unstructured data. Unstructured data are undefined content that does not fit into database tables. Examples include videos, blog posts, customer reviews, social media posts, video feeds, audio etc. They are heavy texts lumped together. Sorting these type of data is difficult because they are not streamlined"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Most people referred to unstructured data as ‘dark analytics” because of its complexity. Working with unstructured data helps you to unravel insights that can be useful for decision making. As a data scientist, you must have the ability to understand and manipulate unstructured data from different platforms"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"Curiosity can be defined as the desire to acquire more knowledge. As a data scientist, you need to be able to ask questions about data because data scientists spend about 80 percent of their time discovering and preparing data. This is because data science field is a field that is evolving very fast and you have to learn more to keep up with the pace"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"You need to regularly update your knowledge by reading contents online and reading relevant books on trends in data science. Don’t be overwhelmed by the sheer amount of data that is flying around the internet, you have to be able to know how to make sense of it all. Curiosity is one of the skills you need to succeed as a data scientist. For example, initially, you may not see much insight in the data you have collected. Curiosity will enable you to sift through the data to find answers and more insights"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"As well as speaking the same language the company understands, you also need to communicate by using data storytelling. As a data scientist, you have to know how to create a storyline around the data to make it easy for anyone to understand. For instance, presenting a table of data is not as effective as sharing the insights from those data in a storytelling format. Using storytelling will help you to properly communicate your findings to your employers"
Top 13 Skills To Become a Rockstar Data Scientist - KDnuggets,"A data scientist cannot work alone. You will have to work with company executives to develop strategies, work product managers and designers to create better products, work with marketers to launch better-converting campaigns, work with client and server software developers to create data pipelines and improve workflow. You will literally have to work with everyone in the organization, including your customers"
Fantastic Four of Data Science Project Preparation - KDnuggets,"Reed Richards aka Mr. Fantastic can stretch, reshape, and contort his body in inhuman ways, and is the group leader. Susan Storm aka Invisible Woman is Reed Richards' wife who possesses the ability to become invisible. Johnny Storm aka Human Torch is Sue Storm's brother who has the mastery of fire, which allows him to engulf his body in flame, and also has the power of flight for some reason. Ben Grimm aka The Thing is Reed Richards' best friend who has been transformed by the cosmic rays into a rock-like monster with superhuman strengh"
Fantastic Four of Data Science Project Preparation - KDnuggets,"Context-dependent minimal levels of expertise aside, you also need to understand the questions being asked. This is separate from understanding the domain. You may be a"
Fantastic Four of Data Science Project Preparation - KDnuggets,"You know the domain. You know the questions being asked. You have a handle on what is in the data, and how that maps to the ability to answer the questions you want answered"
Fantastic Four of Data Science Project Preparation - KDnuggets,"It's not about any single or particular framework; it's about ensuring that you have some reasonable procedural approach in mind which is standardized, reliable, and measurable. Well known examples of such formal frameworks include Knowledge Discovery in Databases (KDD) Process, the cross-industry standard process for data mining (CRISP-DM), and Joe Blitzstein and Hanspeter Pfister's Data Science Process. You can read more about these approaches"
Fantastic Four of Data Science Project Preparation - KDnuggets,"So, which framework should you use? Are there really any important differences? [.] Does this simplified framework provide any real benefit? As long as the bases are covered, and the tasks which explicitly exist in the overlap of the frameworks are tended to, the outcome of following [any of the] models would equal that of [any] other. Your vantage point or level of experience may exhibit a preference for one"
Fantastic Four of Data Science Project Preparation - KDnuggets,"So there is our Fantastic Four of Data Science Project Preparation. Our version may not do battle with the likes of super villains such as Dr. Doom or Galactus, but they play a crucial role in the success of our analytics projects. Hopefully the value of preparation has been reinforced"
"Easy, One-Click Jupyter Notebooks - KDnuggets","All of that can take a load of time, especially steps 1 and 2. Many Data Scientists never even see these things happening — they only see the final product. But it is a real challenge to setup all that infrastructure, some of it requiring constant updates"
"Easy, One-Click Jupyter Notebooks - KDnuggets","I already created my notebook in the video below. Once your notebook is created, you can start coding! I’ve prepared some code ahead of time for plotting the Iris flowers dataset. Saturn Cloud is able to run everything smoothly and seamlessly"
Is Bias in Machine Learning all Bad? - KDnuggets,"Learning involves the ability to generalize from past experience in order to deal with new situations that are ”related to” this experience. The inductive leap needed to deal with new situations seems to be possible only under certain biases for choosing one generalization of the situation over another. This paper defines precisely the notion of bias in generalization problems, then shows that biases are necessary for the inductive leap"
Is SQL needed to be a data scientist? - KDnuggets,"The short answer is yes. As long as there is ‘data’ in data scientist, Structured Query Language (or see-quel as we call it) will remain an important part of it. In this blog, let us explore data science and its relationship with SQL, including answers to the 5 Ws and 1 H – how, why, where, when, who and what. We will also learn the basics of Database Management Systems (DBMS) and understand how being a data scientist could be the best choice for your career"
Is SQL needed to be a data scientist? - KDnuggets,"The digital world is at its peak, and with growing demands and extensive marketing strategies data has become the key to all marketing purposes. For example, if I want to buy a new phone, I go to online shops like Amazon or Flipkart, browse through different brands, put a few in my cart, but decide to buy it later after some more research. Internally, the online shop would save my shopping cart and browsing history and show me suggestions for more phones when I come back later. Even if I don’t buy, the company would send me emails to remind me that my shopping cart is ""still waiting for me. The more data customer shells out, the more customized a stream is presented to the buyer. This is not just true for e-commerce, but data science is proving extremely useful in many other domains like healthcare, manufacturing, banking, finance and transport"
Is SQL needed to be a data scientist? - KDnuggets,"You buy the product and leave. Later you realize you want more of the same product and come back. You tell your friends how useful and inexpensive the product is and they are convinced to buy it too. Manufacturers use this data to understand the likes of customers and update their inventory to have more of the products that are popular. Further, constant feedback helps them bring improvements to their existing product"
Is SQL needed to be a data scientist? - KDnuggets,"Yes, you got it – everything is stored in a database. SQL is thus vital for handling humongous amounts of data that need to be processed on a regular basis. It also acts as an important tool for the right marketing and feedback that data science intends to do. For example, if you don’t like a video that Facebook is suggesting you – you would say ‘hide this’ and Facebook will immediately ask you for a reason. These user preferences also need to be stored somewhere"
Is SQL needed to be a data scientist? - KDnuggets,"There are more developers who understand SQL technology and hence the support and documentation are more plentiful. Further, data integrity is one key factor that makes SQL stand apart from any NoSQL database, by way of the assurance that no duplicates or unauthorized data can enter the system. Also, for complex queries and joins, a well-structured relational database works better"
Is SQL needed to be a data scientist? - KDnuggets,"In a relational database model, all the data points are related or connected to each other. While creating this kind of database, the relationships between various tables and columns has to be defined in the design stage itself. In our above example, the three tables are related. The customer table’s"
Is SQL needed to be a data scientist? - KDnuggets,"Loads of data is generated every day and needs to be converted into new business solutions, designs and products which can only come from the creative mind of a data scientist. This need will only increase by the day at least for a few decades. In addition to the fat package that the industry offers to a data scientist, it is the challenge and ever-growing roles that attract professionals towards this job. From data administrator, data architect, data analyst, business analyst to a data manager or business intelligence manager, there are plenty of opportunities to choose within the data science circle. Knowledge of SQL, programming languages like R and Python, statistics and applied math, paired with critical thinking and industry knowledge will get you there sooner than you would think"
Is SQL needed to be a data scientist? - KDnuggets,"After working for a decade in Infosys and Sapient, he started his first startup, Leno, to solve a hyperlocal book-sharing problem. He is interested in product marketing, and analytics. His latest venture"
Rethinking Mentoring In Data Science - KDnuggets,"Looking back at the numerous companies where I worked, I was fortunate that at times, particularly earlier in my career, I had inspiring managers or colleagues who were knowledgeable, patient and generous with their time. But, for the majority of organizations, my managers were generally unavailable, uncommunicative and indifferent about offering opportunities or encouraging career growth. I think it is almost expected that one should leave the company for better opportunities. I think that could be due in large part to being in the field of statistics"
Rethinking Mentoring In Data Science - KDnuggets,"Traditionally, corporate America assigns managerial and leadership positions to those who have advanced degrees (preferably Ph.D. Rarely are they assessed on their management, leadership or communication expertise. And yet, it is these skills that make a formidable and impactful leader. Companies do not consider allocating these positions to managers who have the requisite “people skills"
Rethinking Mentoring In Data Science - KDnuggets,"You will still have to do the hard work. Give up the idea of a “Prince Charming” who will step in, solve all problems and offer the perfect career opportunity. It is a fantasy, not reality"
Rethinking Mentoring In Data Science - KDnuggets,"Role models seem out of reach if you only see them at their peak. To make them relatable, rewind the clock. What looked like natural talent is revealed as hard-earned skill. Study the trials and tribulations that tested—and shaped—their character"
Rethinking Mentoring In Data Science - KDnuggets,"To share, I have never asked anyone to be my mentor. I have no official mentor. And yet, I feel abundandance with the resources available to me"
Rethinking Mentoring In Data Science - KDnuggets,"Yet, I observed how he involved the community in contributing to fastai open source by his video lectures. Last fall, when I was feeling overwhelmed with the WiMLDS meetup group as all 3 of my co-organizers were on maternity leave, it occurred to me that I could reach out to the WiMLDS community to get assistance for the events. I learned from Jeremy without ever meeting him or speaking with him"
Rethinking Mentoring In Data Science - KDnuggets,"A mentor does not have to be in data science. One of my key confidants is a college friend who works in government, hundreds of miles away from me, doing non-quantitative work. I often share workplace dilemnas with her via phone calls and her perspectives are insightful and useful"
Rethinking Mentoring In Data Science - KDnuggets,"You can learn quite a bit from observation. When you observe someone, whether on stage, in conversation, on video, in writing, observe what is appealing about that person and emulate it. Look at how they conduct themselves. Ask yourself:"
Rethinking Mentoring In Data Science - KDnuggets,"There has been discussion in data science community organizations about offering mentorship programs. However, all of these organizations are operated primarily by volunteers who are already overworked and in high demand. They include meetup organizers, conference organizers, open source contributors, educators and more. It is recognized that there is a demand in the data science community for mentorship. But, the problem with open source is that it is not really free. They are products and services that are primarily created and maintained by volunteers. There is a challenge in obtaining funding by corporations and donations by users. So, if a Mentorship Program is offered, for free, someone is dedicating hours of their personal time for that"
Rethinking Mentoring In Data Science - KDnuggets,"You’ve got to be a bit careful, right? Because sometimes I get messages from random people saying “I’ve got lots of good ideas, can we have coffee?” I don’t want to… you know, I can have coffee in my office anytime, thank you. But it’s very different to say, hey, I took your ideas and I wrote a paper and I did a bunch of experiments and I figured out how your code works. They added documentation to it. Should we submit this to a conference? You see what I mean? There is nothing to stop you doing amazing work and if you do amazing work that helps someone else, like in this case, ok, I’m happy that we have a paper. I don’t particularly care about papers. But, I think it’s cool that these ideas now have this rigorous study. Let me show you what he did"
Rethinking Mentoring In Data Science - KDnuggets,"I had another mentor before, but both of us did not really know what to do. The first session was great but then we were not able to maintain a contact, because I did not have specific aims for that particular interaction. Also, many people are ok with being mentors, but have no clue how to do it"
Rethinking Mentoring In Data Science - KDnuggets,"Hi salaried people. Please don't get annoyed when a self-employed person sends you their rates in response to your invite to a 3.5 hour meeting where you want to 'share ideas'. That's called consulting, and if I do it for free, I can't pay my rent"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,Hackathon held by Junction Ltd. Our team made an iOS application that brings an optimized shopping service experience for Korean beauty product powered by image search engines. This application can help travelers who want to explore Korean beauty products but not knowing what to try due to an obvious language barrier. Users can find useful data about a product just by scanning it and the information is translated into their local languages. There is also a searching history page which could be utilized for further customized service. You can find more about it
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"Rather than talking about what I made, however, I’d like to share what I learned in the process. So today, I’m going to talk about the 4 reasons why you should apply for a place in a Hackathon (especially if you’re an aspiring data scientist), how you should prepare and some of my tips. I hope this article can motivate you to go out of your comfort zone and get some profound inspiration just as I did"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"We, data science learners, tend to work alone or study alone. We tend to invest most of our time in data preprocessing and wangling. And we pour most of our energy into studying machine learning or deep learning algorithms. Our project usually starts with importing data (or sometimes with building data by ourselves) and ends with the evaluation of the predictions"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"Almost every real-world project in the world sadly doesn’t work that way. We don’t work alone. We work with other team members. From database to deployment and product management, all the processes should be done through collaboration. Having toolkits beyond the “modeling” territory can be a great plus for aspirants"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"A hackathon can be a great chance to collaborate with others and make a real-world project. Although there are some limitations, still you can learn how to communicate with others (who do not have much of the knowledge in machine learning as you do) and understand the overall workflow for releasing an actual product. And if you get a chance of being on the team with someone who knows a lot better than yourself in machine learning, I believe it’ll be such a great time to push your limits as well"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"The first step of the project will be deciding what to build. And I can tell half of the winning chance is determined here. You should get off on the right foot. Implementation is also a critical point but the business value plays an essential role. If it’s neglected, your work can be nothing but just showing off how technically competent you are"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"You’d have seen many people pointing out the importance of business mindset already. Finding a hidden need of users and observing a problem in the market. Suggesting a solution with your technical knowledge. Developing a service that can become profitable. At a hackathon, you are asked to think and discuss deeply with this viewpoint"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"I heard of a guy who had won programming competitions more than 15 times. He retired in his ‘prize hunting’ career after he won one of Google’s competitions. Do you know what his secret was? It was just business sense and a little bit of wit. By making a clever and creative product, he made people shout “Brilliant!” which definitely leaves an impression on panels. So if you’re looking for an additional tip for winning, having business values with some wit can be a great bonus"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"The goal of a hackathon is creating a functional product during a short period (commonly 2 or 3 days). You are asked to present a prototype or demo of your product during these days. It’s all about limited time, limited resources and limited energy problems. Sometimes, you could have situations where things go wrong and you’d have to instantly find solutions. Sometimes you have to prioritize your work, which means what to come first and when to stop. You could feel intense pressure to do all your tasks and not to be a burden on the team. And as time passes, becoming completely famished and exhausted is literally inevitable! (More like energy drain or depletion until finally “falling” asleep)"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"If you’re someone like me, who tend to freeze at an extremely stressful moment, you’d be overwhelmed even by the air at the competition hall. It was my first time in a Hackathon so I experienced my brain being paralyzed several times. I had to get some fresh air and relax to ease off the tension. But this also became a great chance for me to learn how to dealing with stress. Because it’s unavoidable meeting those unexpected problems in the workplace and we have to manage those demanding moments"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"Meeting new people is also a great pleasure, especially if they are like-minded people. At a hackathon, you can meet various people from different regions and with different specialties but with the same interest. They can be web developers, app developers or designers but they are people who’re willing to invest their weekend in developing something cool"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"I met people who came from Russia to Korea just for this event. I also met a Ukrainian lady studying Computer Science in Korea. There were people from Denmark, French, China, and Japan. Meeting new people itself is a great joy. But more than that, I could hear what kind of pain points they have and what they are really passionate about. My horizon could become broadened in programming by chatting with fellow participants. And this is a reason why I think you should try a Hackathon with generous topics rather than heavily focused on the data science fields"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"Also, it’d be good to care about what others do as well. Although you’ll be too busy to take extra care of others, this will help you understand the whole process as I mentioned above. We’re here to learn new things, anyway!"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"So now does a Hackathon sound worth the try? If so, then your next question could be how you should prepare. Actually, nothing. Just go for it. Whatever skillsets you have, don’t hesitate to apply for. Hackathons are not a stage only for professionals. But if you’d like to prepare something before the day, then I’d like to list some of the skills I find useful and applicable"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"This is not a project that starts with importing the data and ends with some evaluation plots. You need to collect the data and build a database according to your team project. After you finish with data analysis or modeling part, you have to pass your model to frontend. Or you might need to deploy your work on a web by yourself. Therefore having database and Flask in your toolkits can be a huge asset for this event"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"Conditions could be different for each hackathon, but the one I took part in highly depended on API and Cloud Services. There were several sub tracks and the participants were asked to use particular API to participate in a given track. But besides this condition, you’d need to collect data by yourself anyway. Where will you get the data? And what about the computing power for training? You’d have probably left your “super-computer” with GPUs at home"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"Get enough sleep. Take control of your condition. This is not a joke. During the competition, I slept for 4 hours in total. Sleeping less can be a good strategy but you need to be in good condition when you enter the hall. Also, a neck pillow and a blanket for a catnap can be as much useful item as “Flask” or “AWS"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"If you’re a solo applicant, you have to build your team from the bottom. There can be a social network platform like Slack where you can get the information from other applicants and make a team before the event. You might get a chance to join a premade team with one opening"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"If you have an idea on what you want to develop, divide it into parts and look for crew members who can take on each part. You need to find someone who has the skill different from us (like data modeling). The common list of members you’d need to have is a backend engineer, a web/app developer, and a UI/UX designer"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"You might think your ability is not enough for doing this. So you might want to hide your level or pretend knowing more than you do. But you really don’t have to. Hackathons are open to everyone. And no matter how much you know about programming, there is a portion that can be filled only by you! Is the person next to you know much more than you do? Awesome! You get more chances to learn. Asking for help is a better way to “help” your team than struggling alone and wasting precious time"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"It’s more than winning. Although you should do your best during the time, don’t lose a smile on your face. Enjoy the time for learning, meeting new people and working with them. Enjoy the time for being creative and developing a new thing that doesn’t exist in the world. Enjoy the time for being stressed and going beyond your limits. It will be extremely demanding. At the end of the event, however, you’ll go back to your home with new thoughts and lessons on your mind even if you don’t win a prize. And you’d find yourself looking forward to the next Hackathon like me"
The Hackathon Guide for Aspiring Data Scientists - KDnuggets,"Did this story resonate with you? Please share your insight with us. I’m always open to talk, so feel free to leave comments below and share your thoughts. I also share interesting and useful resources on"
Top 10 Data Science Leaders You Should Follow - KDnuggets,I failed forward. Picked myself up. And moved forward again
"Are We Ready to Partner With Machines? Data Science Salon Miami, September 10-11 - KDnuggets","When it comes to AI, there’s plenty of talk of the future of machines. But it’s the people behind AI development who have the insights needed to shape that future. Register now to catch all of our speakers at the Data Science Salon Miami, Sep 10-11, 2019"
"Are We Ready to Partner With Machines? Data Science Salon Miami, September 10-11 - KDnuggets","When it comes to AI, there’s plenty of talk of the future of machines. But it’s the people behind AI development who have the insights needed to shape that future. We asked a DSS Miami panel of speakers to weigh in on business applications of AI and data science for optimal efficiency and growth"
"Are We Ready to Partner With Machines? Data Science Salon Miami, September 10-11 - KDnuggets","However, AI itself, in the form of predictive analytics, can also enhance businesses’ abilities to make customer experiences mode personalized. Building on this fact, Associate Data Scientist in Customer Care Strategy & Analytics at Florida Light and Power explained, “Data science helps us be more precise in our planning and execution. We’re able to better forecast our call volumes, predict when people will call and why they are calling"
"Are We Ready to Partner With Machines? Data Science Salon Miami, September 10-11 - KDnuggets","As AI’s influence on customer-centric systems grows, issues of ethics and privacy will become increasingly prevalent. Nathan Black, Co-founder and Chief Data Scientist at QuantHub, claimed that the rise of “citizen data scientists, employees and business managers who apply data science methodologies in non-data science roles,” will make the use of AI commonplace across company departments. This may amplify the growth of which AI is capable of engineering, while also increasing the risks of technology misuse. Mediating these risks will require more and more diligence from data science teams"
How to Showcase the Impact of Your Data Science Work - KDnuggets,It could also be useful to depend on analogies. Relate your data science work to examples that virtually everyone can understand regardless of their backgrounds. One professional needed to convince a team of people without data expertise about the worthiness of building a centralized metadata repository. She ultimately used Andy Warhol's famous soup can pop art to
How to Showcase the Impact of Your Data Science Work - KDnuggets,"She asked them to imagine soup as the data, the can as the database, and the label as metadata. Then, they envisioned how hard it would be if they had a pantry full of soup cans without labels. She drove her message home by relating that scenario to how the lack of a metadata repository created unnecessary confusion for end users and got the understanding she wanted"
What’s wrong with the approach to Data Science? - KDnuggets,"The job ‘Data Scientist’ has been around for decades, it was just not called “Data Scientist”. Statisticians have used their knowledge and skills using machine learning techniques such as Logistic Regression and Random Forest for prediction and insights for decades. Those same statisticians were also likely very knowledgeable in Linear Algebra and Calculus. Statisticians were even responsible for one of the greatest gifts to Data Science —"
What’s wrong with the approach to Data Science? - KDnuggets,"While a university education would be adequate in most of the cases, a lot of people are opting for MOOC’s to support their career path into Data Science. Coursera, edX, Data Camp and Data Quest are some of the most famous MOOC’s that people turn to and trust. The problem here is that the people taking these courses might not be qualified in the basics of statistics to at least understand what is being taught"
What’s wrong with the approach to Data Science? - KDnuggets,"Some people are forgetting that Data Science or whatever the name it gets is based on years of hard work, learning and passion to do the job. The statisticians of past were not glamoured like “Data Scientists” are today. The profession is at risk of being cheapened to a job that someone could get started with by paying less than $100 online. I am all for more access to education and online learning but the profession should not be cheapened"
What’s wrong with the approach to Data Science? - KDnuggets,"Handling outliers, missing values, encoding categorical variables, binning, type conversions, incorrect spelling duplicate rows, class imbalances etc. According to media and current data scientists, data preprocessing takes up to 80% of a data scientist's job. While automation can help to a certain extent, the inference from Exploratory Data Analysis should be made by a person with industry knowledge or someone with an adequate grasp of statistics"
What’s wrong with the approach to Data Science? - KDnuggets,"Machine learning packages come with models that have default hyperparameters set for it to be trained and tested. One can simply change the arguments to tune the model. However, just changing arguments without exploring the result of the changes is a waste of time. Changing a hyperparameter could lead to a model being over-fitted, under-fitted, biased etc. Different models will have different ways of being explored as well. For example when tuning a decision tree, it would be best to plot the tree to see the results of tuning because by changing the complexity parameter, you may prune the tree to an extent that you underfit/overfit your model"
What’s wrong with the approach to Data Science? - KDnuggets,This is a problem with all real world data. Class imbalances are not an abnormality but actually something that needs to be accommodated/accounted for in the modelling and evaluation stage. Either they need to resample the data or change the threshold probability to predict each class (i
What’s wrong with the approach to Data Science? - KDnuggets,"While there are many people that do make the transition successfully, many do not see transitioning into a doctor, engineer or lawyer so willfully. The profession of data science should not be diluted to a point where we say anyone can be a data scientist. Everyone should have the opportunity to join any profession of their choice but to say anyone can do it just dilutes the profession into something that would not require the amount of work an undergraduate, post graduate or doctoral candidate would put into becoming a Data Scientist"
DSGO19 Announces Speakers and Training Sessions - KDnuggets,"SAN DIEGO, CA, SEPTEMBER 27-29: DataScienceGO is now announcing its full lineup of expert speakers. DSGO19 is the only conference dedicated to career advancement for data science managers, practitioners and beginners. Three days of immersive talks, panels, and training sessions are designed to teach, inspire, and guide you. DataScienceGO 2019 mainstage speakers are from Intuit, Salesforce, New York Times, Amazon Alexa, Google, Facebook and many more"
DSGO19 Announces Speakers and Training Sessions - KDnuggets,"Multiple conference tracks focus on topics that matter. Newcomers can expect a data science overview with a guide to growing skills and landing a top-notch job. Current practitioners can expect technical talks on topics like Natural Language Processing & Deep Learning, Building Recommendation Engines, and Object Detection. Tracks dedicated to current business use cases will interest data science managers and executives. Use DSGO’s exclusive executive mentorship exercise to find mentors whose expertise can fuel your future ideas"
DSGO19 Announces Speakers and Training Sessions - KDnuggets,"Get plugged into the DataScienceGO community via targeted networking with 700+ attendees. Past attendees have walked away from DSGO with 300+ new connections on LinkedIn. At DSGO, you’ll have the opportunity to connect with executive mentors to guide your path forward. DSGO’s exclusive Executive Mentorship exercise is designed to put you in front of top data decision makers at companies like Levi Strauss, CoreLogic, and Hyperloop. Expand your network instantly and take advantage of the collective knowledge of your community"
DSGO19 Announces Speakers and Training Sessions - KDnuggets,Attending DataScienceGO 2019 will put you in the same room as data science thought leaders and decision makers. This is your chance to elevate your career with a laser focus on both technical and soft skills. Learn the strategies that work!
How Data Science Is Used Within the Film Industry - KDnuggets,"Streaming services are at the forefront of the data science revolution. Production companies, including Amazon, Hulu, and Netflix, analyze patterns in big data to determine the types of content they create and make personalized viewing recommendations. In this way,"
How Data Science Is Used Within the Film Industry - KDnuggets,"The top five actual movies were all in the predicted list: X-Men: Apocalypse; John Wick: Chapter 2; Doctor Strange; Batman v. Superman: Dawn of Justice; and Suicide Squad. Generally, the audience was looking for a superhero movie that featured a “rugged male action lead"
How Data Science Is Used Within the Film Industry - KDnuggets,"When big data first hit the scene around 2010, it effectively changed the methods used to turn data analytics into useful insight and profit. Big data is often externally sourced, using information drawn from the internet, public data sources, and more to make more accurate predictions. In the entertainment industry, big data can be used to provide a personalized user experience and reduce churn rates among streaming site audiences"
How Data Science Is Used Within the Film Industry - KDnuggets,"Among streaming services, the user interface plays an important role in viewer retention. If viewer recommendations are inaccurate, for example, it could lead that viewer to turn to other platforms for entertainment. Streaming services are well aware of the importance of a positive user experience"
How Data Science Is Used Within the Film Industry - KDnuggets,"The streaming giant adjusts the audio and visual quality of the media to optimize the experience. They also use predictive caching to allow a video to play faster or at a higher quality. For example, if a viewer is watching a series, the next episode will be partially cached"
How Data Science Is Used Within the Film Industry - KDnuggets,"Buzz is notable in the sense that information on the phenomenon can be gained from numerous sources, such as social media and critical reviews. The buzz surrounding a film is only a small piece of the larger analytical picture, however. Data analytics must be used at every life cycle stage of the movie, from development to post-production and distribution"
A Data Science Playbook for explainable ML/xAI - KDnuggets,"Model ethics, interpretability, and trust will be seminal issues in data science in the coming decade. This technical webinar discusses traditional and modern approaches for interpreting black box models. Additionally, we will review cutting edge research coming out of UCSF, CMU, and industry. This new research reveals holes in traditional approaches like SHAP and LIME when applied to some deep net architectures and introduces a new approach to explainable ML/xAI where interpretability is a hyperparameter in the model building phase rather than a post-modeling exercise. We will provide step-by-step guides that practitioners can use in their work to navigate this interesting space"
A Data Science Playbook for explainable ML/xAI - KDnuggets,We will review code examples of interpretability techniques. You can follow along with the presentation by running your own notebook hosted in Domino's trial environment. Create a free trial account
How do you check the quality of your regression model in Python? - KDnuggets,"But there is a piece of bad news. We can never know the true errors, no matter how much data we have. We can only estimate and draw inference about the distribution from which the data is generated"
How do you check the quality of your regression model in Python? - KDnuggets,"The main model fitting is done using the statsmodels.OLS method. It is an amazing linear model fit utility which feels very much like the powerful ‘lm’ function in R. Best of all, it accepts R-style formula for constructing the full or partial model (i"
How do you check the quality of your regression model in Python? - KDnuggets,Cook’s distance essentially measures the effect of deleting a given observation. Points with a large Cook’s distance need to be closely examined for being potential outliers. We can plot the Cook’s distance using a special
Why do we need AWS SageMaker? - KDnuggets,"Today, there are several platforms available in the industry that aid software developers, data scientists as well as a layman in developing and deploying machine learning models within no time. Some platforms have made big claims to democratize AI and data science to an extent where you can solve your data science problem with almost no coding skills, while some others claim the fastest and easiest way to deploy a solution in the cloud. All these claims, some very profound, would make you wonder one simple question —"
Why do we need AWS SageMaker? - KDnuggets,"The answer to this question is definitely not a straightforward one. Based on your priority, flexibility, scale, cost, preferred ML framework, ease of use, and other aspects, there isn’t just one size fits for all choice that you get. For a meaningful solution, we have to really start by understanding WHY do we need a platform in the first place. Once we understand the ‘"
Why do we need AWS SageMaker? - KDnuggets,"This entire process is highly iterative and changes can be expected to loop back the progress to any state in the entire process (as shown in the diagram). In most of the Proof of Concepts (PoC) we experiment and the available study resources over the internet (blogs/tutorials/books/courses), this entire process is highly simplified. The biggest abstraction rendered in this process is in the dimension of ‘"
Why do we need AWS SageMaker? - KDnuggets,"Consider a fairly complex ML problem we would deal in an enterprise. Say, you have a very large dataset with millions of records (~10GB — 1TB) to process and would need at least a few hundred iterations (say, for a deep neural network) and around 100,000 API calls per minute once the service is deployed. Then, the vanilla PoCs we developed during experimentation is no longer a comparison for the use-case. Each group (Build, Train, Deploy) would need a different kind of compute infrastructure, a different approach to execute the problem and a distinct set of disciplines to accomplish. Fig 2 (below), helps in understanding how each of these groups within the workflow, fork out as an independent branch with diverse requirements of skills and infrastructure to execute"
Why do we need AWS SageMaker? - KDnuggets,"Data scientists are not software engineers and similarly, software engineers are not data scientists. It is indeed a mammoth task for a data scientist to deploy a machine learning solution (that he researched and prototyped) into a full-scale web service (an API that can be integrated into a software ecosystem). Data scientists lack the required software skills to take a research prototype and offline machine learning model into a large complex model (service) that can inference thousands of API calls in real-time"
Why do we need AWS SageMaker? - KDnuggets,"On the other hand, software engineers are proficient in this task and can easily have this done for a large software system. However, the lack of understanding of machine learning and math skills brings in several impediments for the implementation. Developing a full-scale ML service does require the developer to have a deeper understanding of the process which in most cases is quite overwhelming"
Why do we need AWS SageMaker? - KDnuggets,"Most organization follow the same practice of hiring data scientists to research, experiment, prototype and develop a PoC that can validate an ML business use-case. Later, hire a software development team to take the PoC into a full-blown and scalable web-service or software product as per the required business standards. However, the collaboration between software engineers and data scientists are again a not so easy one to accomplish. It is indeed far too overwhelming for each group to understand the other group’s requirements, tech-language, and ideas"
Why do we need AWS SageMaker? - KDnuggets,"The most effective way to solve large ML problems is by aiding a data scientist with the necessary software skills in neat abstract yet effective way to deliver an ML solution as a highly scalable web-service (API). The software development team can integrate the API into the required software systems and abstract the ML service as just another service wrapped around an API. Therefore, we need a platform that can enable a data scientist with the necessary tools to independently execute a machine learning project in a truly end-to-end way"
Why do we need AWS SageMaker? - KDnuggets,"It provides Jupyter NoteBooks running R/Python kernels with a compute instance that we can choose as per our data engineering requirements on demand. We can visualize, process, clean and transform the data into our required forms using the traditional methods we use (say Pandas + Matplotlib or R +ggplot2 or other popular combinations). Post data engineering, we can train the models using a different compute instance based on the model’s compute demand, say memory optimized or GPU enabled. Leverage a smart default high-performance hyperparameter tuning settings for a variety of models. Leverage performance-optimized algorithms from the rich AWS library or bring our own algorithms through industry standard containers. Also, deploy the trained model as an API, again using a different compute instance appropriate to meet business requirements and scale elastically. And the entire process of provisioning hardware instances, running high capacity data jobs, orchestrating the entire flow with simple commands while abstracting the mammoth complexities and finally enabling serverless elastic deployment works with a few lines of code and yet is cost effective. Sagemaker is a game-changing solution for the enterprise"
Why do we need AWS SageMaker? - KDnuggets,"The problems we discussed above though represent a vast majority of ML projects, there are certainly few scenarios where this might not be a valid option. Sagemaker, expects you to code your data engineering and analysis needs(in your choice of language). The entire orchestration of infrastructure and transition from one phase to another though happens with few lines code; definitely needs a small amount of code. For professionals, who prefer a drag and drop (absolutely no code) solutions for the data engineering and modeling phase, Azure ML studio would be a more ideal choice. Both Azure ML Studio and AWS Sagemaker are great platforms for developing ML solutions but are targetted for a completely different set of users. This"
Why do we need AWS SageMaker? - KDnuggets,"AWS Sagemaker has been a great deal for most data scientists who would want to accomplish a truly end-to-end ML solution. It takes care of abstracting a ton of software development skills necessary to accomplish the task while still being highly effective and flexible and cost-effective. Most importantly, it helps you focus on the core ML experiments and supplements the remainder necessary skills with easy abstracted tools similar to our existing workflow"
The Data Fabric for Machine Learning – Part 2: Building a Knowledge-Graph - KDnuggets,"We want the facts — where those facts come from is less important. The data here can represent concepts, objects, things, people and actually whatever you have in mind. The graph fills in the relationships, the connections between the concepts"
The Data Fabric for Machine Learning – Part 2: Building a Knowledge-Graph - KDnuggets,We are in a different here. A one where it’s possible to set up a framework to study data and its relation to other data. In a knowledge-graph information represented in a particular formal
The Data Fabric for Machine Learning – Part 2: Building a Knowledge-Graph - KDnuggets,Google is a basically a huge knowledge (with more additions) graph and they created maybe the biggest data fabric there is upon that. Google has billions of facts that includes information about and relationships between millions of objects. And allow us to search through their system to discover insights inside it
The Data Fabric for Machine Learning – Part 2: Building a Knowledge-Graph - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Here’s how you can accelerate your Data Science on GPU - KDnuggets,"When installing, you can set your system specs such as CUDA version and which libraries you would like to install. For example, I have CUDA 10.0 and wanted to install all the libraries, so my install command was:"
Here’s how you can accelerate your Data Science on GPU - KDnuggets,"For those 100, 000 points, the run time was 8.31 seconds. The resulting plot is shown below"
Here’s how you can accelerate your Data Science on GPU - KDnuggets,"The GPU version has a run time of 4.22 seconds — almost a 2X speedup. The resulting plot is the exact same as the CPU version too, since we are using the same algorithm"
Here’s how you can accelerate your Data Science on GPU - KDnuggets,The amount of speedup we get from Rapids depends on how much data we are processing. A good rule of thumb is that larger datasets will benefit from GPU acceleration. There is some overhead time associated with transferring data between the CPU and GPU — that overhead time becomes more “worth it” with larger datasets
Here’s how you can accelerate your Data Science on GPU - KDnuggets,"The amount of rises quite drastically when using the GPU instead of CPU. Even at 10,000 points (far left) we still get a speedup of 4.54X. On the higher end of things, with 10,000,000 points we get a speedup of 88.04X when switching to GPU!"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","One of the best ways to measure the popularity or market share of software for data science is to count the number of job advertisements that highlight knowledge of each as a requirement. Job ads are rich in information and are backed by money, so they are perhaps the best measure of how popular each software is now. Plots of change in job demand give us a good idea of what is likely to become more popular in the future"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","Indeed.S. As their co-founder and former CEO Paul Forster stated, Indeed. It used to have a job trend plotter, but that tool has apparently been shut down"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","Searching for jobs using Indeed. Some software is used only for data science (e. General-purpose languages (e. To level the playing field, I developed a protocol to focus the search for each software within only jobs for data scientists. The details of this protocol are described in a separate article,"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","I collected the job counts discussed in this section on May 27, 2019 and February 24, 2017. One might think that a sample from a single day might not be very stable, but the large number of job sources makes the counts in Indeed. Data collected in 2017 and 2014 using the same protocol correlated r=.94, p=.002"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","Figure 1a shows that Python is in the lead with 27,374 jobs, followed by SQL with  25,877. Java and Amazon’s Machine Learning (ML) tools are roughly 25% further below, with jobs in the 17,000s. R and the C variants come next with around 13,000. People frequently compare R and Python, but when it comes to getting a data science job, there are only half as many for R as for Python. That doesn’t mean they’re the same sort of job, of course. I still see more statisticians using R and machine learning people preferring Python, but Python is definitely on a roll! From Hadoop on down, there is a slow decline in jobs. R is also frequently compared to SAS, which has only 8,123 compared to R’s 13,800"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","To let us compare the less popular software, I plotted them separately in Figure 1b. Mathematica and Julia are the leaders of this set, with around 219 jobs each. The ancient FORTRAN language is still hanging on to life with 195 jobs. The open source WEKA software and IBM’s Watson are next, with around 185 each. From XGBOOST on down, there is a fairly steady slow decline"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","There are several tools that use a workflow interface: Enterprise Miner, KNIME, RapidMiner, and SPSS Modeler. They’re all around the same area between 50 and 100 jobs. In many of the other measures of popularity, RapidMiner beats the very similar KNIME tool, but here there are 50% more jobs for the latter. Alteryx is also a workflow-based tool, however, it has pulled away from the pack, appearing back on Figure 1a with 901 jobs"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","It’s important to note that the values shown in Figures 1a and 1b are single points in time. The number of jobs for the more popular software do not change much from day-to-day. Therefore, the relative rankings of the software shown in Figure 1a is unlikely to change much over the coming year or two. The less popular packages shown in Figure 1b have such low job counts that their ranking is more likely to shift from month-to-month, though their position relative to the major packages should remain more stable"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","Next, let’s look at the change in jobs from the 2017 data to now (2019). Figure 1c shows the percent change for those packages that had at least 100 job listings back in 2017. Without such a limitation, software that goes from 1 job in 2017 to 5 jobs in 2019 would have a 500% increase, but still would be of little interest. Software whose job market is heating up, or growing, is shown in red, while those that are cooling down are shown in blue"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets","Tensorflow, the deep learning software from Google, is the fastest growing at 523%. Next is Apache Flink, a tool that analyzes streaming data, at 289%. H2O is next, with 150% growth. Caffe is another deep learning framework and its 123% growth reflects the popularity of artificial intelligence algorithms"
"Data Science Jobs Report 2019: Python Way Up, TensorFlow Growing Rapidly, R Use Double SAS - KDnuggets",Robert A. Muenchen is an ASA Accredited Professional Statistician™ with 35 years of experience and is currently the manager of OIT Research Computing Support (formerly the Statistical Consulting Center) at the University of Tennessee. The author of
7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition - KDnuggets,"Combination strategies may also be employed: drop any instances with more than 2 missing values and use the mean attribute value imputation those which remain. Clearly the type of modeling methods being employed will have an effect on your decision — for example, decision trees are not amenable to missing values. Additionally, you could technically entertain any statistical method you could think of for determining missing values from the dataset, but the listed approaches are tried, tested, and commonly used"
7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition - KDnuggets,"Outliers can be the result of poor data collection, or they can be genuinely good, anomalous data. These are 2 different scenarios, and must be approached differently, and so no ""one size fits all"" advice is applicable here, similar to that of dealing with missing values. A particularly good point of insights from the Analysis Factor article from above is as follows:"
7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition - KDnuggets,One option is to try a transformation.  Square root and log transformations both pull in high numbers.  This can make assumptions work better if the outlier is a dependent variable and can reduce the impact of a single point if the outlier is an independent variable
7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition - KDnuggets,"So, what if your otherwise robust dataset is made up of 2 classes: one which includes 95 percent of the instances, and the other which includes a mere 5 percent? Or worse, 99.8 vs 0.2 percent?"
7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition - KDnuggets,"If so, your dataset is imbalanced, at least as far as the classes are concerned. This can be problematic, in ways which I'm sure do not need to be pointed out. But no need to to toss the data to the side yet; there are, of course, strategies for dealing with this"
7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition - KDnuggets,"Data used in these areas often have less than 1% of rare, but “interesting” events (e. However, most machine learning algorithms do not work very well with imbalanced datasets. The following seven techniques can help you, to train a classifier to detect the abnormal class"
7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition - KDnuggets,"Transforming data is one of the most important aspects of data preparation, requiring more finesse than some others. When missing values manifest themselves in data, they are generally easy to find, and can be dealt with by one of the common methods outlined above — or by more complex measures gained from insight over time in a domain. However, when and if data transformations are required is often not as easily identifiable, to say nothing of the type of transformation required"
7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition - KDnuggets,"Standardization and normalization are a pair of often employed data transformations in machine learning projects. Both are data scaling methods: standardization refers to scaling the data to have a mean of 0 and a standard deviation of 1; normalization refers to the scaling the data values to fit into a predetermined range, generally between 0 and 1. Read this article by Shay Geller,"
7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019 Edition - KDnuggets,"If you want to go right to feeding your data into a machine learning algorithm in order to attempt building a model, you probably need your data in a more appropriate representation. In the Python ecosystem, that would generally be a Numpy ndarray (or matrix). This Stack Overflow discussion,"
Demystifying Data Science: Free Online Conference July 30-31 - KDnuggets,"From 2:30-5pm ET the program transitions to workshops, where attendees will learn from the talented team at Metis in an interactive format. All sessions will include real-time chat, with opportunities to ask questions, answer polls, and share socially. Registrants also receive post-conference access to presentations. Workshops include:"
How to Learn Python for Data Science the Right Way - KDnuggets,"First, you should learn Numpy. It is the most fundamental module for scientific computing with Python. Numpy provides the support of highly optimized multidimensional arrays, which are the most basic data structure of most Machine Learning algorithms"
How to Learn Python for Data Science the Right Way - KDnuggets,Pandas is the most popular Python library for manipulating data. Pandas is as an extension of NumPy. The underlying code for Pandas uses the NumPy library extensively. The primary data structure in Pandas is called a data frame
How to Learn Python for Data Science the Right Way - KDnuggets,"Data Scientists manipulate data using both SQL and Pandas. Because there are certain data manipulation tasks that are easy to perform using SQL, and there are certain tasks that can be done efficiently using Pandas. I personally like to use SQL for retrieving data and do the manipulation in Pandas"
How to Learn Python for Data Science the Right Way - KDnuggets,"So, you should know how to efficiently use SQL and Python together. To learn that, you can install the SQLite database on your computer and store a CSV file in it and analyze it using Python and SQL. Here’s an amazing blog post that shows you how to do that:"
How to Learn Python for Data Science the Right Way - KDnuggets,"Unfortunately, for Python lovers like me, the code examples in the book are written in R. I would recommend you to read the first four chapters of the book. Go through the first 4 chapters of the book to understand the basics statistical concepts that I mentioned previously, ignore the code examples and just understand the concepts. The rest of chapters in the book mostly focus on Machine Learning. I will talk about how to learn Machine Learning in the next section"
How to Learn Python for Data Science the Right Way - KDnuggets,"After this, your goal is to implement the basic concepts you learned in Python. StatsModels is a popular Python library used to build statistical models in Python. StatsModels website has"
How to Learn Python for Data Science the Right Way - KDnuggets,"Your final step is to do a data science project that covers all of the above steps. You can find a data set you like and then come up with interesting business questions that you can answer by analyzing it. But, don't choose generic datasets like Titanic"
5 Useful Statistics Data Scientists Need to Know - KDnuggets,"In a practical sense, statistics allows us to define concrete mathematical summaries of our data. Rather than trying to describe every single data point, we can use statistics to describe some of its properties. And that’s often enough for us to extract some kind of information about the structure and make-up of the data"
5 Useful Statistics Data Scientists Need to Know - KDnuggets,"The Median value is quite different from the Mean value of 63.6. Neither of them are right or wrong, but we can pick one based on our situation and goals"
5 Useful Statistics Data Scientists Need to Know - KDnuggets,"Percentiles combined with the mean and standard deviation can give us a good idea of where a specific point lies within the spread / range of our data. If it’s an outlier, then its percentile will be close to the ends — less than 5% or great than 95%. On the other hand, if the percentile is calculated as close to 50 then we know that it’s close our our central tendency"
5 Useful Statistics Data Scientists Need to Know - KDnuggets,Correlation is simply the normalised (scaled) covariance where we divide by the product of the standard deviation of the two variables being analysed. This effectively forces the range of correlation to always be between -1.0 and 1.0
5 Useful Statistics Data Scientists Need to Know - KDnuggets,"If the correlation of two feature variables is 1.0, then the variables have a perfect positive correlation. This means that if one variable changes by a given amount, the second moves proportionally in the same direction"
"If you’re a developer transitioning into data science, here are your best resources - KDnuggets","A new research paper is published every 30 seconds yet scientists currently only use a fraction of the knowledge available to understand the cause of disease and propose new treatments. Our platform ingests, ‘reads’ and contextualises vast quantities of information drawn from written documents, databases and experimental results. It is able to make infinitely more deductions and inferences across these disparate, complex data sources, identifying and creating relationships, trends and patterns, that would be impossible for a human being to make alone"
A Step-by-Step Guide to Transitioning your Career to Data Science – Part 2 - KDnuggets,"Coming back to my example, if most of my prospects say that they analyze web site data from google analytics and present these insights to the director of digital marketing. And they predominantly use SQL and Tableau to perform this analysis but rarely use Python or R. Adding to this, if they also mention that most of the Marketing Data Analysts learn R or Python on the job"
Secrets to a Successful Data Science Interview - KDnuggets,"That you are reading this document is a reflection of your seriousness in being a successful data scientist. Being interviewed in Data Science, a field that is already wide, and rapidly expanding, is as much a challenge for you as a candidate as it is for interviewers. Interviews try to assess your Data Science competency in a few rounds that last for one hour each while your learning has been for an entire lifetime. This puts interviewers (we have been there!) in an unenviable position, namely,"
Secrets to a Successful Data Science Interview - KDnuggets,It all starts with what you have written in the resume. Expect questions to be asked based on anything that is related to Data Science from your resume. It is understandable that they are deep in your past (> three years) but don’t be fazed by them. We believe you have it in you to give a good acquittal of yourself on anything that is on your resume. Please revisit memory lane. Get reacquainted with your past
Secrets to a Successful Data Science Interview - KDnuggets,"These are open-ended questions, they may not have complete solutions within the scope of the interview. Remember, these questions are not for getting billion-dollar startup ideas or patentable ideas from you for free; far from it, these are purely for assessing your problem-solving skills. In fact, some of them may have resonance in data science problems the interviewers are trying to solve as part of their work"
Secrets to a Successful Data Science Interview - KDnuggets,"Now that we have done with open-ended questions, here’s a sample of close-ended questions: derive back propagation for CNN, explain gradient boosting etc. Former helps the interviewer assess your reaction to unplanned circumstances, while the latter establishes a baseline filter rooted in standard understanding. Strive your best to ace both. Practise is the key"
Secrets to a Successful Data Science Interview - KDnuggets,"Data Science uses Machine Learning as one of the key techniques. Yes, it may also use neuroscience, behavioural economics, game theory, statistical mechanics, complexity theory, non-Euclidean geometry and myriad areas you are an expert in. However, problems you are expected to solve in the industry context require a sound knowledge of ML techniques as the primary skills; interviewers will be delighted if you are an expert in other topics, but please be an expert in ML as well"
Secrets to a Successful Data Science Interview - KDnuggets,"You are a deep learning ninja. Despite your strong feelings, why do we still think you need to have a sound understanding of ‘conventional’ Machine Learning also? ‘Black-box’ nature of deep learning models (you may entirely disagree) makes it difficult for the interviewers to know what you did versus what the model did. However, ML in its conventional form seems to be a good common denominator for all candidates. How does it look to be a person who has built a bi-directional LSTM but doesn’t know how SVM’s work? Please go ahead, dazzle the interviewers with your DL skills, but after you’ve proven a point or two with your ML chops"
Secrets to a Successful Data Science Interview - KDnuggets,You should know why you chose one model / algorithm / approach / architecture above others. Hyperparameters play a key role in DL. Develop a sound understanding of model tuning. Difference between a good model and a not-so-good one may lie in your choice of hyper parameters
Secrets to a Successful Data Science Interview - KDnuggets,"Companies give priority to candidates who not only articulate good data science solutions but also can efficiently implement using the right tools. It will be great if you can make yourself comfortable with the tools available in the industry. You should be at least aware of python or R from the language point of view, scikit learn library for ML algorithms, Keras / tensorflow / pytorch / caffe for deep learning. Do not neglect querying languages eg. HQL,SQL and distributed frameworks like hadoop and spark. Good organizations have training programs for all these skills. Selected candidates often go through these training courses. Familiarity with the above tools gives your candidature the much needed edge. By all means undergo training after you get selected. This will give exposure to the problems the organisation solves and opportunity to interact with other scientists in the organization. These are very important experiences to acquire as a new-joiner"
Secrets to a Successful Data Science Interview - KDnuggets,"Interviewers expect you to know how a machine learning model is used, how it is productionised and deployed and the overall end-to-end architecture of your model. It is very difficult to hire a person who does not know how his model will be used by the client/service/customer/product. Without deployment, what you have built is a POC. At the very best, it works on your laptop for demo purposes. For your ML model to be useful it needs to be part of a software pipeline. Know how to interact with engineers towards deploying models. Be prepared to roll up your sleeves and get into the act without anyone prompting you. By doing so, you are enhancing your value and standing in the team"
Secrets to a Successful Data Science Interview - KDnuggets,"As a great data scientist, you should understand the full life cycle of the ML model. You should know how your model should change when the world it tries to model changes. You may also want automate the retraining process for saving your precious time. Re-purpose the time saved in iterating model building towards improving performance. Create opportunities for yourself to explain how you’ve managed a model through its life cycle"
Secrets to a Successful Data Science Interview - KDnuggets,"Debugging is a very important skill in software industry. A software is highly risky if it cannot be debugged. You should understand your model in depth. You are expected to be aware of the internals of the algorithms that you have used. You should know how to do root cause analysis and debug your models. Interviewers expect you to know how you will improve the results when models have high bias or high variance, what you will do to avoid exploding gradients and vanishing gradients and how you will optimize memory during training etc"
Secrets to a Successful Data Science Interview - KDnuggets,"While it is true that Data Structures and Algorithms are outsourced to packages, your ability to make inferences from data is aided by your understanding of algorithms and data structures. While interviewers won’t ask you to implement skip lists or balance k-d trees, they would still expect you to understand order complexity of algorithms, be familiar with basic data structures such as linked lists, stacks, trees, hash-tables and heap, and be comfortable with algorithms such as sorting, shortest paths, string processing and the like. In other words, hygiene questions from data structures and algorithms. You may think it is inappropriate to judge you through this lens, but remember you are making it easy for the competition. We are sure you are more than up to it"
Secrets to a Successful Data Science Interview - KDnuggets,"This is a great opportunity to display your understanding of how to function as part of a data science team. It’s also an opportunity to direct the interviewer to your strengths that weren’t covered in the interview. For example you might be good at code reviews. You could enquire how code review works. You may highlight your favourite approach, say walkthroughs. You might want to know the dev platforms used. You may ask if there are any open source contributors in the team (this is the time to re-emphasize if you are one). You could ask about the delivery cycle. If the team is distributed across timezone, check how the interactions work. Share your experience working in such teams if applicable"
Secrets to a Successful Data Science Interview - KDnuggets,"Ask if it’s OK for you to know at a high level the project(s) the interviewer is part of. These are far better questions to ask than wanting to know about the WFH policy or how the typical day for the data scientist is like. Phrase these questions carefully, for you aren’t the interviewer! Of course, it’s best to ask just two questions as the conversation that ensues do not give you more openings than that. So chose them carefully and be guided by the interview context. Most importantly, be a patient listener, that’s a key to memorable conversations"
Secrets to a Successful Data Science Interview - KDnuggets,"Note: Don’t try to hypnotise the interviewer by going over the top. As always, be dignified. Remember, no job is greater than your dignity"
Secrets to a Successful Data Science Interview - KDnuggets,"It is honourable to admit that you are not very familiar with the topic. You may still want to try based on first principles if possible and also ask clarifying questions. Else no worries, the interviewer will move to another topic from your resume"
Secrets to a Successful Data Science Interview - KDnuggets,"Also, consider the fact that, in addition to you, many other smart and capable candidates are interviewed for the position you have applied for. It is in the very nature of the process to select only a few candidates. If it is not you, it is not the end of the world for you. But don’t analyse yet; allow one or two days to pass for you to regain your composure. Then reflect without rancour how you could be a better candidate next time. Identify opportunities for improvement and put in a plan of action, and start acting on it, for it is of no use living in the past or in a state of inaction. Companies very much expect you to apply again in the next cycle (check with the HR about the period) –many of us have applied more than once before getting selected. Your HR contact also would strive to provide feedback, but please… please… please… do not start a rebuttal chain. Behave like a data scientist in such situations: If the data (interview result) is at variance with your hypothesis (your preparation), move to a better hypothesis, i"
Secrets to a Successful Data Science Interview - KDnuggets,"OK, this may sound a little philosophical. However we think it is practical and important in making you a better candidate and a better person in general. Develop two kinds of self-awareness: Internal and External. Former is about how aware you are of yourself; latter is about how aware you are of others’ perception of you. It should be clear as to how these two come together in creating a fruitful interview experience for you. We recommend you to go through Insight by Tasha Eurich"
Secrets to a Successful Data Science Interview - KDnuggets,You might be wondering what exactly you would get in return as part of a fair interview process. Here’s what we offer to our candidates. You may expect similar experience from other reputed organizations as well
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"Data scientists want to build the ""best"" model. But beauty is in the eye of the beholder. If you don't know what the goal and objective function is and how it behaves, it is unlikely you will be able to build the ""best"" model. And fwiw the objective may not even be a mathematical function but perhaps improving a business metric"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"Commonly data scientists want to build ""models"". They heard XGBoost and random forests work best so lets use those. They read about deep learning, maybe that will improve results further. They throw models at the problem without having looked at the data and without having formed a hypothesis which model is most likely to best capture the features of the data. It makes explaining your work really hard too because you are just randomly throwing models at data"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"Another problem with not looking at the data is that your results can be heavily driven by outliers or other artifacts. This is especially true for models that minimize squared sums. Even without outliers, you can have problems with imbalanced datasets, clipped or missing values and all sorts of other weird artifacts of real-life data that you didn't see in the classroom"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"Modern ML libraries almost make it too easy. Just change a line of code and you can run a new model. And another. And another. Error metrics are decreasing, tweak parameters - great - error metrics are decreasing further. With all the model fanciness, you can forget the dumb way of forecasting data. And without that naive benchmark, you don't have a good absolute comparison for how good your models are, they may all be bad in absolute terms"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"21 and 0.45 respectively. But wait! By just taking the last known value, the MSE drops to 0.003!"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"This is the one that could derail your career! The model you built looked great in R&D but performs horrible in production. The model you said will do wonders is causing really bad business outcomes, potentially costing the company $m+. It's so important all the remaining mistakes bar the last one focus on it"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,048 vs ols mse 0.183 but out-sample it does a lot worse with mse 0.259 vs linear regression mse 0.187. The random forest overtrained and would not perform well live in production!
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"You probably know that powerful ML models can overtrain. Overtraining means it performs well in-sample but badly out-sample. So you need to be aware of having training data leak into test data. If you are not careful, any time you do feature engineering or cross-validation, train data can creep into test data and inflate model performance"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"Preprocessing is applied to the full dataset BEFORE it is split into train and test, meaning you do not have a true test set. Preprocessing needs to be applied separately AFTER data is split into train and test sets to make it a true test set. The MSE between the two methods (mixed out-sample CV mse 0.187 vs true out-sample CV mse 0.181) in this case is not all that different because the distributional properties between train and test are not that different but that might not always be the case"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"If you randomly split data you make accurate predictions using data you did not actually have available during test, overstating model performance. You think you avoided mistake #5 by using cross-validation and found the random forest performs a lot better than linear regression in cross-validation. But running a roll-forward out-sample test which prevents future data from leaking into test, it performs a lot worse again! (random forest MSE goes from 0.047 to 0.211, higher than linear regression!)"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"When you run a model in production, it gets fed with data that is available when you run the model. That data might be different than what you assumed to be available in training. For example the data might be published with delay so by the time you run the model other inputs have changed and you are making predictions with wrong data or your true y variable is incorrect"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"The more time you spend on a dataset, the more likely you are to overtrain it. You keep tinkering with features and optimizing model parameters. You used cross-validation so everything must be good"
Top 10 Statistics Mistakes Made by Data Scientists - KDnuggets,"Counterintuitively, often the best way to get started analyzing data is by working on a representative sample of the data. That allows you to familiarize yourself with the data and build the data pipeline without waiting for data processing and model training. But data scientists seem not to like that - more data is better"
Mongo DB Basics - KDnuggets,This command will show the default DB’s and the one’s created by the client. However If there are no collections in the DB created then it won’t be shown in the list. Also to check which database is currently selected then use the command- db. The below image shows the database which
Mongo DB Basics - KDnuggets,Use the command — db. Follow the below commands where the databases are shown before and after dropping the selected Database. We cannot specify a selected database name. The Database which is currently selected will be automatically deleted. Also we have inserted some data into sampledb which is why it is appearing in the list. If no data was there in sampledb then would not have appeared after typing the command — show dbs
Mongo DB Basics - KDnuggets,The below process shows show to create a collection in a database. In the Collection lies the document. Use the command — db. The below image shows only one document created in the collection ‘Country’. We can create multiple documents in the same collection. This collection refers to the Table for HBase or either a relation DBMS
Mongo DB Basics - KDnuggets,The below example shows a collection having multiple documents. Notice the number of fields in both the documents. The 1st document does not have the suburb field which means we have to insert only those information which is available with us
Mongo DB Basics - KDnuggets,Use the command- db. The object ID’s would be appended to the each document on it’s own. This would make each document unique
Mongo DB Basics - KDnuggets,We can also use the command db.6. Earlier the command db. This is equivalent to the truncate command in SQL
Mongo DB Basics - KDnuggets,"To select the required fields we can use the below given command. We need to append either ‘1’ or ‘0’ to the field in the command. If we give a ‘1’ then all the rows for that field will appear and ‘0’ will give vice versa results. If the field name is not specified in the command then it default takes a ‘1’ for the Object ID field, this case does not applied for all the other field s in the list. The fields here correspond to the columns in RDBMS"
Mongo DB Basics - KDnuggets,This is the advanced version of Projecting the rows. In Projection technique all the rows would appear resulting in us getting unwanted data. Here we can limit the columns as well as skip few of the rows. Use the command -db. This method will retrieve only one row
Mongo DB Basics - KDnuggets,"In this method we can sort the rows in the ascending or descending manner. Select the fields that are needed and then append a 1 or -1 to the sort command. Default is 1, hence in such a case the sort command need not be specified. The ascending or descending will be based on the column selected in the Sort command"
Mongo DB Basics - KDnuggets,"These are the few methods to operate with Mongo DB. The syntax are user friendly and simple to understand. An another advantage of this database is, we need not include all the fields in all the documents within the same collection. For instance, If one document has a field named city and the other document does not need it then we need not include it. This helps in saving a lot of space. Refer to the below example"
A Step-by-Step Guide to Transitioning your Career to Data Science – Part 1 - KDnuggets,"If you are looking to transition your career to data science, don't immediately start learning Python or R. Instead, leverage the domain expertise you have accumulated over the years. Here's a foolproof guide on how to do that"
A Step-by-Step Guide to Transitioning your Career to Data Science – Part 1 - KDnuggets,This approach makes complete sense if you are a programmer or if you have a Ph.D. If you are coming from a non-technical background the easiest way to get started in data science is to take a domain knowledge focused approach
A Step-by-Step Guide to Transitioning your Career to Data Science – Part 1 - KDnuggets,You've picked a target job role and a few companies. You've done your homework. But there's only so much you can do from your room
A Step-by-Step Guide to Transitioning your Career to Data Science – Part 1 - KDnuggets,"Remember that these Informal meetings are for you to test your ideas and gain new insights. They are NOT to get hooked up with a job. Never ask for one here. You are in the research phase. Right now you're just gathering information. They're also not to talk about yourself, so kindly listen. Your job is to learn here, not to talk about yourself"
A Step-by-Step Guide to Transitioning your Career to Data Science – Part 1 - KDnuggets,Try to ask smart questions. You try to answer each question yourself before you actually ask it. Scenario-plan each answer out
Animations with Matplotlib - KDnuggets,"Animations are an interesting way of demonstrating a phenomenon. We as humans are always enthralled by animated and interactive charts rather than the static ones. Animations make even more sense when depicting time series data like stock prices over the years, climate change over the past decade, seasonalities and trends since we can then see how a particular parameter behaves with time"
Animations with Matplotlib - KDnuggets,"Matplotlib simulates raindrops on a surface by animating the scale and opacity of 50 scatter points. Today Python boasts of a large number of powerful visualisation tools like Plotly, Bokeh, Altair to name a few. These libraries are able to achieve state of the art animations and interactiveness. Nonetheless, the aim of this article is to highlight one aspect of this library which isn’t explored much and that is"
Animations with Matplotlib - KDnuggets,"Most of the people start their Data Visualisation journey with Matplotlib. One can generate plots, histograms, power spectra, bar charts, error charts, scatterplots, etc easily with matplotlib. It also integrates seamlessly with libraries like Pandas and Seaborn to create even more sophisticated visualisations"
Animations with Matplotlib - KDnuggets,"This is the basic intuition behind creating animations in Matplotlib. With a little tweak in the code, interesting visualisations can be created. Let’s have a look at some of them"
Animations with Matplotlib - KDnuggets,"Live updating graphs come in handy when plotting dynamic quantities like stock data, sensor data or any other time-dependent data. We plot a base graph which automatically gets updated as more data is fed into the system. Let’s plot stock prices of a hypothetical company in a month"
Animations with Matplotlib - KDnuggets,Creating 3D graphs is common but what if we can animate the angle of view of those graphs. The idea is to change the camera view and then use every resulting image to create an animation. There is a nice section dedicated to it at
Animations with Matplotlib - KDnuggets,"This will create multiple PNG files in the Volcano folder. Now, use ImageMagick to transform them into animation. Open Terminal and navigate to the Volcano folder and enter the following command:"
Animations with Matplotlib - KDnuggets,Animations help to highlight certain features of the visualisation which otherwise cannot be communicated easily with static charts. Having said that it is also important to keep in mind that unnecessary and overuse of visualisations can sometimes complicate things. Every feature in data visualisation should be used judiciously to have the best impact
The Data Fabric for Machine Learning Part 1-b – Deep Learning on Graphs - KDnuggets,"If you are running this outside remember that the framework is tested for Ubuntu 16.04 and 18.04, and you should install:"
The Data Fabric for Machine Learning Part 1-b – Deep Learning on Graphs - KDnuggets,"So this data lives in a graph. And what we did was loading that data to the library. Actually you can transform your data from and to NetworkX, numpy and sdf format in the library"
The Data Fabric for Machine Learning Part 1-b – Deep Learning on Graphs - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"Everything we express (either verbally or in written) carries huge amounts of information. The topic we choose, our tone, our selection of words, everything adds some type of information that can be interpreted and value extracted from it. In theory, we can understand and even predict human behaviour using that information"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"It is messy and hard to manipulate. Nevertheless, thanks to the advances in disciplines like machine learning a big revolution is going on regarding this topic. Nowadays it is no longer about trying to interpret a text or speech based on its keywords (the old fashioned mechanical way), but about understanding the meaning behind those words (the cognitive way). This way it is possible to detect figures of speech like irony, or even perform sentiment analysis"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"This technology is improving care delivery, disease diagnosis and bringing costs down while healthcare organizations are going through a growing adoption of electronic health records. The fact that clinical documentation can be improved means that patients can be better understood and benefited through better healthcare. The goal should be to optimize their experience, and several organizations are already working on this"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"The main drawbacks we face these days with NLP relate to the fact that language is very tricky. The process of understanding and manipulating language is extremely complex, and for this reason it is common to use different techniques to handle different challenges before binding everything together. Programming languages like Python or R are highly used to perform these techniques, but before diving into code lines (that will be the topic of a different article), it’s important to understand the concepts beneath them. Let’s summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms:"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"Is a commonly used model that allows you to count all words in a piece of text. Basically it creates an occurrence matrix for the sentence or document, disregarding grammar and word order. These word frequencies or occurrences are then used as features for training a classifier"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"Through TFIDF frequent terms in the text are “rewarded” (like the word “they” in our example), but they also get “punished” if those terms are frequent in other texts we include in the algorithm too. On the contrary, this method highlights and “rewards” unique or rare terms considering all texts. Nevertheless, this approach still has no context nor semantics"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"Pretty simple, right? Well, although it may seem quite basic in this case and also in languages like English that separate words by a blank space (called segmented languages) not all languages behave the same, and if you think about it, blank spaces alone are not sufficient enough even for English to perform proper tokenizations. Splitting on blank spaces may break up what should be considered as one token, as in the case of certain names (e. San Francisco or New York) or borrowed foreign phrases (e"
Your Guide to Natural Language Processing (NLP) - KDnuggets,These can be pre-selected or built from scratch. A potential approach is to begin by adopting pre-defined stop words and add words to the list later on. Nevertheless it seems that the general trend over the past time has been to go from the use of large standard stop word lists to the use of no lists at all
Your Guide to Natural Language Processing (NLP) - KDnuggets,"The thing is stop words removal can wipe out relevant information and modify the context in a given sentence. For example, if we are performing a sentiment analysis we might throw our algorithm off track if we remove a stop word like “not”. Under these conditions, you might select a minimal stop word list and add additional terms depending on your specific objective"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"A possible approach is to consider a list of common affixes and rules (Python and R languages have different libraries containing affixes and methods) and perform stemming based on them, but of course this approach presents limitations. Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To offset this effect you can edit those predefined methods by adding or removing affixes and rules, but you must consider that you might be improving the performance in one area while producing a degradation in another one. Always look at the whole picture and test your model’s performance"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"Has the objective of reducing a word to its base form and grouping together different forms of the same word. For example, verbs in past tense are changed into present (e. Although it seems closely related to the stemming process, lemmatization uses a different approach to reach the root forms of words"
Your Guide to Natural Language Processing (NLP) - KDnuggets,"Is as a method for uncovering hidden structures in sets of texts or documents. In essence it clusters texts to discover latent topics based on their contents, processing individual words and assigning them values based on their distribution. This technique is based on the assumptions that each document consists of a mixture of topics and that each topic consists of a set of words, which means that if we can spot these hidden topics we can unlock the meaning of our texts"
The Data Fabric for Machine Learning – Part 1 - KDnuggets,"If you search for machine learning online you’ll find around 2,050,000,000 results. Yeah for real. It’s not easy to find that description or definition that fits every use or case, but there are amazing ones. Here I’ll propose a different definition of machine learning, focusing on a new paradigm, the data fabric"
The Data Fabric for Machine Learning – Part 1 - KDnuggets,"We want the facts — where those facts come from is less important. The data here can represent concepts, objects, things, people and actually whatever you have in mind. The graph fills in the relationships, the connections between the concepts"
The Data Fabric for Machine Learning – Part 1 - KDnuggets,"In Einstein theory of gravity (General Relativity) he proposed mathematically that mass can deform space-time, and that deformation is what we understand as gravity. I know that if you are not familiar with the theory it can sound weird. Let me try to explain it"
The Data Fabric for Machine Learning – Part 1 - KDnuggets,That’s exactly what I’m proposing what machine learning can be in the data fabric. I know I sound crazy. Let me explain myself
The Data Fabric for Machine Learning – Part 1 - KDnuggets,Thanks also for reading this. I hope you found something interesting here :). If these articles are helping you please share them with your friends!
The Data Fabric for Machine Learning – Part 1 - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Data Literacy: Using the Socratic Method - KDnuggets,"Unlike gold and oil, which are precious because they are rare, data is ubiquitous. So, it is not the data itself that is valuable--it is everywhere. The value lies in an organization’s ability to engage with it meaningfully and extract business"
Data Literacy: Using the Socratic Method - KDnuggets,"If it is not providing business value, then data is useless. It goes from being the new gold to the new dirt (very costly dirt). So, in addition to investing in data infrastructure, organizations need to prioritize"
Data Literacy: Using the Socratic Method - KDnuggets,"The key is to recognize that at the core of it, data literacy skills are critical thinking skills. And, good questions are the key to developing critical thinking. No one understood this better than Socrates. He believed that:"
Data Literacy: Using the Socratic Method - KDnuggets,"J.A. Binker,"
Data Literacy: Using the Socratic Method - KDnuggets,"This method is used in law schools around the country to teach how to expose any logical fallacies in arguments. The beauty of this framework is that it can be adapted to any topic of interest. In our case, Data!"
Data Literacy: Using the Socratic Method - KDnuggets,"An even more effective application of Socratic questioning is in stimulating a guided discussion among the stakeholders of a data project. By examining data together and reasoning through it together, the group can impart more context to it and construct a stronger statistical narrative; all the while developing their data literacy skills. Since the goal of a Socratic seminar is to think better, who better to lead the discussion than a"
Data Literacy: Using the Socratic Method - KDnuggets,Below are example questions for each category. Not every example question listed here will be applicable to every situation and some questions may fall in more than one category. The main goal should be to ask questions from all six categories
The Infinity Stones of Data Science - KDnuggets,"Don't forget, Thanos was a bit of a data scientist himself. He identified a problem and its solution, though you might take issue with his conclusions, not to mention the complete lack of scientific process is his line of thinking. But that's beside the point"
The Infinity Stones of Data Science - KDnuggets,"Let's look at this first one metaphorically. Presumably, in order to manipulate reality, we have to understand it. This seems to be a good (enough) analogy for the importance of domain knowledge"
The Infinity Stones of Data Science - KDnuggets,"But how much data exploration, and exactly what kind of exploration are we talking? This is going to surprise you, but. If we are interested in descriptive analysis — that is, no prediction, along the lines of a straightforward data analysis — the more intimately we are familiar with the data the better. The end"
The Infinity Stones of Data Science - KDnuggets,"When it comes to predictive analytics, and machine learning undertakings, there are differing opinions on how much exploratory data exploration is helpful. There are also differing opinions on the level of exploratory analysis of datasets which are not being used for training (i. This aside, in order to ensure maximum power over your data space is achieved, be sure to guard against the potential pitfalls of poor exploratory data analysis or shoddy visualizations, such as the"
The Infinity Stones of Data Science - KDnuggets,"Computational power (or ""compute"") is the collective computational resources we have to throw at a particular problem. Unlimited compute was once thought to be the be all and end all of computing, and for good reason. Consider how little compute there was one, two, or three decades ago, in comparison to today. Imagine scientists sitting around and thinking about problems they could solve, if only they had more than a handful of MHz worth of compute at their disposal. The sky would be the limit!"
The Infinity Stones of Data Science - KDnuggets,"Of course, that's not exactly how things have turned out. Sure, we have a lot more compute at our disposal now than we ever have in the past in the form of supercomputers, the cloud, publicly available APIs backed by heaping amounts of compute, and even our own notebooks and smartphones, comparatively. All sorts of problems we could never have imagined would have enough compute to solve are now tractable, and that's a great development. We need to keep in mind, however, that ""clever"" is a great counterbalance to compute, and lots of advancement in data science and its supportive technologies have been made possible by brain over brawn"
The Infinity Stones of Data Science - KDnuggets,"The Soul Stone, then, is analogous to the power of prediction, which is to say it lies at the absolute core of data science. What are data scientists trying to accomplish? They are trying to answer interesting questions with available data in order to make predictions which align as closely as possible with reality. That prediction piece seems pretty crucial"
The Infinity Stones of Data Science - KDnuggets,"The modeling is complete. The predictions have been made. The insights are. It's now time to inform the project stakeholders of the outcomes. But the non-data scientists among us don't have the same interests in, or understandings of, data and the data science process, so we need to be effective in presenting our findings to them in a way they will appreciate"
The Infinity Stones of Data Science - KDnuggets,"Remember, if your insights don't translate to being useful, then your work isn't complete. It's up to you to convince others of the value of your work. Once convinced, they can take action, and"
PyCharm for Data Scientists - KDnuggets,"I have recently started using PyCharm as an alternative to Spyder, and am loving it. This article talks about some of the features of PyCharm that made me completely transition to PyCharm from Spyder. The below features are in comparison with Spyder, and not general IDEs"
7 Steps to Mastering SQL for Data Science — 2019 Edition - KDnuggets,"This time around, we aim to, once again, lay out the path to SQL mastery. Let's make sure we view 'mastery' in a relative sense, however, and not expect to be scripting at database guru-levels after getting through the materials. The learning path is aimed at those with some understanding of databases, programming, computer science concepts, and/or data management in an abstract sense, who are wanting to be able to use SQL to manage and query their own data and scale up to larger systems"
7 Steps to Mastering SQL for Data Science — 2019 Edition - KDnuggets,"Let's approach this initial exposure to SQL as follows. First watch the video below, titled ""SQL - A Brief Review,"" by Charles Germany, for some very quick insight into where SQL came from, along with some syntax by way of a few short examples. As with the resources and examples below the video, don't worry about fully understanding the syntax, just get a feel for what SQL is doing, what it can be used for, and what it looks like"
7 Steps to Mastering SQL for Data Science — 2019 Edition - KDnuggets,"This second resource in particular should make for a handy reference later on. Some of these commands are covered in greater detail in subsequent steps, so, again, don't worry about understanding everything. At the same time, these initial examples are rather intuitive, and so should be helpful for your understanding"
7 Steps to Mastering SQL for Data Science — 2019 Edition - KDnuggets,"Finally, in preparation of the of next step, get an SQL environment up and running. You may not want to enter every SQL statement you encounter, but having an SQL interpreter up and running just makes sense. I suggest installing SQLite locally; it is a simple, but capable, SQL installation"
7 Steps to Mastering SQL for Data Science — 2019 Edition - KDnuggets,"Joins come in different flavors, and likely one of the more complex topics you will cover while learning SQL is getting them straight. That's really more of a testament to the ease of SQL than the actual difficulty of learning about joins. Get a better understanding of joins with this Socratica video"
7 Steps to Mastering SQL for Data Science — 2019 Edition - KDnuggets,"Hopefully you have learned enough SQL at this point to consider yourself a ""master"" of the subject as relates to data science. But don't stop here; there are plenty more free quality resources online which you can use to build on this new foundation of mastery. You can never know enough SQL, and as with so many other skills, practice in the key to reinforcement"
Mathematical programming —  Key Habit to Build Up for Advancing Data Science - KDnuggets,This is the core of data science and analytics. It is not enough to write a function which prints the expected output and stop there. The essential programming may be done but the scientific experiment does not stop there without further exploration and testing of the hypothesis
Mathematical programming —  Key Habit to Build Up for Advancing Data Science - KDnuggets,"The core function of throwing dart uses a random generator at its heart. Now, a computer-generated random number is not truly random, but for all practical purpose, it can be assumed to be one. In this programming exercise, we used a uniform random generator function from the"
Mathematical programming —  Key Habit to Build Up for Advancing Data Science - KDnuggets,"We demonstrate what it means to develop a habit of mathematical programming. Essentially, it is thinking in terms of programming to test out the mathematical properties or data patterns that you are developing in your mind. This simple habit can aid in the development of good practices for an upcoming data scientist"
Machine Learning in Agriculture: Applications and Techniques - KDnuggets,"Species selection is a tedious process of searching for specific genes that determine the effectiveness of water and nutrients use, adaptation to climate change, disease resistance, as well as nutrients content or a better taste. Machine learning, in particular, deep learning algorithms, take decades of field data to analyze crops performance in various climates and new characteristics developed in the process. Based on this data they can build a probability model that would predict which genes will most likely contribute a beneficial trait to a plant"
Machine Learning in Agriculture: Applications and Techniques - KDnuggets,"For specialists involved in agriculture, soil is a heterogeneous natural resource, with complex processes and vague mechanisms. Its temperature alone can give insights into the climate change effects on the regional yield. Machine learning algorithms study evaporation processes, soil moisture and temperature to understand the dynamics of ecosystems and the impingement in agriculture"
Machine Learning in Agriculture: Applications and Techniques - KDnuggets,"Both in open-air and greenhouse conditions, the most widely used practice in pest and disease control is to uniformly spray pesticides over the cropping area. To be effective, this approach requires significant amounts of pesticides which results in a high financial and significant environmental cost. ML is used as a part of the general precision agriculture management, where agro-chemicals input is targeted in terms of time, place and affected plants"
Machine Learning in Agriculture: Applications and Techniques - KDnuggets,"Apart from diseases, weeds are the most important threats to crop production. The biggest problem in weeds fighting is that they are difficult to detect and discriminate from crops. Computer vision and ML algorithms can improve detection and discrimination of weeds at low cost and with no environmental issues and side effects. In future, these technologies will drive robots that will destroy weeds, minimizing the need for herbicides"
Machine Learning in Agriculture: Applications and Techniques - KDnuggets,"This is an application that can be called a bonus: imagine a farmer sitting late at night and trying to figure out the next steps in management of his crops. Whether he could sell more now to a local producer or head to a regional fair? He needs someone to talk through the various options to take a final decision. To help him, companies are now working on development specialized chatbots that would be able to converse with farmers and provide them with valuable facts and analytics. Farmers’ chatbots are expected to be even smarter than consumer-oriented Alexa and similar helpers, since they would be able not only to give figures, but analyze them and consult farmers on tough matters"
Machine Learning in Agriculture: Applications and Techniques - KDnuggets,"ANNs are inspired by the human brain functionality and represent a simplified model of the structure of the biological neural network emulating complex functions such as pattern generation, cognition, learning, and decision making. Such models are typically used for regression and classification tasks which prove their usefulness in crop management and detection of weeds, diseases, or specific characteristics. The recent development of ANNs into deep learning that has expanded the scope of ANN application in all domains, including agriculture"
Machine Learning in Agriculture: Applications and Techniques - KDnuggets,"SVMs are binary classifiers that construct a linear separating hyperplane to classify data instances. SVMs are used for classification, regression, and clustering. In farming, they are used to predict yield and quality of crops as well as livestock production"
Ten random useful things in R that you might not know about - KDnuggets,It provides simple HTML shortcuts that allow easy construction of sidebars and the organization of your display into rows and columns. It also has a super flexible title bar where you can organize your app into different pages and put in icons and links to Github code or an email address or whatever. As a package which operates within
Ten random useful things in R that you might not know about - KDnuggets,"R Shiny development can be frustrating, especially when you get generic error messages that don’t help you understand what is going wrong under the hood. As Shiny develops, more and more validation and testing functions are being added to help better diagnose and alert when specific errors occur. The"
Ten random useful things in R that you might not know about - KDnuggets,"Its been a tough day, you’ve had a lot on your plate. Your code isn’t as neat as you’d like and you don’t have time to line edit it. Fear not. The"
Ten random useful things in R that you might not know about - KDnuggets,"So you write a lovely R Markdown document where you’ve analyzed a whole bunch of facts about dogs. And then you get told — ‘nah, I’m more interested in cats’. Never fear. You can automate a similar report about cats in just one command if you parameterize your R markdown document"
Ten random useful things in R that you might not know about - KDnuggets,"It can be used inside R Markdown and has very intuitive HTML shortcuts to allow you to create a nested, logical structure of pretty slides with a variety of styling options. The fact that the presentation is in HTML means that people can follow along on their tablets or phones as they listen to you speak, which is really handy. You can set up a"
Ten random useful things in R that you might not know about - KDnuggets,"Most people don’t take full advantage of the HTML tags available in R Shiny. There are 110 tags which offer shortcuts to various HTML formatting and other commands. Recently I built a shiny app that took a long time to perform a task. Knowing that the user would likely multitask while waiting for it to complete, I used"
Modeling 101 - KDnuggets,Let me give you an illustration with a simple example of supervised learning. Supervised learning refers to statistical models with one or more dependent variables which we try to understand or predict using one or more independent variables. Note that
Modeling 101 - KDnuggets,"Say, for instance, we have purchase data for a certain product category. We'd like to use these data to better understand why some people purchase this type of product more (or less) often than others do. Based on what we've learned from previous research,"
Modeling 101 - KDnuggets,Intercept is the value of purchase frequency when the predictors are all zero. Error denotes deviations of our model's predicted purchase frequency from actual purchase frequency. It's what the model is unable to explain and is a very important part of the equation
Modeling 101 - KDnuggets,"Even uncomplicated regressions such as this hypothetical one aren't simply a case of clicking and dragging variable names into boxes. We have many decisions to make, including those when we're setting up and cleaning our data. Regression comes in many flavors and the nature of the dependent variable determines which type of regression we should use. This is a critical decision"
Modeling 101 - KDnuggets,"On the other hand, we might have a record of the number of times each consumer had bought the product over a three-month period, for example. Here, Poisson regression or another kind of regression designed for data of this type would be most suitable. There are more than twenty kinds of regressions for count data I know of"
Modeling 101 - KDnuggets,"There are other decisions that pertain to the right-hand side of the equation. We might need to recode gender as Female = 1, Male = 0, for example. This is usually called dummy coding, though other terms are also used. Likewise, we might collapse household income and household size into fewer categories and recode them as dummy variables - that is just one possibility and not a wise choice in every situation"
Modeling 101 - KDnuggets,"Age might need to be transformed in some way - it's common to center variables such as age on the data mean or some value that makes intuitive sense. For example, if the mean age of the consumers in our data is 40, a 50 year-old would be recoded as 10 and a 30 year-old as -10. This way, the intercept would be meaningful since none of our consumers would be zero years of age. The relationship between age and purchase frequency could be curvilinear, in which case some variety of spine or polynomial regression might be used. There are many options"
Modeling 101 - KDnuggets,"Multilevel, mixture or longitudinal modeling might also be considered. These are advanced topics but, in our example, multilevel modeling could be used to account for geographic patterns in our data. Mixture modeling is quite complex, but one type essentially blends regression and cluster analysis. We might find, for instance, that separate regression equations are needed for different segments of consumers. Longitudinal analysis would be used if we'd like to study how the behavior of consumers changes over time, perhaps in response to marketing activity"
Modeling 101 - KDnuggets,"Our final model might look quite different from the one shown earlier. We might conclude, for example, that some independent variables aren't needed and can be dropped from the equation. How to decide what is the ""best"" model is a large topic and involves the use of fit indices and information criteria as well as other model diagnostics. Cross-validation, in which part of the data is held out from the model building, may also be advisable. Most importantly, unless we're only concerned with making predictions, our final model should be interpretable and make sense to those who will use our results"
Modeling 101 - KDnuggets,"Building ""simple"" models with just a few variables can become quite complicated, and not only what we do but the order in which we do it is important. Automated modeling will usually provide us with models which fit the data reasonably well but, in my experience, they are often hard to interpret and, therefore, useless beyond prediction. Models we cannot understand, even if they predict well, do not inform us about the"
Modeling 101 - KDnuggets,"I haven't mentioned unsupervised learning, factor and cluster analysis being two examples. There is no distinction between dependent and independent variables in unsupervised learning and no dependent variables to ""supervise"" the modeling. Generally speaking, more subjective judgment is required in unsupervised learning"
Modeling 101 - KDnuggets,"In the past couple of decades, innovation in statistics and machine learning has been increasing at a rapid pace and we are now able to do things unimaginable when I began my career. Progress has downsides too, though, and there is now much more to learn in the same space of time and many more decisions for modelers to make. The probability of serious errors has probably risen as a result"
Choosing Between Model Candidates - KDnuggets,"Let’s fit a model to some data. These are the annual temperatures for the last 120 years in a fictional Midwestern town. There’s one point per year, the annual median of the daily high temperatures. When we look at it, our eye is really good at pulling out a pattern. There’s a clear lift toward the right hand side. We would like to capture that in a model"
Choosing Between Model Candidates - KDnuggets,"There are a lot of models that can represent this. A really nice starting point, because it's so simple, is a straight line. Here’s what the best fit straight line looks like. It does a pretty good job. We can see that it definitely captures the upward tilt of the data, but it doesn’t capture the bend in it. It’s clear when we examine it, that a straight line doesn’t do quite as well as we would like"
Choosing Between Model Candidates - KDnuggets,"Luckily, we have a lot of other options. A reasonable next candidate is a quadratic, a polynomial with a squared term instead of just a linear term. These have curvature to them. We can see that the best fit quadratic clearly captures the lift at the right hand side of the plot and the bend in the middle, but it also imposes a little lift on the left-hand side of the plot which is not obviously reflected in the data"
Choosing Between Model Candidates - KDnuggets,"Now the fit appears to be getting better, but the line is taking on extra personality. It’s adopting wiggles. If we take this to an extreme, we can imagine a model that passes through every single data point perfectly. This model would have zero error, zero deviation from our measured data. So does that make it the best fit model?"
Choosing Between Model Candidates - KDnuggets,"Models are useful because they allow us to generalize from one situation to another. When we use a model, we’re working under the assumption that there is some underlying pattern we want to measure, but it has some error on top of it. The goal of a good model is to look through the error and find the pattern"
Choosing Between Model Candidates - KDnuggets,"The most common way to do this is to split our data into two groups. We can use one group to train our model, and then we can test it to see how closely it fits the second group. The first group is the training data set, and the second group is the testing data set. There are lots of ways to do this, and we will revisit them later, but for now, we will randomly sort our years into two bins. We’ll put 70% of them into our training data set, and 30% of them into our testing data set"
Choosing Between Model Candidates - KDnuggets,"As the models get higher order, we can see that the wiggles they have developed may have been helpful for fitting the training data, but don't necessarily help them fit the testing data better. We can see an extreme example of this in the full interpolation model, where we just connect all the training data points with straight lines. It really struggles to match the testing data points"
Choosing Between Model Candidates - KDnuggets,"Looking at the errors on the training data set, a few things jump right out. First is the wide gap between the training errors (hollow circles) and testing errors (solid circles). Right away we can see that there is a substantial difference between the two data sets"
Choosing Between Model Candidates - KDnuggets,"Second, there is a precipitous drop in error going from a linear to a quadratic model (order-1 to order-2 polynomial). This makes sense. When we were eyeballing it, we could see that the linear fit failed to capture the curvature of the data, one of its most prominent features"
Choosing Between Model Candidates - KDnuggets,"So which model fits best? When we look carefully at the errors on the training data, it appears that the error on the fifth order polynomial model is the lowest. The differences are subtle so you might have to squint. All the higher order models have low error too, but they are just a little higher than an order-5 polynomial. But as we mentioned, that's not the ultimate test. It's the error on the testing data that we really care about"
Choosing Between Model Candidates - KDnuggets,"Careful inspection of testing error (solid circles) shows that the fourth-order model does the best job. At higher orders of polynomials, the error on the test data set goes up. The more wiggly the line gets in fifth-and-higher order polynomial models, the more it captures the quirks of the training data, rather than the underlying pattern of the testing data that we are interested in"
5 Things to Review Before Accepting That Data Scientist Job Offer - KDnuggets,"It drops to $88,973 for an entry-level position. Hopefully, you've already researched the wages for data scientists with experience comparable to yours before starting the job search. If not, now is the time to get those numbers and see how they stack up to your job offer"
5 Things to Review Before Accepting That Data Scientist Job Offer - KDnuggets,"Consider things like the cost of living, too, especially if the job requires you to move somewhere new. Moreover, evaluate whether you will have to endure a significant commute to get to work each day. Are public transit routes available? Answering those questions could help you determine whether the salary offered will help you have a comfortable lifestyle"
5 Things to Review Before Accepting That Data Scientist Job Offer - KDnuggets,"In a case like that, imagine the distress you'll experience if you're under the impression the contract will last for at least three years, but an at-will clause allows the employer to terminate you, and they do because their data science expectations haven't come to pass. That likely happened because of inadequate resources, not a failure on your part. But, an at-will clause could still cause you to lose your job"
5 Things to Review Before Accepting That Data Scientist Job Offer - KDnuggets,"What the job entails has likely already come up in many of your previous discussions. But, you should still look closely at the job offer and ensure everything is as you expected. While looking at the job specifics, you might notice some red flags. For example, maybe the contract says you'll be the sole person involved in data science at the company"
5 Things to Review Before Accepting That Data Scientist Job Offer - KDnuggets,"It's tempting to accept a job offer almost as soon as you get it. But, the best approach to take is to ask for time to look it over and review the factors above and others that matter to you. Showing caution like that helps you determine if the job is a good fit for you"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"We are constantly surrounded by data that represents the business, product, and customers at scale. This allows us to see things from a 30,000-foot view, where other roles spend most of their time at ground-level, working on execution. It’s important that we realize this fact, and more important that we make the most of it"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"Most technical ICs within established companies focus on execution. This is pretty intuitive. In order for a company to be successful, it has to get things done that provide value"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"Data science roles are a little different. They vary greatly depending on the team structure and size, but generally speaking, execution isn’t where we’re at our best. Our most valuable work often comes from exploration"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"When it comes to complex questions and hypotheses, execution isn’t the answer. Someone has to dive in and figure things out on a deeper level. They have to thoroughly analyze and explore the problem. Data scientists are the perfect candidates to take this on"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"The act of thinking, coming up with a hunch, and then exploring that hunch is criminally underrated. When done right, not only does this work produce interesting results — it drives decision-making. This is where data scientists really thrive"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"This isn’t to say that data scientists can’t or shouldn’t execute. We spend a good amount of time building models, writing production code, and automating common tasks. The reality is that we have a diverse skill set that allows us to both explore and execute. This is why data scientists are hard to find, and also what makes the field so"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,Should data scientists go completely rogue and do whatever they want? Probably not. We can’t blatantly ignore a backlog of JIRA tickets while looking into a hypothesis that came to mind at 2:00 AM the previous night. There has to be a balance here
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"But the question remains: what does this look like in practice? It’s not easy to think this way in a world of constant focus on execution. Recently, I’ve been doing three different things to stay exploration-first. I’m pretty happy with the results so far"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"First, I recommend blocking off an hour or so daily for deep thought and exploration. The best time for you will vary from person to person. I prefer first thing in the morning, but you could just as easily set aside an hour in the afternoon. It’s extremely important to schedule this time"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,This is a meeting that you can’t afford to miss or reschedule. Hold yourself accountable. This is your time to think
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"In case you haven’t heard, documentation is kind of important. Your thinking practice is no exception to this rule. No matter the quality of your idea, get it down somewhere. Create a running document or keep a notepad where you can allow these ideas, questions, and hypotheses to live on and be revisited"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,"As a data scientist, curiosity is your north star. Sometimes you’ll get caught up in execution-mode and forget to develop and explore your own ideas. When this inevitably happens, curiosity is what will bring you back. I highly recommend this excellent article from Multithreaded for more on the topic of"
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,The execution-based work gets most of the love in data science. And can you blame us? It’s easier to quantify. You can see the results that come from building a model or pushing code to production
Data Scientists Are Thinkers: Execution vs. exploration and what it means for you - KDnuggets,It’s more difficult to see concrete results from an afternoon tinkering with a new idea. This new idea probably won’t lead to anything significant. Maybe only 10% of these bets actually turn out to be anything. Don’t let this discourage you. The 10% is worth it. The 10% is where the truly transformative work comes from — and it all starts with thinking
The Whole Data Science World in Your Hands - KDnuggets,My favorite programming language of the moment is Python. There are lots of great tools and features that can help you using this language. One of the most popular ones is Jupyter Notebook. To launch a notebook in MatrixDS do this:
The Whole Data Science World in Your Hands - KDnuggets,"I started my data science career on R. It’s a great tool for doing data analysis, data cleaning, plotting and much more. I think right now the machine learning part it’s better with Python, but to be a successful data scientist you need to know them both"
The Whole Data Science World in Your Hands - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Becoming a Level 3.0 Data Scientist - KDnuggets,"This post aims to shed light on what’s expected and what’s outside of the scope of each Data Science Career Level. While companies might have different job titles, this post provides a general baseline. Furthermore, the post concludes with hands-on tips on how to prepare your career for the switch into AI or the well-deserved promotion"
Becoming a Level 3.0 Data Scientist - KDnuggets,"A Data Scientist is expected to have knowledge in three areas: Statistics, Engineering, and Business. However, you’re not expected to master all three areas right from the get-go. Which skills should you focus on when seeking an entry-level position? Which skills become more important as you progress through the career ladder?"
Becoming a Level 3.0 Data Scientist - KDnuggets,The graphic below shows the market expectations for each Data Science Level from 1.0 to 3.0. The results are based on my personal experience in the field and conversations with experts and influencers from
Becoming a Level 3.0 Data Scientist - KDnuggets,"To avoid confusion, we will refer to the positions as Level 1.0 to 3.0"
Becoming a Level 3.0 Data Scientist - KDnuggets,"The prototypical Junior Data Scientist is a young graduate. Popular fields of study include Computer Science, Mathematics, or Engineering. A Junior Data Scientist has 0–2 years of work experience and is familiar with creating prototypes with structured data sets in Python or R. She has participated in kaggle competitions and has a GitHub profile"
Becoming a Level 3.0 Data Scientist - KDnuggets,"Junior Data Scientists can provide tremendous value to companies. They are fresh off taking online courses and can provide immediate help. They are usually self-taught since few universities offer Data Science degrees and thus show tremendous commitment and curiosity. They are enthusiastic about the field they have chosen and are eager to learn more. The Junior Data Scientist is good at prototyping solutions, but still lacks proficiency in engineering and business mindset"
Becoming a Level 3.0 Data Scientist - KDnuggets,"If a company is hiring Junior Data Scientists, usually a Data Science team is already in place. The company is then looking for help to make life easier for more experienced colleagues. This involves rapidly testing new ideas, debugging, and refactoring existing models. You will discuss ideas as sparring partners with the team. You pitch new ideas on how to do things better. You take responsibility for your code, continuously striving to improve code quality and impact. You’re a great team player, constantly looking to support your teammates on their mission of building great data products"
Becoming a Level 3.0 Data Scientist - KDnuggets,"Junior Data Scientists don’t have experience in engineering complex product solutions. Ergo, she works in a team to put Data Science models into production. Since the Junior Data Scientist just joined the company, she is not immersed in the business of the company. Hence, she is not expected to come up with new products to impact the"
Becoming a Level 3.0 Data Scientist - KDnuggets,"I’m interested in a Junior Data Scientists ability to complete a non-trivial project. By complete, I mean that the project was completed from start to end by the person — or within a group — and culminated in a fully flushed-out product. I find it to correlate with the Data Scientist’s ability to lead projects on the job"
Becoming a Level 3.0 Data Scientist - KDnuggets,"The Senior Data Scientist has already worked as a Junior Data Scientist, Software Engineer, or completed a Ph.D. He has 3–5 years of relevant experience in the field, writes reusable code, and builds resilient data pipelines in cloud environments"
Becoming a Level 3.0 Data Scientist - KDnuggets,Senior Data Scientists should be able to frame Data Science problems. Good candidates have great insights from past Data Science experiences. I also dig deeper into their ability to write production code
Becoming a Level 3.0 Data Scientist - KDnuggets,"They are also not as expensive as Principal Data Scientists, while still being expected to deliver Data Science models in production. It’s a very fun level to play, having surpassed Level 1.0 and yet having room to grow to Level 3.0"
Becoming a Level 3.0 Data Scientist - KDnuggets,"The Senior Data Scientist masters the art of putting mathematical models into production. While Principal Data Scientists or Business Managers assign tasks, the Senior Data Scientist takes pride in building well-architected products. He avoids logical flaws in the model, doubts systems that perform too well, and takes pride in preparing data correctly. The Senior Data Scientist mentors Junior Data Scientists and answers business questions to management"
Becoming a Level 3.0 Data Scientist - KDnuggets,"The Senior Data Scientist is not expected to lead entire teams. It is not the responsibility of the Senior Data Scientist to come up with ideas for new products since they are generated by more experienced colleagues and managers. While the Senior Data Scientist knows the details of the products they have built, they are not expected to know the overall architecture of all data-driven products. The Level 2.0 Data Scientist is skilled in statistics and better in engineering than a Level 1.0 Data Scientist but strays away from the non-fun business part from Level 3.0"
Becoming a Level 3.0 Data Scientist - KDnuggets,The Senior Data Scientist is measured by the impact their models generate. He has a good intuition about the inner workings of statistical models and how to implement them. He is in the process of understanding the business of the company better but isn’t expected to provide solutions to business problems just yet
Becoming a Level 3.0 Data Scientist - KDnuggets,"Now that we’ve investigated Level 2.0, let’s see what the final Level 3.0 looks like"
Becoming a Level 3.0 Data Scientist - KDnuggets,The Principal Data Scientist is the most experienced member of a Data Science team. She has 5+ years of experience and is well-versed in various types of Data Science models. She knows the best practices when putting models to work. She knows how to write code computationally efficient and is lurking around to find high-impact business projects
Becoming a Level 3.0 Data Scientist - KDnuggets,"Principal Data Scientists need to have a very good understanding of the business problem they are solving before writing one line of code. Meaning, they need to have the ability to validate ideas prior to implementation. This approach increases the Data Science project success"
Becoming a Level 3.0 Data Scientist - KDnuggets,"The Principal Data Scientist is responsible for creating high-impact Data Science projects. In close coordination with stakeholders, she is responsible for leading a potentially cross-functional team in providing the best solution to a given problem. Hence, her leadership skills have developed since Level 1.0 and 2.0. The Principal Data Scientist acts as a technical consultant to Product Managers from different departments. With her vast experience and skills in the major Data Science categories, she becomes a highly valued asset to any project"
Becoming a Level 3.0 Data Scientist - KDnuggets,"While shaping the discussion about desired skills, it is not the responsibility of the Principal Data Scientist to recruit new team members. Although she understands the business of her company and suggests impactful new products, the Product Managers are still responsible for market adoption. She also leads teams, but career progression decisions are still taken by the team lead"
Becoming a Level 3.0 Data Scientist - KDnuggets,"The Principal Data Scientist has seen why products fail and thus she drives new projects successfully. She is a valued contributor to product discussions and enjoys educating the company about Data Science. With her experience in delivering impactful Data Science solutions, she is the most valuable asset in the Data Science department"
Becoming a Level 3.0 Data Scientist - KDnuggets,"Now that you’ve seen the different expectations of a Data Scientist from Level 1.0 to 2.0 until 3.0, let’s find out how you can use this knowledge to advance your career"
Becoming a Level 3.0 Data Scientist - KDnuggets,It doesn’t matter if you’re looking to enter the Data Science Career Game on Level 1.0 or you’re looking to progress into a higher Level. Take the following steps to make your next career move
Becoming a Level 3.0 Data Scientist - KDnuggets,"Do you want to break into AI? Then get rock solid about statistical models and learn how to solve problems with structured datasets. Do you look to enter Level 3.0? Make sure you have your math, engineering, and"
Becoming a Level 3.0 Data Scientist - KDnuggets,"Nuances exist within the different Data Science Career Levels. For instance, Séb Foucaud is rather looking for strong engineering than math skills in Junior Data Scientists. Some Senior Data Scientists might discover their passion for building scalable data pipelines and transition to a Data Engineering role. Some Principal Data Scientists prefer to develop technical expertise while others rejoice in focusing on business skills. Whatever career path you take, developing your skills around the three main areas of Data Science expertise will get you far"
Becoming a Level 3.0 Data Scientist - KDnuggets,"This post continues the ongoing series of educating Data Scientists to become business-savvy. The series aims at helping you polish your overall Data Science skill set. If you enjoy the format, please"
Becoming a Level 3.0 Data Scientist - KDnuggets,"Jan currently works in the realm of self-driving cars as a Project Lead Data Science for Carmeq GmbH, the innovation vehicle of Volkswagen AG. Jan is passionate about advancing the automotive industry through machine learning and sharing his knowledge in the fields of Business and Data Science. He is a monthly Contributor to the “Towards Data Science” Publication on Medium. He is also a Deep Learning"
Data Science vs. Decision Science - KDnuggets,"Data science has become a widely used term and a buzzword as well. It is a broad field representing a combination of multiple disciplines. However, there are adjacent areas that deserve proper attention and should not be confused with data science. One of them is decision science. Its importance should not be underestimated, so it is useful to know the actual differences and peculiarities of these two fields. Data science and decision science are related but still separate fields, so at some points, it might be hard to compare them directly"
Data Science vs. Decision Science - KDnuggets,"In general, data scientist is a specialist involved in finding insights from data after this data has been collected, processed, and structured by data engineer. Decision scientist considers data as a tool to make decisions and solve business problems. To demonstrate other differences, we decided to prepare an infographic which puts data science and decision science in contrast according to several criteria. Let’s dive right in"
Data Science vs. Decision Science - KDnuggets,"In terms of definition, data science appears to be an interdisciplinary field that uses scientific algorithms, methods, techniques and various approaches to extract valuable insights. Thus, its primary purpose is to reveal the insights from data for further application to the benefit of the various industries. In contrast, decision science is an application of a complex of quantitative techniques to the decision-making process. Its purpose is to apply the data-driven insights in combination with the elements of cognitive science to policies planning and development. So, data is equally important for both, yet the mechanisms are quite different"
Data Science vs. Decision Science - KDnuggets,"Now, let's move on to the areas of application. Data science is applied in numerous industries like retail, FMCG, entertainment, media, healthcare, insurance, telecommunication, finance, travel, manufacturing, agriculture, sports, etc. Decision science touches more theoretical areas of business and management, law and education, environmental regulation, military science, public health, and public policy"
Data Science vs. Decision Science - KDnuggets,"Critical challenges the specialists face in these areas also vary. For instance, data scientists struggle with the problems of dirty data, difficulties in sourcing development, security issues, etc. Decisions scientists search for new ways to overcome the lack of reliable data, difficulties caused by complex data environments, and complexity of applied techniques. They should possess knowledge in math, finance, and analytics to make the right decision"
Data Science vs. Decision Science - KDnuggets,"Finally, let’s consider future trends shedding the light on further development and prospects of data science and decision science. According to our expectations, data science will continue its way towards automation, further evolution and extensive use of chatbots and virtual assistants. There will be widespread use of augmented reality elements, further robotization of industries and increasing popularity of reinforcement learning. In contrast, decision science will continue to move us towards automated decision-making and data empowerment. For sure, it is going to achieve vital importance and broad application in industries which will result in increasing demand in specialists"
Data Science vs. Decision Science - KDnuggets,"Data science can be a crucial component of decision science and quite often business owners rely on data science as on a solution to all their problems and worries. However, it is not enough to only use data science. The truth is somewhere in between data science and decision science"
Naive Bayes: A Baseline Model for Machine Learning Classification Performance - KDnuggets,Naive Bayes is a supervised Machine Learning algorithm inspired by the Bayes theorem. It works on the principles of conditional probability. Naive Bayes is a classification algorithm for binary and multi-class classification. The Naive Bayes algorithm uses the probabilities of each attribute belonging to each class to make a prediction
The Third Wave Data Scientist - KDnuggets,"There seems to be no consensus on the data science skill set. Additionally, as the field evolves, shortcomings become obvious and new challenges arise. How can we describe this evolution?"
The Third Wave Data Scientist - KDnuggets,"Test your code and use version control. Follow an established coding style (e. PEP8) and learn how to use an IDE (e. PyCharm). Try pair programming. Modularize and document your code, use meaningful variable names and refactor, refactor, refactor"
The Third Wave Data Scientist - KDnuggets,Bridge the deployment gap for agile prototyping of data products: Learn to use tools for logging and monitoring. Know how to build a REST API (e. Learn how to ship your work inside a Docker container or deploy it to a platform like Heroku
Learn About Data Science & the Future of Investing from Hedge Fund Leaders at Rev 2 - KDnuggets,"Point72 also weights its investments toward industries fueled by model-driven innovations, allocating more than 22% to healthcare, 21% to technology, and 16% to energy. The firm has expanded its geographic footprint in the last year, and recently launched operations in Sydney to focus on macro investing. The hedge fund raised more than $4 billion in outside capital in 2018, bringing total assets under management to roughly $13 billion (as of November 2018)"
Which Deep Learning Framework is Growing Fastest? - KDnuggets,"In September 2018, I compared all the major deep learning frameworks in terms of demand, usage, and popularity. TensorFlow was the champion of deep learning frameworks and PyTorch was the youngest framework. How has the landscape changed?"
Which Deep Learning Framework is Growing Fastest? - KDnuggets,"PyTorch v1.0 was pre-released in October 2018, at the same time fastai v1.0 was released. Both releases marked major milestones in the maturity of the frameworks"
Which Deep Learning Framework is Growing Fastest? - KDnuggets,"TensorFlow 2.0 alpha was released March 4, 2019. It added new features and an improved user experience. It more tightly integrates Keras as its high-level API, too"
Which Deep Learning Framework is Growing Fastest? - KDnuggets,"I won’t be exploring other deep learning frameworks in this article. I expect I will receive feedback that Caffe, Theano, MXNET, CNTK, DeepLearning4J, or Chainer deserve to be discussed. While these frameworks each have their virtues, none appear to be on a growth trajectory likely to put them near TensorFlow or PyTorch. Nor are they tightly coupled with either of those frameworks"
Which Deep Learning Framework is Growing Fastest? - KDnuggets,TensorFlow had a slightly larger increase in listings than PyTorch. Keras also saw listings growth — about half as much as TensorFlow. Fastai still isn’t showing in hardly any job listings
Which Deep Learning Framework is Growing Fastest? - KDnuggets,Web searches on the largest search engine are a gauge of popularity. I looked at search history in Google Trends over the past year. I searched for worldwide interest in the
Which Deep Learning Framework is Growing Fastest? - KDnuggets,"TensorFlow had the most GitHub activity in each category. However, PyTorch was quite close in terms of growth in watchers and contributors. Also, Fastai saw many new contributors"
Which Deep Learning Framework is Growing Fastest? - KDnuggets,"Job listings make up a little over a third of the total score. As the cliche goes, money talks. Unlike my"
Which Deep Learning Framework is Growing Fastest? - KDnuggets,"TensorFlow is both the most in demand framework and the fastest growing. It’s not going anywhere anytime soon. Its large increase in job listings is evidence of its increased usage and demand. Keras has grown a good bit in the past six months, also. Finally, fastai has grown from a low baseline. It’s worth remembering that it’s the youngest of the lot"
Which Deep Learning Framework is Growing Fastest? - KDnuggets,Tensorflow 2.0 is using Keras as its high-level API through tf. Here’s a quick getting started intro to TensorFlow 2.0 by
Which Deep Learning Framework is Growing Fastest? - KDnuggets,I’ve consistently heard that folks enjoy using PyTorch more than TensorFlow. PyTorch is more pythonic and has a more consistent API. It also has native
Which Deep Learning Framework is Growing Fastest? - KDnuggets,You’ve seen that both TensorFlow and PyTorch are growing. Both now have nice high-level APIs — tf. You’ve also heard a bit about recent developments and future directions
Interview Questions for Data Science – Three Case Interview Examples - KDnuggets,"Case interviews scare many graduates who have never seen a test that does not have an answer key. In industry, any problem worth solving does not come with an answer key. Critical thinking is the antithesis of formulaic thinking. The best way to master case interviews is to practice, practice, practice"
Interview Questions for Data Science – Three Case Interview Examples - KDnuggets,"I set three sample case interviews for you to practice. Find some friends, and collaborate on your answers. With five people, your answers should cover most possibilities. Then find a hiring manager who’s willing to give you feedback. Remember: you are judged not only on the contents but the presentation"
Interview Questions for Data Science – Three Case Interview Examples - KDnuggets,"If you are asking for an answer key, you’ve missed the point about case interviews. Case interviews are open-ended by design. If these questions have answer keys, then they’d be useless to assess critical thinking. The hiring manager is listening for how you approach problems, and structure the analysis"
Interview Questions for Data Science – Three Case Interview Examples - KDnuggets,"Try your answer out on a few friends, or better, a hiring manager. Then, try again. If you’re doing it right, these different attempts should move along different paths, because the interview questions are designed to be open-ended"
Interview Questions for Data Science – Three Case Interview Examples - KDnuggets,"That is exactly what every real-world data problem is like. You never have enough data, or all the right data, which means you need to make sensible assumptions, and keep moving along. A good case interview is a dialogue – you gather more information by asking your interviewer questions"
Normalization vs Standardization — Quantitative analysis - KDnuggets,"Nice results. By looking at the CV_mean column, we can see that at the moment, MLP is leading. SVM has the worst performance"
Pandas DataFrame Indexing - KDnuggets,"To start with, we create a small data frame using data from Wikipedia on the highest mountains in the world. For each mountain, we have its name, height in meters, year when it was first summitted, and the range to which it belongs. If this is your first exposure to a pandas DataFrame, each mountain and its associated information is a row, and each piece of information, for instance name or height, is a column"
Pandas DataFrame Indexing - KDnuggets,"Each column has a name associated with it, also known as a label. The labels for our columns are 'name', 'height (m)', 'summitted', and 'mountain range'. In pandas data frames, each row also has a name. By default, this label is just the row number. However, you can set one of your columns to be the index of your DataFrame, which means that its values will be used as row labels. We set the column 'name' as our index"
Pandas DataFrame Indexing - KDnuggets,"It is a common operation to pick out one of the DataFrame's columns to work on. To select a column by its label, we use the . One thing that we can do that makes our commands easy to interpret is to always include both the row index and the column index that we are interested in. In this case, we are interested in all of the rows. To show this, we use a colon. Then, to indicate the column that we're interested in we add its label. The command mountains"
Pandas DataFrame Indexing - KDnuggets,"It’s worth noting that it this command returns a Series, the data structure that pandas uses to represent a column. If instead of a Series, we just wanted an array of the numbers that are in the 'summitted' column, then we add '. This returns a numpy array containing [1953, 1954, 1955, and 1956]"
Pandas DataFrame Indexing - KDnuggets,"We don’t have to limit ourselves to a single row or single column using this method. Here, in the row position we pass a list of labels. This returns a set of rows, rather than just one"
Pandas DataFrame Indexing - KDnuggets,"We can also get a subset of the columns, by specifying the start and end column, and putting a ':' in between. In this case, 'height': 'summitted' will give us all of the columns between and including the startpoint, 'height', and the endpoint, 'summitted'. Note that this is different than numerical indexing in numpy, where the endpoint is omitted. Also, because we have already specified the name column as the index, it will also be returned in the data frame that we get back"
Pandas DataFrame Indexing - KDnuggets,"In addition, we can select rows or columns where the value meets a certain condition. In this case, we want to find the rows where the values of the 'summitted' column are greater than 1954. In the rows position, we can put any Boolean expression that has the same number of values as we have rows. We could do the same for columns if we wished"
Pandas DataFrame Indexing - KDnuggets,"As an alternative to selecting rows and columns by their labels, we can also select them by their row and column number. The ordering of the columns, and thus their positions, depends on how the data frame is initialized. The index column, our 'name' column, doesn’t get counted"
Pandas DataFrame Indexing - KDnuggets,"To select data by its position, we use the . Again, the first argument is for the rows, and the second argument is for the columns. To select all the columns in the zeroth row, we write"
Pandas DataFrame Indexing - KDnuggets,"We can also use the colon range operator to get a contiguous set of rows or columns by position. Note that unlike the . In this case, it returns only columns zero and one, and does not return column two"
Projects to Include in a Data Science Portfolio - KDnuggets,"The more carefully tailored your portfolio is to the specific jobs you’re applying for, the better the results you’re likely to get. But if you’re applying for entry-level positions, you’re probably casting a wide net, and you’re also likely to be looking at positions that require a lot of the same skills regardless of industry. If you put together a portfolio with at least one project in each of these categories, you’ll be off to an excellent start"
Projects to Include in a Data Science Portfolio - KDnuggets,"This project should show that you’re capable of building a system that can perform the same analysis on new data sets as they’re input, as well as capable of building a system that can be understood and run with relative ease by others. The simplest version of this would be well-commented code that can take data from a public, regularly-updated data set, and perform some analysis. Its"
The most desired skill in data science - KDnuggets,That’s not quite how I’d describe it. The process of developing the question requires collaboration between the data scientists who know much more about the data and the analytical tools and the business owners who know much more about the business goals and metrics. The collaboration leads to sharing of knowledge and symbiotic problem solving
The most desired skill in data science - KDnuggets,"STEM training is particularly lacking in these two aspects of critical thinking. A typical problem in a math, science or engineering class includes (a) a well-posed question, and (b) nicely-shaped data, and the student’s challenge is to figure out which formula or method can use the provided data to answer the specified question. There is no need to develop the question further; in fact, any student trying to change the question will be penalized! There is also no need to question the data. If the data should be questioned, then the problem will have no single correct answer, which doesn’t fit well with traditional academic STEM training"
The most desired skill in data science - KDnuggets,"Analysts at the National Highway Traffic Safety Administration (NHTSA) failed to notice gaping holes in the data submitted by Tesla when they endorsed Tesla’s claim that the auto-pilot feature would reduce crash rates by 40 percent. An independent consultant succeeded in getting the data released, and noticed a large number of blank entries. When the missing values were imputed using a standard method (mean imputation), the reported benefit of auto-pilot vanished"
The problem with data science job postings - KDnuggets,"Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people aren’t talking about it"
The problem with data science job postings - KDnuggets,"This is not good, for several reasons. First, a misleading job description means that recruiters get a *ton* of irrelevant applications, and that candidates waste a *ton* of time applying to irrelevant positions. But there’s another problem: job descriptions are the training labels that any good aspiring data scientist will use to prioritize their personal and technical skills development"
The problem with data science job postings - KDnuggets,"There are many reasons. For one, companies make hiring decisions based on a candidate’s (perceived) ability to solve a real problem that they actually have. Because there are many ways to solve any given data science problem, it can be hard to narrow down the job description to a specific set of technical skills or libraries"
2019 Best Masters in Data Science and Analytics – Online - KDnuggets,"Back in 2017, we ran a series of articles looking at the best of these degrees in America, Europe and Online. Luckily for you, we’ve now updated these comprehensive lists and added lots more masters programs. The first of these lists has already been published -"
2019 Best Masters in Data Science and Analytics – Online - KDnuggets,"They provide individual CS rankings for top 50 schools, and then ranges (51-100, 101-150, etc) for the next 450 schools. To separate schools that have the same range of rankings, we added the Top Universities ranking for the divided by 10 to the start of the range. Of course, this ranking isn’t 100% scientific in how it is calculated, but it does give a good idea of the schools that offer these courses"
Was it Worth Studying a Data Science Masters? - KDnuggets,"It was the spring of 2016 and I was coming towards the end of a 6 month internship at one of the largest consulting firms in the City of London. I had taken this role to gain experience and figure out whether becoming an Actuary was the correct route for my career. I quickly found passion in the data analytics of the role as I was being pulled into meetings to discuss numbers I had crunched or was able hack together a tool to automate previously manual tasks. However, I also found that the traditional track I would be heading on if I moved onto the graduate scheme no longer interested me due to years of standardised exams and little to no creativity. Furthermore, most of my work at this point was within Excel and I was gaining little to no coding experience"
Was it Worth Studying a Data Science Masters? - KDnuggets,"I had come from a background in mathematics and, due to the nature of the job market in the UK, had been nudged towards the well-founded traditional roles such as accountancy and actuarial consulting. And yet, here was a new role that defied all the expectations I had set for myself of my future career. Where becoming an Actuary I would be solving problems by learning regulatory standards, Data Scientists are encouraged to creatively find solutions that fit within the commercial environment. Furthermore, the role opportunities were no longer fixed into a few select firms but almost all companies were looking for some variation of a Data Scientist and the idea that I could move into a completely new industry, from fashion to finance, greatly appealed to my interests"
Was it Worth Studying a Data Science Masters? - KDnuggets,"I considered online boot camps but they were often fixed in their content and I was unsure of how repeating someone else’s work would be received by employers. Furthermore, there was no guarantee that these were credible to employers and were expensive to self fund. Today, only three years later, the list of courses supported by universities makes this much more of a viable option and is something that is definitely worth considering but at the time these were lacking. Unfortunately, boot camps were a costly risk that I was uncertain would pay off"
Was it Worth Studying a Data Science Masters? - KDnuggets,"Therefore, I decided to look for the options available at universities. At the time there were two types of courses that fit within my goals; business analysts courses and computer science machine learning. The former focused on applying analytics within commercial environments but, as this was run through business schools, was far more expensive at over £25,000 for one year of studying. The latter provided the teaching through academic research and focused more on teaching the underlying theory than the application. Furthermore, as this was an academic course run through the Computer Science department, the cost was considerably less for the year at £9,000 (for UK citizen)"
Was it Worth Studying a Data Science Masters? - KDnuggets,"The first term consisted of the three main topics of Data Science: fundamentals of data science, machine learning and visualisations. Each module consisted of a coursework component that we were given a choice of any publicly available dataset to apply our newly learned methods on. With these, I was quickly able to improving my coding skills and even built the confidence to start sharing"
Was it Worth Studying a Data Science Masters? - KDnuggets,"In the second term we had two core modules, Big Data and Neural Computing, and were given the choice of two optional modules. The list of options was comprehensive and enabled us to pick specialisms from computer vision to data architectures. I chose Data Visualisations (a continuation of the first term’s module) and Software Agents (the basics of AI by applying Reinforcement Learning). Again, these modules included coursework and with the fundamentals from the first term, I was really able to expand my applications and think creatively. Big Data also introduced text data and natural language processing"
Was it Worth Studying a Data Science Masters? - KDnuggets,"Over the two terms, I had been given a broad overview of most of the Data Science topics and had a deep knowledge of Machine Learning and Neural Networks from the core modules. As we moved into the first component of the course, the dissertation, we were given the choice to complete this whilst in an internship role (and be provided an extension on the deadline to account for balancing whilst working). I found a suitable role, defined my research topic and over the following months applied all the skills I had gained so far towards"
Was it Worth Studying a Data Science Masters? - KDnuggets,"I had two goals to achieve; to demonstrate that I understand machine learning and apply these with coding. The course not only provided a clear ‘box tick’ against these on my CV but enabled me to continue to expand my skills after with more and more interesting projects. Part of any job application is to get past the initial checks and I was now doing this much more consistently. Furthermore, as I moved into interview stages, I had all these project to discuss and truly demonstrate confidence in my understanding far more than I would have achieved on my own"
Was it Worth Studying a Data Science Masters? - KDnuggets,"The Masters opened up all the doors that I had previously been knocking on and even had recruiters contacting me directly following the projects I had posted publicly. In the end, I found that I enjoyed the research aspect of my dissertation and the freedom to pursue the field and have since move onto a PhD in Artificial Intelligence. Ironically, this is the last thing I would have considered back in 2016 but as the field is constantly expanding it is incredible to be at the forefront of this, particularly because many problems require an applied mindset and fit within commercial problems and are not simply theoretical"
How To Go Into Data Science: Ultimate Q&A for Aspiring Data Scientists with Serious Guides - KDnuggets,You’re a self-motivated person who is very passionate about data science and bringing values to companies by solving complex problems. Great. But you have ZERO experience in data science and have no clue how to get started in this field
How To Go Into Data Science: Ultimate Q&A for Aspiring Data Scientists with Serious Guides - KDnuggets,I’ll be very honest with you. To learn ALL the skills sets in data science is next to impossible as the scope is way too wide. There’ll always be some skills (technical/non-technical) that data scientists don’t know or haven’t learned as different businesses require different skill sets
How To Go Into Data Science: Ultimate Q&A for Aspiring Data Scientists with Serious Guides - KDnuggets,"Business problems give you WHAT and WHY. To solve a business problem, one has to first how to solve the problem. And the HOW comes from technical skills. Again, the approach depends on situation and my suggestion is mainly based on personal experience"
How To Go Into Data Science: Ultimate Q&A for Aspiring Data Scientists with Serious Guides - KDnuggets,"We could easily lose our focus by getting overwhelmed with all the advice and resources (Online courses, workshops, webinars, meetups, you name it…) that come from different directions. Stay focused. Know what you have and what you need and ALL IN"
How To Go Into Data Science: Ultimate Q&A for Aspiring Data Scientists with Serious Guides - KDnuggets,"I hope my sharing has answered the burning questions for you. Whenever you face any obstacles in your data science journey, remember that you’re not alone and we’re all here to help as a part of the community. Just ping me up and I’ll be more than happy to help!"
How To Go Into Data Science: Ultimate Q&A for Aspiring Data Scientists with Serious Guides - KDnuggets,"No action is too small to make a difference. Just move forward one step at a time. When you’re on the verge of giving up, PERSISTENCE is key"
Unleash a faster Python on your data - KDnuggets,"Intel’s optimized Python packages deliver quick repeatable results compared to standard Python packages. Intel offers optimized Scikit-learn, Numpy, and SciPy to help data scientists get rapid results on their Intel® hardware. Download now"
What’s Going to Happen this Year in the Data World - KDnuggets,"There are lots of avances in automation regarding machine learning, deep learning and deployment. But as I said before data is an important asset (maybe the most important one) for companies right now. So before you can apply machine learning or deep learning, at all, you need to have it, know what you have, understand it, govern it, clean it, analyze it, standardize it (maybe more) and then you can think of using it"
What’s Going to Happen this Year in the Data World - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Data Science in the Senses - KDnuggets,"That’s on Thursday, May 23 in NYC. We’ll be showcasing some amazing projects that leverage data and machine learning for sensory experiences. Here’s a sneak peek at what you’ll experience:"
Data Science in the Senses - KDnuggets,"Now that definitely sparks joy. Their works are the result of big data, some text analytics, probability, and ample doses of irony, which Botnik performs as a live show, sometimes even as karaoke-style AI-augmented antics. At our"
Data Science in the Senses - KDnuggets,"Rumor has it that Botnik and folk RNN have been collaborating based on an introduction made through Rev conference. One uses AI to generate new music scores, while the other writes lyrics. We’re eager to see what follows!"
2019 Best Masters in Data Science and Analytics – Europe Edition - KDnuggets,"April 2019), for simplicity and generality and the fees shown are for the 2019-20 academic year. We’ve also included a World University Computer Science ranking where applicable. The programs listed here are all campus-based, i"
The 3 Biggest Mistakes on Learning Data Science - KDnuggets,"The time has come. We’ve created a new field, or something like that. There’s a lot of things to say and study in this field. It doesn’t matter the name, maybe data science is just a temporary name for a bigger field, but the scientific study of data, getting insights from it and then be able to predict something"
The 3 Biggest Mistakes on Learning Data Science - KDnuggets,If we don’t follow those basic principles it would be very hard to conduct a proper data science practice. Data science should be implemented in a way that enables decision making to follow a systematic process. But more on that later
The 3 Biggest Mistakes on Learning Data Science - KDnuggets,"Well you are wasting you time. Everything you learn, even though if the professor doesn’t tell you, practice and try it. This is fundamental to really comprehend things and when you are working in the field you will be doing a lot of different practical stuff"
The 3 Biggest Mistakes on Learning Data Science - KDnuggets,"A good knowledge on statistics, math and python won’t make you a successful data scientist. You need more, you need to master your craft. Be able to use these tools to solve business problems. So if you are learning something new, and you want to understand it for real, find a scenario where you can apply it or play with it"
The 3 Biggest Mistakes on Learning Data Science - KDnuggets,"Modeling is something very important in the machine learning and data science space, but they must have a purpose. And you have to understand them before using them. Now what they assume from the data before training it, understand the different metrics they use to learn, how to evaluate them and more"
The 3 Biggest Mistakes on Learning Data Science - KDnuggets,"I’m not saying here that you need to start a course with your BFF but make use of what the online platforms give us today. We have forums, chats, discussion boards and more where you can meet people learning the same things you are learning. It will be much easier to learn with more people, and don’t be afraid on asking questions"
The 3 Biggest Mistakes on Learning Data Science - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Join the new generation of AI technologists - KDnuggets,"Copyright © 2018 The Innovation Enterprise Ltd. All Rights Reserved. Innovation Enterprise Ltd is a division of Argyle Executive Forum. Registered in England and Wales, Company Registered Number 6982151, 131 Finsbury Pavement London EC2A 1NT"
Attention Craving RNNS: Building Up To Transformer Networks - KDnuggets,"RNNs let us model sequences in neural networks. While there are other ways of modeling sequences, RNNs are particularly useful. RNNs come in two flavors, LSTMs (Hochreiter et al, 1997) and GRUs (Cho et al, 2014)"
Attention Craving RNNS: Building Up To Transformer Networks - KDnuggets,"RNNs let us model sequences in neural networks. While there are other ways of modeling sequences, RNNs are particularly useful. RNNs come in two flavors, LSTM's"
How Optimization Works - KDnuggets,"There is a best temperature for tea. If your tea is too hot, it will scald your tongue and you won't be able to taste anything for three days. If it’s lukewarm, it’s entirely unsatisfying. There is a sweet spot in the middle where it is comfortably hot, warming you from the inside out all the way down your throat and radiating through your belly. This is the ideal temperature for tea"
How Optimization Works - KDnuggets,"Finding how to get things just right turns out to be a very common problem. Mathematicians and computer scientist love it because it’s very specific and well formulated. You know when you’ve got it right, and you can compare your solution against others to see who got it right faster"
How Optimization Works - KDnuggets,"When a computer scientist tries to find the right temperature for tea, the first thing they do is flip the problem upside down. Instead of trying to maximize tea drinking enjoyment, they try to minimize suffering while drinking tea. The result is the same, and the math works out in the same way. It's not that all computer scientists are pessimists, it's just that most optimization problems are naturally described in terms of costs - money, time, resources - rather than benefits. In math it's convenient to make all your problems look the same before you work out a solution, so that you can just solve it the one time"
How Optimization Works - KDnuggets,"In machine learning, this cost is often called an error function, because error is the undesirable thing, the suffering, that is being minimized. It can also be called a cost function, a loss function, or an energy function. They all mean pretty much the same thing"
How Optimization Works - KDnuggets,"There are a handful of ways to go about finding the best temperature for serving tea. The most obvious is just to look at the curve and pick the lowest point. Unfortunately, we don't actually know what the curve is when we start out. That is implicit in the optimization problem"
How Optimization Works - KDnuggets,"But we can make use of our original idea and just measure the curve. We can prepare a cup of tea at a given temperature, serve it, and ask our unwitting test subject how they enjoyed it. Then we can repeat this process for every temperature across the whole range we care about. By the time we're done with this, we do know what the whole curve looks like, and then we can just pick temperature for which our tea drinker reported the most enjoyment, that is, the least suffering"
How Optimization Works - KDnuggets,"This way of finding the best tea temperature is called exhaustive search. It is straightforward and effective, but may take a while. If our time is limited, it's worth it to check out a few other methods"
How Optimization Works - KDnuggets,"To use gradient descent we start at an arbitrary temperature. Before beginning, we don't know anything about our curve, so we make a random guess. We brew a cup of tea at that temperature and see how well our tea drinker likes it"
How Optimization Works - KDnuggets,"From there, the next trick is to figure out which direction is downhill and which is up. To figure this out, we choose a direction, and choose a new temperature a very small distance away. Let's say we choose a temperature to the left. Then we brew up another cup of tea at this slightly cooler temperature and see whether or not it is better than the first. We discover that it is actually inferior. Now we know that ""downhill"" is to the right - that we need to make our next cup warmer to make it better"
How Optimization Works - KDnuggets,"There are lots of gradient descent methods. Most of them are clever ways to measure the slope as efficiently as possible and to get to the bottom of the bowl in as few steps as possible - to brew as few cups of tea as we can get away with. They use different tricks to avoid completely calculating the slope or to choose a step size that is as large as can be gotten away with, but the underlying intuition is the same"
How Optimization Works - KDnuggets,"Curvature, this slope-of-the-slope or Hessian, to give it its rightful name, can be very helpful if you are trying to take as few steps as possible, however it can also be much more expensive to compute. This is a trade-off that comes up a lot in optimization. We end up choosing between the number of steps we have to take and how hard it is to compute where the next step should be"
How Optimization Works - KDnuggets,"There are a lot of ways that this drop-a-marble approach can fail. If there is more than one valley for a marble to roll into, we might miss the deepest one. Each of these little bowls is called a local minimum. We are interested in finding the global minimum, the deepest of all the bowls. Imagine that we are testing our tea temperatures on a hot day. It may be that once tea becomes cold enough, it makes a great iced tea, which is even more popular. We would never find that out by gradient descent alone"
How Optimization Works - KDnuggets,"If the error function is not smooth, there are lots of places a marble could get stuck. This could happen if our tea drinkers' enjoyment was heavily impacted by passing trains. The periodic occurrence of trains could introduce a wiggle into our data"
How Optimization Works - KDnuggets,"If the error function you are trying to optimize makes discrete jumps, that presents a challenge. Marbles don't roll down stairs well. This could happen if our tea drinkers have to rate their enjoyment on a 10-point scale"
How Optimization Works - KDnuggets,"If we suspect that our tea satisfaction curve has any of these tricky characteristics, we can always fall back to exhaustive search. Unfortunately, exhaustive search takes an extremely long time for a lot of problems. Luckily for us, there is a middle ground. There is a set of methods that is tougher than gradient descent. They go by names like genetic algorithms, evolutionary algorithms, and simulated annealing. They take longer to compute than gradient descent, and they take more steps, but they don't break nearly so easily. Each has its own quirks, but one characteristic that most of them share is a randomness to their steps and jumps. This helps them discover the deepest valleys of the error function, even when they are difficult to find"
How Optimization Works - KDnuggets,"Optimization algorithms that rely gradient descent are like Formula One race cars. They are extremely fast and efficient, but require a very well behaved track (error function) to work well. A poorly-placed speed bump could wreck it. The more robust methods are like four-wheel-drive pickup trucks. They don't go nearly as fast, but they can handle a lot more variability in the terrain. And exhaustive search is like traveling on foot. You can get absolutely anywhere, but it may take you"
DATAx San Francisco | 14-15 May | Over 500 Data Professionals - KDnuggets,"Just a few weeks left until DATAx San Francisco, May 14 & 15. Find out about a number of our stand out sessions taking place at the biggest data festival on the West Coast. Secure your ticket to DATAx San Francisco on May 14 & 15 now!"
How to Recognize a Good Data Scientist Job From a Bad One - KDnuggets,"Ideally, a data science job description should be specific and include language that indicates a familiarity with data science terms and the applications used for the work. If it's vague, you should worry that perhaps the company merely wants to hire a data scientist because they have the impression they should. In other words, since their competitors are hiring data scientists, they should follow suit"
How to Recognize a Good Data Scientist Job From a Bad One - KDnuggets,"That means you'll demonstrate your skills in a scenario that may mimic the kind of work you'll do for the company. It's also possible the test will involve showing your coding skills. Data scientists use various programming languages, including R and Python, so you should be prepared to go through a coding assessment, too"
How to Recognize a Good Data Scientist Job From a Bad One - KDnuggets,"Maybe you're interviewing for a data science job 10 days before earning your degree, or you might work at a place where you haven't provided your two-week notice. In instances like those, a decent employer will allow you to tie up loose ends without feeling under pressure. However, if a company insists they need to hire you as soon as possible, that could be a red flag for several reasons"
How to Recognize a Good Data Scientist Job From a Bad One - KDnuggets,"First, the company may be rushing the hiring process, which could mean you get hired for a job that doesn't suit you. Second, they might not have thought through the position itself. In that case, you could enter a chaotic environment with little direction or leadership"
How to Recognize a Good Data Scientist Job From a Bad One - KDnuggets,"An arguably worse situation is if the employer tries to make you feel guilty for not starting immediately. They may even say something like, ""Well, you're really leaving us in a bind, especially since the person you'd replace left so suddenly. Was it because they disliked the job?"
How to Recognize a Good Data Scientist Job From a Bad One - KDnuggets,"However, some companies want to get into data science without having a relevant team first. You should always thoroughly research any company before going for an interview. Doing that often starts by going to the enterprise's website"
How to Recognize a Good Data Scientist Job From a Bad One - KDnuggets,"Keep in mind that you may see some things as positive that others deem as downsides. For example, maybe someone worked as a data scientist for the company that offered you a job and said they had to do too much independent work. If you're a highly motivated person who doesn't require a lot of direction, that could be a plus for you"
Another 10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,Our previous collections of free machine learning and data science courses was well received. Here are another 10 courses to help with your spring learning season. Courses range from introductory machine learning to deep learning to natural language processing and beyond
Another 10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,0 preview may contain bugs and may not behave exactly like the final 2.0 release. Hopefully this code will run fine once TF 2 is out. This is extreme bleeding edge stuff people! :)
Another 10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,"This course will cover two areas of deep learning in which labeled data is not required: Deep Generative Models and Self-supervised Learning. Recent advances in generative models have made it possible to realistically model high-dimensional raw data such as natural images, audio waveforms and text corpora. Strides in self-supervised learning have started to close the gap between supervised representation learning and unsupervised representation learning in terms of fine-tuning to unseen tasks. This course will cover the theoretical foundations of these topics as well as their newly enabled applications"
Another 10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,"This class provides a practical introduction to deep learning, including theoretical motivations and how to implement it in practice. As part of the course we will cover multilayer perceptrons, backpropagation, automatic differentiation, and stochastic gradient descent. Moreover, we introduce convolutional networks for image processing, starting from the simple LeNet to more recent architectures such as ResNet for highly accurate models. Secondly, we discuss sequence models and recurrent networks, such as LSTMs, GRU, and the attention mechanism. Throughout the course we emphasize efficient implementation, optimization and scalability, e. The goal of the course is to provide both a good understanding and good ability to build modern nonparametric estimators"
Another 10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,"The course introduces students to the design of algorithms that enable machines to learn based on reinforcements. In contrast to supervised learning where machines learn from examples that include the correct decision and unsupervised learning where machines discover patterns in the data, reinforcement learning allows machines to learn from partial, implicit and delayed feedback. This is particularly useful in sequential decision making tasks where a machine repeatedly interacts with the environment or users. Applications of reinforcement learning include robotic control, autonomous vehicles, game playing, conversational agents, assistive technologies, computational finance, operations research, etc"
Another 10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,"This is an applied course; each class period will be divided between a short lecture and in-class lab work using Jupyter notebooks (roughly 50% each). Students will be programming extensively during class, and will work in groups with other students and the instructors. Students must prepare for each class and submit preparatory materials before class; attendance in class is required"
Another 10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,"This is a collection of course material from various courses that I've taught on machine learning at UBC, including material from over 80 lectures covering a large number of topics related to machine learning. The notation is fairly consistent across the topics which makes it easier to see relationships, and the topics are meant to be gone through in order (with the difficulty slowly increasing and concepts being defined at their first occurrence). I'm putting this in one place in case people find it useful for educational purposes"
Northwestern’s MS in Data Science - KDnuggets,Advance your data-driven career with an online MS in Data Science at Northwestern. You’ll learn from an accomplished faculty of leading industry experts. You can choose from a wide range of specializations and electives to suit your goals
"Grow your data career at DataScienceGO, San Diego, Sep 27-29 - KDnuggets","For newcomers, it’s a place to learn the current data landscape and get your bearings; for practitioners and managers, it’s a chance to meet other experts and explore new workflows and techniques. Use DSGO’s exclusive executive mentorship exercise to find mentors whose expertise can fuel your future ideas. And gain the technical and conceptual insights you need to succeed in your data career at DSGO with a mix of business use cases, technical talks, keynotes, and workshops presented by top leaders in the data science industry. Your next career move awaits you at DSGO"
Data Science with Optimus Part 2: Setting your DataOps Environment - KDnuggets,"Breaking down data science with Python, Spark and Optimus. Today: Data Operations for Data Science. Here we’ll learn to set-up Git, Travis CI and DVC for our project"
Data Science with Optimus Part 2: Setting your DataOps Environment - KDnuggets,"DataOps can accelerate the ability of data-analytics teams to create and publish new analytics to users. It requires an Agile mindset and must also be supported by an automated platform which incorporates existing tools into a DataOps development pipeline. DataOps spans the entire analytic process, from data acquisition to insight delivery"
Data Science with Optimus Part 2: Setting your DataOps Environment - KDnuggets,"Travis CI (Continuous Integration) is my favorite CI tool. Continuous Integration is the practice of merging in small code changes frequently, rather than merging in a large change at the end of a development cycle. The goal is to build healthier software by developing and testing in smaller increments"
Data Science with Optimus Part 2: Setting your DataOps Environment - KDnuggets,"The hidden concept here is automatic testing of what you are doing. When we are programming we are doing a lot of stuff all the time, we are testing new things, trying new libraries and more, and it’s not uncommon to mess things up. CI helps you with that because you will begin doing your work, commit a little bit of it with Git, and you should have the necessary tests to see if the new piece of code or analysis you made impacts (in a good or bar way) your project"
Data Science with Optimus Part 2: Setting your DataOps Environment - KDnuggets,"Ok so right now this is empty because we don’t have anything to test yet. That’s fine, will get to that in following articles. But right now we need to built the basic file that will trigger “travis builds”"
Data Science with Optimus Part 2: Setting your DataOps Environment - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"The understanding of the data value for optimization and improvement of gaming makes specialists search for new ways to apply data science and its benefits in the gaming business. Therefore, various specific data science use cases appear. Here is our list of the most efficient and widely applied data science use cases in gaming"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"The understanding of the data value for optimization and improvement of gaming makes specialists search for new ways to apply data science and its benefits in the gaming business. Therefore, various specific data science use cases appear. Here is our list of the most efficient and widely applied data science use cases in gaming"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"One of the most exciting applications of data science in gaming is its use in the game development process. The whole idea of the game, its functionality, and design play a critical part in keeping the player engaged and interested in playing. Insights gained from gaming data are very much appreciated in this case. A game should be regarded as a kind mechanism which performance may be measured, and as a result, it may be tuned according to the need of a customer. Data science is utilized to build models, to analyze and identify optimization points, make predictions and empower machine learning algorithms, identify patterns and trends to guide service maps and improve gaming models"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Game monetization is an essential factor in the general increase in revenues. Developing a well-designed, engaging and popular game requires a lot of time, money, and finance. Thus, companies principal goal is to make this game profitable for them. In this regard, there exist three significant models of video games subscription: pay-to-play, free-to-play, and freemium. In any case, big data analytics tools will help you make sure your game is profitable for you"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Game design has turned into art with the fast development of modern technologies. Moreover, game design has become an incredibly popular area to build a successful career for developers. It is a complex process requiring various programming, visualization, and animation skills"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Application of the marvelous visual effects is no longer to keep the players engaged. Gaming data insights along with the developers' creativity help to create an interactive and complex scenario for the games. The insights from the gaming analytics are used to obtain the specific knowledge of what the player wants, to predict the gaming bottlenecks, reasoning, and timing. New game concepts, storylines, and mechanics are designed using the data gained previously"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Realistic graphics, application of artificial intelligence, and pushing the limits of graphics realism are now among key activities of game developers and designers. Image recognition technologies are predicted to revolutionize gaming industry. Along with the object detection models, they are used by the developers to create natural change of scenes and movement in the space of the gaming reality"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"For instance, these models are often used to differentiate the players belonging to different teams and to give commands to the specific character among the group. Distinction between forms, objects, obstacles, and figures becomes easier and much faster for the player. Moreover, object identification models and algorithms are used to identify body movements in order to transmit and reflect these actions on the screen for interactive games"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Throughout the video games development, a vast number of computer graphic techniques were invented. The rise of modern technologies also caused a huge advancement in the mechanisms used for the creation of visual effects in gaming. Among them, there is motion capture in games, real-time rendering, photogrammetry, and many others"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Modern video games developers try to use advanced algorithms to push the visual boundaries of the game. Real-time rendering techniques are used for this purpose.  Photogrammetry, in its turn, involves taking photographic data and converting into engaging, realistic digital models"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Personalized marketing is actively applied in various industries to avoid useless, annoying and ineffective advertisement. Both the marketers and game developers are interested in highly targeted interaction with customers and leads by creation of meaningful marketing messages and delivering them to proper people. In any case, video games providers collect data that will help to appeal to the audience better"
Top 8 Data Science Use Cases in Gaming - KDnuggets,Personalized marketing in gaming helps to increase the activity of the users and at the same time attract new ones. This may be achieved due to precise tailoring of the advertising message. To assure your ads are perceived correctly you need to know which players are ad responsive and which are not
Top 8 Data Science Use Cases in Gaming - KDnuggets,"All the action and decisions in the world of gaming are fast. A high speed of all these processes presents a matter of high interest for the fraudsters. Thus, the companies face the need to prevent fraudulent activity, yet to keep the level of customer satisfaction high enough. Security matters are challenging in all industries"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Various solutions for verification of the players are widely applied in gaming. The matter is those game developers are obliged to use players verification by law. Also, various verification techniques allow detecting doubtful accounts and action at early stages. Moreover, these techniques are used to avoid identity theft, which is quite widespread in the virtual world of gaming"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Payment fraud is also quite often in gaming. The fraudsters tend to create special bots to get the information necessary for payment. Therefore, gaming companies need to assure a high level of security to the player's personal information and transactions performed"
Top 8 Data Science Use Cases in Gaming - KDnuggets,Machine learning algorithms come to the rescue of gaming companies. Their application allows fast identification of suspicious activity. They make fraud detection much more automated and efficient due to the amount of data they can process
Top 8 Data Science Use Cases in Gaming - KDnuggets,Gaming industry proves to be very successful financially. Its growing popularity results in the attraction of new players every few seconds. Millions of people play video games actively every day and all over the world. All these people leave a significant amount of valuable data that may become of great use for the game developers. Social data and customer data analysis are essential to understand the customer's perception of the games and develop effective product strategies
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Social and customer data analysis allows the video industry to get customers insights into their attitude towards the brand and predict customers purchase decisions and brand loyalty. The so-called social network games form multiplayer online networks. Within these networks players actively interact. This interaction often results in the creation of separate social communities based on competition and interest in achieving the same goals. Customer data, in its turn, contains feedbacks along with the data that helps to segment the audience and tailor the product better"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"The gaming industry has been growing exponentially. The number of active users tends to increase every minute and so does the overall income of the companies developing games. The inner infrastructure of the games gets even more complex providing more opportunities for players. An entirely new world and realities are created for the users. Top level visualization and design techniques, the latest visual effects, graphic elements and augmented reality effects provide customers with a high level of satisfaction"
Top 8 Data Science Use Cases in Gaming - KDnuggets,"Data science has entered various industries and improved the principles of their functioning forever. It has brought various businesses to a qualitatively new level of their development. The industry of gaming is no exception here. Moreover, data science techniques and methodologies have become integral parts of games development, design, operation, and many other stages of their functioning"
How to DIY Your Data Science Education - KDnuggets,"After doing that and feeling like you've gained a thorough understanding of it, consider learning R. It's another programming language often used in data science. Swirl offers"
How to DIY Your Data Science Education - KDnuggets,"One of the great things about the internet is that it offers people countless opportunities to learn at a speed that suits them. You can sign up for a data science course and work through it in a way that matches your schedule. For example, Udemy has"
How to DIY Your Data Science Education - KDnuggets,"Plan to clear your calendar now to attend at least one data science event. You'll likely be surprised by how much you learn in such a short time. Plus, you'll have a deeper understanding of how data science applies to other fields, letting you keep up with trends and developments"
How to DIY Your Data Science Education - KDnuggets,"YouTube is an excellent resource for furthering your data science studies. Be careful which videos you choose, especially since some of them are merely ads for courses and don't give you the learning content. However, you still have plenty of possibilities to watch"
How to DIY Your Data Science Education - KDnuggets,"Investigate to find out if your community has regular meetings of data scientists and learners. The MeetUp website is a fantastic place to start that could reveal new opportunities to you. For example,"
How to DIY Your Data Science Education - KDnuggets,"You may choose to go through the sections of this list in any order. At some point, though, you need to put all the learning into practice. The best way to do that is to get engrossed in a data science project"
Top 10 Coding Mistakes Made by Data Scientists - KDnuggets,"A data scientist is a ""person who is better at statistics than any software engineer and better at software engineering than any statistician"". Many data scientists have a statistics background and little experience with software engineering. I'm a senior data scientist ranked top 1% on Stackoverflow for python coding and work with a lot of (junior) data scientists. Here is my list of 10 common mistakes I frequently see"
Top 10 Coding Mistakes Made by Data Scientists - KDnuggets,"Data science needs code AND data. So for someone else to be able to reproduce your results, they need to have access to the data. Seems basic but a lot of people forget to share the data with their code"
Top 10 Coding Mistakes Made by Data Scientists - KDnuggets,"I get it, you're in a hurry to produce some analysis. You hack things together to get results to your client or boss. Then a week later they come back and say ""can you change xyz"" or ""can you update this please"". You look at your code and can't remember why you did what you did. And now imagine someone else has to run it"
Top 10 Coding Mistakes Made by Data Scientists - KDnuggets,"Back data, it's DATA science after all. Just like functions and for loops, CSVs and pickle files are commonly used but they are actually not very good. CSVs don't include a schema so everyone has to parse numbers and dates again. Pickles solve that but only work in python and are not compressed. Both are not good formats to store large datasets"
Top 10 Coding Mistakes Made by Data Scientists - KDnuggets,"Lets conclude with a controversial one: jupyter notebooks are as common as CSVs. A lot of people use them. That doesn't make them good. Jupyter notebooks promote a lot of bad software engineering habits mentioned above, notably:"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,"But as the amount of these posts grows larger, it requires a bit more work on the part of the reader to seek out, sift through, and process all of the available information. This post is designed to make it a little easier for aspiring data scientists to find all of the excellent advice out there from experts in the field. The majority of the ideas below are condensed from the following 6 posts that I found especially helpful:"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,"First, I went through each article and plucked out each individual insight or piece of advice. Then I looked over the list of ideas and made note of any common themes among the different resources, seen below. Later on in this post, I include all of the other pieces of advice that stood alone. Let’s get to it"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,"This was probably the most popular theme of them all. The importance of communication in data science is often harped on, and for good reason. Sifting through data to"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,"As data scientists, we make assumptions constantly, whether we know it or not. These assumptions might be related to the data we’re working with or the problem that we’re trying to tackle, but they need to be questioned. By keeping some level of paranoia about our outputs, we ensure that we’re on the right track. This skill is often associated with exploratory, research-oriented work, but it’s much more applicable than you think"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,"Similar to the last point, it also pays to be curious. Curiosity can lead you to interesting insights that you would never have found otherwise. It drives you to adopt a"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,"Whether it’s through blog posts, projects, tweets, or something else — it doesn’t really matter. What matters is that you are putting something out there. The tweet below pretty much sums up my stance on this:"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,"When working on learning projects, make sure you’re interested in the topic. This seems pretty straightforward, but plenty of aspiring data scientists get caught up trying to produce the project that seems the most complex or impressive to would-be employees and colleagues. Stick to what you enjoy and use real-world data instead of super-clean"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,"It’s clear that data science is a broad, complex field. You could spend your whole working life practicing it and not even skim the surface. There’s always going to be another technique to master, another tool to learn, and another paper to read. This is why"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,"Along the same lines, just because you can’t master every tool out there doesn’t mean that you shouldn’t master some of them. There will probably be a couple of building blocks that you spend most of your day working with. That might be R, SQL, Vim, Airflow, Scikit-learn, anything really. It doesn’t matter as long as you hone in on your critical tools and learn them well"
Compilation of Advice for New and Aspiring Data Scientists - KDnuggets,The journey to data scientist isn’t an easy one. Starting out as a data scientist is no difference. But the beauty of information sharing makes things that much easier. It lets us learn from those that came before us. I think that’s a pretty cool thing. Pass it on and enjoy the ride
The Deep Learning Toolset — An Overview - KDnuggets,"Every problem worth solving needs great tools for support. Deep learning is no exception. If anything, it is a realm in which good tooling will become ever more important over the coming years. We are still in the relatively early days of the deep learning supernova, with many deep learning engineers and enthusiasts hacking their own way into efficient processes. However, we are also observing an increasing number of great tools that help facilitate the intricate process that is deep learning, making it both more accessible and more efficient. As deep learning is steadily spreading from the work of researchers and pundits into a broader field of both DL enthusiasts looking to move into the field (accessibility), and growing engineering teams that are looking to streamline their processes and reduce complexity (efficiency), we have put together an overview of the best DL tools"
The Deep Learning Toolset — An Overview - KDnuggets,"The first step towards any deep learning application is sourcing the right data. Sometimes you are lucky and have historical data readily available. Sometimes you need to search for open-source datasets, scrape the web, buy the raw data or use a simulated dataset. Since this step is often very specific to the application at hand we did not include it in our tooling landscape. Please note, however, that there are websites like Google’s"
The Deep Learning Toolset — An Overview - KDnuggets,"Most supervised deep learning applications deal with images, videos, text or audio, and before you train your model you need to annotate this raw data with ground-truth labels. This can be a cost-intensive and time-consuming task. In an ideal setup, this process is interwoven with model training and deployment and tries to leverage your trained deep learning models (even if their performance is not perfect yet) as much as possible"
The Deep Learning Toolset — An Overview - KDnuggets,"This step is relevant for both model training and deployment: getting access to the right hardware. When moving from local development to large-scale experiments during model training, your hardware needs to scale appropriately. The same goes for scaling according to user demand when your model is deployed"
The Deep Learning Toolset — An Overview - KDnuggets,"There would be no point in training a neural network if you could not distinguish between a good and a bad model. During model evaluation you usually choose one metric to optimize for (while possibly observing many others). For this metric you try to find the best performing model that generalizes from your training data to the validation data. This involves keeping track of the different experiments (with possibly differing hyperparameters, architectures and datasets) and their performance metrics, visualizing outputs of the trained models and comparing experiments to each other. Without the right tooling, this can quickly become convoluted and confusing especially when collaborating with multiple engineers on the same deep learning pipeline"
The Deep Learning Toolset — An Overview - KDnuggets,"Note: The flowchart already hints at the circular nature of the typical deep learning workflow. In fact, treating the feedback loop between deployed models and new labels (often called human-in-the-loop) as a first-class citizen in your deep learning workflow can be one of the most important success factors for many applications. In real-life deep learning work things are often more complicated than the flowchart suggests. You will find yourself jumping over steps (e"
Best Practices for Using Notebooks for Data Science - KDnuggets,"Like every other story, a data science notebook follows a certain structure that is typical for its genre. Usually there are four parts - (1) It starts with defining a data set, (2) continue to clean and prepare the data, (3) perform some modeling using the data and (4) interpret the results. In essence, a notebook should record an explanation of why experiments were initiated, how they were performed and then display the results"
Best Practices for Using Notebooks for Data Science - KDnuggets,"A notebook segments a computation in individual steps called paragraphs. A paragraph contains an input and an output section. Each paragraph executes separately and modifies the global state of the notebook. State can be defined as the ensemble of all relevant variables, memories, and registers. Paragraphs must not contain computations, but can contain text or visualizations to illustrate the workings of the code"
Best Practices for Using Notebooks for Data Science - KDnuggets,"The power of the notebook roots in its ability to segment and then slow down computation. Common executions of computer programs are done at machine speed. Machine speed suggests that when a program is submitted to the processor for execution, it will run from start to end as fast as possible and only block for IO or user input. Consequently, the state of the program changes so fast that it is neither observable nor modifiable by humans. Programmers would typically attach debuggers to stop programs during execution at so-called breakpoints and read out and analyze their state. Thus, they would slow down execution to human speed"
Best Practices for Using Notebooks for Data Science - KDnuggets,"Notebooks make interrogating the state more explicit. Certain paragraphs are dedicated to make progress in the computation, i. Moreover, it is possible to rewind state during execution by overwriting certain variables. It is also simple to kill the current execution, thereby deleting the state and starting anew"
Best Practices for Using Notebooks for Data Science - KDnuggets,"Notebooks increase productivity, by facilitating incremental improvement. It is cheap to modify code and rerun only the relevant paragraph. So when developing a notebook, the user builds up state and then iterates on that state until progress is made. Running a stand-alone program on the contrary will incur more setup time and might be prone to side-effects. A notebook will most likely keep all its state in the working memory whereas every new execution of a stand-alone program will need to build up the state on every time it is run"
Best Practices for Using Notebooks for Data Science - KDnuggets,"This takes more time and the required IO operations might fail. Iterating on a program state in the memory proved to be very efficient. This is particularly true for data scientists, as their programs usually deal with a large amount of data that has to be loaded in and out of memory as well as computations that can be time-consuming"
Best Practices for Using Notebooks for Data Science - KDnuggets,"A notebook contains a complete record of procedures, data, and thoughts to pass on to other people. For that purpose, they need to be focused. Although it is tempting to put everything in one place, this might be confusing for readers. Better write two or more notebooks than overloading a single notebook"
Best Practices for Using Notebooks for Data Science - KDnuggets,A common source of confusion is when program state gets passed on between paragraphs through hidden variables. The set of variables that represent the interface between two subsequent paragraphs should be made explicit. Referencing variables from other paragraphs than the previous one should be avoided
Best Practices for Using Notebooks for Data Science - KDnuggets,"A notebook integrates code, it is not a tool for code development. The tool for code development is an Integrated Development Environment (IDE). Therefore, a notebook should one contain glue code and maybe one core algorithm. All other code should be developed in an IDE, unit tested, version controlled, and then imported via libraries into the notebook. Modularity and all other good software engineering practices are still valid in notebooks. As in practice number one too much code clutters the notebook and distracts from the original purpose or analysis goal"
Best Practices for Using Notebooks for Data Science - KDnuggets,"Notebooks are meant to be shared and read by others. Others might not have an easy time following our thought process, if we did not come up with good, self-explaining names. Tidying up the code goes a long way, too. Notebooks impose an even higher standard than traditional code on quality"
Best Practices for Using Notebooks for Data Science - KDnuggets,"A picture is worth a thousand words. A diagram, however, will need some words to label axes, describe lines and dots, and comprehend other important informations such sample size, etc. A reader can have a hard time to seize the proportion or importance of a diagram without that information. Also keep in mind that diagrams are easily copy-pasted from the notebook into other documents or in chats. Then they lose the context of the notebook in which they were developed"
Best Practices for Using Notebooks for Data Science - KDnuggets,Bottom line here - The segmentation of a thought process is what fuels the power of the notebook. Facilitating incremental improvements when iterating on a problem boosts productivity. A notebook should provide a stat-of-the-art user experience coupled with access to machine learning frameworks to unlock the value of data
Best Practices for Using Notebooks for Data Science - KDnuggets,"Previously, he spent many happy years as a researcher in academia and industry. His interests are machine learning, security and the internet of things. Armin holds PhD and MSc degrees from Technical University Vienna, Austria and he was a Marie Curie Fellow at University of California, Berkeley"
10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,"6.0002 is the continuation of 6.0001 Introduction to Computer Science and Programming in Python and is intended for students with little or no programming experience. It aims to provide students with an understanding of the role computation can play in solving problems and to help students, regardless of their major, feel justifiably confident of their ability to write small programs that allow them to accomplish useful goals. The class uses the Python 3.5 programming language"
10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,"This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include: supervised learning (generative/discriminative learning, parametric/non-parametric learning, neural networks, support vector machines); unsupervised learning (clustering, dimensionality reduction, kernel methods); learning theory (bias/variance tradeoffs; VC theory; large margins); reinforcement learning and adaptive control. The course will also discuss recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing"
10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,"An introductory course on deep learning methods with applications to machine translation, image recognition, game playing, image generation and more. A collaborative course incorporating labs in TensorFlow and peer brainstorming along with lectures. Course concludes with project proposals with feedback from staff and panel of industry sponsors"
10 Free Must-See Courses for Machine Learning and Data Science - KDnuggets,Welcome to the 2018 edition of fast. Learn how to build state of the art models without needing graduate-level math—but also without dumbing anything down. Oh one other thing
Data Science “Paint by the Numbers” with the Hypothesis Development Canvas - KDnuggets,"Well, the design world is applying the “Paint by the Numbers” concept using design canvases.  A design canvas outlines what’s important given the subject area, and then allows the “painter” to color in the right information.  A design canvas is a one-page operational template that is designed to capture all of the different perspectives necessary for successful execution depending upon the problem being solved.  A great example of a canvas is the"
Data Science “Paint by the Numbers” with the Hypothesis Development Canvas - KDnuggets,"Now to complete the loop, I introduce the Machine Learning Canvas.  I stumbled upon the Machine Learning Canvas v0.4 from Louis Dorard at the web site “"
Data Science with Optimus Part 1: Intro - KDnuggets,"Optimus V2 was created to make data cleaning a breeze. The API was designed to be super easy for newcomers and very familiar for people that come from working with pandas. Optimus expands the Spark DataFrame functionality, adding"
Data Science with Optimus Part 1: Intro - KDnuggets,"It’s super easy to use. It’s like the evolution of pandas, with a piece of dplyr, joined by Keras and Spark. The code you create with Optimus will work on your local machine, and with a simple change of masters, it can run on your local cluster or in the cloud"
Data Science with Optimus Part 1: Intro - KDnuggets,Creating a data science environment should be easy. Both for trying stuff and for production. When I was starting thinking on these series of articles I was shocked to find out how hard is to prepare a reproducible environment for data science with free tools
Data Science with Optimus Part 1: Intro - KDnuggets,"With this tool you have a free environment for Python (with JupyterLab) and for R (with R Studio), also tools for presenting like Shiny and Bokeh, and much more. And for free. You will be able to run everything in the repo:"
Data Science with Optimus Part 1: Intro - KDnuggets,"He has a passion for science, philosophy, programming, and music. He is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
"Make Your Own Job in Data Science: A High-Risk, High-Reward Approach - KDnuggets",Find a challenge that you think you can help with. Come up with a solution. That’s the best advice that I can give somebody. That’s a way to stand out
"Make Your Own Job in Data Science: A High-Risk, High-Reward Approach - KDnuggets","It requires a lot of prep time: you’ve got to conduct extensive research on the company in question, identify a real problem, and do a little data science project to solve that problem in some way. Then you’ve got to find the right person to contact and communicate your work to them very quickly and clearly. If any part of that goes wrong, you’ll have spent a"
How Data Science Is Improving Higher Education - KDnuggets,"Temple University in Philadelphia has been using data to increase its graduation rate. It found students that were moderately low-income and received partial Pell Grants were most likely to drop out, even compared the lowest-income students who received full Pell Grants. It also discovered that students who had four years of a foreign language in high school were less likely to drop out"
How Data Science Is Improving Higher Education - KDnuggets,"The secret to the school's success is a data-backed advising system. This program uses an algorithm to monitor student performance. If the system detects a student's work is beginning to suffer, their advisor will reach out to them to provide assistance. The school also offers small grants to students who find themselves struggling to pay tuition"
The Most in Demand Skills for Data Scientists - KDnuggets,I scoured job listing websites to find which skills are most in demand for data scientists. I looked at general data science skills and at specific languages and tools separately. I searched job listings on
The Most in Demand Skills for Data Scientists - KDnuggets,Terms with over 400 listings on LinkedIn for general skills and over 200 listings for specific technologies were included in the final analyses. There was certainly some cross posting. The results are recorded in this
The Most in Demand Skills for Data Scientists - KDnuggets,The results show that analysis and machine learning are at the heart of data scientist jobs. Gleaning insights from data is a primary function of data science. Machine learning is all about creating systems to predict performance and it is very in demand
The Most in Demand Skills for Data Scientists - KDnuggets,"AI and deep learning don’t show up as frequently as some other terms. However, they are subsets of machine learning. Deep learning is being used for more and more of the machine learning tasks that other algorithms were used for previously. For example, the best machine learning algorithms for most natural language processing problems are now deep learning algorithms. I expect deep learning skills will be sought more explicitly in the future and that machine learning will become more synonymous with deep learning"
The Most in Demand Skills for Data Scientists - KDnuggets,"These tools have considerably less written about them on Medium and in tutorials than many others. I expect many fewer job candidates have these skills than Python, R, and SQL. If you have or can gain experience with Hadoop and Spark it should give you a leg up on the competition"
The Most in Demand Skills for Data Scientists - KDnuggets,"The results are fairly similar. Both my analysis and GlassDoor’s found Python, R, and SQL to be the most in demand. We also found the same top nine technology skills, albeit in slightly different orders"
Are you buying an apartment? How to hack competition in the real estate market - KDnuggets,"In the last couple of years, real estate companies have shifted their focus to the digital world, and now almost all investments have an online system showing what apartments are available. This is very convenient for their potential clients, as they can easily become familiar with the apartments on offer. Things become interesting when all available data is monitored on a weekly basis, and sales progress is analysed"
Are you buying an apartment? How to hack competition in the real estate market - KDnuggets,"The monitored investment is built in the eastern Warsaw district of Praga Południe. This is an area with great potential. Currently, there are a lot of apartment buildings and places waiting to be renovated. However, it is very well connected with the city centre (15 minutes by public transport or 10 minutes by car). There are also a lot of facilities nearby, such as shopping malls, parks, schools, or medical service points"
Are you buying an apartment? How to hack competition in the real estate market - KDnuggets,"Another interesting observation is about apartments that had sold, but later became available again. There are multiple reasons why this happens. It is important to understand how the selling process works. The buyer has to sign an agreement with the developer before requesting a loan from a bank. There is always a possibility that after officially signing for an apartment, the bank can reject your loan request. The buyer can also change their mind and withdraw for other reasons"
Are you buying an apartment? How to hack competition in the real estate market - KDnuggets,"This system didn’t have any captcha or anomaly detection protection. The only impediment here is that the content is loaded dynamically using JavaScript, and there is no open API endpoint where we can just request the data. You are able to view the data only after clicking through the interface"
Are you buying an apartment? How to hack competition in the real estate market - KDnuggets,"As a private investor, I need to excel in my investments. There is no place for bad bets, which is why I use all available cards in the deck to make sure I do everything in my data science power. This analysis gave me the information about the investment’s financial performance, and I could predict how much time I had to close the deal for my dream apartment. I had more time for the final decision, and I was more confident during negotiations. Remember that buying an apartment at the latest possible moment gives you short-term alternatives, instead of freezing your capital right away"
KDnuggets Offer: Save 20% on Strata in London - KDnuggets,"Strata Data Conference is coming to London Apr 29-May 2. Discover what's coming in data and AI. Save 20% on Gold, Silver, and Bronze passes with code KDNU (up to £231 on a Gold pass)"
New Jobs Sure to Emerge Alongside Artificial Intelligence - KDnuggets,"Many of the most important uses for AI are directly related to new problems created by the development of digital industries. Big data is one example of this relationship. As our technology improved, our ability to generate new kinds of data in much larger amounts outpaced our ability to analyze it effectively. From the"
New Jobs Sure to Emerge Alongside Artificial Intelligence - KDnuggets,"In fact, AI is also being used in recruitment to help narrow down large pools of applicants to the best candidates, which enables recruiters to spend less time sifting through resumes and more focusing on the absolute best match for the position. Just like replacing horses with tractors on the farm, AI is enabling people in many professions to be more efficient and productive. The time spent sifting through resumes makes recruiters better at their jobs — it doesn’t make their jobs obsolete"
New Jobs Sure to Emerge Alongside Artificial Intelligence - KDnuggets,"A lot of top minds in tech are doomsaying about AI, and others are much more optimistic. Some industries will certainly be hit harder than others, and that may require major political changes and ethical discussions. Still, the future may well be optimistic, and AI may be set to"
Graphs Are The Next Frontier In Data Science - KDnuggets,"It is worth noting the community and user growth of Neo4j. I spoke to a participant who attended the first conference 6 years prior who shared that back then about 50 to 100 people attended. This year, over 1000 people attended"
Graphs Are The Next Frontier In Data Science - KDnuggets,Adobe has a creative cloud.  Users can follow other creatives. Used to use noSQL. Was very resource intensive
Graphs Are The Next Frontier In Data Science - KDnuggets,"This was a private event to bring together the top Neo4j open source contributors, influencers and inspirational community leaders. Discussions were held in a smaller room. It was an intimate and casual discussion with Neo4j executives"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,"In the example we were discussing, we were limited to analysis of a single variable i. However to get more meaningful insights we have to connect other variables layer by layer to the initial variable which we have analysed to get more insights on the problem. As far as battery is concerned some of the critical variables other than conductance are voltage and discharge. Let us connect these two variables along with the conductance profile to gain more intuitions from the data"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,The above figure is a plot which depicts three variables across the same time span. The idea of plotting multiple variables together across a common time span is to unearth any discernible trends we can see together. A cursory look at this plot will reveal some obvious observations
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,"Having made some observations,we now need to ascertain whether these observations can be codified to some definitive trends. This can be verified only by observing plots for many samples of similar variables. By sampling data pertaining to many batteries if we can get similar observations, then we can be sure that we have unearthed some trends explaining behaviors of different variables. However just unearthing some trends will not suffice. We have to get some intuitions from such trends which will help in transforming the raw variables to some form which will help in the modelling task. This is achieved by feature engineering the raw variables"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,"Many a times the given set of raw variables will not suffice for extracting the required predictive power from the model. We will have to transform the raw variables to generate new variables giving us the extra thrust towards better predictive metrics. What transformation has to be done, will be based on the intuitions we build during the exploratory analysis phase and also by combining domain knowledge. For the case of batteries let us revisit some of the intuitions we build during the exploratory analysis phase and see how these intuitions we build can be used for feature engineering"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,"So a probable feature we can extract from the conductance variable is the slope of the data points over a fixed time span.The rationale for such a feature is this, if precipitous fall in conductance over time is an indicator of failing health of a battery  then the slope of data points for a battery which is failing will be more steeper than the battery which is healthy. It was observed that through such transformation there was a positive influence on predictive metrics. The dynamics of such transformation is as follows, if we have conductance data for the battery for three years, we can take consecutive three month window of conductance data and take the slope of all the data points and make it as a feature.  By doing this, the number of rows of data for the variable also gets consolidated to much fewer numbers"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,"Let us also look at another example of feature engineering which we can introduce to the variable, discharge voltage. As seen from the above figure, the discharge voltage follows a wave like profile. It turns out that when a battery discharges the voltage first drops and then it rises. This behavior is called the “Coupe De Fouet” (CDF) effect. Now our thought should be, how do we combine the observed wave like pattern and the knowledge about CDF into a feature ? Again we have to dig into domain knowledge. As per theory on the state of health of batteries there are standards for the CDF profile of a healthy battery and that of a failing battery. These are prescribed by the manufacturer of the battery. For example the manufacturing standards prescribe certain depth to which the voltage will fall during discharge and certain height to which it will go up during a typical CDF effect. The deviance between the observed CDF and the manufacture prescribed standard can be taken as another feature. Similarly we can also think of other features related to voltage, like depth of discharge ( DOD), number of cycles etc. Our focus should be in using the available domain knowledge to transform raw variables into features"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,As seen from the above two examples the essence of feature engineering is all about translating the domain knowledge and the trends seen in the data to more meaningful features. The veracity of the models which are built depends a lot on the strength of  the features built. Now that we have seen the feature engineering phase let us now look at modelling strategy for this use case
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,"Since the use case is to predict which battery would fail and at what period of time, we have to look back in time from the failure point label for creating different classes related to periods of failure. In this specific case, the different features were created by consolidating 3 months of data into a single row. So one period before failure would denote 3 months before failure. So if the requirement is to predict failure 6 months prior to when it is likely to happen, then we will have 4 different classes i. All periods prior to 6 months can be labelled as normal state"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,"With respect to modelling, we can spot check with different classification algorithms ( logistic regression, Naive bayes, SVM, Random Forest, XGboost . The choice of final model will be based on the accuracy metrics ( sensitivity , specificity etc) of the spot checked models. Another aspect which might be useful to note is also that, data set could be highly unbalanced i. It will be a good idea to try out class balancing methods on the data set before modelling"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,This post brings down curtains to the three part series on predictive analytics for industrial batteries. Any use case within the manufacturing sector can be quite challenging as the variables involved are very technical and would require lot of interventions from related domain teams. Constant engagement of domain specialist as part of the data science team is very important for the success of such projects
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 3 - KDnuggets,I have tried my best to write the nuances of such a difficult use case. I have tried to cover the critical elements in the process. In case of any clarifications on the use case and details of its implementation you can connect with me through the following email id bayesianquest@gmail. Looking forward to hearing from you.  Till then let me sign off
Calling All Data Geeks to Come Home and Ignite Their Powers - KDnuggets,"GOJEK is a super app based in Indonesia (the country’s first and fastest-growing unicorn!), and we have offices in more than 5 countries in Asia. We offer more than 20 services, from ride-hailing and food delivery to digital wallet, on-demand cleaning, and on-demand pampering and massage. Our app has been downloaded more than 125 million times and we will continue to grow and expand to other countries!"
Calling All Data Geeks to Come Home and Ignite Their Powers - KDnuggets,"As home of the #DataAvengers, GOJEK is looking for people whose worlds revolve around data. We’re calling all data geeks, data aficionado, data enthusiasts to come home and put their power to good use. If you love solving business optimization problems, coming up with algorithmic models or turning data into actionable insights, you may find a home with us"
"Will Models Rule the World? Data Science Salon Miami, Nov 6-7 - KDnuggets","But despite the power and wide adoption of models, there are still some limitations. Ghosh goes further, “Biases in the data consequently bias models and decisions. And it can’t disrupt - creative decisions still need the human brain"
Using Confusion Matrices to Quantify the Cost of Being Wrong - KDnuggets,"There are so many confusing and sometimes even counter-intuitive concepts in statistics.  I mean, come on…even explaining the differences between Null Hypothesis and Alternative Hypothesis can be an ordeal.  All I want to do is to understand and quantify the cost of my analytical models being wrong"
Using Confusion Matrices to Quantify the Cost of Being Wrong - KDnuggets,"Now let’s get back to our shepherd example.  We want to determine the costs of the model being wrong, or the savings the neural network provides.  We need to determine if the there is sufficient improvement in what the model provides over what the shepherd already does himself"
Using Confusion Matrices to Quantify the Cost of Being Wrong - KDnuggets,Not all Type I and Type II errors are of equal value.  One needs to invest the time to understand the costs of Type I and Type II errors in relationship to your specific case.  The real challenge is determining whether the improvement in performance from the analytic model is “good enough
How To Learn Data Science If You’re Broke - KDnuggets,"Over the last year, I taught myself data science. I learned from hundreds of online resources and studied 6–8 hours every day. All while working for minimum wage at a day-care"
How To Learn Data Science If You’re Broke - KDnuggets,"In the following article, I give guidelines and advice so you can make your own data science curriculum. I hope to give others the tools to begin their own educational journey. So they can begin to work towards a more passionate career in data science"
How To Learn Data Science If You’re Broke - KDnuggets,Programming is a fundamental skill of data scientists. Get comfortable with the syntax of Python. Understand how to run a python program many different ways
How To Learn Data Science If You’re Broke - KDnuggets,If you love making discoveries about the world. If you are fascinated by artificial intelligence. Then you can break into the data science industry no matter what your situation is
How To Learn Data Science If You’re Broke - KDnuggets,"Over the last 9 months, Harrison left hisjob, started studying machine learning full time, and enrolled in a Master's Program in Computer Science. Harrison is doing all of this because his passion and goal is implementing machine learning applications in the real world. This means a strong understanding of predictive modeling and production environments"
Why do I Call Myself a Data Scientist? - KDnuggets,"After the important distinction between Knowledge and Truth by Descartes, we can mark the beginning of science as we know it. But the scientists in the old era did not call themselves “scientists” back them. If you see the Principia by Newton:"
Why do I Call Myself a Data Scientist? - KDnuggets,"He mentions that he talks about Natural Philosophy. Not Physics. Actually that was the name of Physics back them. And while the concept of science was a thing in Newton’s time, he consider himself more a philosopher than a scientist. The philosopher of nature"
Why do I Call Myself a Data Scientist? - KDnuggets,"So, Newton, one of the most important physicists of history, did not study Physics in a formal school, he actually studied math, philosophy and more. But we call him a physicists too. I don’t believe he proclaimed himself a physicist, we did that for him, but in the end is what we think he is"
Why do I Call Myself a Data Scientist? - KDnuggets,"And that’s what I did. I studied, a lot, about data science (actually I did not know what was Data Science when I started learning all this things that took me to the field), machine learning, deep learning, programming, and more. From courses, self-study, projects and then I learned a lot from my data-related jobs"
Why do I Call Myself a Data Scientist? - KDnuggets,"Also, I’m talking about the scientific method because I think we should think of Data Science as a modern Science. I’m more than happy to discuss this with you, just reach me. You can listen and read more about that here:"
Why do I Call Myself a Data Scientist? - KDnuggets,"What you’ll do day by day will have tremendous impact in your organization, so you better do it well. You have to be prepared to fail, and fail again. To have an open mind, to be able to criticize yourself and your ideas"
Why do I Call Myself a Data Scientist? - KDnuggets,"He has a passion for science, philosophy, programming, and music. Right now he is working on data science, machine learning and big data as the Principal Data Scientist at Oxxo. Also, he is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
Bring Essential Data Governance Training to Your Team - KDnuggets,"Technology trends such as big data, cloud, self-service, and agile challenge traditional data governance practices. TDWI Onsite Education sends top-rated instructors to teach the skills you need at your location. Discounts available for training scheduled prior to Dec 31, 2018"
We Sized Washington’s Edible Marijuana Market Using AI - KDnuggets,"We were given a 60-gigabyte dataset on seed-to-sale transactions, including 10 million sales of edible marijuana products. We needed to estimate the quantity of THC sold in all these products, but all the values for THC content among edibles were missing. The information we received looked a bit like this [1]:"
We Sized Washington’s Edible Marijuana Market Using AI - KDnuggets,"He judged that these weights referred to the quantities of THC in the product [2]. These weights might appear as “10mg”, “10 mg”, “0.01 grams” or in numerous other forms, but to the human eye, the implication is clear: products with such labels contain 10 milligrams of THC"
We Sized Washington’s Edible Marijuana Market Using AI - KDnuggets,"For the other 5.6 million sales, the product names didn’t give us a crutch. The simplest way to deal with this would be to assume that these other transactions have the same amount of THC per sale as the former 44% had. If we could scrape THC estimates from 2 million solid edibles, each of which contained 1 mg of THC, and there were 1 million solid edibles left, then we might say that solid edibles were linked to 3 million mg of THC (2 million x 1 mg + 1 million x 1 mg)"
We Sized Washington’s Edible Marijuana Market Using AI - KDnuggets,"So, we thought we could do better by just exploiting price and other numbers in the data. Products that are more expensive probably have more of the main active chemicals, including THC. And by taking sale times and household incomes surrounding each store into account, we controlled for two key trends: how prices for THC decline over time and how high-income consumers inflate them. This regression-based model did better —it had an error of 20 mg THC per transaction — but there was still plenty of uncertainty"
We Sized Washington’s Edible Marijuana Market Using AI - KDnuggets,"In the data table above, it is not unreasonable to guess that “PButter_Triples_10mg” and “PButter_Triples” are likely similar products, potentially with similar THC content. They share much of the same name. Likewise, “PButter_Triples” is probably pretty different from “Tincture Wintermint” — they don’t have similar names. To exploit this information, we calculated the"
We Sized Washington’s Edible Marijuana Market Using AI - KDnuggets,"Using all the available information — price, sale times, local incomes, product names, potency, and all other variables — our AI created a “random forest” that tried to minimize error in predicting THC amounts per sale. And with a cross-validated error of just 5 mg THC per transaction, its performance far-exceeded that of all previous models. It proved to be the best way to estimate the THC content in each of the remaining 5.6 million transactions"
We Sized Washington’s Edible Marijuana Market Using AI - KDnuggets,"But with our estimates’ granularity, they give policymakers the capability to answer deeper questions on how THC legally gets to consumers. Exactly which products are consumers using to get high? We see that solid edibles dominate liquid edibles. However, with just 0.3 metric tons of THC legally sold through edibles from July 2016 through June 2017, they represent a small portion of the 24 metric tons of THC sold in WA’s licit market in the same time frame"
We Sized Washington’s Edible Marijuana Market Using AI - KDnuggets,"Are edibles a cheap way to get THC? Apparently not — their price per unit of THC is about nine times as high as that of usable marijuana products. Is the demand for pot entirely fulfilled by sales of licit products? Nope — there’s a big gap. [5] If not, is there a black market satisfying it? Maybe"
We Sized Washington’s Edible Marijuana Market Using AI - KDnuggets,"We’ve illuminated just another piece of the puzzle on the industry’s scale and its character. Going abroad, the picture that we’ve painted here just might foretell how Canada’s market will change over the years. And I am sure data scientists will be watching it too"
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"The frontiers between software and hardware are fascinating. We, Data Scientists, are used to develop Machine Learning algorithms for business purposes, and frequently the outputs of these models are measured in terms of profit, cost savings and even intangible outputs, like customer satisfaction. Of course we can visualize the outputs of our models using dashboards like Power BI, Tableau or in the cloud, with Kibana, Quick Sight or Tago. But it’s quite rewarding to see outcomes of our models in the physical world, outside a screen"
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"I’ve been working with IoT for finance and telecommunications. One can easily generate alerts in dashboards regarding malfunctions in a given system, like saturation of BTSs (base transceiver station), the nodes where telecommunications signals pass through. Now imagine that besides of an alert in a dashboard, we automatically redirect telecom data traffic to a different anthem so that Point-of-Sales (POS) remain connected all the time. This would leverage acquirers’ profit and merchants’ satisfaction. We may even adjust settings of a machine in the real world"
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"In IoT projects we work close to hardware. Devices like POS,  power and water meters, cane grinders, Libelium sets (for agriculture), elevators, health appliances send telemetry data that can be analyzed in order to develop Predictive Maintenance models, so that businesses can cut costs and increasy efficiency, at the same time they optimize logistics. In these cases, connectivity can be achieved using SimCards, Wi-Fi, Bluetooth or radio signals (LoRa)"
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"One of the parts of the IoT solution is the gateway. It intermediates the communication between the device (temperature, humidity sensor) and the cloud. In my previous post I explained how to turn your notebook into an IoT device, providing details on how to send data to AWS, from the device to the dashboard, in an end-to-end solution ("
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"In this post, I will explain how to run an IoT project from the command line, without graphical interface, using Ubuntu Core in a Raspberry Pi 3. An easier way is to use Raspbian, that provides a graphical interface for your projects. Raspbian is a Debian-based computer operating system. However, I decided to try Ubuntu Core, given that security should be a main concern in IoT. Ubuntu Core can be used for digital signage, robotics, drones and edge gateways. A more challenging and secure decision"
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"Ubuntu Core is a lightweight system, with “security at its heart”. It is composed of snaps, “self contained, isolated and protected bits of code”. It’s quite difficult to alter a read-only file in the system, even using ESC :w !sudo tee % at vim command"
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"As it is an ARM system, it’s quite different to install Machine Learning libraries in it, since there are some specific packages for this type of system and lots of dependencies are not included, not even “wget”. So let’s start. When you buy a Raspberry, it comes without the energy source (top right below) and without the SDCard (prominence at right). Model 3 B comes with 4 USB entries and an ethernet entry. It also contains the HDMI, audio and camera entries. In this project we will not use the mouse, and monitor, ethernet cable and keyboard are only used in the first run of the system, to set up the internet connection"
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"This will take a couple of seconds in the first time it runs, and you will be directed to the network configuration. Configure DHCP and your Wi-Fi username and password. Then Ubuntu Core will provide you with the address you are supposed to ssh"
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"The -vvv command will bring you details about the ssh connection. When connecting, a window will open asking for your password. You are in and the login will change:"
Raspberry Pi IoT Projects for Fun and Profit - KDnuggets,"In Ubuntu Core, is not that simple to generate and edit notebooks. You cannot open Jupyter, Spyder or even Notepad and create a new file, as there is no graphical interface. You have to use the command “touch” to create a file:"
Diversity in Data Science: Overview and Strategy - KDnuggets,"Many recent articles have highlighted the shortage of women and underrepresented minorities in the tech industry, as well as problems associated with the “Bro Culture” of Silicon Valley. However, few articles have offered suggestions other than hiring practices as a way to increase diversity. Just how dire is this problem, and what can be done about it, other than suggesting a cultural change initiated by teams that are largely comprised of the exact same backgrounds?"
Diversity in Data Science: Overview and Strategy - KDnuggets,"Approximately one third of the tech industry is female, and approximately one sixth is underrepresented minorities (Hispanic or African American). Management is approximately five sixths Caucasian. Large tech companies such as Google or Apple report even less diversity among their workforces, and within tech-specific positions, the industry is 24% women and 5% underrepresented minorities. On development teams, women make up as few as 13% of roles, and women in technical positions are twice as likely to leave a position as men. These statistics suggest that women and underrepresented minorities are in jobs not related to technical development or leadership in tech companies"
Diversity in Data Science: Overview and Strategy - KDnuggets,"Companies can benefit in many ways from increased diversity. A recent report from McKinsey suggests that the top quarter of companies with respect to racial and ethnic diversity enjoy a 35% boost in likelihood of exceeding industry median returns compared to baseline rates. Having a female in an executive position boosted return rates for the Standard & Poor 1500 companies’ revenue by $42 million. This suggests an opportunity for boosted revenue through hiring a diverse tech team and a diverse executive team. In addition, the experiences women and minorities bring to tech teams can aid in design and cover potential blind spots in user experience and product development"
Diversity in Data Science: Overview and Strategy - KDnuggets,"However, addressing this issue is more complex than simply changing hiring practices. Women and underrepresented minorities make up disproportionately small fraction of the educational pipeline associated with tech positions; the number of women graduating with computer science degrees continues to fall (from 37% in 1984 to 18% in 2014). Given the ubiquity and ease of online training programs and MOOCs these days, it would be easy to assume that those opportunities bridge the gap. However, data science programs/bootcamps lag in female participation (35% of participants are women) and in underrepresented minority participation (4% African-American and 8% Hispanic), as well. With low diversity in academic programs and industry bootcamps, the tech position figures are unlikely that the trend will change in the near future. Even reductions in discrimination may not help if young students do not see anyone who looks like them or comes from the same socioeconomic background within a classroom"
Diversity in Data Science: Overview and Strategy - KDnuggets,"Industry conferences offer another opportunity to encourage women and minorities to pursue tech positions and career growth. Stanford’s Women in Data Science (WiDS) conference surpassed 100,000 attendees in hundreds of cities this past March, and this has allowed women to present their work to other women and to the men who attend the conference. Creating a conference similar to WiDS that highlights the tech-based accomplishments and work of African-Americans and Hispanics or those who came from low-income families might create similar visibility within the larger community that portrays tech as a viable career choice"
Diversity in Data Science: Overview and Strategy - KDnuggets,"My own path into the tech industry involved three professors—one in an introductory calculus course and two during graduate school—who were successful women in mathematics, and my first roles within the industry were for companies with involved female leaders. Without an opportunity to learn from these women, I’m not sure I would have seen leadership or technical roles as a good fit for me. Creating visibility of women and minorities in technical education/programs, particularly early on, and making those opportunities financially viable through scholarship and grant programs can increase diversity at all stages of a technical career"
Diversity in Data Science: Overview and Strategy - KDnuggets,"Visibility of role models and resources may be more effective at increasing tech diversity (and STEM diversity in general) than hiring guidelines and other equality measures. Campaigns like the UK-based WISE initiative, which focuses on helping and encouraging young women to pursue STEM careers after age 16, and Miami’s Women in Biomedical Science Initiative may be one route to achieving this. More conferences like WiDS can also aid in the visibility effort. With visible leadership by women and minorities in the field, wider knowledge of opportunities and funding available, and programs that connect a new generation of potential tech workers with those further along in their careers"
"GitHub Python Data Science Spotlight: High Level Machine Learning & NLP, Ensembles, Command Line Viz & Docker Made Easy - KDnuggets","It serves as a toolkit for regression and classification using these ensembled machines, and also for visualisation of the performance of the new machine and constituent machines. Here, when we say machine, we mean any predictor or machine learning object - it could be a LASSO regressor, or even a Neural Network. It is scikit-learn compatible and fits into the existing scikit-learn ecosystem"
6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case Study - KDnuggets,"This goes back to what I originally stated. If you don’t understand the basics, don’t tackle an algorithm from scratch. At the very least, you should be able to answer the following questions:"
6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case Study - KDnuggets,"After you have a basic understanding of the model, it’s time to start doing your research. I recommend using numerous sources. Some people learn better with textbooks, some people learn"
6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case Study - KDnuggets,"Now that we’ve gathered our sources, it’s time to start learning. Start by grabbing some paper and a pencil. Rather than read a chapter or blog post all the way through, start by skimming for section headings, and other important info. Write down bullet points, and try to outline the algorithm"
6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case Study - KDnuggets,"Breaking the algorithm up into chunks like this makes it easier to learn. Basically I’ve outlined the algorithm with pseudocode, and now I can go back and fill in the fine details. Here’s a picture of my notes for the second step, which is the dot product of the weights and inputs:"
6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case Study - KDnuggets,"After I’ve put together my notes on the algorithm, It’s time to start implementing it in code. Before I dive in to a complicated problem, I like to start with a simple example. For the Perceptron, a"
6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case Study - KDnuggets,"Now that I have a simple data set, I’ll start implementing the algorithm that I outlined in Step 3. It’s good practice to write the algorithm in chunks and test it, rather than trying to write it all in one sitting. This makes it easier to debug when you’re first starting out. Of course at the end you can go back and clean it up to make it look a little nicer"
6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case Study - KDnuggets,"At first, I didn’t get the same weights, and this is because I had to tweak the default settings in the scikit-learn Perceptron. I wasn’t implementing a new random state every time, just a fixed seed, so I had to turn this off. The same goes for shuffling, I also needed to turn that off. To match my learning rate, I changed eta0 to 0.1. Finally, I turned off the fit_intercept option. I included a dummy column of 1’s in my feature dataset, so I was already automatically fitting the intercept (aka bias term)"
6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case Study - KDnuggets,"This brings up another important point. When you validate against an existing implementation of a model, you need to be very aware of the inputs to the model. You should never blindly use a model, always question your assumptions, and exactly what each input means"
6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case Study - KDnuggets,"This last step in the process is probably the most important. You’ve just gone through all the work of learning, taking notes, writing the algorithm from scratch, and comparing it with a trusted implementation. Don’t let all that good work go to waste! Writing up the process is important for two reasons:"
A Winning Game Plan For Building Your Data Science Team - KDnuggets,"One of the most exciting challenges I have at Hitachi as the Vice-Chairmen of Hitachi’s “Data Science 部会” is to help lead the development of Hitachi’s data science capabilities. We have a target number of people who we want trained and operational by 2020, so there is definitely a sense of urgency. And I like urgency because it’s required to sweep aside the inhibitors and resistors to change"
A Winning Game Plan For Building Your Data Science Team - KDnuggets,"But that blog only addressed the Data Engineer role.  To achieve the goals for the “Data Science 部会” – which is to become more effective at leveraging data and analytics to optimize key business and operational processes, mitigate compliance and security risks, uncover new revenue opportunities and create a more compelling, differentiated user experience – we need to consider three key roles, and the interaction between those three key roles, that round out the data science community. We need to understand the responsibilities, capabilities, expectations and competencies of the Data Engineer, Data Scientist and Business Stakeholder"
A Winning Game Plan For Building Your Data Science Team - KDnuggets,"This is a great foundation that helps us understand what skills we are going to need to hire and/or develop.  However, the chart in of itself isn’t yet enough. We now need to turn this research into something actionable"
A Winning Game Plan For Building Your Data Science Team - KDnuggets,"While our goal is not to turn our Business Stakeholders into data scientists, we do want to train our Business Stakeholders to “Think like a data scientist”.  That way the Business Stakeholders can understand how best to collaborate with a data scientist and a data engineer to uncover the customer, product, service and operational insights that will drive business success.  See the blog “"
A Winning Game Plan For Building Your Data Science Team - KDnuggets,"Data science is a team sport comprised of Data Engineers, Data Scientists and Business Stakeholders.  And like a baseball team can’t function effectively with only shortstops and catchers. One’s data science initiative MUST clearly articulate the roles, responsibilities and expectations of the Data Engineers, Data Scientists and Business Stakeholders"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"In cross-validation, we do more than one split. We can do 3, 5, 10 or any K number of splits. Those splits called Folds, and there are many strategies we can create these folds with"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"Cross-Validation is an essential tool in the Data Scientist toolbox. It allows us to utilize our data better. Before I present you my five reasons to use cross-validation, I want to briefly go over what cross-validation is and show some common strategies"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"When we’re building a machine learning model using some data, we often split our data into training and validation/test sets. The training set is used to train the model, and the validation/test set is used to validate it on data it has never seen before. The classic approach is to do a simple 80%-20% split, sometimes with different values like 70%-30% or 90%-10%. In cross-validation, we do more than one split. We can do 3, 5, 10 or any K number of splits. Those splits called Folds, and there are many strategies we can create these folds with"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"If we have 3000 instances in our dataset, We split it into three parts, part 1, part 2 and part 3. We then build three different models, each model is trained on two parts and tested on the third. Our first model is trained on part 1 and 2 and tested on part 3. Our second model is trained to on part 1 and part 3 and tested on part 2 and so on"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"When we have very little data, splitting it into training and test set might leave us with a very small test set. Say we have only 100 examples, if we do a simple 80–20 split, we’ll get 20 examples in our test set. It is not enough. We can get almost any performance on this set only due to chance. The problem is even worse when we have a multi-class problem. If we have 10 classes and only 20 examples, It leaves us with only 2 examples for each class on average. Testing anything on only 2 examples can’t lead to any real conclusion"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"For each instance, we make a prediction by a model that didn’t see this example, and so we are getting 100 examples in our test set. For the multi-class problem, we get 10 examples for each class on average, and it’s much better than just 2. After we evaluated our learning algorithm (see #2 below) we are now can train our model on all our data because if our 5 models had similar performance using different train sets, we assume that by training it on all the data will get similar performance"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"As mentioned in #1, when we create five different models using our learning algorithm and test it on five different test sets, we can be more confident in our algorithm performance. When we do a single evaluation on our test set, we get only one result. This result may be because of chance or a biased test set for some reason. By training five (or ten) different models we can understand better what’s going on. Say we trained five models and we use accuracy as our measurement. We could end up in several different situations. The best scenario is that our accuracy is similar in all our folds, say 92.0, 91.5, 92.0, 92.5 and 91.8. This means that our algorithm (and our data) is consistent and we can be confident that by training it on all the data set and deploy it in production will lead to similar performance"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"5, 92.5 and 91.8. These results look very strange. It looks like one of our folds is from a different distribution, we have to go back and make sure that our data is what we think it is"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"Sometimes we want to (or have to) build a pipeline of models to solve something. Think about Neural Networks for example. We can create many layers. Each layer may use previews layer output and learn a new representation of our data so eventually, it will be able to produce good predictions. We are able to train those different layers because we use the back-propagation algorithm. Each layer computes its error and passes it back to the previous layer"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"The best solution here is to use two different datasets for each model. We train our Random Forest on dataset A. Then we use dataset B to make a prediction using it. Then we use the dataset B predictions to train our second model (the logistic regression) and finally, we use dataset C to evaluate our complete solution. We make predictions using the first model, pass them to our second model and then compare it to the ground truth"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"When we have limited data (as in most cases), we can’t really do it. Also, we can’t train both our models on the same dataset because then, our second model learns on predictions that our first model already seen. These will probably be over-fitted or at least have better results than on a different set. This means that our second algorithm is trained not on what it will be tested on. This may lead to different effects in our final evaluations that will be hard to understand"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"When we perform a random train-test split of our data, we assume that our examples are independent. That means that by knowing/seeing some instance will not help us understand other instances. However, that’s not always the case"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,Consider a speech recognition system. Our data may include different speakers saying different words. Let’s look at spoken digits recognition. In
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"The proper way to do it is to split the speakers, i. However, then we’ll test our algorithm only on one speaker. It is not enough. We need to know how our algorithm performs on different speakers"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"We can use cross-validation on the speakers level. We will train 3 models, each time using one speaker for testing and two others for training. This way we’ll be able to evaluate better our algorithm (as described above) and finally build our model on all speakers"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"This is one of the most common and obvious reasons to do cross validation. Most of the learning algorithms require some parameters tuning. It could be the number of trees in Gradient Boosting classifier, hidden layer size or activation functions in a Neural Network, type of kernel in an SVM and many more. We want to find the best parameters for our problem. We do it by trying different values and choosing the best ones. There are many methods to do this. It could be a manual search, a grid search or some more sophisticated optimization. However, in all those cases we can’t do it on our training test and not on our test set of course. We have to use a third set, a validation set"
5 Reasons Why You Should Use Cross-Validation in Your Data Science Projects - KDnuggets,"Cross-Validation is a very powerful tool. It helps us better use our data, and it gives us much more information about our algorithm performance. In complex machine learning models, it’s sometimes easy not pay enough attention and use the same data in different steps of the pipeline. This may lead to good but not real performance in most cases, or, introduce strange side effects in others. We have to pay attention that we’re confident in our models. Cross-Validation helps us when we’re dealing with non-trivial challenges in our Data Science projects"
Understand Why ODSC is the Most Recommended Conference for Applied Data Science - KDnuggets,"Running 4 days, 40 training sessions, 50 workshops, and over 200 speakers, an ODSC conference offers unparalleled depth and breadth in deep learning, machine learning, and other data science topics. Save 20% offer ends tomorrow. Register now!"
The Growing Participation of Women in the Data Science Community - KDnuggets,Dell previously performed an investigation and found a lack of workplace readiness was a substantial barrier for Middle Eastern countries wishing to be competitive in the digital age. The PNU program helps solve that issue. People in computer science must know numerous things
The Growing Participation of Women in the Data Science Community - KDnuggets,"The University of California at Berkeley established a Data Science Division in the Spring 2016 semester and enrolled 300 students then. However, enrollment numbers for the Spring 2018 semester were over 1000, making the program the fastest-growing option in Berkeley’s history. A report indicates the gender balance"
The Growing Participation of Women in the Data Science Community - KDnuggets,"Kennesaw State University (KSU) in Georgia also has a minor in applied statistics and data analysis. Representatives from the college say it stresses that people can study numerous things besides data science and still end up with that career path. At KSU, about"
The Growing Participation of Women in the Data Science Community - KDnuggets,"When girls show an interest in science, technology, engineering and math (STEM) careers during their youth, the foundations could be laid for later data science work as adults. However, they often lose interest before it’s time to choose their college courses. A study by Microsoft and KRC Research aimed to"
Datmo: the Open Source tool for tracking and reproducible Machine Learning experiments - KDnuggets,"We initially built it as an internal solution for tracking our experiments, making them reproducible, and with easy setup of environments. As we started to grow this out, we strove towards a tool that had an open and simple interface that integrated seamlessly with the way we were already doing machine learning; generic with respect to frameworks yet powerful that it provides complete reproducibility. Basically, something we could give to our friends so that they could run their experiments with a few commands on the command line and still repeat them reliably"
Datmo: the Open Source tool for tracking and reproducible Machine Learning experiments - KDnuggets,"Let’s first make sure we have the pre-requisites for datmo. Docker is the main pre-requisite, so let’s ensure docker is installed (and running!) before starting. You can find the instructions based on your OS here:"
Robust Quality – Powerful Integration of Data Science and Process Engineering - KDnuggets,"With rapid growth in data and its usage, data quality is becoming quite important. It is important to connect these two aspects of quality to ensure better performance. The book by Rajesh Jugulum provides a strong connection between the concepts in data science and process engineering that is necessary to ensure better quality levels and takes you through a systematic approach to measure holistic quality with several case studies"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"Mathematics is the bedrock of any contemporary discipline of science. It is no surprise then that, almost all the techniques of modern data science (including all of the machine learning) have some deep mathematical underpinning or the other. In this article, we discuss the essential math topics to master to become a better data scientist in all aspects"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"However, having a solid understanding of the math behind the cool algorithm you are using to create meaningful product recommendation for your users, will never hurt you. More often than not, it should give you an edge among your peers and make you more confident. It always pays to know the machinery under the hood (even at a high level) than being just the guy behind the wheel with no knowledge about the car"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"That’s a problem. There is no universal blueprint. Data science, by its very nature, is not tied to a particular"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"The importance of having a solid grasp over essential concepts of statistics and probability cannot be overstated in a discussion about data science. Many practitioners in the field actually call classical (non neural network) machine learning nothing but statistical learning. The subject is vast and endless, and therefore focused planning is critical to cover most essential concepts"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"Trust me. As a prospective data scientist, if you can master all of the concepts mentioned above, you will impress the other side of the table really fast. And you will use some concept or other pretty much every day of your job as data scientist"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"Song recommendation in Spotify. Transferring your selfie to a portrait drawing Salvador Dali style using Deep Transfer learning. What is common? Matrices and matrix algebra in all of them. This is an essential branch of mathematics to study for understanding how most machine learning algorithms work on a stream of data to create insight. Here are the essential topics to learn,"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"It lurks behind the simple looking analytical solution of ordinary least square problem in linear regression, or it is embedded in every back-propagation your neural network makes to learn a new pattern. It is an extremely valuable skill to add to your repertoire. Here are the topics to learn,"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"However, a basic understanding of these powerful techniques can be immensely fruitful in the practice of machine learning. Virtually every machine learning algorithm/technique aims to minimize some kind of estimation error subject to various constraints. That, right there, is an optimization problem. Topics to learn,"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"But logistic regression problems don’t. To understand the reason, you need to know the concept of convexity in optimization. This line of investigation will also illuminate why we have to remain satisfied with ‘approximate’ solutions in most machine learning problems. That’s a powerful truth to know deeply about"
Essential Math for Data Science:  ‘Why’ and ‘How’ - KDnuggets,"Do not need to be feel scared or lost. These are lot of things to learn and master, especially if you are not practicing them on a regular basis. But there are excellent resources online including wonderful videos. With some time and effort, you can make your own curated list of learning resource according to your personal need and level of comfort"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"Have you ever wanted to start a new project but you can’t decide what to do? First, you spend a couple hours brainstorming ideas. Then days. Before you know it, weeks have gone by without shipping anything new"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,This is extremely common for self-driven projects in all fields; data science is no different. It’s easy to have grand ambitions but much more difficult to execute on them. I’ve found the hardest part of a data science project is getting started and deciding which path to go down
5 Resources to Inspire Your Next Data Science Project - KDnuggets,Data science is an extremely diverse field; this means that it’s virtually impossible to squeeze every concept and tool into one single project. You need to pick and choose which skills you want to focus on developing further. A few relevant examples could include:
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"Basically, if you want to become a more effective machine learning engineer, chances are that you won’t accomplish that by doing a data viz project. Your project should reflect your goals. That way, even if it doesn’t blow up or uncover any groundbreaking insights, you still walk away with a win and a bunch of applied knowledge to show for it"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"Like we touched on before, side projects should be enjoyable. Whether we realize it or not, we all ask ourselves hundreds of questions a day. Try tuning into these questions for the rest of today a little more than you usually would. You’ll be surprised by what happens. You may see that you’re a bit more creative and interested in certain things that you thought"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"The possibilities are truly endless. Let your interests, curiosities, and goals drive your next project. Once you’ve checked those boxes, let’s get inspired"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"It’s easy to think that we’re on our own, but turns out that this is rarely the case. There’s always others out there with similar interests and goals if you look hard enough. This effect can be incredibly powerful for ideation"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"Find projects that you like or admire, and then put your own twist on them. Use them as jumping off points to generate new, original work that stands alone. Some of my favorite resources for inspiration are as follows:"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"I could spend hours just browsing this subreddit of data visualizations. You’ll be interested in all of the unique ideas and questions that people think up. There’s also monthly challenge where a dataset is chosen, and users are tasked with visualizing it in the most effective way possible. Sort by best all time for instant gratification"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"I would be remiss if I didn’t mention the poster child of online data science. There’s a couple ways to use Kaggle effectively for inspiration. First, you can look at the"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"It really is true that visual essays are an emerging form of journalism. The Pudding embodies this movement like none other. The team uses original datasets, primary research, and interactivity in order to explore tons of interesting topics"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,"Side projects have not only helped me out immensely throughout my development, but they’re also generally a lot of fun. Recently, there’s been more and more awesome content coming out on data science portfolios. If interested, I highly recommend checking out the following links:"
5 Resources to Inspire Your Next Data Science Project - KDnuggets,The hardest part of anything is getting started. I hope that the tips and resources above help you on your path to completing and shipping your next data science project. I’ll be on the lookout
"Interpreting a data set, beginning to end - KDnuggets","Machine learning may rescue you from some disasters, but there are times when it won’t help. At some point,the model’s accuracy has to be improved. That’s when it hits that understanding and exploring a data set is critically important"
"Interpreting a data set, beginning to end - KDnuggets","Don’t jump into modelling before digging into the data.Remember that the quality of the output depends on the quality of input.Garbage in, garbage out"
"Interpreting a data set, beginning to end - KDnuggets","Data scientists spend much of their time on data preparation before they jump into modelling, because understanding, generating and selecting useful features impacts model performance. It helps the data scientists to check assumptions required for fitting models.In addition to understanding the input data set, it’s also important to understand the data generated throughout the model building process—e"
"Interpreting a data set, beginning to end - KDnuggets","Depending on size and type of data, understanding and interpreting data sets can be challenging. It’s impossible to understand the data set and make conclusions just looking at the first or randomly selected 100 observations from millions of them. Don’t expect to plot statistics for each feature if the data has thousands of variables. It’s better to have a handful of features or a way to summarize them. And remember that each variable can’t be treated in a similar way if the data has heterogeneous variables"
"Interpreting a data set, beginning to end - KDnuggets","What can be done? Use different exploratory data analysis and visualization techniques to get a better understanding. This includes summarizing main data set characteristics, finding representative or critical points and discovering relevant features. After gaining an overall understanding of the data set, you can think about which observations and features to use in modeling"
"Interpreting a data set, beginning to end - KDnuggets","Summary statistics help to analyze information about the sample data. It indicates something about the continuous (interval) and discrete (nominal) data set variables. Analyze those variables individually or together because they can help find: unexpected values; proportion of missing values compared to the whole data set; skewness and other issues. The distribution of feature values across different features can be compared, as can feature statistics for training and test data sets. This helps uncover differences between them"
"Interpreting a data set, beginning to end - KDnuggets",Be careful about summary statistics. Excessive trust of summary statistics can hide problems in the data set. Consider using additional techniques for a full understanding
"Interpreting a data set, beginning to end - KDnuggets",Assume the data set has millions of observations with thousands of variables. It’s challenging to understand this data without any abstraction. One approach to solve this problem is to use example-based explanations; techniques that can help pick important observations and dimensions. They can help interpret highly complex big data sets with different distributions
"Interpreting a data set, beginning to end - KDnuggets","In the illustrations below, robot pictures in each category consist of robots with different head and body shapes. Robots in costumes can also belong to one of those categories, although they can be very different from a typical robot picture. Those pictures are needed to understanding the data since they are important minorities"
"Interpreting a data set, beginning to end - KDnuggets",The most important features for those selected observations must be considered. Subspace representation is a solution to that problem. Using the prototype and subspace representation helps in interpretability. One method that can be used to achieve this is
"Interpreting a data set, beginning to end - KDnuggets","In addition to understanding important features, it’s also necessary to understand differences between clusters for many applications such as differential diagnosis. For that, find distinguishing dimensions in the data. A mind the gap model (MGM) combines extractive and selective approaches and reports a global set of distinguishable dimensions to assist with further exploration"
"Interpreting a data set, beginning to end - KDnuggets","An embedding is a mapping from discrete values, such as words or observations, to vectors. Different embedding techniques help visualize lower-dimensional representation. Embeddings can be in hundreds of dimensions. The common way to understand them is to project them into two or three dimensions. They are useful for many things:"
"Interpreting a data set, beginning to end - KDnuggets",It can be used to highlight the variations and eliminate dimensions. It’s possible to retain the first few principal components that consider a significant amount of variation if needed to interpret the data.  Remaining principal components account for trivial amounts of variance. They should not be retained for interpretability and analysis
"Interpreting a data set, beginning to end - KDnuggets","It’s very useful to visualize and interpret datasets, but there are many things that require caution. While preserving local structure, it may distort global structure. If more information is needed about t-SNE, check out a great article at distill"
"Interpreting a data set, beginning to end - KDnuggets","Using t-SNE embeddings can help reduce the dimension of the data and find structures. However, if it’s a very large data set, understanding the projections can still be hard. It’s useful to check the geometry of the data to get a better understanding"
"Interpreting a data set, beginning to end - KDnuggets","Topology studies geometric features preserved when we deform the object without tearing it. Topological data analysis provides tools to study the geometric features of data using topology. This includes detecting and visualizing features, and the statistical measures related to those. Geometric features can be distinct clusters, loops and tendrils in the data. If there is a loop in this network, the conclusion is that a pattern occurs periodically"
"Interpreting a data set, beginning to end - KDnuggets","When it comes to understanding and interpreting data, there is no one solution that fits all.Pick the one that best meets your need. When there’s big raw data, use representative examples to explain the underlying distribution. If it’s a wide data set, find the important dimensions to understand the representative samples because listing all features of representative samples will be difficult to understand"
"Interpreting a data set, beginning to end - KDnuggets",Her work involves building scalable machine learning algorithms that help solve big data problems. Kabul is co-founder of the North Carolina chapter of Women in Machine Learning and Data Science. She holds a doctorate of computer science from the University of North Carolina
"Project Hydrogen, new initiative based on Apache Spark to support AI and Data Science - KDnuggets","Big data and AI are joined at the hip: the best AI applications require massive amounts of constantly updated training data to build state-of-the-art models. AI has always been one of the most exciting applications of big data and Apache Spark. In part driven by deep learning, we see Increasingly more Spark users want to integrate Spark with distributed machine learning frameworks built for state-of-the-art training"
"Project Hydrogen, new initiative based on Apache Spark to support AI and Data Science - KDnuggets","The problem is, big data frameworks like Spark and distributed deep learning frameworks don’t play well together due to the disparity between how big data jobs are executed and how deep learning jobs are executed. As an example, on Spark, each job is divided into a number of individual tasks that are independent of each other. This is called ""embarrassingly parallel,"" and this is a massively scalable way of doing data processing that can scale up to petabytes of data"
"Project Hydrogen, new initiative based on Apache Spark to support AI and Data Science - KDnuggets","However, deep learning frameworks use different execution schemes. They assume complete coordination and dependency among the tasks. What that means is this pattern is optimized for constant communication, rather than large-scale data processing to scale to petabytes of data"
"Project Hydrogen, new initiative based on Apache Spark to support AI and Data Science - KDnuggets","In this mode, the tasker schedules ""all or nothing"". Which means either all of the tasks are scheduled in one shot, or none of the tasks are scheduled at all. This actually reconciles the fundamental incompatibilities of how Spark works with the distributed ML frameworks needs"
"Project Hydrogen, new initiative based on Apache Spark to support AI and Data Science - KDnuggets","Artificial Intelligence (AI) has massive potential to drive disruptive innovations affecting most enterprises on the planet. However, most enterprises are struggling to succeed with AI​. Why is that? Simply put, AI and Data are siloed in different systems and different organizations"
"Project Hydrogen, new initiative based on Apache Spark to support AI and Data Science - KDnuggets","Very few companies have been successful at doing AI at scale and company wide. This is what we call the 1% problem. The majority of companies - the 99% - continue to struggle due to having disparate systems and technologies, and organizational divides between data engineering and data scientists. To achieve AI, organizations need to unify data and AI"
"Project Hydrogen, new initiative based on Apache Spark to support AI and Data Science - KDnuggets","Apache Spark was the first step to unify data and AI, but that alone is not enough - organizations still need to manage a lot of infrastructure. To eliminate obstacles for AI, companies have to leverage unified analytics. Unified Analytics brings together data processing with AI technologies, making AI much more achievable for enterprise organizations and enabling them to accelerate their AI initiatives. Unified Analytics makes it easier for enterprises to build data pipelines across various siloed data storage systems and to prepare labelled datasets for model building, which allows organizations to do AI on their existing data and iteratively do AI on massive data sets"
How to Set Up a Free Data Science Environment on Google Cloud - KDnuggets,"When moving from a laptop / desktop setup to a cloud data science environment, it's important to think about what criteria is the most important to you. To work with larger data sets in pandas, for example, it's important to have a large amount of memory. To work with Spark to process large data sets, we need to rent a pool of many computers with a large amount of total memory to share the load"
How to Set Up a Free Data Science Environment on Google Cloud - KDnuggets,"By making two main tweaks, we can replicate this access capability for our cloud instance as well. By default, the firewall for our cloud instance is setup to prevent incoming network access. In addition, most cloud providers change our instance's IP address often (really whenever they feel like it!). This means if our instance's current IP address is"
How to Set Up a Free Data Science Environment on Google Cloud - KDnuggets,"Now that we have a static IP address, our local computer can talk to our cloud instance. Unfortunately, most cloud providers have a firewall in place that disables incoming access to most of the ports. Thankfully, we can manually add an exception for the port"
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets","How much has data changed our lives over the past decade? Just over 10 years ago the iphone was launched. Back then, our phones took grainy photos and video was just wishful thinking. It was still weird to buy shoes over the internet and we still had to carry stacks of maps when we visited a new city. And Netflix was only a DVD company"
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets","Now, your phones take photos and videos, with more than 4,000 photos uploaded to Facebook every second and more than 400 hours of video uploaded to YouTube every minute. We worry more about having connectivity than ensuring we have a map. And our mapping apps give us real-time traffic and options to navigate through traffic. Don’t want to drive? No problem, use a ride-sharing app that leverages trillions of data points. The fundamental shift behind this radical change is a combination of massive increases in computational power, storage, and data. And of course data scientists, designers, and other technologists that make these ideas real"
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets",The transformation due to data is just starting. We’re about to go from sequencing the human genome to enabling tailored medical treatments (precision medicine). Autonomous vehicles have started to appear on our roads and we’ll see efforts to build cargo ships and airplanes. And artificial intelligence has shown new ways to think about games as they’ve beat the best humans
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets","At the same time, we’ve seen data used to cause harm though a combination of negligence, naivety, and sophisticated attacks. From the U.S. And we only beginning to come to grips with the social impacts due to job displacement from automation"
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets","And hearings from the U.S. Congress (with no action). There have been books that have highlighted the risks ahead, such as"
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets",I’m a fan of these efforts. They are very much needed. The question I want to ask is: what about the data scientist and the rest of the team that are responsible for building these technologies? What is their role in implementing “good” data science? (
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets","Given how much we expect to change on this topic, we think of this book like an open source project and this is the 0.1 release. We’ve also make sure that it will always be"
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets","Ideas about how to implement more ethical behaviors in product development process including a dissent channel if you disagree with the team. How we can start interviewing talent for cultural fit as well as ethical fit. Also what we call the 5 C’s — five framing guidelines help us think about building data products (consent, clarity, consistency, control & transparency, and consequences & harm). Finally, we’ve included a set of case studies from Ed Felten’s team at Princeton for you and your team to work through"
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets","The impact of data is data is happening now, and we need to get head of it. It starts with us. Those that are building these technologies"
"Ethics + Data Science: opinion by DJ Patil, former US Chief Data Scientist - KDnuggets",Former U.S. Chief Data Scientist
"GitHub Python Data Science Spotlight: AutoML, NLP, Visualization, ML Workflows - KDnuggets",Auto-Keras is an open source software library for automated machine learning (AutoML). The ultimate goal of AutoML is to allow domain experts with limited data science or machine learning background easily accessible to deep learning models. Auto-Keras provides functions to automatically search for architecture and hyperparameters of deep learning models
"GitHub Python Data Science Spotlight: AutoML, NLP, Visualization, ML Workflows - KDnuggets","MLflow is library-agnostic. You can use it with any machine learning library, and in any programming language, since all functions are accessible through a REST API and CLI. For convenience, the project also includes a Python API"
Data Science Cheat Sheet - KDnuggets,"There are lots of cheat sheets out there of varying quality covering vastly different topics which are all considered to be under the ""data science"" banner. Some are great, some are good, many are not worth your time. Occasionally a gem can be found, covering some particular niche to some acceptable level of understanding"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,You have just been hired as a Data Scientist at a small software company. You are feeling ecstatic! Your hard work and perseverance has finally paid off. It is time to put your statistics and machine learning knowledge into action. You have finally joined the data revolution. Congrats!
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"Day 1 arrives, and everyone is excited to meet this “Data Scientist”. The company has never hired a data scientist before, so expectations were unrealistically high. But, you are not worried. Your supervisor, who probably is not a data scientist herself, asks how she can assist you on your first day. You may have believed that the data would be easy to retrieve, or at least it would be stored in a clean and tidy format, or may be not. Clearly, the company that hired you must have had a grand plan for your amazing data skills!"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"Rarely is the above scenario the case for most junior data scientists joining small companies (or even large organizations outside the tech giants of the world? I don’t really know). As someone who has been there, I’d like to outline a few practical ideas to help junior data scientists get started at a small software company. The steps were drawn from my personal journey and that of others before me"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"As a data scientist, you need to understand the ins and outs of the industry you’re currently a part of. How else can you conduct exploratory data analysis, critique your findings and investigate anomalies? Strong domain expertise enables you to perform better feature selection and engineering. Indeed, building a model to optimize a system without understanding the underlying nuances of how a current system works is a recipe for failure"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"Just because your company put out a job description for a data scientist does not mean that they have a deep understanding of what that role entails. I mean let’s face it: sometimes neither do we. I once read about a data science manager who, upon starting a new role, spent 30 per cent or more of his time building a common understanding of data science and machine learning across the organization (here is the original"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"This is an excellent first step for a data scientist starting work at an organization foreign to machine learning. You can opt to teach courses in R or Python, or give classes to build intuition around statistical analysis and machine learning. This can be extremely important in helping colleagues identify Machine Learning and Data Science opportunities for you to work on, and in helping others around you understand what is it exactly that you do"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"Understanding how data is created and collected is crucial because it enables you to identify whether you can trust the data as is, or if it requires further preprocessing before you can make use of it or present it. Knowing the schema of your data will speed up your query process and help you minimize the mistakes you do when pulling data. It is also important to identify what data needs to be collected to enable the company’s data science strategy (which you should play a big part in building)"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"The role of a data scientist should not be confined to A/B tests, building models and finding correlations. Rather, a data scientist should play a key role in creating a data-driven culture at her organization. A good starting point is to democratize access to the work you are doing to all employees. Airbnb has a great article on building what it terms as the “"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"The objective of the knowledge repo is to facilitate knowledge sharing across the organization. The simplest way to do this is by documenting all of your data science work using Jupyter notebooks and R markdown files, and make them easily accessible to anyone in your organization. You can take it to the next level by sharing simple apps created using Shiny, enabling your colleagues to manipulate input and observe how the output, it be a number or plot, can change"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"When starting as the first data scientist at a small company, chances are there wont be a planned out machine learning strategy. Attempting to start your job by identifying a machine learning opportunity and building sophisticated models right off the bat may prove to be a frustrating experience. That is because you are still unfamiliar with the business domain, you haven’t immersed yourself in your company’s data infrastructure, and you probably won’t even have a data pipeline setup!"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"Many of us data scientists get stuck in the allure of solving mathematically complex problems and building machine learning algorithms. That said, the reality is that a significant chunk of what we deem as “interesting” problems will not bring back any return to our employer. Such problems can only act as cool conversation starters at best"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"In data science, it is important to think ahead. What is your data science play for the next quarter? What about till year end? What about next year? This task, from my humble experience, is difficult to do alone; you need the assistance of Product Management and senior level executives to understand where data science best fits and where ROI can be maximized. Nevertheless, building and evangelizing a data science roadmap is crucial to communicate the role and importance of data science in your organization"
Seven Practical Ideas For Beginner Data Scientists - KDnuggets,"The underlying theme tends to be that data scientists are not challenged enough, and thus they leave looking for ‘sexier’ things to do. Nevertheless, the crude reality at most small-to-medium software companies is that data science is not a predefined role with a thought out strategy and laid out objectives. It is a new field of discovery with great untapped potential, most of which requires identifying and establishing the right bridge between profit, data analysis, statistics and machine learning, and targeted data communication. All in all, data science is rather a process with a beginning and sometimes not-so-clear end"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"When looking up UX design strategies for AI products, I found little to no relevant material. Among the few I found, most were either too domain specific or completely focused on visual designs of web UIs. The best articles I have came across on this subject matter were"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"Realizing that there is a legitimate knowledge gap between UX Designers and Data Scientists, I have decided to attempt addressing the needs from the Data Scientist’s perspective. Hence, my assumption is that the readers have some basic understanding of data science. For UX Designers"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"AI is taking over almost every aspect of our daily lives. This will change how we behave and what we expect from these products. As designers, we aim to create useful, easy-to-understand products in order to bring clarity to this shady new world. Most importantly, we want to use the power of AI to make people’s lives easier and more joyful"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"While in the past, most business problems relied on simple explainable models, “black box” models like Neural Network have started becoming very popular. This is because Neural Networks provide much higher accuracy at lower costs for problems having complex decision boundaries (e. However, Neural Networks are a lot hard to explain compared to most traditional models. This is especially true for teams with non-technical players as analyzing a neural network is a non-trivial task"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"Other times, we can infer the engineered features based on the prediction and input. For example, the model may not have a feature for ice being present on the road. However, if the presence of water on the road and the road’s surface temperature are being picked up by the model as inputs, we can intuitively say that the existence of ice can be derived by the hidden layers through feature engineering"
UX Design Guide for Data Scientists and AI Products - KDnuggets,Model interpretation doesn’t always need to be mathematical. Qualitatively looking at the inputs and their corresponding outputs can often provide valuable insight. Below is a
UX Design Guide for Data Scientists and AI Products - KDnuggets,"AI can generate content and take actions no one had thought of before. For such unpredictable cases, we have to spend more time testing the products and finding weird, funny, or even disturbingly unpleasant edge cases. One example is the misinterpretation of chatbots ٍbelow"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"Extensive testing in the field can help minimize these errors. Collecting clear logs on the production model can help debug unexpected issues when they arise. For more information on testing and DevOps related topics, read my “"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"Financial products are good examples. As per government regulations, your credit score may only be taking assets of value larger than $2000 into account. Hence, any asset lower than $2000 will not be taken into account by the model when calculating your score. Similar case with cyber-security products. You cannot expect the model to monitor DDoS attacks if it is collecting data at a daily or weekly rate"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"A natural reaction would be that the camera did not pick up on the bicycle. However, the camera did recognize the pedestrian but the system chose not to stop. This is because the model was not designed to completely replace the driver"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"The goal of a good UX design is to deliver a product that the end user is satisfied with In order to ensure that your users are getting what they wants, provide them the opportunity to give feedback about the AI content. This can include the rating system on movies or letting banks know that their credit card transaction is not fraudulent. Below is an example from Google Now"
UX Design Guide for Data Scientists and AI Products - KDnuggets,"At the end of the day, AI is being designed to solve a problem and make lives easier. The human-centric design must be kept at its core at all times while developing a successful product. Not addressing client needs is the reason why major industries go bankrupt and get replaced by better service providers. Understanding clients needs is key to moving forward"
Data Scientist guide for getting started with Docker - KDnuggets,"Docker is an increasingly popular tool designed to make it easier to create, deploy and run applications within a container. Containers are extremely useful as they allow developers to package up an application with all the parts it needs, such as libraries and other dependencies, and ship it all out as one package. It’s commonly used by software engineers, but how can Data Scientist’s get started with this powerful tool? Well, before we get into the guide for getting started, let’s discuss some of the reasons you may want to use Docker for Data Science"
Data Scientist guide for getting started with Docker - KDnuggets,"One of Docker’s biggest draws is its reproducibility. Aside from sharing the Docker image itself, you could in theory share a python script with the results baked inside the Docker. A colleague could then run this to see for themselves what’s in the Docker image"
Data Scientist guide for getting started with Docker - KDnuggets,"Data scientists can spend hours preparing their machines to accommodate a specific framework. For example, there are 30 + unique ways for someone to setup a Caffe environment. Docker provides a consistent platform to share these tools, reducing the time wasted in searching for operating system specific installers and libraries"
Data Scientist guide for getting started with Docker - KDnuggets,"The Docker eco-system – docker compose and docker machine – make it easily accessible for anyone. It means that a member of the company who isn’t familiar with the code inside it can still run it. Perfect for members of the sales team, or higher management to show off that new data science application you’ve been building!"
Data Scientist guide for getting started with Docker - KDnuggets,"OK, now that’s up and running let’s dive into sharing Jupyter notebooks between the host and the container. Firstly, we need to create a directory on our host machine that will store the notebooks, we’ll call it /jupyter-notebooks. Sharing directories when running the Docker command is similar to how the ports work and we need to add the following:"
Data Scientist guide for getting started with Docker - KDnuggets,"As you can see, we managed to get a working use case for Docker with data science up and running very quickly. We barely scratched the surface with what you could do, but thanks to Docker’s fantastic library, the possibilities are endless! Becoming a master at Docker can not only assist you with local development, but can save a vast amount of time, money and effort when working with a team of data scientists. Stay tuned to KDnuggets, as we’ll be posting a Docker Cheat Sheet article very soon"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"When most data scientists start working, they are equipped with all the neat math concepts they learned from school textbooks. However, pretty soon, they realize that the majority of data science work involve getting data into the format needed for the model to use. Even beyond that, the model being developed is part of an application for the end user. Now a proper thing a data scientist would do is have their model codes version controlled on Git. VSTS would then download the codes from Git. VSTS would then be wrapped in a Docker Image, which would then be put on a Docker container registry. Once on the registry, it would be orchestrated using Kubernetes. Now, say all that to the average data scientist and his mind will completely shut down. Most data scientists know how to provide a static report or CSV file with predictions. However, how do we version control the model and add it to an app? How will people interact with our website based on the outcome? How will it scale!? All this would involve confidence testing, checking if nothing is below a set threshold, sign off from different parties and orchestration between different cloud servers (with all its ugly firewall rules). This is where some basic DevOps knowledge would come in handy"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"Developers have their own chain of command (i. For data scientists, this would mean changing model structure and variables. They couldn’t care less what happens to the machinery. Smoke coming out of a data center? As long as they get their data to finish the end product, they couldn’t care less. On the other end of the spectrum is IT. Their job is to ensure that all the servers, networks and pretty firewall rules are maintained. Cybersecurity is also a huge concern for them. They couldn’t care less about the company’s clients, as long as the machines are working perfectly. DevOps is the middleman between developers and IT. Some common DevOps functionalities involve:"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"An important note before reading the rest of the blog. Understand the business problem and do not get married to the tools. The tools mentioned in the blog will change, but the underlying problem will remain roughly the same (for the foreseeable future atleast)"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"Imagine pushing your code to production. And it works! Perfect. No complaints. Time goes on and you keep adding new features and keep developing it. However, one of these features introduce a bug to your code that badly messes up your production application. You were hoping one of your many unit tests may have caught it. However, just because something passed all your tests doesn’t mean it’s bug free. It just means it passed all the tests currently written. Since it’s production level code, you do not have time to debug. Time is money and you have angry clients. Wouldn’t it all be simple to revert back to a point when your code worked??? That’s where version control comes in. In"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"A data science specific problem with version control is the use of Jupiter/Zeppelin notebooks. Data scientists absolutely LOVE notebooks. However, if you store your codes on a notebook template and try to change the code in version control, you will be left with insane HTML junk when performing diff and merge. You can either completely abandon the use of notebooks in version control (and simply import the math functions from the version controlled libraries) or you can use existing tools like"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"From a data scientist’s perspective, testing usually fall into one of two camps. You have the usual unit testing which checks if the code is working properly or if the code does what you want it to do. The other one, being more specific to the domain of data science, are data quality checks and model performance. Does your model produce for you an accurate score? Now, I am sure many of you are wondering why that’s an issue. You have already done the classification score and ROC curves and the model is satisfactory enough for deployment. Well, lot’s of issues. The primary issue is that, the library versions on the development environment maybe completely different from production. This would mean different implementation, approximations and hence, different model outputs"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"Another classic example is the use of different languages for development and production. Let’s imagine this scenario. You, the noble data scientist, wishes to write a model in R, Python, Matlab, or one of the many new languages whose white paper just came out last week (and may not be well tested). You take your model to the production team. The production team looks at you skeptically, laughs for 5 seconds, only to realize that you are being serious. Scoff they shall. The production code is written in Java. This means re-writing the entire model code to Java for production. This, again, would mean completely different input format and model output. Hence why, automated testing is required"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,Sand-boxing is an essential part of coding. This might involve having different environments for various applications. It could simply be replicating the production environment into development. It could even mean having multiple production environments with different software versions in order to cater a much larger costumer base. If the best you have in mind is using a VM with
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"A simpler alternative is using a container instead of a full on VM. A container is simply a unix process or thread that looks, smells and feels like a VM. The advantage is that it is low powered and less memory intensive (meaning you can spin it up or take it down at will… within minutes). Popular containerization technologies include"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"Containerization technologies help, not only with tests, but also scalability. This is especially true when you need to think about multiple users using your model based application. This may either be true in terms of training or prediction"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"Security is important but often underestimated in the field of data science. Some of the data used for model training and prediction involves sensitive data such as credit card information or healthcare data. Several compliance policies such as GDPR and HIPPA needs to be addressed when dealing with such data. It is not only the client that needs security. Trade secret model structure and variables, when deployed them on client servers, require a certain level of encryption. This is often solved by deploying the model in encrypted executables (e. JAR files) or by encrypting model variables before storing them on the client database (although, please DO NOT write your own encryption unless you absolutely know what you are doing…)"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"Also, it would be wise to build models on a tenant-by-tenant basis in order to avoid accidental transfer learning that might cause information leaks from one company to another. In the case of enterprise search, it would be possible for data scientists to build models using all the data available and, based on permission settings, filter out the results a specific user is not authorized to see. While the approach may seem sound, part of the information available in the data used to train the model is actually learned by the algorithm and transferred to the model. So, either way, that makes it possible for the user to infer the content of the forbidden pages. There is no such thing as perfect security. However, it needs to be good enough (the definition of which depends on the product itself)"
DevOps for Data Scientists: Taming the Unicorn - KDnuggets,"When working with DevOps or IT, as a data scientist, it is important to be upfront about requirements and expectations. This may include programming languages, package versions or framework. Last but not the least, it is also important to show respect to one another. After all, both DevOps and Data Scientists have incredibly hard challenges to solve. DevOps do not know much about data science and Data Scientists are not experts in DevOps and IT. Hence, communication is key for a successful business outcome"
What on earth is data science? - KDnuggets,"Pretty please, don’t taxonomize by histograms vs t-tests vs neural networks. Frankly, if you’re clever and you have a point to make, you can use the same algorithm for any part of data science. It might look like Frankenstein’s monster, but I assure you it can be forced to do your bidding"
What on earth is data science? - KDnuggets,"I’d promised you we were going to talk about making data useful. To me, the idea of usefulness is tightly coupled with influencing real-world actions. If I believe in Santa Claus, it doesn’t particularly matter unless it might influence my behavior in some way. Then, depending on the potential consequences of that behavior, it might start to matter an awful lot. It’s through our actions — our decisions — that we affect the world around us (and invite it to affect us right back)"
What on earth is data science? - KDnuggets,"Unless you know how you intend to frame your decision-making, start here. The great news is that this one is easy. Think of your dataset as a bunch of negatives you found in a darkroom. Data-mining is about working the equipment to expose all the images as quickly as possible so you can see whether there’s anything inspiring on them. As with photos, remember not to take what you see too seriously. You didn’t take the photos, so you don’t know much about what’s off-screen. The golden rule of data-mining is:"
What on earth is data science? - KDnuggets,"The darkroom’s intimidating at first, but there’s not that much to it. Just learn to work the equipment. Here’s a tutorial in"
What on earth is data science? - KDnuggets,"Inspiration is cheap, but rigor is expensive. If you want to leap beyond the data, you’re going to need specialist training. As someone with undergrad"
What on earth is data science? - KDnuggets,"If you intend to make high-quality, risk-controlled, important decisions that rely on conclusions about the world beyond the data available to you, you’re going to have to bring statistical skills onto your team. A great example is that moment when your finger is hovering over the launch button for an AI system and it occurs to you that you need to check it works before releasing it (always a good idea, seriously). Step away from the button and call in the statistician"
Data Science For Business: 3 Reasons You Need To Learn The Expected Value Framework - KDnuggets,"Passionate about learning new tools, building software and working with people to gain insights and make better decisions. Enjoys working with teams and individuals to fuel operational excellence. Based in State College, PA, USA"
Programming Best Practices For Data Science - KDnuggets,"Unlike a bank, Lending Club doesn't lend money itself. Lending Club is instead a marketplace for lenders to lend money to individuals who are seeking loans for a variety of reasons (home repairs, wedding costs, etc. We can use this data to build models that will predict if a given loan application will be successful or not. We won't dive into building a machine learning pipeline for making predictions in this post, but we cover it in our"
Programming Best Practices For Data Science - KDnuggets,"In the production mindset, we want to focus on writing code that will generalize to more situations. In our case, we want our data cleaning code to work for any of the data sets from Lending Club (from other time periods). The best way to generalize our code is to turn it into a"
"Why Germany did not defeat Brazil in the final, or Data Science lessons from the World Cup - KDnuggets","Well, lets just say it was mixed. 13 of the last 16 (81.25%) were correctly predicted, with only Poland, Germany and Egypt missing out and Japan, Sweden and hosts Russia taking their places"
"Why Germany did not defeat Brazil in the final, or Data Science lessons from the World Cup - KDnuggets","Sport in general contains a lot of external factors that can hinder results. For example for football ( soccer), the result may be affected by an unfair referee, adverse weather conditions, the climate, the player’s personal lives and much more.  It’s very tricky to factor in these features, as they can be difficult to measure and collect"
How to Build a Data Science Portfolio - KDnuggets,"Besides the benefit of learning by making a portfolio, a portfolio is important as it can help get you employment. For the purpose of this article, let’s define a portfolio as public evidence of your data science skills. I got this definition from"
How to Build a Data Science Portfolio - KDnuggets,"The most effective strategy for me was doing public work. I blogged and did a lot of open source development late in my PhD, and these helped give public evidence of my data science skills. But the way I landed my first industry job was a particularly noteworthy example of the public work. During my PhD I was an active answerer on the programming site Stack Overflow, and an engineer at the company came across one of my answers (one explaining the intuition behind the beta distribution). He was so impressed with the answer that he got in touch with me [through Twitter], and a few interviews later"
How to Build a Data Science Portfolio - KDnuggets,"I want to hear about a project they’ve worked on recently. I ask them about how the project started, how they determined it was worth time and effort, their process, and their results. I also ask them about what they learned from the project. I gain a lot from answers to this question: if they can tell a narrative, how the problem related to the bigger picture, and how they tackled the hard work of doing something"
How to Build a Data Science Portfolio - KDnuggets,"The image below shows partial examples of classification of Titanic (A), MNIST (B), and iris (C) datasets. There aren’t a lot of ways to use these datasets to distinguish yourself from other applicants. Make sure to list novel projects"
How to Build a Data Science Portfolio - KDnuggets,"Have a portfolio. If you are looking for a serious paid job in data science do some projects with real data. If you can post them on GitHub. Apart from Kaggle competitions, find something that you love or a problem you want to solve and use your knowledge to do it"
How to Build a Data Science Portfolio - KDnuggets,"I applied to almost 125 jobs (for real, maybe you applied for much more), I got only like 25–30 replies. Some of them were just: Thanks but nope. And I got almost 15 interviews. I learned from each one. Got better. I had to deal with a lot of rejection. Something I was actually not prepared to. But I loved the process of getting interviewed (not all of them to be honest). I studied a lot, programmed everyday, read a lot of articles and posts. They helped a lot"
How to Build a Data Science Portfolio - KDnuggets,"As you learn more and improve yourself, your portfolio should also be updated. This same sentiment is echoed in many other advice articles. As"
How to Build a Data Science Portfolio - KDnuggets,"Take note of all the interview questions you got asked, especially those questions you failed to answer. You can fail again, but don’t fail at the same spot. You should always be learning and improving"
How to Build a Data Science Portfolio - KDnuggets,"One of the ways someone finds your portfolio is often through your resume so it is worth a mention. A data science resume is a place to focus on your technical skills. Your resume is a chance to succinctly represent your qualifications and fit for that particular role. Recruiters and hiring managers skim resumes very quickly, and you only have a short time to make an impression. Improving your resume can increase your chance of getting an interview. You have to make sure every single line and every single section of your resume counts"
How to Build a Data Science Portfolio - KDnuggets,"They don’t help you distinguish yourself from other people. They take away space from the more important things (skills, projects, experience etc). Cover letters are extremely optional unless you really personalize it"
How to Build a Data Science Portfolio - KDnuggets,"Show results and include links. If you participated in Kaggle competition, put percentile rank as it helps the person reading your resume understand where you are in the competition. In projects sections, there is always room for links to writeups and papers as they let the hiring manager or recruiter dig in deeper (bias to real world messy problems where you learn something new)"
How to Build a Data Science Portfolio - KDnuggets,"Fill our your online presence. The most basic is a LinkedIn profile. It is kind of like an extended resume. Github and Kaggle profiles can help show off your work. Fill out each profile and include links to other sites. Fill out descriptions for your GitHub repositories. Include links to your knowledge sharing profiles/blog (Medium, Quora). Data science specifically is about knowledge sharing and communicating what the data means to other people. You don’t have to do all of them, but pick a few and do it (More on this later)"
How to Build a Data Science Portfolio - KDnuggets,"Experience is the core of your resume, but if you don’t have work experience what do you do? Focus your resume on independent projects, like capstone projects, independent research, thesis work, or Kaggle competitions. These are substitutes for work experience if you don’t have work experience to put on your resume. Avoid putting irrelevant experience on your resume"
How to Build a Data Science Portfolio - KDnuggets,"This is very similar to the Importance of a Portfolio section, just divided into subsections. Having a Github page, a Kaggle profile, a Stack Overflow, etc can provide support for your resume. Having online profiles filled out can be a good signal for hiring managers"
How to Build a Data Science Portfolio - KDnuggets,"A lot of Data science is about communication and presenting data so it is good to have these online profiles. Besides from the fact that these platforms help provide valuable experience, they can also help you get noticed and lead people to your resume. People can and do find your resume online through various sources (LinkedIn, GitHub, Twitter, Kaggle, Medium, Stack Overflow, Tableau Public, Quora, Youtube, etc). You will even find that different types of social media feed into eachother"
How to Build a Data Science Portfolio - KDnuggets,"A Github profile is a powerful signal that you are a competent data scientist. In the projects section of a resume, people often leave links to their GitHub where the code is stored for their projects. You can also have writeups and markdown there. GitHub lets people see what you have built and how you have built it. At some companies, hiring managers look at an applicants GitHub. It is another way to show employers you aren’t a false positive. If you take the time to develop your GitHub profile, you can be better evaluated than others"
How to Build a Data Science Portfolio - KDnuggets,"It is true, doing one Kaggle competition does not qualify someone to be a data scientist. Neither does taking one class or attending one conference tutorial or analyzing one dataset or reading one book in data science. Working on competition(s) adds to your experience and augments your portfolio. It is a complement to your other projects, not the sole litmus test of one’s data science skillset"
How to Build a Data Science Portfolio - KDnuggets,"I never, never, never applied to any companies without an introduction to someone who worked at the company…once I was interested in a company, I would use LinkedIn to find a first- or second- degree connection at the company. I would write to that connection, asking to talk to them about their experience at the company and, if possible, whether they’d be able to connect me to someone on the Data Science team. Whenever I could, I did in-person meetings (coffee or lunch) instead of phone calls. As an aside, Trey Causey recently wrote"
How to Build a Data Science Portfolio - KDnuggets,"I would never ask for a job directly, but they would usually ask for my resume and offer to submit me as an internal referral, or put me in touch with a hiring manager. If they didn’t seem comfortable doing so.I’d just thank them for their time and move on"
How to Build a Data Science Portfolio - KDnuggets,"Notice that he doesn’t right away ask for a referral. While common job advice when applying to a company is to get a referral, it is VERY IMPORTANT to note that you still need a portfolio, experience, or some sort of proof you can do a job. Jason even mentions the importance of a portfolio in that and"
How to Build a Data Science Portfolio - KDnuggets,Having some form of blog can be highly beneficial. A lot of data science is about communication and presenting data. Blogging is a way of practicing this and showing you can do this. Writing about a project or a data science topic allows you to share with the community as well as encourages you to write out your work process and thoughts. This is a useful skill when interviewing
How to Build a Data Science Portfolio - KDnuggets,"By writing a blog, you are can practice communicate findings to others. It also is another form of advertising yourself. Blogs about"
How to Build a Data Science Portfolio - KDnuggets,"I thought of creating my own website on a platform such as WordPress or Squarespace. While those platforms are amazing to host your own portfolio, I wanted a place where I would get some visibility, and a pretty good tagging system to reach greater audiences. Luckily Medium, as we know, has those options (and it’s also free)"
How to Build a Data Science Portfolio - KDnuggets,Being active on Twitter is a great way to identify and interact with people in your field. You can also promote your blog on Twitter so that your portfolio can be that much more visible. There are so many opportunities to interact with people on twitter. One of them as
How to Build a Data Science Portfolio - KDnuggets,"Not every data science job uses Tableau or other BI tools. However, if you are applying to jobs where these tools are used, it is important to note that there are websites where you can put dashboards for public consumption. For example, if you say you are learning or know Tableau, put a couple dashboards on"
How to Build a Data Science Portfolio - KDnuggets,"Having a strong resume has long been the primary tool for job seekers to relay their skills to potential employers. These days, there is more than one way to showoff your skills and get a job. A portfolio of public evidence is a way to get opportunities that you normally wouldn’t get. It is important to emphasize that a portfolio is an iterative process. As your knowledge grows, your portfolio should be updated over time. Never stop learning or growing. Even this blog post will be updated with feedback and with increasing knowledge. If you want interview advice/guides, time to check out"
Cookiecutter Data Science: How to Organize Your Data Science Project - KDnuggets,"That being said, once started it is not a process that lends itself to thinking carefully about the structure of your code or project layout, so it's best to start with a clean, logical structure and stick to it throughout. We think it's a pretty big win all around to use a fairly standardized setup like this one. Here's why:"
Building A Data Science Product in 10 Days - KDnuggets,"Here is the problem. After adding items to the shopping cart on Instacart, a customer can select a delivery window during checkout (illustrated in Figure 1). Then, an Instacart shopper would try to deliver the groceries to the customer within the window. During peak times, our system often accepted more orders than our shoppers could handle, and some orders would be delivered late"
Building A Data Science Product in 10 Days - KDnuggets,"The percentage of late deliveries per day was used to measure lateness. We didn’t want to close delivery windows too early and fail to capture the orders that could be delivered on time. So, the number of deliveries per day was used as a counter metric"
Building A Data Science Product in 10 Days - KDnuggets,"We followed a typical modeling process: feature engineering, creating training and testing data, and comparing different models. However, once we felt the models were reasonably accurate, we did not invest more time in models. Firstly, models were only one part of the system. Secondly, the improvement in model accuracy did not necessarily translate to the same degree of improvement in metrics"
Building A Data Science Product in 10 Days - KDnuggets,"We built a random forest model for comparison. The predicted values vs. The random forest model was not substantially better than the linear model, and so we felt comfortable to proceed with the linear model that is easier to interpret and implement"
Building A Data Science Product in 10 Days - KDnuggets,"We used databases as the interfaces between data science and engineering components. In this way, the dependency between data science and engineering can be reduced (vs. Figure 6 illustrates how the system works"
Building A Data Science Product in 10 Days - KDnuggets,"There were two data science jobs, model training job and prediction job, both triggered by cron (a time-based scheduler) at pre-defined frequencies. The model training job ran every week, fetched the most recent order_shoppers data (orders and shoppers’ time spent on the orders), fitted the models and saved them into a database table (models). The prediction job ran every night, fetched the models and scheduled_hours (future scheduled shopper hours) data, and estimated the capacity for future delivery windows. The estimates were then saved to the capacity_estimates table"
Building A Data Science Product in 10 Days - KDnuggets,"The capacity counting job was created to consume capacity estimates and provide the delivery availability of each window for the customer app. It was scheduled to run every minute, got the capacity estimates and existing orders, calculated if a delivery window was available, and saved the availability information to the delivery_availabilities table. Also, when a customer placed an order, the order information would be saved to the orders table, and the capacity counting job would be triggered"
Building A Data Science Product in 10 Days - KDnuggets,"We ran the prediction job that generated the capacity estimates for future delivery windows. Then, after a window became obsolete, we compared the estimated capacity of the window to the orders actually accepted by the existing system. We found that in some cases the existing system took fewer orders than the estimated capacity but with substantial lateness (illustrated in Figure 7). This indicated the capacity was over-estimated in those cases. Based on this insight, we found two issues"
Building A Data Science Product in 10 Days - KDnuggets,"The models we built predicted the mean. It can be seen from Figure 5 (left) that there are data points below the mean line. The mean predictions would over-estimate the capacity for those data points. To solve this, a prediction interval was constructed, and a lower percentile level was used. Figure 8 shows the 25th percentile and 75th percentile levels"
Building A Data Science Product in 10 Days - KDnuggets,"Figure 9 shows the percentage of late deliveries by day around the product launching time. The new system achieved our goal of substantially reducing late deliveries (without reducing the number of deliveries). It was a quick success. Since the initial launch, we’ve continued to iterate, including estimating the capacity for same-day delivery windows"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","WMDs are algorithms that decide important criteria to reject a certain category of people.  They are secret – there is no appeal system and those rejected are not even aware of the existence of these all-powerful algorithms.  WMDs are unfair, make mistakes and are destructive for the society.  They encode “"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","WMDs are pretty much everywhere a human being interacts with the bureaucracy.  They seem perfect and sanitary, innocuously deciding who is getting into high school/college, who gets a job or keeps one, who gets a loan or who is allowed to buy insurance.  They are used in policing and the court system.  And in health and human services.  These algorithms are not well thought out and are used to replace the difficult questions"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","WMDs matter because of the frightfully vital role they play.  They don’t work.  They undermine their own goals – e.  They undermine science and erode people’s trust in science.  Thus, they decrease accountability.  Nobody, in particular, is in charge when they make a mistake.  Perhaps, mulls Cathy, it is the primary goal to take away that accountability.  These algorithms are everywhere and constantly chip away at the equality and threaten democracy.  In politics, the information we hear is specifically tailored and we no longer are informed citizens of a well-functioning democracy"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","And, as they hide the problems, other people suffer.  We need to focus on causality, as essentially we are creating the past by training the algorithms on past data loaded with historical biases.  For instance, if we trained a hiring algorithm on twenty-two years of Fox News data, it would predict that a qualified woman is going to fail.  Because they were systemically harassed and treated badly in Fox News.  There really is, no company that can claim to be perfect.  Big Data does not remove the need for causality"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","The graphs suggest that the default behavior changes with race.  Asians default less at lower FICO scores (from the left-side graph).  From the right-side graph: percentage wise, there are more blacks who have lower FICO scores"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","In the first graph, the focus is on profit maximization and this makes it a much higher bar for Blacks to get a loan.  Thus, it would seem to be unfair if a lending company sets higher standards for Blacks, especially when the historical data could be wrong and biased against them.  In the second graph, if the threshold is the same for everyone, then very few Blacks would qualify for a loan"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","And finally, this graph above is a function of profitability as a function of the false positive/negative trade-off.  The stricter the fairness (based on Demography), the lower the profits.  Imagine what businesses would do in a competitive scene"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets",Red means – it's something to worry about – e.  Concerns (in red) may be positive too – “we want this to be fair”.  Yellow is something bad may happen.  The False negatives are higher for minority Customers – which means they may not get the loan
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets",It’s to be noted that the Ethical Matrix is not a solution.  It helps to frame the problems better and highlights the limitations of the conversations.  It makes the infinite problem space finite and the associated technical problems solvable
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","Cathy’s expanded ethical matrix would represent the maximum concern for Data Quality and Fairness and False Positives.  It is important to note that False Positives are not symmetrical to False Negatives.  False Positives is getting imprisoned longer for something you are not going to do.  False Negatives is that you get out earlier and you are a little more criminal and more likely to be arrested.  The target variable is arrest within two years.  And, this is not for violent crimes.  The model is what it is because you could be training your model on addicts.  Or, because of training the model on those with mental-health issues.  So, think about that – it is not even about crime!"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","Thus, the Ethical Matrix would allow the conversation around the moral question to occur and then the data science follows.  Ideally, the conversation should occur before the algorithm is even built.  Cathy then concluded by taking a couple of audience questions"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","Banks tend to have a large inventory of algorithmic models, perhaps in thousands, which degrade continuously over time.  Such a model is at its peak just when released.  Banks deal with data in large volumes and so the statistical significance of the variations is large.  Banks are extremely risk-averse"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","The triple line of defense in the figure (on the right) is like the one that the US has.  In the first line of defense, is the modeler who is encouraged to be honest about what can you actually predict, what you cannot and what is the actual question asked.  In the second line of defense, verification and validation rigor is enforced.  Internal audit forms the third line of defense – having proper information flow and a right governance process around it is critical"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","Federal guidelines around model risk management (from the OCC and FDIC) provide a framework around how validation is done.  Do we know why a model is designed the way it is designed?  In Jacob’s team there are about fifty models that they manage, regularly calibrate and tune.  The typical cycles though are long in finance"
"Weapons of Math Destruction, Ethical Matrix, Nate Silver and more Highlights from the Data Science Leaders Summit - KDnuggets","What the Moody’s team has learned, is to begin the model validation as early as possible.  In 1998, when it was early in the game, Moody’s had unique intellectual property in its models.  In 2008, Moody’s Analytics had unique intellectual property in the data.  Today, in 2018, Moody’s has unique intellectual property around its process and its experienced resources, what the team has gained as part of its domain expertise"
How to Lie with Data Science - KDnuggets,"Recently I read the book “How to lie with statistics” by Darrel Huff. The book talks about how one can use statistic to make people conclude wrong. I found this an exciting topic, and I think that it is very relevant to Data Science. This why I want to make the “Data Science” version of the examples shown in the book. Some of them are as in the book, others, are examples of what I saw may happen in real life Data Science"
How to Lie with Data Science - KDnuggets,"Consider yourself as a new data scientist in some company. This company already has a data science team that builds a model to predict something important. You are a very talented data scientist, and just after one month, you were able to improve their model accuracy by 3 percent. Incredible! You want to show your progress to someone, so you prepare this chart:"
How to Lie with Data Science - KDnuggets,All you need to do to show this same data more impressively is to change the chart a bit. You need to make it focus on the change. There’s no “real” need in all those numbers below 80% or above 85%. So it can look like this:
How to Lie with Data Science - KDnuggets,"We can do the same things with process over time. Let’s say you and your team work on some model, and you had a breakthrough in the recent weeks, so your model performance improved by 2%, very nice. It looks like this:"
How to Lie with Data Science - KDnuggets,"Very often, junior data scientist don’t pay enough attention to what metric to use to measure their model performance. This may lead to usage of some default and most of the time wrong metric. Take accuracy for example, in real life (in most cases) it is a very bad metric. This is because in most problems in real life, the data is unbalanced. Consider a model that predicts survivors on Titanic, A very popular tutorial on Kaggle. What if I told you that I built a model that archives 61% accuracy. Is it good? It is hard to say. We don’t have anything to compare it to (more on this later). It sounds ok. It is probably much better than nothing, right? Let me show what I did exactly:"
How to Lie with Data Science - KDnuggets,"That’s right, all I did is predict “zero” ( or “No”) for all the instances. I can get this accuracy (61%) simply because the number of people who survived is lower than people who didn’t. There are far more extreme cases where the data is very unbalanced, in those cases, even 99% accuracy may say nothing. One example of such an extreme unbalanced data is when we want to classify some rare disease correctly. If there’s only 1% of people who have this disease, then just by predicting “No” every time, will give us 99% accuracy!"
How to Lie with Data Science - KDnuggets,"Another important thing we need to do with measurements is to understand how good or bad the results are. Even when we use the right metric, it is sometimes hard to know how good or bad they are. 90% precision may be excellent for one problem, but very bad for others. This is why a good practice is to create a benchmark. Create a very simple (or even random) model and compare your/others results against it. For the Titanic problem, we already know that just by saying “No” to everyone will give us 61% accuracy, so when some algorithm gives us 70%, we can say that this algorithm contributes something, but probably it can do better"
How to Lie with Data Science - KDnuggets,"For example one of our features may be the deviation from the mean. Say we have house size-price prediction, and we want to use how different current house size from the average house size as a feature. By calculating the mean on the whole data (and not just the train set), we introduce information about the test set to our model! (the mean of the data is part of the model). In this case, we might get outstanding results on our test set, but when we use this model in production, it will produce different/worse results"
How to Lie with Data Science - KDnuggets,"In my thesis work, I build a system that tries to classify recordings of utterances into typical and atypical speech. I have 30 participants with 15 utterances each repeated 4 times. So a total of 30*15*4=1800 recordings. This is very little data, so instead of just splitting it into train and test, I want to do cross-validation to evaluate my algorithm. However, I need to be very careful, even without cross-validation, when I randomly select some percent of my data to be a test set, I will get (in high probability) recordings of all the participants in the test set! That means that my model is trained on the participants it will be tested on! Of course, my results will be great, but my model will learn to recognize the different voices of different participants and not typical or atypical speech! I will get a high score, but in reality, my model isn’t worth much"
How to Lie with Data Science - KDnuggets,This type of dependent data may appear in different datasets. Another example of this is when we try to create a matching algorithm between jobs and candidates. We don’t want to show to our model jobs that will appear in the test set. We need to make sure that all parts of our model never saw any data from the test set
How to Lie with Data Science - KDnuggets,"This is a very common one. Sometimes, we have columns in our data that won’t be available for us in the future. Here’s a simple example: We want to predict user satisfaction regarding products on our site. We got a lot of historical data, so we built the model using it. We have a field called"
How to Lie with Data Science - KDnuggets,"We got excellent results, and we are happy. However, when we use our model in production, it predicts absolute non-sense. It turns out that in addition to general user satisfaction, other fields provided by the user. Fields like whether or not the user is satisfied with the delivery, the shipping, the customer support and so on. These fields are not available for us in prediction time and are very correlated (and predictive) to general user satisfaction. Our model used them to predict general satisfaction and did it very well, but when those fields are not available (and we impute them), the model doesn’t have to contribute much"
How to Lie with Data Science - KDnuggets,"Let’s get back to the typical-atypical speech problem. As I said there are only 30 participants, so if I do a simple 20%-80% train-test split, I’ll get only 6 participants to test on. Six participants are very little. I might classify correctly 5 of them just by chance. I even may classify all of them correctly just because I was lucky. This will give me 100% accuracy! (or 83% in case of only 5 correct). It might look excellent, and when I’ll publish my results it will look very impressive, but the reality is that this score is not significant (or even real)"
How to Lie with Data Science - KDnuggets,"If we assume that there’s a 50% of people have atypical speech, then just by randomly guessing I’ll be right 50% of the times, that means that if I try to guess for 6 participants, I’ll classify all of them correctly 0.5⁶=0.01 (1%) of the times"
How to Lie with Data Science - KDnuggets,"It is very tempting to compare learning algorithms to humans. This is very common in many medical fields. However, comparing humans and machines is not trivial at all. Let’s say we have an algorithm that can diagnose a rare disease. We already saw that using accuracy as measurements is not a good idea with unbalanced data. It this case, it might be much better if we use precision and recall for our model evaluation and comparison. We can use precision and recall of some doctor and compare it to our algorithm. However, there’s always a tradeoff between precision and recall and it not always clear what do we want more, high precision or high recall. If our algorithm got 60% precision and 80% recall and the doctor got 40% precision and 100% recall, who’s better? We can say that the precision is higher and thus our algorithm is “better than human”. Also, as an algorithm, we can control this tradeoff, all we need to do is to change our classification threshold, and we can set the precision (or the recall) to the point we want it to be (and see what happens to recall). So an even better option is to use ROC AUC score or “Average Precision” for model evaluation. These metrics take into consideration the precision-recall tradeoff and provide a better metric about how our model is “predictive”. Humans don’t have ROC AUCs nor “Average Precision”. We can’t control (in most cases) this threshold in any doctor. There are different techniques to provide the precision-recall curve for a set of human decision makers, but those techniques almost never used. Here’s a great and much more detailed post about this:"
How to Lie with Data Science - KDnuggets,"In this post, I showed different pitfalls that might occur when we try to publish some algorithm results or interpret others. I think the main idea to take from this is “When it looks too good to be true, it probably is”. When our model (or others) looks surprisingly good, we have to make sure that all of the steps in our pipeline are correct"
Beating the 4-Year Slump: Mid-Career Growth in Data Science - KDnuggets,"There are a lot of resources for early career and aspiring data scientists today, and many bootcamps, books, and MOOCs cater to those hoping to enter the field. However, after a few years, one learns the industry, is a veteran of random forest and boosting algorithms, and can set up a solid A/B test design in a short period of time. It’s an awkward period in which one is no longer a novice but not yet an expert in the field, and for many data scientists, career development is not straight-forward. Recent polls show a satisfaction slump somewhere around 4 years on the job ("
Beating the 4-Year Slump: Mid-Career Growth in Data Science - KDnuggets,"Attending conferences can be a great way to learn new skills, network with other data scientists in the area, and explore new technologies/methods in data science and machine learning. Many academic conferences offer workshops focused on a particular method, as well as opportunities for publication. For those in the US, the American Statistical Association, Institute of Electrical and Electronics Engineers, and American Mathematical Society all provide conference lists for upcoming years, and many of these conferences welcome data scientists and industrial mathematicians, as well as academics. Industry conferences abound, and most US cities/international data hubs now host one or more data science/big data/machine learning conferences. Some conferences even allow for remote participation, providing access for data scientists in less-populated areas or working abroad"
Beating the 4-Year Slump: Mid-Career Growth in Data Science - KDnuggets,"Academic journals and conference publications are another good source of continued learning. Sites like arXiv, open-source journals like Journal of Machine Learning Research or Data Science (Methods, Infrastructure, and Applications), and material published by professional societies (such as Casualty Actuarial Society) often overview state-of-the-art methods covering a broad range of problems. Methods like XGBoost, persistent homology, or HodgeRank often appear in one of these publications before trickling into software packages and blogs. ArXiv and ResearchGate in particular have been a constant in my career since switching from academia to industry. Some employers even allow data scientists doing research to publish in academic journals or though professional societies"
Beating the 4-Year Slump: Mid-Career Growth in Data Science - KDnuggets,"Blogs, discussion forums, and LinkedIn data science groups can also aid in mid-career development and provide data scientists with meaningful connections within the field, particularly for those in rural areas or cities in which data science positions are scarce. The LinkedIn Data Scientists group is quite active (and has >50,000 members). Posts range from career guidance to technical papers to business topics/skills, and many articles/papers are shared every day in the group.KDnuggets (of course), Quora, Data Science Central, Kaggle, and GitHub all provide blogs and/or discussion topics ranging from early career topics to advanced algorithms/managerial topics. Many of these online communities also post local and online conference notices"
Beating the 4-Year Slump: Mid-Career Growth in Data Science - KDnuggets,"Finally, many metropolitan areas have local meetups for data professionals featuring local speakers or math/tech workshops. I’ve only recently been able to attend the meetups in my general area, but it’s been a good experience. I learned a lot putting together my first workshop, and one of the booster sessions provided some good insight on data wrangling from another industry that seems to work well in my industry, as well. It’s also a great way to stay in touch with other data scientists in the area who have varied expertise and span many industries"
Beating the 4-Year Slump: Mid-Career Growth in Data Science - KDnuggets,"While not as many resources exist for continued career development after working in data science for a few years as exist for new data scientists, many good options exist for mid-career development, both online and in-person. Conferences, professional societies/groups, publications, and local events offer a wide range of options for a variety of career trajectories. Happy learning!"
Beating the 4-Year Slump: Mid-Career Growth in Data Science - KDnuggets,"Colleen M. Farrelly is a data scientist whose industry experience includes positions related to healthcare, education, biotech, marketing, and finance. Her areas of research include topology/topological data analysis, ensemble learning, nonparametric statistics, manifold learning, and explaining mathematics to lay audiences (https://www. When she isn’t doing data science, she is a poet and author (https://www"
"Data science of the connected vehicle: perspectives, applications and trends - KDnuggets","The application of streaming and real-time data analytics to connected vehicles is gaining traction around the world. Underpinning this shift is the increased availability of data from vehicles and from the underlying transport infrastructure. In addition to the streaming nature of the data, these rich data feeds also constitute a highly distributed (spatial extent) and highly dynamic (temporal extent) IoT grid. These considerations present enormous challenges but also many opportunities"
"Data science of the connected vehicle: perspectives, applications and trends - KDnuggets","The “Connected Vehicle” is any vehicle that is able to communicate with the cloud and/or the transport infrastructure, and broadcast relevant information (e. Here the vehicle itself can be equipped with relevant capability (SIM card embedded within the vehicle),or the relevant capability can be provided by pairing the vehicle with a mobile phone device or an embedded device that can read the sensor data and broadcast the data (e. OBD port)"
"Data science of the connected vehicle: perspectives, applications and trends - KDnuggets","Before that, Boris was the lead data scientist at BuildingIQ (venture-backed by Siemens and Schneider Electric) where he led the development of machine learning algorithms for the optimisation of energy usage in large-scale buildings (skyscrapers, hospitals etc), from the early stage through to a successful IPO. Boris holds a PhD in Applied Mathematics from UNSW. Boris’ experience spans the Australian, US and European markets"
The 4 Levels of Data Usage in Data Science - KDnuggets,"Overton noted that 5 years ago the idea of extracting value from data was new to businesses, and that businesses had to be convinced that it was worth their effort to collect data and analyze it for meaningful patterns from which they could benefit. He then contrasted that with the views of today, noted that it is now understood that not using data is losing out on a potential competitive edge. It seems a forgone conclusion that data science is necessary to some degree, yet businesses now have a different set of questions. What should our data scientists be doing? What areas of our business should we be focusing on? How exactly can our data add value?"
The 4 Levels of Data Usage in Data Science - KDnuggets,"One topic that Ben and Jerry then discussed were the levels, or ""buckets,"" of data usage in business, where Jerry defined and outlined 4 such categories. The discussion was interesting, and the description of the levels, while intuitive, was useful, in my view. Below I have summarized the basics of these categories, and I would encourage everyone to listen to the entire episode for more solid discussion"
Why a Professional Association for Data Scientists is a Bad Idea - KDnuggets,"The focus in the majority of data science courses is on the final point. Granted, there are some courses that also focus on communication, but the fact remains that data science is an applied discipline, and is learnt best by doing. The above reasons are why aspiring data scientists are encouraged to share their work online (via blog, github, kaggle, etc) - it's a far stronger signal than a degree or an accreditation"
Why a Professional Association for Data Scientists is a Bad Idea - KDnuggets,"The assumption in the above paragraphs is that technical qualification is a requirement for such a body. Certainly, there are other industry groups that have no such requirement, but few could argue that membership of such a group provides much of a signal. The field is far too large for any operator to hope to license members with any objectivity apart from standardised testing, and thus we are drawn to this mechanism as a foundational plinth"
Top 20 Python Libraries for Data Science in 2018 - KDnuggets,"During the year, a large number of improvements have been made to the library. In addition to bug fixes and compatibility issues, the crucial changes regard styling possibilities, namely the printing format of NumPy objects. Also, some functions can now handle files of any encoding that is available in Python"
Top 20 Python Libraries for Data Science in 2018 - KDnuggets,"Another core library for scientific computing is SciPy. It is based on NumPy and therefore extends its capabilities. SciPy main data structure is again a multidimensional array, implemented by Numpy. The package contains tools that help with solving linear algebra, probability theory, integral calculus and many more tasks"
Top 20 Python Libraries for Data Science in 2018 - KDnuggets,"Pandas is a Python library that provides high-level data structures and a vast variety of tools for analysis. The great feature of this package is the ability to translate rather complex operations with data into one or two commands. Pandas contains many built-in methods for grouping, filtering, and combining data, as well as the time-series functionality. All of this is followed by impressive speed indicators"
Top 20 Python Libraries for Data Science in 2018 - KDnuggets,"Matplotlib is a low-level library for creating two-dimensional diagrams and graphs. With its help, you can build diverse charts, from histograms and scatterplots to non-Cartesian coordinates graphs. Moreover, many popular plotting libraries are designed to work in conjunction with matplotlib"
Top 20 Python Libraries for Data Science in 2018 - KDnuggets,"Seaborn is essentially a higher-level API based on the matplotlib library. It contains more suitable default settings for processing charts. Also, there is a rich gallery of visualizations including some complex types like time series, jointplots, and violin diagrams"
Top 20 Python Libraries for Data Science in 2018 - KDnuggets,"Plotly is a popular library that allows you to build sophisticated graphics easily. The package is adapted to work in interactive web applications. Among its remarkable visualizations are contour graphics, ternary plots, and 3D charts"
Top 20 Python Libraries for Data Science in 2018 - KDnuggets,"Pydot is a library for generating complex oriented and non-oriented graphs. It is an interface to Graphviz, written in pure Python. With its help, it is possible to show the structure of graphs, which are very often needed when building neural networks and decision trees based algorithms"
Explaining the 68-95-99.7 rule for a Normal Distribution - KDnuggets,"To get the probability of an event within a given range we will need to integrate. Suppose we are interested in finding the probability of a random data point landing within 1 standard deviation of the mean, we need to integrate from -1 to 1. This can be done with SciPy"
5 Key Takeaways from Strata London 2018 - KDnuggets,"General Data Protection Regulation (GDPR) is here to stay. Many of the topics throughout the conference were related to GDPR. It affects how Data Science will be performed, forcing companies and professionals who handle information to follow a guideline on how user data should be used. However, this regulation will create a new opportunity for companies to develop products based on trust"
5 Key Takeaways from Strata London 2018 - KDnuggets,"On the Internet, there are plenty of tutorials, workshops, MOOCs, etc. However, how many MOOCs are there on Ethics? It would be interesting to find a report on how much attention is given to ethics within all these resources when we are reading about Machine Learning. We shouldn’t be concerned only about how robust is our model; the ethical implication of the use case should be considered as a critical part of the Machine Learning pipeline"
5 Key Takeaways from Strata London 2018 - KDnuggets,"That’s a challenge when most users want to understand how the model works in order to trust it. For instance, a person would probably have more confidence in their lawyer than in an AI suggestion. Currently, there are many black-box models which are doing amazing things in the field and the good news is we’re also advancing in developing methods to explain these models. One of them is"
5 Key Takeaways from Strata London 2018 - KDnuggets,"One important message delivered over and over again across all ranges of sessions was the importance of the Data Science process. I would say that Pipeline + Teamwork + Documentation are three important components to deliver good projects. A good pipeline and a well-documented registration of the outcome from every step lets you bring faster results and avoid mistakes made in previous projects. In a talk explaining the lessons learned from several Data Science projects at Microsoft,"
5 Key Takeaways from Strata London 2018 - KDnuggets,"A lot of data related tools were covered in the conference. It’s essential to bear in mind that most of the technologies used today may not be the same in a few years. Hadoop, Spark, Rstudio, Python, Tensorflow, Kafka, Neo4j, Dock, and Apache Flink were some of the examples mentioned across the event. Although, in the session given by"
Key Takeaways from the Strata San Jose 2018 - KDnuggets,"There were about a similar number of sessions on Data Engineering / Architecture as on Data Science and Machine Learning, with a lot of emphasis on cloud, streaming and real-time applications. The shift away from just Hadoop was evident. You can view the Keynotes on the O’Reilly YouTube channel"
Data Science Predicting The Future - KDnuggets,"In fact, everything is connected. Once the BI reports and dashboards have been prepared and insights – extracted from them – this information becomes the basis for predicting future values. And the accuracy of these predictions lies in the methods used"
Data Science Predicting The Future - KDnuggets,"We can make a similar distinction regarding predictive analytics and their methods: traditional data science methods vs. Machine Learning. One deals primarily with traditional data, and the other – with big data"
Data Science Predicting The Future - KDnuggets,"In data science, the linear regression model is used for quantifying causal relationships among the different variables included in the analysis. Like the relationship between house prices, the size of the house, the neighborhood, and the year built. The model calculates coefficients with which you can predict the price of a new house, if you have the relevant information available"
Data Science Predicting The Future - KDnuggets,"Since it's not possible to express all relationships between variables as linear, data science makes use of methods like the logistic regression to create non-linear models. Logistic regression operates with 0s and 1s. Companies apply logistic regression algorithms to filter job candidates during their screening process, for instance. If the algorithm estimates that the probability that a prospective candidate will perform well in the company within a year is above 50%, it would predict 1, or a successful application. Otherwise, it will predict 0"
Data Science Predicting The Future - KDnuggets,"If clustering is about grouping observations together, factor analysis is about grouping features together. Data science resorts to using factor analysis to reduce the dimensionality of a problem. For example, if in a 100-item questionnaire each 10 questions pertain to a single general attitude, factor analysis will identify these 10 factors, which can then be used for a regression that will deliver a more interpretable prediction. A lot of the techniques in data science are integrated like this"
Data Science Predicting The Future - KDnuggets,"When companies launch a new product, they often design surveys that measure the attitudes of customers towards that product. Analysing the results after the BI team has generated their dashboards includes grouping the observations into segments (e. The results of these operations often corroborate the conclusion that the product needs slight but significantly different adjustments in each segment in order to maximize customer satisfaction"
Data Science Predicting The Future - KDnuggets,"This is the type of analysis where time series comes into play. Sales data has been gathered until a certain date, and the data scientist wants to know what is likely to happen in the next sales period, or a year ahead. They apply mathematical and statistical models and run multiple simulations; these simulations provide the analyst with future scenarios. This is at the core of data science, because based on these scenarios, the company can make better predictions and implement adequate strategies"
Data Science Predicting The Future - KDnuggets,"The data scientist. But bear in mind that this title also applies to the person who employs machine learning techniques for analytics, too. A lot of the work spills from one methodology to the other"
Data Science Predicting The Future - KDnuggets,"The main advantage machine learning has over any of the traditional data science techniques is the fact that at its core resides the algorithm. These are the directions a computer uses to find a model that fits the data as well as possible. The difference between machine learning and traditional data science methods is that we do not give the computer instructions on how to find the model; it takes the algorithm and uses its directions to learn on its own how to find said model. Unlike in traditional data science, machine learning needs little human involvement. In fact, machine learning, especially deep learning algorithms are so complicated, that humans cannot genuinely understand what is happening “inside”"
Data Science Predicting The Future - KDnuggets,"To be clear, here we must note that machine learning methods STEP ON traditional ones. Supervised learning, for example, has two subtypes – regression and classification (e. Naturally, many traditional methods also fall under the ‘machine learning’ umbrella term. That is logical becauselinear regression is the basis of many other methods, including deep neural networks"
Data Science Predicting The Future - KDnuggets,"Either way, the distinction between traditional methods and ML is more or less subjective. Some draw a line, others don’t. In our framework, the simplicity (which is also elegance in a way) of traditional methods is the main reason for the distinction. An interesting point of view on that issue can be explored here:"
Data Science Predicting The Future - KDnuggets,"Supervised learning rests on using labeled data. The machine gets data that is associated with a correct answer; if the machine’s performance does not get that correct answer, an optimization algorithm adjusts the computational process, and the computer does another trial. Bear in mind that, typically, the machine does this on 1000 data points at once"
Data Science Predicting The Future - KDnuggets,"When the data is too big, or the data scientist is under too much pressure for resources to label the data, or they do not know what the labels are at all, data science resorts to using unsupervised learning. This consists of giving the machine unlabeled data and asking it to extract insights from it. This often results in the data being divided in a certain way according to its properties. In other words, it is clustered"
Data Science Predicting The Future - KDnuggets,"This is a type of machine learning where the focus is on performance (to walk, to see, to read), instead of accuracy. Whenever the machine performs better than it has before, it receives a reward, but if it performs sub-optimally, the optimization algorithms do not adjust the computation. Think of a puppy learning commands. If it follows the command, it gets a treat; if it doesn’t follow the command, the treat doesn’t come. Because treats are tasty, the dog will gradually improve in following commands. That said, instead of minimizing an error, reinforcement learning maximizes a reward"
Data Science Predicting The Future - KDnuggets,"With machine learning algorithms, corporate organizations can know which customers may purchase goods from them. This means the store can offer discounts and a ‘personal touch’ in an efficient way, minimizing marketing costs and maximizing profits. A couple of prominent names come to mind: Google, and Amazon"
Data Science Predicting The Future - KDnuggets,R and Python are the two most popular tools across all data science sub-disciplines. Their biggest advantage is that they can manipulate data and are integrated within multiple data and data science software platforms. They are not just suitable for mathematical and statistical computations; they are adaptable
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","This post includes the debate which has arisen from Kevin's original response article. While KDnuggets takes no side, we present the informative and respectful back and forth as we believe it has value for our readers. We hope that you agree"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","Kevin’s prediction that many statisticians may find my views “odd or exaggerated” is accurate. This is exactly what I have found in numerous conversations I have had with statisticians in the past 30 years. However, if you examine my views more closely, you will find that they are not as whimsical or thoughtless as they may appear at first sight"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","Of course many statisticians will scratch their heads and ask: “Isn’t this what we have been doing for years, though perhaps under a different name or not name at all?” And here lies the essence of my views. Doing it informally, under various names, while refraining from doing it mathematically under uniform notation has had a devastating effect on progress in causal inference, both in statistics and in the many disciplines that look to statistics for guidance. The best evidence for this lack of progress is the fact that, even today, only a small percentage of practicing statisticians can solve any of the causal toy problems presented in the Book of Why"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","A valid question to be asked at this point is what gives humble me the audacity to state so sweepingly that no statistician (in fact no scientist) was able to properly solve those toy problems prior to the 1980’s. How can one be so sure that some bright statistician or philosopher did not come up with the correct resolution of the Simpson’s paradox or a correct way to distinguish direct from indirect effects? The answer is simple: we can see it in the syntax of the equations that scientists used in the 20th century. To properly define causal problems, let alone solve them, requires a vocabulary that resides outside the language of probability theory, This means that all the smart and brilliant statisticians who used joint density functions, correlation analysis, contingency tables, ANOVA, etc"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","It is this notational litmus test that gives me the confidence to stand behind each one of statements that you were kind enough to cite from the Book of Why. Moreover, if you look closely at this litmus test, you will find that it not just notational but conceptual and practical as well. For example, Fisher’s blunder of using ANOVA to estimate direct effects is still haunting the practices of present day mediation analysts. Numerous other examples are described in the Book of Why and I hope you weigh seriously the lesson that each of them conveys"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","I appreciate Judea Pearl taking the time to read and respond to my blog post. I’m also flattered, as I have been an admirer and follower of Pearl for many years. For some reason - this has happened before - I am having difficulty with Disqus and have asked Matt Mayo to post this (admittedly hastily written) response on my behalf"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","The suggestion of his opening comment is that I am only superficially acquainted with his views. Obviously, I do not feel this is the case. Also, nowhere in the article do I describe any of Pearl’s views as whimsical or thoughtless. I take them very seriously, or I wouldn’t have bothered writing my blog article in the first place"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","Like many other statisticians, including academics, I feel his characterizations of statisticians are inaccurate, however, and that his approach to caution is overly simplistic. That is the essence of my views (and, as I indicated, I am not alone). There are many possible reasons for these discrepancies, I will not speculate here as to Why"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","That “you can’t answer a question if you have no words to ask it” is certainly true. Practicing statisticians - as opposed to theoretically focused academics - work closely with their clients, who are specialists in a specific area. The language of that field largely defines the language of causation used in a particular context. That is why there has been no universal causal framework, and why statisticians in psychology, healthcare and economics, for instance, have different approaches and often use different language and mathematics. I myself draw upon all three, as well as Pearl’s. The utility of each, in my 30+ years’ experience, is case-by-case. There are also ad hoc approaches (some questionable)"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","He claims that “…even today, only a small percentage of practicing statisticians can solve any of the causal toy problems presented in the Book of Why” yet provides no evidence of this. They are not difficult, and claims are not evidence. Furthermore, the examples he gives in “Every chapter of The Book of Why” of the failure of statistics are, in general, not compelling and were part of the motivation for the article in the first place"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets",Simpson’s paradox in its various forms is something that generations of researchers and statisticians have been trained to look out for. And we do. There is nothing mysterious about it
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","Mediation, often confused with moderation, can be a tough nut to crack. Simple path diagrams or DAG with only few variables will frequently be inadequate and may badly mislead us. In the real world of a statistician, there are frequently a vast number of potentially relevant variables, including those we are unable to observe. There are also critical variables that, for many reasons, are not included in the data we have to work with and cannot be obtained. Measurement error may be substantial - this is not rare - and different causal mechanisms for different classes of subjects (e. These classes are often unobserved and unknown to us"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","I should make clear that I greatly appreciate Pearl’s efforts at bringing the analysis of causation into the spotlight. I would urge statisticians to read him but also consult (as I do) with other veteran practitioners and academics for their views on his writings and on the analysis of causation. There are many ways to approach causation and Pearl’s is but one. As Pearl himself will be aware, to this day, philosophers disagree about what causation is, thus to suggest he has found the answer to it is not plausible in my view. Real statisticians know better than to look for a Silver Bullet"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","I am not suggesting that you are only superficially acquainted with my works. You actually show much greater acquaintance than most statisticians in my department, and I am extremely appreciative that you are taking the time to comment on The Book of Why. You are showing me what other readers with your perspective would think about the Book, and what they would find unsubstantiated or difficult to swallow. So let us go straight to these two points (i"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","Let us take the first example that you bring, Simpson’s paradox, which is treated in Chapter 6 of the Book, and which is familiar  to every red-blooded statistician. I characterized the paradox in these words: “It has been bothering statisticians for more than sixty years – and it remains vexing to this very day” (p. 201). This was, as you rightly noticed, a polite way of saying: “Even today, the vast majority of statisticians cannot solve Simpson’s paradox,” a fact which I strongly believe to be true"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","This distinction appears vividly in the debate that took place in 2014 on the pages of The American Statistician, which you and I cite.  However, whereas you see the disagreements in that debate as evidence that statisticians have several ways of resolving Simpson’s paradox, I see it as evidence that they did not even come close. In other words, none of the other participants presented a method for deciding whether the aggregated data or the segregated data give the correct answer to the question: “Is the treatment helpful or harmful?”"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","I dont blame Liu and Meng for erring on this point, it is not entirely their fault (Rosenbaum and Rubin made the same error). The correct solution to Simpson’s dilemma rests on the back-door criterion, which is almost impossible to articulate without the aid of DAGs. And DAGs, as you are probably aware, are forbidden from entering a 5 mile no-fly zone around Harvard"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","One thing I do agree with you — your warning about the implausibility of the Causal Revolution. Quoting: “to this day, philosophers disagree about what causation is, thus to suggest he has found the answer to it is not plausible”.  It is truly not plausible that someone, especially a semi-outsider, has found a Silver Bullet. It is hard to swallow. That is why I am so excited about the Causal Revolution and that is why I wrote the book. The Book does not offer a Silver Bullet to every causal problem in existence, but it offers a solution to a class of problems that centuries of statisticians and Philosophers tried and could not crack. It is implausible, I agree, but it happened. It happened not because I am smarter but because I took Sewall Wright’s idea seriously and milked it to its logical conclusions as much as I could"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets",It took quite a risk on my part to sound pretentious and call this development a Causal Revolution. I thought it was necessary. Now I am asking you to take a few minutes and judge for yourself whether the evidence does not justify such a risky characterization
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","Thank you once again for taking the time to respond to my blog post and earlier response.  And thank you for the kind words, especially given that UCLA is on the cutting edge of both SEM and AI. No no-fly zones there"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","The orthodoxy is that the best approach is through randomized experiments. While I concur, experiments in many cases are infeasible or unethical. They also can be botched or be so artificial that they do not generalize to real world conditions. They may also fail to replicate. They are not magic"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets",Non-experimental research may be our only option in many instances. But this does not mean data dredging or otherwise shoddy research. There are better and worse ways to conduct non-experimental research. That's the hitch - it ain't easy to get right and mistakes can prove very costly
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","My last point is most important, I feel. Some folks seem to have misinterpreted your writings as suggesting theory and judgement are irrelevant and that the computer can find the “best model” automatically. Shotgun empiricism in its most modern form…This is exacerbated by point-and-click and click-and-drag software which is arguably easier to use than Microsoft Word. Shotguns (and sometimes DAG) in the hands of children…"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","In the big data age, people in my sort of role are sometimes tasked with determining the relative importance of thousands of variables, sometimes individually for each consumer. It is not generally recognized that that this is causal analysis. At the other extreme, sweeping causal implications are sometimes drawn based on a few focus groups. Again, this is frequently not recognized as causal research"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","Regarding Simpson’s paradox, I’ll let you continue to slug it out with your academic colleagues. I approach this case-by-case with the data on hand and according to the objectives of the research. Surely, some statisticians and researchers who should know better drop the ball on this. To be clear, I am I referring to practical solutions, not to theoretical mathematical solutions. While not personally acquainted with Professor Meng, I have watched several of his lectures on YouTube and read a few of his papers. He has much to say regarding common big data fallacies"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","In the applied world, I do note some people with excellent math and programing skills who are, nevertheless, not so hot at using statistics to solve real-world problems. This is not an original observation. Perhaps the ASA, RSS and other professional organizations would be interested in periodic skills assessments, purely voluntary and with the aim of professional development?"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","I imagine an on-line quiz with a small number of applied stats questions – not math, except, for some essential areas. It would be confidential. As an incentive, participants would be shown the correct answers after taking the test. Over time, participants would be able to get a sense of where they are strong and where they need to improve. We would also have some empirical evidence (if not from a truly representative sample) regarding the level of skill in key areas of practicing statisticians and researchers, which would be publishable. I would hope you would contribute some of the questions!"
"Statistics, Causality, and What Claims are Difficult to Swallow: Judea Pearl debates Kevin Gray - KDnuggets","For statisticians and researchers in any field with a good background in stats, I regard Causation as must reading. The Book of Why I feel also has value, as I also noted.  You’ll be familiar of course with"
9+ Rising Stars of Data Science - KDnuggets,"Geoffrey Hinton, Yann LeCun, and Andrew Ng are some of the most influential data scientists on the planet. There’s a new cluster of rising stars and ODSC West 2018 is delighted to be hosting many of them. Here’s why they matter"
9+ Rising Stars of Data Science - KDnuggets,D. He’s at the forefront of the autonomous vehicle revolution as Director of AI at Tesla and also a prolific blogger on topics like neural net enabled Software 2.0
A FREE Live Online Conference For Aspiring Data Scientists & Data-Curious Business Leaders - KDnuggets,"Demystifying Data Science - a completely free, live online conference for aspiring data scientists and data-curious business professionals, July 24-25. Experience 28 interactive data science talks from industry-leading speakers. Register now!"
A FREE Live Online Conference For Aspiring Data Scientists & Data-Curious Business Leaders - KDnuggets,"On day two,14 speakers will explain how data science applies to you and what needs to be done to integrate it into your organization. Attendees will hear a keynote talk from Beth Comstock, change maker, author, and former Vice Chair of General Electric. Her mission is to understand what’s next, navigate change, and help people and organizations do the same"
"The What, Where and How of Data for Data Science - KDnuggets","Data Science is a term that escapes any single complete definition, which makes it difficult to use, especially if the goal is to use it correctly. Most articles and publications use the term freely, with the assumption that it is universally understood. However, data science – its methods, goals, and applications – evolve with time and technology. Data science 25 years ago referred to gathering and cleaning datasets then applying statistical methods to that data. In 2018,"
"The What, Where and How of Data for Data Science - KDnuggets","Of course, this might look like a lot of overwhelming information, but it really isn’t. In this article, we will take data science apart and we will build it back up to a coherent and manageable concept. Bear with us!"
"The What, Where and How of Data for Data Science - KDnuggets","You have data. To use this data to inform your decision-making, it needs to be relevant, well-organized, and preferably digital. Once your data is coherent, you proceed with analyzing it, creating dashboards and reports to understand your business’s performance better. Then you set your sights to the future and start generating predictive analytics. With predictive analytics, you assess potential future scenarios and predict consumer behavior in creative ways"
"The What, Where and How of Data for Data Science - KDnuggets","Before anything else, there is always data. Data is the foundation of data science; it is the material on which all the analyses are based. In the context of data science,"
"The What, Where and How of Data for Data Science - KDnuggets","Traditional data is data that is structured and stored in databases which analysts can manage from one computer; it is in table format, containing numeric or text values. Actually, the term “traditional” is something we are introducing for clarity. It helps emphasize the distinction between big data and other types of data"
"The What, Where and How of Data for Data Science - KDnuggets","That said, before being ready for processing, all data goes through pre-processing. This is a necessary group of operations that convert raw data into a format that is more understandable and hence, useful for further processing. Common processes are:"
"The What, Where and How of Data for Data Science - KDnuggets","When collecting data on a mass scale, this aims to ensure that any confidential information in the data remains private, without hindering the analysis and extraction of insight. The process involves concealing the original data with random and false data, allowing the scientist to conduct their analyses without compromising private details. Naturally, the scientist can do this to traditional data too, and sometimes is, but with big data the information can be much more sensitive, which masking a lot more urgent"
"The What, Where and How of Data for Data Science - KDnuggets","Big data, however, is all-around us. A consistently growing number of companies and industries use and generate big data. Consider online communities, for example, Facebook, Google, and LinkedIn; or financial trading data. Temperature measuring grids in various geographical locations also amount to big data, as well as machine data from sensors in industrial equipment. And, of course, wearable tech"
"The What, Where and How of Data for Data Science - KDnuggets","The data specialists who deal with raw data and pre-processing, with creating databases, and maintaining them can go by a different name. But although their titles are similar sounding, there are palpable differences in the roles they occupy. Consider the following"
"The What, Where and How of Data for Data Science - KDnuggets","The former creates the database from scratch; they design the way data will be retrieved, processed, and consumed. Consequently, the data engineer uses the data architects’ work as a stepping stone and processes (pre-processes) the available data. They are the people who ensure the data is clean and organized and ready for the analysts to take over"
"The What, Where and How of Data for Data Science - KDnuggets","The Database Administrator, on the other hand, is the person who controls the flow of data into and from the database. Of course, with Big Data almost the entirety of this process is automated, so there is no real need for a human administrator. The Database Administrator deals mostly with traditional data"
"The What, Where and How of Data for Data Science - KDnuggets","Notably, analysts apply data science to inform things like price optimization techniques. They extract the relevant information in real time, compare it with historicals, and take actions accordingly. Consider hotel management behavior: management raise room prices during periods when many people want to visit the hotel and reduce them when the goal is to attract visitors in periods with low demand"
"The What, Where and How of Data for Data Science - KDnuggets","The BI consultant is often just an ‘external BI analysts’. Many companies outsource their data science departments as they don’t need or want to maintain one. BI consultants would be BI analysts had they been employed, however, their job is more varied as they hop on and off different projects. The dynamic nature of their role provides the BI consultant with a different perspective, and whereas the BI Analyst has highly specialized knowledge (i"
5 of Our Favorite Free Visualization Tools - KDnuggets,"It’s becoming increasingly apparent that the collection and application of data is more important than ever for businesses looking to continue to grow. The downside? As we continually discover ways in which data is beneficial, the more difficult it can become for individuals to compile, present and implement findings. Thankfully, there’s an influx of free data visualization tools that are able to take your unique data, both spatial and tabular, and present the information back to you through the use of advanced graphs and charts"
5 of Our Favorite Free Visualization Tools - KDnuggets,"R Shiny is an open source package that provides the web framework for building data visualizations, interactive charts, and applications using R. This tool helps you to turn your analyses into sleek interactive web visuals without it requiring in-depth knowledge of HTML, CSS or JavaScript. Much like a spreadsheet, this reactive programming model allows for data to be manipulated with ease rather than waiting for the entire page to be reloaded"
5 of Our Favorite Free Visualization Tools - KDnuggets,"With the ability to display graphs, charts, maps and more, Tableau Public is a popular data visualization tool that's also totally free. With up to 10 GB of storage and a drag-and-drop interface, users can watch their data update in real-time while collaborating with others on their team. The “public” portion of Tableau means that you can only save your data to public profile where others have access to your data, but if you’re not a highly public company whose privacy is your #1 concern, there are a plethora of upsides to Tableau Public for business analysts and managers. The newest version is optimized for mobile devices, can connect to a variety of data sources beyond Excel, and can link directly to Google Sheets"
5 of Our Favorite Free Visualization Tools - KDnuggets,"Datawrapper is a great open source tool for the complete visualization of data and the ability to embed live and interactive charts. Simply upload your data in a CSV file and the online tool is able to build customized visuals such as bar and line graphs. Datawrapper is great for small business or presentation use, as it allows for only 10,000 views per chart, but it may not be ideal for big businesses with a large clientele. However, most people agree that the easy to use interface and ability to quickly present statistics in a straightforward manner is helpful"
5 of Our Favorite Free Visualization Tools - KDnuggets,"Pivot is an intuitive UI designed to enable exploratory analytics on event data while utilizing the much appreciated drag-and-drop interface. One of the attributes that sets Pivot apart is that it’s centered around two operations: Filter and Split. Filter narrows the view of data and is equivalent to the “WHERE” clause in SQL, where as Split is very similar to SQL’s “GROUP BY” function. However, Split allows for data to be cut across multiple dimensions -- we’ve seen great success in grocery price/promotional analysis and optimization"
5 of Our Favorite Free Visualization Tools - KDnuggets,"D3, which stands for data driven documents, is a JavaScript library that binds to arbitrary data to a Document Object Model (DOM), and then applies data-driven transformations to the document. Though D3 may appeal more towards programmers as this tool involves creating code, D3 is able to build an array of truly engaging charts, maps, diagrams within web pages, and more. If you’re willing to put in a bit of extra work, the visual payout absolutely more than worth it"
DIY Deep Learning Projects - KDnuggets,"To perform video tracking an algorithm analyzes sequential video frames and outputs the movement of targets between the frames. There are a variety of algorithms, each having strengths and weaknesses. Considering the intended use is important when choosing which algorithm to use. There are two major components of a visual tracking system: target representation and localization, as well as filtering and data association"
DIY Deep Learning Projects - KDnuggets,The concept of facenets was originally presented in a research paper. The main concepts talked about triplet loss function to compare images of different person. This concept uses inception network which has been taken from source and fr_utils. I have added several functionalities of my own for providing stability and better detection
DIY Deep Learning Projects - KDnuggets,"Emojis are ideograms and smileys used in electronic messages and web pages. Emoji exist in various genres, including facial expressions, common objects, places and types of weather, and animals. They are much like emoticons, but emoji are actual pictures instead of typographics"
DIY Deep Learning Projects - KDnuggets,"I want to thank Akshay and his friends for making this great Open Source contributions and for all the others that will come. Try them, run them, and get inspired. This is only a small example of the amazing things DL and CV can do, and is up to you to take this an turn it into something that can help the world become a better place"
DIY Deep Learning Projects - KDnuggets,"He has a passion for science, philosophy, programming, and music. Right now he is working on data science, machine learning and big data as the Principal Data Scientist at Oxxo. Also, he is the creator of Ciencia y Datos, a Data Science publication in Spanish. He loves new challenges, working with a good team and having interesting problems to solve. He is part of Apache Spark collaboration, helping in MLlib, Core and the Documentation. He loves applying his knowledge and expertise in science, data analysis, visualization, and automatic learning to help the world become a better place"
How (dis)similar are my train and test data? - KDnuggets,"They always say that don’t compare apples to oranges. But how about if we’re comparing one mix of apples and oranges with another mix of apples and oranges, but the distribution is different. Can you still compare ? And how will you go about it ?"
How (dis)similar are my train and test data? - KDnuggets,"I’m not referring to overfitting here. Even if I’ve picked my best model based on cross-validation and it still performs poorly on test data, there is some inherent patterns in test data that we are not capturing. Now if my train and test data looks like below then you can clearly see the issue here"
How (dis)similar are my train and test data? - KDnuggets,"The model will be trained on customers with lower average age compared to test. This model would have never seen age patterns like the ones in test data. If age is an important feature in your model, then it’ll not perform well on the test"
How (dis)similar are my train and test data? - KDnuggets,"Let’s understand why. Consider the above example where age was the drifting feature between test and train. If we take a classifier like Random Forest and try to classify rows into test and train, age will be come out be very important feature in splitting the data"
How (dis)similar are my train and test data? - KDnuggets,"If the classifier is able to classify the rows into train and test with good accuracy, our AUC score should be on the higher side (greater than 0.8). This implies strong covariate shift between train and test"
How (dis)similar are my train and test data? - KDnuggets,AUC value of 0.49 implies that there is no evidence of strong covariate shift. This means that majority of the observations comes from a feature space which is not specific to test or train
How (dis)similar are my train and test data? - KDnuggets,So for the first row our classifier thinks that it belongs to training data with .34 probability. Let’s call this P(train). Or we can also say that it has .66 probability of being from the test data. Let’s call this as P(test). Now here is the magic trick:
Netflix Data Science Interview Questions: Acing the AI Interview - KDnuggets,This highlights the focus Netflix has on Deep Learning and Data Science. The site is extremely well designed showing vertical classification of the different areas that Netflix research works on along with the horizontal business areas where Data Science is deployed at Netflix. It has some great articles with everything from video encoding to A/B testing where they use Data Science. I found the website to be very comprehensive making it a go to destination for things Netflix Data Science from different verticals to jobs
Netflix Data Science Interview Questions: Acing the AI Interview - KDnuggets,"To maximize the impact of their research, Netflix does not centralize research into a separate organization. Instead, they have many teams that pursue research in collaboration with business teams, engineering teams, and other researchers. From our publications we can deduce that they are focused on the applied side of the research spectrum, though they do pursue fundamental research and think that has the potential for high impact, such as improving our understanding of causality in our"
Netflix Data Science Interview Questions: Acing the AI Interview - KDnuggets,"Netflix moves quite fast. There is one phone interview with the recruiter and another detailed one with the hiring manager. There are two onsite interviews with around 4 people first time (data scientists/engineers) and 3 people (higher level execs) second time. There is a mix of product, business, analytical and statistical questions. Statistical questions mostly revolve around A/B testing: hypothesis testing. There are a couple of SQL questions too. Analytical questions usually includes a hypothetical problem to analyze and metrics to evaluate product performance. Higher level executives mostly focus on background and past experience"
Netflix Data Science Interview Questions: Acing the AI Interview - KDnuggets,"The data around Netflix questions is sparse. The high level questions resolve around A/B testing, recommender systems and foundational knowledge questions around regularization and activation functions. This is different from the other companies we have looked at"
Automated Machine Learning vs Automated Data Science - KDnuggets,"Just think about how broad and inclusive this description is. So inclusive is it, in fact, that there is no agreed upon definition or scope for data science. Sure, lots of attempts at both can be found, and there may be some agreement on where very loose boundaries are, but no one will convince me that there exists some definition to any notable degree of specificity that a majority of data scientists would accept. Also, what exactly is a"
Automated Machine Learning vs Automated Data Science - KDnuggets,"I (and many others) would argue that machine learning is a relatively well-defined and comparatively narrow field, while ""data science"" is most definitely not. Just by adding the term ""automated"" in front of these 2 separate, distinct concepts does not somehow make them equivalent. Machine learning and data science are not the same thing, just like"
Automated Machine Learning vs Automated Data Science - KDnuggets,"Since, in my view, the practice of machine learning comes down to 2 main overarching tasks, in a restrained practical definition we can consider the core of automated machine learning to be 1) the automated optimization of feature engineering and/or selection, and 2) hyperparameter tuning. Semantically, the training of a machine learning model, while the result of these automated steps, is incidental to the automated machine learning process, while automated steps such as model evaluation and model selection are ancillary to the core. See Figure 1"
Automated Machine Learning vs Automated Data Science - KDnuggets,"Some aspects of data science are obviously more difficult to automate than others (hypothesis testing, communication, domain knowledge, formulating data-based strategies, etc. Human involvement, for the foreseeable future, is paramount, not only for overseeing and correcting course for any level of automation, but also to kick off searches for insight. We may be able to automate exploratory investigations of what questions we should be looking to potentially apply the data science process to in the hopes of answering, and even have this phase augmented by facts and figures, but the human element will need to make nuanced decisions on which courses of action are worthy of pursuit"
5 Data Science Projects That Will Get You Hired in 2018 - KDnuggets,"You’ve been taking MOOCs and reading a bunch of textbooks, but now what do you do? Getting a job in data science can seem daunting. The best way to showcase your skills is with a portfolio. This shows employers that you can use the skills you’ve been learning"
5 Data Science Projects That Will Get You Hired in 2018 - KDnuggets,"This is a huge pain point for teams. If you can show that you’re experienced at cleaning data, you’ll immediately be more valuable. To create a data cleaning project, find some messy data sets, and start cleaning"
5 Data Science Projects That Will Get You Hired in 2018 - KDnuggets,"Another important aspect of data science is exploratory data analysis (EDA). This is the process of generating questions, and investigating them with visualizations. EDA allows an analyst to draw conclusions from data to drive business impact. It might include interesting insights based on customer segments, or sales trends based on seasonal effects. Often you can make interesting discoveries that weren’t initial considerations"
5 Data Science Projects That Will Get You Hired in 2018 - KDnuggets,"Interactive data visualizations include tools such as dashboards. These tools are useful for both data science teams, as well as more business-oriented end users. Dashboards allow data science teams to collaborate, and draw insights together. Even more important, they provide an interactive tool for business-oriented customers. These individuals focus on strategic goals rather than technical details. Often the deliverable for a data science project to a client will be in the form of a dashboard"
5 Data Science Projects That Will Get You Hired in 2018 - KDnuggets,"A machine learning project is another important piece of your data science portfolio. Now before you run off and start building some deep learning project, take a step back for a minute. Rather than building a complex machine learning model, stick with the basics. Linear regression and logistic regression are great to start with. These models are easier to interpret and communicate to upper level management. I’d also recommend focusing on a project that has a business impact, such as predicting customer churn, fraud detection, or loan default. These are more real-world than predicting flower type"
5 Data Science Projects That Will Get You Hired in 2018 - KDnuggets,"Communication is an important aspect of data science. Effectively communicating results is what separates the good data scientists from the great ones. It doesn’t matter how fancy your model is, if you can’t explain it to teammates or customers, you won’t get their buy-in. Slides and notebooks are both great communication tools. Use one of your machine learning projects and put it into slide format. You could also use a"
5 Data Science Projects That Will Get You Hired in 2018 - KDnuggets,Make sure to understand who your intended audience is. Presenting to executives is very different than presenting to machine learning experts. Make sure to hit on these skills:
Advice For Applying To Data Science Jobs - KDnuggets,"A disclaimer: I have never worked as a recruiter or career coach. This knowledge comes from mainly from my study of Organizational Behavior (including negotiations and women in tech) in graduate school and my own career. This advice also will not fit every single situation. The people who will benefit the most from this post are probably those who are applying for their first data science job and/or are in or recently out of school. If there’s another tip or a caveat I should add, send me a note on twitter!"
Advice For Applying To Data Science Jobs - KDnuggets,"Take stock of your online presence. Check your privacy settings on Facebook or any other social media you use to make sure you’ve limited what’s available publicly. While many online job applications now have places for twitter handles, don’t include yours unless you use it mainly in a professional manner (e. Pets are always appropriate)"
Advice For Applying To Data Science Jobs - KDnuggets,"If you have a GitHub, pin the repos you want people to see and add READMEs that explain what the project is. I also strongly recommend creating a blog to write about data science, whether it’s projects you’ve worked on, an explanation of a machine learning method, or a summary of a conference you attended. If you need more convincing that blogging is a great use of your time, check out"
Advice For Applying To Data Science Jobs - KDnuggets,"For the initial offer, you may get a phone call, an email with the details, or an email asking for time to go over the offer by phone. In all cases, make sure you express your excitement and gratitude for the opportunity. Don’t accept right away: get the full offer in writing, say you need to review it, and ask if you can reconnect in a few days. This sets the stage for negotiating and gives you time if you’re considering other offers. If you need more information (e"
Advice For Applying To Data Science Jobs - KDnuggets,"Unfortunately, you probably won’t get offers (or rejections) from all the companies you’re interested in at once. More likely, when you get an offer, you might be in the final round for another company, just finished with the phone screen in another, and waiting to hear back from others. What if one of the ones you’re waiting on is your “dream job”?"
Advice For Applying To Data Science Jobs - KDnuggets,"If you’re in or very close to the final round with a company, tell them about your other option. Reiterate how interested you are in their company (and if they’re your first choice, tell them!), but explain that you’re received another offer and would appreciate if they would be able to give you a decision by X date. Recruiters understand that you’re likely to be interviewing with other companies and deal with this situation all the time. If you’re very early in the other process, it’s unlikely they’ll be able to speed it along enough to have an offer in time, and you may need to decide whether to accept the offer without knowing your other options"
Advice For Applying To Data Science Jobs - KDnuggets,You can almost always get at least another $5k or a 5% increase in base salary and very likely more depending on the company and original offer. You could also ask for a signing bonus or more stock options. Remember that it’s not selfish or greedy to negotiate:
Advice For Applying To Data Science Jobs - KDnuggets,"Other thoughts: it's not about the salary you make now. It's about the salaries of your peers at the new place. It may be a large bump for you now, but once you're hired you'll be doing the same work for less pay"
Advice For Applying To Data Science Jobs - KDnuggets,"Also think about the non-monetary benefits you’re interested in. For example, maybe you want to be able to work from home one day a week. Maybe you want them to cover the cost of two conferences a year. If it’s a smaller company, you can ask for a different job title"
Advice For Applying To Data Science Jobs - KDnuggets,"How much you can negotiate for depends on your bargaining position and the company. A non-profit will probably have less room on salary. If you have another offer or you’re well compensated in your current job, you have a strong alternative. If you have a rare skill that they recruited you for or the role has been open for a long time, it’s going to be harder for them to find someone else. You should do your research on the salary websites mentioned above so you can explain why you’re asking for the numbers you are. Aim high so you have room to compromise and give a number, not a range - if you say you’re looking for between a 5k and 10k increase, they’ll probably give you the 5k. Reiterate why you’re excited about the company and position and how you bring X, Y, Z to the table. Finally, start the conversation with the set of things you’re interested in instead of raising and settling one issue after another. This way, they’ll both feel they have the picture of what you want and you can do compromises among issues (e"
Advice For Applying To Data Science Jobs - KDnuggets,"Job hunting is stressful, especially if you’re working full-time and/or looking to change industries. I hope this post has given you a good starting point for understanding the hiring process in data science, what mistakes to avoid, and what strategies you can leverage in the process. If you’re still looking more, this is an"
The Book of Why - KDnuggets,"UCLA computer scientist Judea Pearl has made noteworthy contributions to artificial intelligence, Bayesian networks, and causal analysis. These achievements notwithstanding, Pearl holds some views many statisticians may find odd or exaggerated. Here are a few examples from his latest book,"
The Book of Why - KDnuggets,"Though he can come across as an Ivory Tower academic whose arguments at times are muddled and contradictory, I suspect few seriously interested in the subject of causation consider Judea Pearl or his work bland or irrelevant. He is always thought-provoking and has much to say that should be heeded, and has provided statisticians and researchers with another set of useful tools for causal analysis. He should also be read because of his influence. In particular, I would recommend his"
The Book of Why - KDnuggets,"He has not revolutionized the analysis of causation, though, and as noted, many statisticians will probably find at least some of his opinions out of sync with their own perceptions of statistics and their professional brethren, as well as the history of their discipline. He makes many generalizations regarding what “all or most statisticians” would do in a given situation, and then shows us that this would be wrong. He offers no evidence in support of these generalizations, many of which strike me as what a competent statistician would"
The Book of Why - KDnuggets,"His charge that statisticians are focused on summarizing data, ignore the data generating process, and are uninterested in theory and causal analysis is particularly amusing in light of the at times acrimonious discussions between statisticians and data scientists from other backgrounds. He also disregards the myriad complex experimental designs and analyses of data obtained through these designs via ANOVA and MANOVA – which also can become quite complex – that explicitly consider a causal framework. These designs as well as ANOVA and MANOVA have been in use for many decades. Related to this, historically, statisticians have specialized in particular disciplines, such as agriculture, economics, pharmacology and psychology, because subject matter expertise is necessary for them to be effective - statistics is not just mathematics"
The Book of Why - KDnuggets,"More fundamentally, all statisticians are not equally competent or will even define competence in the same way. There are also academic statisticians and applied statisticians and often wide gulfs between the two. All of this is certainly true of just about any profession. Not all lawyers are ambulance chasers, and practicing attorneys are not clones of their former law school professors"
The Book of Why - KDnuggets,It differs starkly from believing causation is immaterial and should not be explored. Statisticians also contribute to the design of research and lengthy discussions about causation are not unusual. Most researchers are interested in the why and statisticians who aren’t are an endangered species
The Book of Why - KDnuggets,"Furthermore, practitioners, who enormously outnumber Ivory Tower statisticians, are sometimes given data with little background information and asked to find something “interesting. In these circumstances, any number of ad hoc causal models created manually or with automated software may fit the data about equally well but suggest very different courses of action for decision-makers. This can turn into a tightrope walk. More commonly, they may be given a set of cross tabulations and asked for their interpretation. It could be that something in the data does not make sense to the researcher or that s/he wants a second opinion"
The Book of Why - KDnuggets,"The Great Depression, WWII, the Korean War and the Cold War surely had some impact on its historical development. Let’s also not forget that it was not that long ago when calculations were done manually, statisticians had limited empirical data to work with and were unable to conduct Monte Carlo studies, now essential for many statisticians. Still, I agree with Pearl that statistics and causal analysis would have progressed more rapidly if Sewall Wright’s contributions, especially path analysis, had received the attention they merited"
The Book of Why - KDnuggets,"Apart from his strange opinions about statistics and statisticians, my main criticism of Pearl is that he overstates his case. For example, in my experience path diagrams and DAG are aids to causal analysis, but not essential. Some people find them confusing. He also appears little interested in sampling and data quality, or with the possibility that dissimilar causal models may operate for latent classes"
The Book of Why - KDnuggets,Most of these experiments are only reported in academic journals and go unnoticed except to specialists working in that area. Many are quite simple and do not require sophisticated mathematics and fancy software. They do not test grand theories that have direct and sweeping implications for public health and welfare. They are the regular guys of research and the unsung heroes of causal analysis
The Book of Why - KDnuggets,"Kevin’s prediction that many statisticians may find  my views “odd or exaggerated” is accurate. This is exactly what I have found in numerous conversations I have had with statisticians in the past 30 years. However, if you examine my views more closely,  you will find that they are not as whimsical or thoughtless as they may appear at first sight"
The Book of Why - KDnuggets,"Of course many statisticians will scratch their heads and ask: “Isn’t this what we have been doing for years, though perhaps under a different name or not name at all?” And here lies the essence of my views. Doing it informally, under various names, while refraining from doing it mathematically under uniform notation has had a devastating effect on progress in causal inference, both in statistics and in the many disciplines that look to statistics for guidance. The best evidence for this lack of progress is the fact that, even today, only a small percentage of practicing statisticians can solve any of the causal toy problems presented in the Book of Why"
The Book of Why - KDnuggets,"A valid question to be asked at this point is what gives humble me the audacity to state so sweepingly that no statistician (in fact no scientist) was able to properly solve those toy problems prior to the 1980’s. How can one be so sure that some bright statistician or philosopher did not come up with the correct resolution of the Simpson’s paradox or a correct way to distinguish direct from indirect effects? The answer is simple: we can see it in the syntax of the equations that scientists used in the 20th century.  To properly define causal problems, let alone solve them, requires a vocabulary that resides outside the language of probability theory, This means that all the smart and brilliant statisticians who used joint density functions, correlation analysis, contingency tables, ANOVA, etc"
The Book of Why - KDnuggets,"It is this notational litmus test that gives me the confidence to stand behind each one of statements that you were kind enough to cite from the Book of Why. Moreover, if you look closely at this litmus test, you will find that it not just notational but conceptual and practical as well. For example, Fisher’s blunder of using ANOVA to estimate direct effects is still haunting the practices of present day mediation analysts. Numerous other examples are described in the Book of Why and I hope you weigh seriously the lesson that each of them conveys"
The Book of Why - KDnuggets,"I appreciate Judea Pearl taking the time to read and respond to my blog post. I’m also flattered, as I have been an admirer and follower of Pearl for many years. For some reason - this has happened before - I am having difficulty with Disqus and have asked Matt Mayo to post this (admittedly hastily written) response on my behalf"
The Book of Why - KDnuggets,"The suggestion of his opening comment is that I am only superficially acquainted with his views. Obviously, I do not feel this is the case. Also, nowhere in the article do I describe any of Pearl’s views as whimsical or thoughtless. I take them very seriously, or I wouldn’t have bothered writing my blog article in the first place"
The Book of Why - KDnuggets,"Like many other statisticians, including academics, I feel his characterizations of statisticians are inaccurate, however, and that his approach to caution is overly simplistic. That is the essence of my views (and, as I indicated, I am not alone). There are many possible reasons for these discrepancies, I will not speculate here as to Why"
The Book of Why - KDnuggets,"That “you can’t answer a question if you have no words to ask it” is certainly true. Practicing statisticians - as opposed to theoretically focused academics - work closely with their clients, who are specialists in a specific area. The language of that field largely defines the language of causation used in a particular context. That is why there has been no universal causal framework, and why statisticians in psychology, healthcare and economics, for instance, have different approaches and often use different language and mathematics. I myself draw upon all three, as well as Pearl’s. The utility of each, in my 30+ years’ experience, is case-by-case. There are also ad hoc approaches (some questionable)"
The Book of Why - KDnuggets,"He claims that “…even today, only a small percentage of practicing statisticians can solve any of the causal toy problems presented in the Book of Why” yet provides no evidence of this. They are not difficult, and claims are not evidence. Furthermore, the examples he gives in “Every chapter of The Book of Why” of the failure of statistics are, in general, not compelling and were part of the motivation for the article in the first place"
The Book of Why - KDnuggets,Simpson’s paradox in its various forms is something that generations of researchers and statisticians have been trained to look out for. And we do. There is nothing mysterious about it
The Book of Why - KDnuggets,"Mediation, often confused with moderation, can be a tough nut to crack. Simple path diagrams or DAG with only few variables will frequently be inadequate and may badly mislead us. In the real world of a statistician, there are frequently a vast number of potentially relevant variables, including those we are unable to observe. There are also critical variables that, for many reasons, are not included in the data we have to work with and cannot be obtained. Measurement error may be substantial - this is not rare - and different causal mechanisms for different classes of subjects (e. These classes are often unobserved and unknown to us"
The Book of Why - KDnuggets,"I should make clear that I greatly appreciate Pearl’s efforts at bringing the analysis of causation into the spotlight. I would urge statisticians to read him but also consult (as I do) with other veteran practitioners and academics for their views on his writings and on the analysis of causation. There are many ways to approach causation and Pearl’s is but one. As Pearl himself will be aware, to this day, philosophers disagree about what causation is, thus to suggest he has found the answer to it is not plausible in my view. Real statisticians know better than to look for a Silver Bullet"
A Beginner’s Guide to the Data Science Pipeline - KDnuggets,"On one end was a pipe with an entrance and at the other end an exit. The pipe was also labeled with five distinct letters: ""O.S.E.M.N"
A Beginner’s Guide to the Data Science Pipeline - KDnuggets,"It all started as Data was walking down the rows when he came across a weird, yet interesting, pipe. On one end was a pipe with an entrance and at the other end an exit. The pipe was also labeled with five distinct letters: “"
A Beginner’s Guide to the Data Science Pipeline - KDnuggets,"You cannot do anything as a data scientist without even having any data. As a rule of thumb, there are some things you must take into consideration when obtaining your data. You must"
A Beginner’s Guide to the Data Science Pipeline - KDnuggets,"Now comes the fun part. Models are general rules in a statistical sense.Think of a machine learning model as tools in your toolbox. You will have access to many algorithms and use them to accomplish different business goals. The better features you use the better your predictive power will be. After cleaning your data and finding what features are most important, using your model as a predictive tool will only enhance your business"
A Beginner’s Guide to the Data Science Pipeline - KDnuggets,People aren’t going to magically understand your findings. The best way to make an impact is telling your story through emotion. We as humans are naturally influenced by emotions
A Beginner’s Guide to the Data Science Pipeline - KDnuggets,"Don’t worry your story doesn’t end here. As your model is in production, its important to update your model periodically, depending on how often you receive new data. The more data you receive the more frequent the update"
A Beginner’s Guide to the Data Science Pipeline - KDnuggets,"You’re old model doesn’t have this and now you must update the model that includes this feature. If not, your model will degrade over time and won’t perform as good, leaving your business to degrade as well. The introduction to new features will alter the model performance either through different variations or possibly correlations to other features"
A Beginner’s Guide to the Data Science Pipeline - KDnuggets,"Most of the problems you will face are, in fact, engineering problems. Even with all the resources of a great machine learning god, most of the impact will come from great features, not great machine learning algorithms. So, the basic approach is:"
10 More Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"Summer, summer, summertime. Time to sit back and unwind. Or get your hands on some free machine learning and data science books and get your learn on. Check out this selection to get you started"
10 More Free Must-Read Books for Machine Learning and Data Science - KDnuggets,It's time for another collection of free machine learning and data science books to kick off your summer learning season. Because that's a thing. Right?
10 More Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"Most books on Bayesian statistics use mathematical notation and present ideas in terms of mathematical concepts like calculus. This book uses Python code instead of math, and discrete approximations instead of continuous mathematics. As a result, what would be an integral in a math book becomes a summation, and most operations on probability distributions are simple loops"
10 More Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"This is a work in progress, which I add to as time allows. The purpose behind it is to have a balance between theory and implementation for the software engineer to implement machine learning models comfortably without relying too much on libraries. Most of the time the concept behind a model or a technique is simple or intutive but it gets lost in details or jargon. Also, most of the time existing libraries would solve the problem at hand but they are treated as black boxes and more often than not they have their own abstractions and architectures that hide the underlying concepts. This book's attempt is to make the underlying concepts clear"
10 More Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. This book descibes the important ideas in these areas in a common conceptual framework"
10 More Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"The book is intended to be a low cost introduction to the important field of statistical inference. The intended audience are students who are numerically and computationally literate, who would like to put those skills to use in Data Science or Statistics. The book is offered for free as a series of markdown documents on github and in more convenient forms (epub, mobi) on LeanPub and retail outlets"
10 More Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"This book is about convex optimization, a special class of mathematical optimization problems, which includes least-squares and linear programming problems. It is well known that least-squares and linear programming problems have a fairly complete theory, arise in a variety of applications, and can be solved numerically very efficiently. The basic point of this book is that the same can be said for the larger class of convex optimization problems"
10 More Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"This is a book about Natural Language Processing. By ""natural language"" we mean a language that is used for everyday communication by humans; languages like English, Hindi or Portuguese. In contrast to artificial languages such as programming languages and mathematical notations, natural languages have evolved as they pass from generation to generation, and are hard to pin down with explicit rules. We will take Natural Language Processing — or NLP for short — in a wide sense to cover any kind of computer manipulation of natural language"
10 More Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"The growth of social media over the last decade has revolutionized the way individuals interact and industries conduct business. Individuals produce data at an unprecedented rate by interacting, sharing, and consuming content through social media. Understanding and processing this new type of data to glean actionable patterns presents challenges and opportunities for interdisciplinary research, novel algorithms, and tool development. Social Media Mining integrates social media, social network analysis, and data mining to provide a convenient and coherent platform for students, practitioners, researchers, and project managers to understand the basics and potentials of social media mining"
Top 20 R Libraries for Data Science in 2018 - KDnuggets,"But this list of articles will not be complete without R. All of these programming languages are popular for different data science tasks and projects and have their supporters and opponents. So while we are arranging a comparison of how these programming languages relate to each other, we have prepared some of the most useful R libraries for data scientists and engineers, based on our experience"
Top 20 R Libraries for Data Science in 2018 - KDnuggets,"R is a well-known and increasingly popular tool in the Data Science field. It is a programming language and a software environment primarily designed for statistical computing, so its interface and structure are very well suited for the scientific tasks. Moreover, R has one of the most developed libraries systems that counts thousands of packages to solve a wide variety of problems"
A Better Stats 101 - KDnuggets,"Statistics encourages us to think systemically and recognize that variables normally do not operate in isolation, and that an effect usually has multiple causes. Some call this multivariate thinking. Statistics is particularly useful for uncovering the Why"
A Better Stats 101 - KDnuggets,"Judging from ""Stats 101"" courses in which we plug numbers into simple formulas, or seminars given by software vendors where we click-and-drag or point-and-click for two days, it would be easy to get the impression that there really isn't much to statistics. Of course, there are dweeby types whose worlds revolve around linear algebra, calculus and programming. In a different way, they also succeed in giving us a misleading notion of what stats is really about"
A Better Stats 101 - KDnuggets,"Karl Pearson, regarded by many as the father of modern statistics, dubbed this new field ""the grammar of science. How does it differ from data science? Paraphrasing former Royal Statistical Society President Peter Diggle, data science seeks to maximize the utility of data, whereas statistics seeks to minimize the uncertainty that is associated with the interpretation of data. This distinction reflects very different mentalities, and when I read on LinkedIn that someone is ""passionate about data"", without reading further, I know he or she is not a statistician"
A Better Stats 101 - KDnuggets,"It also encourages us to think systemically and recognize that variables normally do not operate in isolation, and that an effect usually has multiple causes. Some call this multivariate thinking. Statistics is particularly useful for uncovering the"
A Better Stats 101 - KDnuggets,"As the first link above demonstrates, there are a gigantic number of statistical methods designed for a gigantic number of purposes. It is a king-sized discipline, with many specializations and sub-specializations. My own work draws heavily on psychometrics, econometrics, biostatistics and epidemiology, though, when needed, I also utilize machine learners designed primarily for predictive analytics"
A Better Stats 101 - KDnuggets,"Experience, judgement and subject matter knowledge, as well as understanding decision-makers' expectations and priorities are critical. The importance of communication and interpersonal skills is hard to overstate. Programming and computer science skills are increasingly demanded in many occupations and more so in statistics than most"
A Better Stats 101 - KDnuggets,"Data science and analytics programs are popping up all over the world the way MBA programs did a generation ago. Their quality appears even more varied, though, and there is little consistency in their curricula. Many of the topics I've listed above are skipped or crammed into the curricula along with IT and programing courses"
A Better Stats 101 - KDnuggets,"Formal university-level coursework in statistics is still the best way to learn about statistics. Let's not forget real experience analyzing real data, though - there is the classroom and there is the real world. I’ve been using stats in my work for more than 30 years now and still learn something new every day"
How should I organize a larger data science team? - KDnuggets,"I manage a data science team at a fortune 500 company that has grown from a few people to over 40, with an aim to grow even larger in the next 12 to 18 months.  We are primarily machine learning practitioners -- PhD level writing Python / Spark and using best practices tools like GBM and DNN.  When we were small, we got by ""scrappily"" making due without dedicated admins an/or infrastructure team, doing everything ourselves or borrowing resource from engineering.  We do have a few product managers that help manage requirements and interface with other teams"
How should I organize a larger data science team? - KDnuggets,"Getting people who can scale ML algorithms beyond the current state of the art is fundamentally hard. This requires a deep understanding of computer science, machine learning, and math, and very high general intelligence. Demand exceeds supply and large tech companies bid aggressively for these people, sometimes with seven-figure compensation. Consider seriously what the ROI is for your business — does the incremental benefit that you will get relative to using a simpler algorithm, smaller training set , or accepting a slower cycle time have sufficient dollar returns?"
How should I organize a larger data science team? - KDnuggets,"Fortunately, getting people to build and maintain pipelines is not as hard. There are plenty of people who got a BS in CS and then got a masters or continuing-education certificate in ML and are now finding that a Ph.D. I would try to get a manager and/or tech lead with some actual data engineering experience, then build out the rest of the team with generalists willing to learn"
How should I organize a larger data science team? - KDnuggets,"One thing that struck me is that you have a team of 40 people and you're overweight PhDs doing advanced, high-accuracy, expensive ML models. I think this may be related to your innovation issues. The skill set you've selected for is tuned for one stage in the project life cycle - when you've already determined that there is a business opportunity and that you have the data to predict something useful, and you want to maximize accuracy of those predictions. This is not the stage of the project life cycle where new opportunities are discovered. I think I would increase my weighting on people with strong creativity, business, domain knowledge, and statistics/EDA background, which is the skillset that's going to let you discover and get credit for new opportunities instead of optimizing opportunities that somebody else brings you"
How should I organize a larger data science team? - KDnuggets,"Training depends on whether you want people to know ABOUT what your team does, or whether you want them to be able to DO things themselves. For people who just need to know ABOUT the field, there's a good book from Provost and Fawcett which I recommend to managers that are going to be interacting with a data science team; it should work fine for engineers as well. You might organize a ""book group"" where somebody from your team meets with them for half an hour a week to discuss each chapter (and gently make sure they actually read it)"
Scientific debt – what does it mean for Data Science? - KDnuggets,"Technical debt occurs when engineers choose a quick but suboptimal solution to a problem, or don’t spend time to build sustainable infrastructure. Maybe they’re using an approach that doesn’t scale well as the team and codebase expand (such as hardcoding “magic numbers”), or using a tool for reasons of convenience rather than appropriateness (“we’ll write the DevOps infrastructure in PHP since that’s what our team already knows”). Either way, it’s something that seems like it’s working at first but causes real challenges in the long-term, in the form of delayed feature launches and hard-to-fix bugs"
Scientific debt – what does it mean for Data Science? - KDnuggets,"Imagine a small startup WidgetCorp is developing a B2B product, and deciding on their sales strategy. One year they decide to start focusing their sales efforts on larger corporate clients. They notice that as they take on this new strategy, their monthly revenue increases. They’re encouraged by this, and in the following years hire half a dozen salespeople with experience working with large clients, and spend marketing and design effort building that as part of their brand"
Scientific debt – what does it mean for Data Science? - KDnuggets,"Years later, the strategy doesn’t seem to be paying off: their revenue is struggling and the early successes aren’t repeating themselves. They hire an analyst who looks at their sales data, and finds that in fact, it had never been the case that they’d had a higher return-on-investment selling to large companies. In that early year, their revenue had been rising because of a seasonal effect (the demand for widgets goes up in the fall and winter), which was compounded with some random noise and anecdotes (e"
Scientific debt – what does it mean for Data Science? - KDnuggets,"It’s reasonable to take a quick look at metrics and be happy that they’re going in the right direction. But once the company made product, sales and marketing changes, it became difficult to reverse them. Before making a major shift in business, it’s worth making sure that the data supports it: that they’ve accounted for seasonal effects and applied proper statistical tests"
Scientific debt – what does it mean for Data Science? - KDnuggets,"I often take shortcuts in my own analyses. Running a randomized experiment for a feature launch is sometimes too expensive, especially if the number of users is fairly small or the change pretty uncontroversial (you wouldn’t A/B test a typo fix). And while correlation doesn’t imply causation, it’s usually better than nothing when making business decisions"
Scientific debt – what does it mean for Data Science? - KDnuggets,"Again, let’s go back to technical debt. There are lots of reasons individual engineers may want to write “clean code”: they appreciate its elegance, they want to impress their peers, or they’re perfectionists procrastinating on other work. These reasons don’t generally matter to non-technical employees, who care about product features and reliability. The framing of technical debt helps emphasize what the company loses by not investing in architecture: the idea that even if a product looks like it’s working, the flaws have a long-term cost in actual dollars and time"
Scientific debt – what does it mean for Data Science? - KDnuggets,"Similarly, scientists, especially from an academic background, often have a particular interest in discovering truths about reality. So the idea of “I’d like to analyze whether X is a confounding factor here” can sound like an indulgence rather than an immediate business need. Statisticians in particular are often excited by finding flaws in mathematical methods. So when a data scientist says something like “We can’t use that method, Jones et al 2012 proved that it is asymptotically inconsistent,” non-technical colleagues might assume they’re overthinking it or even showing off. Framing it in terms of what we’re actually"
Scientific debt – what does it mean for Data Science? - KDnuggets,"As described earlier, one reason a company might fall into scientific debt is that analysts may not have easy access to the data they need. It could be locked away in a platform that hasn’t been ingested, or in Google sheets that are edited by hand. Ingesting relevant data into a data warehouse or a data lake makes it more likely data scientists can make relevant discoveries"
Command Line Tricks For Data Scientists - KDnuggets,"For many data scientists, data manipulation begins and ends with Pandas or the Tidyverse. In theory, there is nothing wrong with this notion. It is, after all, why these tools exist in the first place. Yet, these options can often be overkill for simple tasks like delimiter conversion"
Command Line Tricks For Data Scientists - KDnuggets,"Aspiring to master the command line should be on every developer’s list, especially data scientists. Learning the ins and outs of your terminal will undeniably make you more productive. Beyond that, the command line serves as a great history lesson in computing. For instance, awk — a data-driven scripting language. Awk first appeared in 1977 with the help of"
Command Line Tricks For Data Scientists - KDnuggets,"File encodings can be tricky. For the most part files these days are all UTF-8 encoded. To understand some of the magic behind UTF-8, check out this"
Command Line Tricks For Data Scientists - KDnuggets,"Nonetheless, there are times where we receive a file that isn’t in this format. This can lead to some wonky attempts at swapping the encoding schema. Here,"
Command Line Tricks For Data Scientists - KDnuggets,Tr is analogous to translate. This powerful utility is a workhorse for basic file cleaning. An ideal use case is for swapping out the delimiters within a file
Command Line Tricks For Data Scientists - KDnuggets,The preceding commands are obvious: they do what they say they do. These two provide the most punch in tandem (i. This is due to
Command Line Tricks For Data Scientists - KDnuggets,"Grep has a lot of power, especially for finding your way around large codebases. Within the realm of data science, it acts as a refining mechanism for other commands. Although its standard usage is valuable as well"
Command Line Tricks For Data Scientists - KDnuggets,"Sed and Awk are the two most powerful commands in this article. For brevity, I’m not going to go into exhausting detail about either. Instead, I will cover a variety of commands that prove their impressive might. If you want to know more,"
Command Line Tricks For Data Scientists - KDnuggets,"The best for last. Awk is much more than a simple command: it is a full-blown language. Of everything covered in this article,"
Command Line Tricks For Data Scientists - KDnuggets,"The command line boasts endless power. The commands covered in this article are enough to elevate you from zero to hero in no time. Beyond those covered, there are many utilities to consider for daily data operations"
The Statistics of Gang Violence - KDnuggets,"Carcach allowed that there might be merit in this, but thought a cancer model was more appropriate. For one thing, cancer tends to mutate as it spreads, changing in form, and the criminal activity he was tracking did likewise. For another, infection tends to produce a defensive response in the body it infects, usually overcoming the infection, while cancer does not excite an immediate counterattack from its host"
The Statistics of Gang Violence - KDnuggets,"Carcach was pessimistic about the potential for an organic evolution of society in which the role of the gangs would wither. He said that their activities at the local level were not new - they had been preceded by local actors whose protection and extortion rackets were similar in character, but not in intensity. The gangs, informed and strengthened by their US experiences and connections, simply took control of these activities, and gave them greater intensity - folding them into a hierarchical organization of units not unlike franchises, with an average size of 15 people at the bottom. When they got bigger than that, they tended to split up"
Resources For Women In Data Science and Machine Learning - KDnuggets,She worked for over 10 years as a biostatistician in the pharmaceutical industry.  She is also an organizer of the meetups group NYC Women in Machine Learning & Data Science and PyLadies.  She earned MS in statistics from Rutgers and her MBA from NYU Stern School of Business
Skewness vs Kurtosis – The Robust Duo - KDnuggets,"Descriptive Statistics can provide great amount of insight about data, however it often lays interesting pitfalls in front of us, sometimes causing misinterpretation of the results. One way of mitigating such risks, is to use a combination of more than one technique to reach an unambiguous conclusion. Today we will see how Kurtosis outputs can be supplemented by Skewness in tackling a very interesting challenge"
Skewness vs Kurtosis – The Robust Duo - KDnuggets,"Figure 1 shows the data for which the value of Kurtosis was 6.227 - clearly above 3 which is default for Gaussian noise. Impulsive content was detected, great! After some time however, a curious Business Analyst detected a somewhat puzzling case, depicted in Figure 2. Even though the nature of the signal changed quite dramatically – the impulse has only the upward-facing part, Kurtosis returned precisely the same value, 6.227 (it actually took me half a dozen tries to synthesize such a signal)"
Skewness vs Kurtosis – The Robust Duo - KDnuggets,"The amplitude distribution of signals shown in Figure 1 and 2 can be seen in Figure 3 and 4 respectively. There is quite a noticeable change to be noticed. Figure 3 shows an almost identically symmetrical distribution, whereas Figure 4 shows a shape which leans towards the left-hand side of the plot. Side note: despite the left-side leaning, such distribution shape is referred to as a right-skewed distribution, because we are really interested in the relative movement of the mean value. In our case it definitely shifted towards the right side, due to the presence of the prominent impulse"
Skewness vs Kurtosis – The Robust Duo - KDnuggets,This is where the helping hand of the Skewness comes into play. Its formula can be seen in Eq. 1 [1]:
Skewness vs Kurtosis – The Robust Duo - KDnuggets,"Skewness formula is virtually identical to formula of Kurtosis apart from the powers in numerator and denominator and now the distinction between the ‘above mean’ and ‘below mean’ values becomes possible. Skewness outputs values which are close to 0 for symmetrically distributed signals, values between 0 and 1 for right-skewed (aka positively skewed) signals, and values between 0 and -1 for right-skewed (aka negatively skewed) signals. The shapes of such distributions together with their corresponding relation of mean, median and mode are shown in Figure 5 (taken from [2]). When applied on signals from Figure 1 and 2, Skewness values are 0.06 and 0.58 respectively. At this point, the Business Analyst will always use Kurtosis with conjunction with Skewness values to be able to not only detect the presence of impulses but also determine the direction of their attack"
Skewness vs Kurtosis – The Robust Duo - KDnuggets,He subsequently moved to The University of Manchester where he obtained PhD on project sponsored by QinetiQ related to data analytics for helicopter gearbox diagnostics. Upon returning to Poland he worked as a Senior Scientist at ABB’s Corporate Research Center and a Senior Risk Modeler in Strategic Analytics at HSBC. Currently he is a Data Scientist at Codewise
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"Why is data science sexy? It has something to do with so many new applications and entire new industries come into being from the judicious use of copious amounts of data. Examples include speech recognition, object recognition in computer vision, robots and self-driving cars, bioinformatics, neuroscience, the discovery of exoplanets and an understanding of the origins of the universe, and the assembling of inexpensive but winning baseball teams. In each of these instances, the data scientist is central to the whole enterprise. He/she must combine knowledge of the application area with statistical expertise and implement it all using the latest in computer science ideas"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"In other words, put effort into understanding how the data is captured, understand exactly how each data field is defined, and understand when data is missing. If the data is missing, does that mean something in and of itself? Is it missing only in certain situations? These little, teeny nuanced data gotchas will really get you. They really will"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"You can use the most sophisticated algorithm under the sun, but it’s the same old junk-in–junk-out thing. You cannot turn a blind eye to the raw data, no matter how excited you are to get to the fun part of the modeling. Dot your i’s, cross your t’s, and check everything you can about the underlying data before you go down the path of developing a model"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"My take on it is that if you’re an undergrad, study a specialty where you can take as many math and physics courses as you can. And it has to be the right courses, unfortunately. What I’m going to say is going to sound paradoxical, but majors in engineering or physics are probably more appropriate than say math, computer science, or economics. Of course, you need to learn to program, so you need to take a large number of classes in computer science to learn the mechanics of how to program. Then, later, do a graduate program in data science. Take undergrad machine learning, AI, or computer vision courses, because you need to get exposed to those techniques. Then, after that, take all the math and physics courses you can take. Especially the continuous applied mathematics courses like optimization, because they prepare you for what’s really challenging"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"It depends where you want to go because there are a lot of different jobs in the context of data science or AI. People should really think about what they want to do and then study those subjects. Right now the hot topic is deep learning, and what that means is learning and understanding classic work on neural nets, learning about optimization, learning about linear algebra, and similar topics. This helps you learn the underlying mathematical techniques and general concepts we confront every day"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"Someone coming from software engineering should take a class in machine learning and work on a project with real data, lots of which is available for free. As many people have said, the best way to become a data scientist is to do data science. The data is out there and the science isn’t that hard to learn, especially for someone trained in math, science, or engineering"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"You have to do it. You can only listen to so many videos and watch it happen. At the end of the day, you have to get on your damn skis and go down that hill. You will crash a few times on the way and that is fine. That is the learning experience you need. I actually much prefer to ask interviewees about things that did not go well rather than what did work, because that tells me what they learned in the process"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"Whenever people come to me and ask, “What should I do?” I say, “Yeah, sure, take online courses on machine learning techniques. There is no doubt that this is useful. You clearly have to be able to program, at least somewhat. You do not have to be a Java programmer, but you must get something done somehow. I do not care how"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"Ultimately, whether it is volunteering at DataKind to spend your time at NGOs to help them, or going to the Kaggle website and participating in some of their data mining competitions — just get your hands and feet wet. Especially on Kaggle, read the discussion forums of what other people tell you about the problem, because that is where you learn what people do, what worked for them, and what did not work for them. So anything that gets you actually involved in doing something with data, even if you are not paid being for it, is a great thing"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"Remember, you have to ski down that hill. There is no way around it. You cannot learn any other way. So volunteer your time, get your hands dirty in any which way you can think, and if you have a chance to do internships — perfect. Otherwise, there are many opportunities where you can just get started. So just do it"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"I also think humility is very important. You’ve got to make sure that you’re not tied up in what you’re doing. You can always make changes and start over. Being able to scrap code, I think, is really hard when you’re starting out, but the most important thing is to just do something"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"Even if you don’t have a job in data science, you can still explore data sets in your downtime and can come up with questions to ask the data. In my personal time, I’ve played around with Reddit data. I asked myself, “What can I explore about Reddit with the tools that I have or don’t have?” This is great because once you’ve started, you can see how other people have approached the same problem. Just use your gut and start reading other people’s articles and be like, “I can use this technique in my approach. I tried reading a lot when I started, but I think that’s not as helpful until you’ve actually played around with code and with data to understand how it actually works, how it moves. When people present it in books, it’s all nice and pretty. In real life, it’s really not"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"I think trying a lot of different things is also very important. I don’t think I’d ever thought that I would be here. I also have no idea where I’ll be in five years. But maybe that’s how I learn, by doing a bit of everything across many different disciplines to try to understand what fits me best"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"What is it they want to do? Right now, data science is a bit of a hot topic, and so I think there are a lot of people who think that if they can have the “data science” label, then magic, happiness, and money will come to them. So I really suggest figuring out what bits of data science you actually care about. That is the first question you should ask yourself. And then you want to figure out how to get good at that. You also want to start thinking about what kinds of jobs are out there that really play to what you are interested in"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"One strategy is to go really deep into one part of what you need to know. We have people on our team who have done PhDs in natural language processing or who got PhDs in physics, where they’ve used a lot of different analytical methods. So you can go really deep into an area and then find people for whom that kind of problem is important or similar problems that you can use the same kind of thinking to solve. So that’s one approach"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"Another approach is to just try stuff out. There are a lot of data sets out there. If you’re in one job and you’re trying to change jobs, try to think whether there’s data you could use in your current role that you could go and get and crunch in interesting ways. Find an excuse to get to try something out and see if that’s really what you want to do. Or just from home there’s open data you can pull. Just poke around and see what you can find and then start playing with that. I think that’s a great way to start. There are a lot of different roles that are going under the name “data science” right now, and there are also a lot of roles that are probably what you would think of data science but don’t have a label yet because people aren’t necessarily using it. Think about what it is that you really want"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"For me, every time I started something new, it’s really boring to just study without a having a problem I’m trying to solve. Start reading material and as soon as you can, start working with it and your problem. You’ll start to see problems as you go. This will lead you to other learning resources, whether they are books, papers, or people. So spend time with the problem and people, and you’ll be fine"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"Understand the basics really deeply. Understand some basic data structures and computer science. Understand the basis of the tools you use and understand the math behind them, not just how to use them. Understand the inputs and the outputs and what is actually going on inside, because otherwise you won’t know when to apply it. Also, it depends on the problem you’re tackling. There are many different tools for so many different problems. You’ve got to know what each tool can do and you’ve got to know the problem that you’re doing really well to know which tools and techniques to apply"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"Additionally, my statistics program also taught a lot about ethics, which is something that we think a lot about at DataKind. You always want to think about how your work is going to be applied. You can give anybody an algorithm. You can give someone a model for using stop-and-frisk data, where the police are going to make arrests, but why and to what end? It’s really like building any new technology. You’ve got to think about the risks as well as the benefits and really weigh that because you are responsible for what you create"
8 Useful Advices for Aspiring Data Scientists - KDnuggets,"No matter where you come from, as long as you understand the tools that you’re using to draw conclusions, that is the best thing you can do. We are all scientists now, and I’m not just talking about designing products. We are all drawing conclusions about the world we live in. That’s what statistics is — collecting data to prove a hypothesis or to create a model of the way the world works. If you just trust the results of that model blindly, that’s dangerous because that’s your interpretation of the world, and as flawed as it is, your understanding is how flawed the result is going to be"
Boost your data science skills. Learn linear algebra. - KDnuggets,"The aim of these notebooks is to help beginners/advanced beginners to grasp linear algebra concepts underlying deep learning and machine learning. Acquiring these skills can boost your ability to understand and apply various data science algorithms. In my opinion, it is one of the bedrock of machine learning, deep learning and data science"
Boost your data science skills. Learn linear algebra. - KDnuggets,These notes cover the chapter 2 on Linear Algebra. I liked this chapter because it gives a sense of what is most used in the domain of machine learning and deep learning. It is thus a great syllabus for anyone who want to dive in deep learning and acquire the concepts of linear algebra useful to better understand deep learning algorithms
Boost your data science skills. Learn linear algebra. - KDnuggets,"So I decided to produce code, examples and drawings on each part of this chapter in order to add steps that may not be obvious for beginners. I also think that you can convey as much information and knowledge through examples than through general definitions. The illustrations are a way to see the big picture of an idea. Finally, I think that coding is a great tool to experiment concretely these abstract mathematical notions. Along with pen and paper, it adds a layer of what you can try to push your understanding through new horizons"
Boost your data science skills. Learn linear algebra. - KDnuggets,"Graphical representation is also very helpful to understand linear algebra. I tried to bind the concepts with plots (and code to produce it). The type of representation I liked most by doing this series is the fact that you can see any matrix as linear transformation of the space. In several chapters we will extend this idea and see how it can be useful to understand eigendecomposition, Singular Value Decomposition (SVD) or the Principal Components Analysis (PCA)"
Boost your data science skills. Learn linear algebra. - KDnuggets,"In addition, I noticed that creating and reading examples is really helpful to understand the theory. It is why I built Python notebooks. The goal is two folds:"
Boost your data science skills. Learn linear algebra. - KDnuggets,"This chapter is mainly on the dot product (vector and/or matrix multiplication). We will also see some of its properties. Then, we will see how to synthesize a system of linear equations using matrix notation. This is a major process for the following chapters"
Boost your data science skills. Learn linear algebra. - KDnuggets,"We will see two important matrices: the identity matrix and the inverse matrix. We will see why they are important in linear algebra and how to use it with Numpy. Finally, we will see an example on how to solve a system of linear equations with the inverse matrix"
Boost your data science skills. Learn linear algebra. - KDnuggets,"In this chapter we will continue to study systems of linear equations. We will see that such systems can’t have more than one solution and less than an infinite number of solutions. We will see the intuition, the graphical representation and the proof behind this statement. Then we will go back to the matrix form of the system and consider what Gilbert Strang call the *row figure* (we are looking at the rows, that is to say multiple equations) and the *column figure* (looking at the columns, that is to say the linear combination of the coefficients). We will also see what is linear combination. Finally, we will see examples of overdetermined and underdetermined systems of equations"
Boost your data science skills. Learn linear algebra. - KDnuggets,"The norm of a vector is a function that takes a vector in input and outputs a positive value. It can be thinks as the *length* of the vector. It is for example used to evaluate the distance between the prediction of a model and the actual value. We will see different kind of norms (L⁰, L¹, L²…) with examples"
Boost your data science skills. Learn linear algebra. - KDnuggets,"We will see some major concepts of linear algebra in this chapter. We will start by getting some ideas on eigenvectors and eigenvalues. We will see that a matrix can be seen as a linear transformation and that applying a matrix on its eigenvectors gives new vectors with same direction. Then we will see how to express quadratic equations into a matrix form. We will see that the eigendecomposition of the matrix corresponding to the quadratic equation can be used to find its minimum and maximum. As a bonus, we will also see how to visualize linear transformation in Python!"
Boost your data science skills. Learn linear algebra. - KDnuggets,"We will see another way to decompose matrices: the Singular Value Decomposition or SVD. Since the beginning of this series, I emphasized the fact that you can see matrices as linear transformation in space. With the SVD, you decompose a matrix in three other matrices. We will see that we can see these new matrices as *sub-transformation* of the space. Instead of doing the transformation in one movement, we decompose it in three movements. As a bonus, we will apply the SVD to image processing. We will see the effect of SVD on an example image of Lucy the goose so keep on reading!"
Boost your data science skills. Learn linear algebra. - KDnuggets,"We saw that not all matrices have an inverse. It is unfortunate because the inverse is used to solve system of equations. In some cases, a system of equation has no solution, and thus the inverse doesn’t exist. However it can be useful to find a value that is almost a solution (in term of minimizing the error). This can be done with the pseudoinverse! We will see for instance how we can find the best-fit line of a set of data points with the pseudoinverse"
Boost your data science skills. Learn linear algebra. - KDnuggets,I hope that you will find something interesting in that series. I tried to be as accurate as I could. If you find errors/misunderstandings/typos… Please report it! You can send me emails or open issues and pull request in the notebooks Github
Boost your data science skills. Learn linear algebra. - KDnuggets,"Goodfellow, I. Deep learning. MIT press"
Boost your data science skills. Learn linear algebra. - KDnuggets,"He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog ("
To Kaggle Or Not - KDnuggets,"The company was founded in 2010 in Melbourne, Australia, and a year later, it moved to San Francisco after receiving funding from Silicon Valley. In 2017, it was acquired by Google. Read more about its history and future in"
To Kaggle Or Not - KDnuggets,"While there are learning benefits to acquiring your own datasets or scraping the web, the downside to that is there is no benchmark, no way to compare your findings. There is the possibility of significant errors, and no one would know because there is no validation being performed. Kaggle competitions provide a platform for “checking your work"
To Kaggle Or Not - KDnuggets,"Despite the fact that the dataset is provided, there remains the requirement to understand the data and the evaluation metrics. Contrary to popular belief, there is still “dirty data” which requires further investigation. Digging deeper into misclassified items begets adjustments to the algorithm"
To Kaggle Or Not - KDnuggets,"It is true, doing one Kaggle competition does not qualify someone to be a data scientist. Neither does taking one class or attending one conference tutorial or analyzing one dataset or reading one book in data science. Working on competition(s) adds to your experience and augments your portfolio. It is a complement to your other projects, not the sole litmus test of one’s data science skillset"
To Kaggle Or Not - KDnuggets,"Often, people are unsure whether to pursue a career in data science. Participating in a competition is one informative way to gauge your abilities and excitement. If you truly enjoy the process of Kaggle, it will point you more clearly in the right direction. If you prefer to spend your time doing something else, that is all right too; it is one way to find out"
To Kaggle Or Not - KDnuggets,"There are kernels, which is code in Jupyter Notebooks that others have shared. You are free to copy and use them to get started on a competition. Code is available in both R and Python"
To Kaggle Or Not - KDnuggets,"I think it is worthwhile to participate in at least one competition. There is a difference in having an opinion on something you have tried versus not. Kaggle is evolving, like everything, especially since its acquisition by Google. Check back periodically and see what is new"
To Kaggle Or Not - KDnuggets,She worked for over 10 years as a biostatistician in the pharmaceutical industry. She is also an organizer of the meetups group NYC Women in Machine Learning & Data Science and PyLadies. She earned MS in statistics from Rutgers and her MBA from NYU Stern School of Business
Data Science: 4 Reasons Why Most Are Failing to Deliver - KDnuggets,"One of today’s organizational dilemmas: it’s pretty well understood that data science is a key driver of innovation, but few organizations know how to consistently turn data science output into business value. Sixty percent of companies plan to double the size of their data science teams in 2018. Ninety percent believe data science contributes to business innovation. However, less than nine percent can actually quantify the business impact of all their models, and only 11 percent can claim more than 50 predictive models working in production. This data stems from a recent"
Data Science: 4 Reasons Why Most Are Failing to Deliver - KDnuggets,What’s at the root of the disconnect? There is a divide between organizations that view data science as a technical practice and those that go further to embrace data science as an organizational capability woven throughout the business fabric. Companies that have mastered the technical and management practices necessary to embed algorithmic-driven decision-making at the core of the business are reaping the most returns. They can be considered “model-driven
Data Science: 4 Reasons Why Most Are Failing to Deliver - KDnuggets,"How these four challenges are addressed will determine much of how the next five to ten years looks for your organization. If you’d consider your company a laggard with respect to data science maturity, seek consolation in the fact that you’re not alone: some 46 percent of all survey respondents, in fact, fell into the laggard category. Forty percent are considered “aspiring,” and a mere 14 percent rose to the top and demonstrated ability to manage data science as an organizational capability"
Data Science: 4 Reasons Why Most Are Failing to Deliver - KDnuggets,"It’s not too late. To start delivering and measuring business value from data science, your organization must develop and implement a consistent and sensible framework around people, process, and technology. Companies that put the time and energy into this framework, and that successfully leverage data science as a core business competency, are reaping the rewards. Netflix, for example, integrates models into each part of their business; they estimate"
Data Science: 4 Reasons Why Most Are Failing to Deliver - KDnuggets,"The message is clear: achieving success with data science is not easy. There are significant barriers that must be overcome. But ultimately, those that solve the hard problems -- figuring out how to develop and deploy and high-impact models at scale, and truly instrumenting their businesses with data science -- are better equipped to leverage them for competitive advantage over the long term"
Data Science vs Machine Learning vs Data Analytics vs Business Analytics - KDnuggets,"With one note, though. There exist data science processes that are not directly and immediately business analytics but are data analytics. For instance, ‘Optimization of Drilling Operations’ requires data science tools and techniques. Data scientists may well do that on a daily basis. However, while in the domain of ‘oil"
Data Science Interview Guide - KDnuggets,"Data Science is quite a large and diverse field. As a result, it is really difficult to be a jack of all trades. Traditionally, Data Science would focus on mathematics, computer science and domain expertise. While I will briefly cover some computer science fundamentals, the bulk of this blog will mostly cover the mathematical basics one might either need to brush up on (or even take an entire course)"
Data Science Interview Guide - KDnuggets,"Python and R are the most popular ones in the Data Science space. However, I have also come across C/C++, Java and Scala. Although, I would personally recommend Python as it has all the math libraries as well as specialized libraries for querying various databases and maintaining interactive web UIs. Common Python libraries of choice are matplotlib, numpy, pandas and scikit-learn"
Data Science Interview Guide - KDnuggets,"It is common to see the majority of the data scientists being in one of two camps: Mathematicians and Database Architects. If you are the second one, the blog won’t help you much (YOU ARE ALREADY AWESOME!). If you are among the first group (like me), chances are you feel that writing a double nested SQL query is an utter nightmare. That being said, it is important to have some knowledge of query optimization (both for SQL and noSQL systems)"
Data Science Interview Guide - KDnuggets,"Big Data technologies are a little hard to follow considering how the Apache project keeps on adding new tools all the time. However, I would recommend learning either Hadoop or Spark (though my personal recommendation is Spark). Both use similar Map Reduce algorithms (except Hadoop does it on disk while Spark does it in memory). Common Spark wrappers are Scala, Python and Java"
Data Science Interview Guide - KDnuggets,"Now that we have covered the software needs, we will start making a smooth transition into the mathematics domain. Around this part of the process, you generally need to have some parsing background. This might either be collecting sensor data, parsing websites or carrying out surveys. After collecting the data, it needs to be transformed into a usable form (e. Once the data is collected and put in a usable format, it is essential to perform some data quality checks. Some common quality checks are as described below:"
Data Science Interview Guide - KDnuggets,"At times, the feature by itself may not provide useful information. For example, imagine using internet usage data. You will have YouTube users going as high as Giga Bytes while Facebook Messenger users use a couple of Mega Bytes. The simplest solution here would be to take the LOG of the values. Another issue is the use of categorical values. While categorical values are common in the data science world, realize computers can only comprehend numbers. In order for the categorical values to make mathematical sense, it needs to be transformed into something numeric. Typically for categorical values, it is common to perform a One Hot Encoding. In One Hot Encoding, a new feature is created for each categorical value to state if it is present in the given record. Example of One Hot Encoding is given below:"
Data Science Interview Guide - KDnuggets,"Certain features are redundant by themselves but are useful when grouped together. For example, imagine you had a predictive model for traffic density and you had a column for each type of car. Naturally, you don’t care about the type of car but the frequency of the total number of cars. Hence, a row wise summation of all the car types can be done to create a new “all_cars” variable"
Data Science Interview Guide - KDnuggets,"Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here. Common methods under this category are Pearson’s Correlation, Linear Discriminant Analysis, ANOVA and Chi-Square"
Data Science Interview Guide - KDnuggets,"In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive. Common methods under this category are Forward Selection, Backward Elimination and Recursive Feature Elimination"
Data Science Interview Guide - KDnuggets,Embedded methods combine the qualities’ of filter and wrapper methods. It’s implemented by algorithms that have their own built-in feature selection methods. LASSO and RIDGE are common ones. The regularizations are given in the equations below as reference:
Torus for Docker-First Data Science - KDnuggets,"Data scientists are becoming more involved in the delivery pipeline of products, and it is a non-trivial task ensuring that their work survives the delivery process. Of course, this isn’t a new problem: in the past, traditional software development teams would throw their work “over the wall” to the operations team to serve in production with little to no context. A community effort to solve the inevitable mess resulted in what we now think of as DevOps, removing the wall between development and operations to drive increased efficiency and improve product quality. New tools and processes to help teams implement streamlined delivery pipelines now help guarantee development/production parity"
Torus for Docker-First Data Science - KDnuggets,"Now, the same problem has reared its head in the ML space, and is only getting worse as demand for AI products continues to grow. There is a new wall that is killing productivity. How does DevOps change with the rise of data science teams in engineering organizations? The pain points we are seeing in the community today feel familiar, but also have unique aspects to ML development"
Torus for Docker-First Data Science - KDnuggets,"Simply put, solutions in the DevOps space provide tools for people working at the intersection of development and operations. Similarly, there needs to be a toolkit for the person working at the intersection of data science and software engineering. We call that person a Machine Learning Engineer (MLE). At a high level, MLEs have the same set of challenges as any software engineer working in a product development team:"
Torus for Docker-First Data Science - KDnuggets,"At Manifold, we developed internal tools for easily spinning up Docker-based development environments for machine learning projects. In order to help other data science teams adopt Docker and apply DevOps best practices to streamline machine learning delivery pipelines, we open-sourced our evolving toolkit. We wanted to make it dead simple for teams to spin up new ready-to-go development environments and move to a Docker-first workflow"
Torus for Docker-First Data Science - KDnuggets,"The Torus 1.0 package contains a Dockerized Cookiecutter for Data Science (a fork of the popular cookiecutter-data-science) and an ML Development Base Docker Image. Using the project cookiecutter and Docker image together, you can go from cold-steel to a new project working in a Jupyter Notebook with all of the common libraries available in under five minutes (and you didn’t have to pip install anything)"
Torus for Docker-First Data Science - KDnuggets,"1. The ML base development image was pulled down to your local machine from Docker Hub. This includes many of the commonly used data science and ML libraries pre-installed, along with a Jupyter Notebook server with useful extensions installed and configured"
Torus for Docker-First Data Science - KDnuggets,"2. A container is launched with the base image, and is configured to mount your top-level project directory as a shared volume on the container. This lets you use your preferred IDE on your host machine to modify code and see changes reflected immediately in the runtime environment"
Torus for Docker-First Data Science - KDnuggets,"3. Port forwarding is set up so you can use a browser on your host machine to work with the notebook server running inside the container. An appropriate host port to forward is dynamically chosen, so no worries about port conflicts (e"
Torus for Docker-First Data Science - KDnuggets,"There is a lot of exciting activity going on in the MLE toolkit space and it’s easy to forget that, before even considering a higher-order platform or framework, you need to make sure your team is set up for success. We need what the DevOps movement did for software engineering in the ML delivery pipeline. Moving to a Docker-first development workflow is a great first step in making life easier for everyone involved with the delivery pipeline—and that includes your customers"
The Executive Guide to Data Science and Machine Learning - KDnuggets,"Big data. Deep learning. Predictive analytics. A lot of jargon is thrown around within data science departments and in the leading business news sites these days, and there’s a big hiring scramble across industries to develop advanced analytics to aid in decision-making. But what is important to know as an executive?"
The Executive Guide to Data Science and Machine Learning - KDnuggets,"Many times, this refers to the number of records, which can be in the billions or trillions, such as transaction data with retail or lead data within large marketing campaigns. Storage becomes an issue with this type of data. However, big data can also refer to data that has a lot of information about a given record, such as genomics data on a patient or academic records of a given student, and this type of data can be very tricky to analyze through statistics, as it violates many assumptions of analysis tools. Data may be large but contain quality control issues, such as missing data, corrupted records, or incorrect capture strategy; this is one of the most common reasons analytics projects fail and a key reason why data capture strategy is so important"
The Executive Guide to Data Science and Machine Learning - KDnuggets,"In this way, a set of computers can work on an analysis or store similar data without needing a large storage space on any given computer. Data and results stored across computers can then be pooled together as needed for a final result. It is common within Hadoop and specialized big data databases/analysis tools"
The Executive Guide to Data Science and Machine Learning - KDnuggets,"Models statistically test relationships between an outcome of interest (such as customer churn, leads acquired in a marketing campaign, or weekly sales) and a set of predictors (such as customer demographics, buying patterns, click behavior, or other characteristics known about leads or customers that might be related to a given outcome). Many times, these models can also be used to predict future behavior given the important predictors found by the model. Machine learning extends many common statistical models to accommodate big data, and it also includes tools to explore the data outside of prediction, which can be used to segment customer bases or group sales trends over time. Common software languages for implementing such models are"
The Executive Guide to Data Science and Machine Learning - KDnuggets,"Unlike other models that plateau in performance within a few thousand training records, deep learning models continue to improve as they are given more and more training data. However, it takes a lot of data to reach good performance with deep learning, and training samples of 5,000 or 10,000 records may not be sufficient for training this type of supervised learning algorithm. Oftentimes, deep learning requires substantial expertise to customize or build from scratch, and it’s helpful to hire an expert in deep learning if the problem or data is complicated or small in size"
The Executive Guide to Data Science and Machine Learning - KDnuggets,"This algorithm performs well for both classification and regression models, and it is well-suited to big data through the map reduce framework. At its core, this method includes a set of models built on samples of data, which are aggregated together into a final model. This helps reduce errors and incorrectly-learned relationships within the set of individual models"
7 Books to Grasp Mathematical Foundations of Data Science and Machine Learning - KDnuggets,"Most people learn Data Science with an emphasis on Programming. However, to be truly proficient with Data Science (and Machine Learning), you cannot ignore the mathematical foundations behind Data Science. In this post, I present seven books that I enjoyed in learning the mathematical foundations of Data Science"
7 Books to Grasp Mathematical Foundations of Data Science and Machine Learning - KDnuggets,"Understanding the Maths will help you understand the evolution of AI better. It will help you distinguish from others who approach AI from a superficial level. It will also help you to see the Intellectual Property(IP) potential of AI better. Finally, understanding the Maths behind Data Science could also lead you to the higher end jobs in AI and Data Science"
7 Books to Grasp Mathematical Foundations of Data Science and Machine Learning - KDnuggets,"Of all the books in this list, Vapnik is the hardest to find. I have an older Indian edition. Vladimir Vapnik is the creator of SVM. His"
7 Books to Grasp Mathematical Foundations of Data Science and Machine Learning - KDnuggets,"Like Dr Vapnik’s book, Duda is another classic from another era. First published in 1973. Updated 25 years later (2000) and nothing since! But yet a vital resource. The book takes a pattern recognition approach and provides extensive coverage of algorithms"
7 Books to Grasp Mathematical Foundations of Data Science and Machine Learning - KDnuggets,"Stephen Marsland’s book is now in its second edition. Marsland was one of the earliest books I have read (I only have the first edition). Both are very good. The second edition I believe has lot more code in Python. Like the first two books, this book also places a heavy emphasis on Algorithms"
7 Books to Grasp Mathematical Foundations of Data Science and Machine Learning - KDnuggets,Hastie is another classic. The version I have is very well printed with colours. This is another reference book
Key Algorithms and Statistical Models for Aspiring Data Scientists - KDnuggets,"With a glut of algorithms from which to choose, it’s hard to know where to start. Courses may include algorithms that aren’t typically used in industry today, and courses may exclude very useful methods that aren’t trending at the moment. Software-based programs may exclude important statistical concepts, and mathematically-based programs may skip over some of the key topics in algorithm design"
Key Algorithms and Statistical Models for Aspiring Data Scientists - KDnuggets,"I’ve put together a short guide for aspiring data scientists, particularly focused on statistical models and machine learning models (supervised and unsupervised); many of these topics are covered in textbooks, graduate-level statistics courses, data science bootcamps, and other training resources (some of which are included in the reference section of the article). Because machine learning is a branch of statistics, machine learning algorithms technically fall under statistical knowledge, as well as data mining and more computer-science-based methods. However, because some algorithms overlap with computer science course material and because many people separate out traditional statistical methods from new methods, I will separate the two branches in the list"
Key Algorithms and Statistical Models for Aspiring Data Scientists - KDnuggets,"Christopher, M. B. Pattern Recognition and Machine Learning. Springer-Verlag New York"
Key Algorithms and Statistical Models for Aspiring Data Scientists - KDnuggets,"Friedman, J. The elements of statistical learning (Vol. 1, pp. 337-387). New York: Springer series in statistics"
Descriptive Statistics: The Mighty Dwarf of Data Science – Crest Factor - KDnuggets,"Nowadays fair part of the community (often influenced by the pressure from the business) seems to show a tendency of applying somewhat complex and rather computationally expensive algorithms to applications that would have been easily accommodated in the past by much simpler (hence faster) and much more interpretable (hence of greater business value) techniques. In the current series of texts I am introducing the power and beauty of descriptive statistics as an approach for quantitatively describing the nature of data and creating solid foundations for any subsequent data investigations. In this post I will introduce another weapon of the mighty dwarf, of similar purpose to the technique introduced in my"
Descriptive Statistics: The Mighty Dwarf of Data Science – Crest Factor - KDnuggets,"In the previous post we used kurtosis to detect impulsive content within the data, which was caused by some seasonal variations in the demand for the goods offered by an imaginary shop. We said that kurtosis is often thought of as a measure of the “peakidness” of the amplitude distribution of a signal and explained what is really meant by this, somewhat controversial to some peers, statement. As a result of some hands-on simulations we concluded that kurtosis is great at detecting single impulses. Today I want to spend some time considering what will happen if we encounter more than one impulse in our data and would the behaviour of kurtosis still be of desirable nature"
Descriptive Statistics: The Mighty Dwarf of Data Science – Crest Factor - KDnuggets,"Figure 1 shows the data and the corresponding value of kurtosis. The metric is 6.227 - clearly above 3 which is default for Gaussian noise. Impulsive content was detected, success! So, this year the encouraged management used the same metric. After the analysis, the value of kurtosis came out even greater: 6.920. Splendid!”. But further investigation revealed something somewhat different, as can be seen in Figure 2. It turned out that apart from exactly the same impulse at day 200 as in the previous year, yet another impulse occurred at day 400, but it was of smaller amplitude. The difference between the two signals is indicated on a histogram by two arrows (Figure 3)"
Descriptive Statistics: The Mighty Dwarf of Data Science – Crest Factor - KDnuggets,"This is the time when the Dragon of (business) Demand asks the Mighty Dwarf for help with alternatives: “Oi, I don’t want my parameter to go up when the maximal value in the data does not go up. What if I am only interested in tracking the behaviour of the maximal peaks, but in a normalized way, so I get roughly the same output no matter the absolute scale of my data?”. The Dwarf is likely to reply the following: “I will not solve your problem entirely, but I have something that might help. Try the power of “crest factor”"
Descriptive Statistics: The Mighty Dwarf of Data Science – Crest Factor - KDnuggets,"OK, but what does this really mean? Let us break this down a bit. The peak value is the maximal of the absolute value of all the samples in the data, and the RMS is a kind of a measure of the total “weight” of the data, or the amount of energy contained in the data. We can see that unlike the kurtosis, the numerator of crest factor (CF) does not include any summation over all the points in the data, it only looks at the maximal peak. At the same time the denominator is the quadratic mean of all the samples in the data. Having such lightweight numerator (just one sample) and heavyweight denominator (all the samples) may imply that the effect of the appearance of additional impulses in the signal (apart from the already present maximal peak) will be greatly offset by all the other samples in the data – it may come quasi-unnoticed. The effect? Far lower sensitivity to the presence of new impulses compared to kurtosis"
Descriptive Statistics: The Mighty Dwarf of Data Science – Crest Factor - KDnuggets,"Indeed, the changes seem almost unnoticeable. Table below compares the values of kurtosis and crest factor applied on both datasets. In addition, percentage changes and baseline, impulse free signal ("
Descriptive Statistics: The Mighty Dwarf of Data Science – Crest Factor - KDnuggets,"If you are a careful analyst, you surely noticed that CF reacted with a very slight, yet interesting decrease in its value. Why? Well, the numerator did not change – the absolute peak stayed the same – but the additional impulse caused the RMS of the signal to increase a tiny bit. Hence, the ratio went down"
Descriptive Statistics: The Mighty Dwarf of Data Science – Crest Factor - KDnuggets,He subsequently moved to The University of Manchester where he obtained PhD on project sponsored by QinetiQ related to data analytics for helicopter gearbox diagnostics. Upon returning to Poland he worked as a Senior Scientist at ABB’s Corporate Research Center and a Senior Risk Modeler in Strategic Analytics at HSBC. Currently he is a Data Scientist at Codewise
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"In May of last year, I needed surgery to fix my left knee in hopes that I could get back to running, one of my favorite activities. Now, you may be thinking ""what the heck does this have to do with Data Science?"", but during my recovery I found more parallels than you might think. I started to make the connection a few months into rehab, with all of the squats, icing, balancing drills, and massages. Getting back into running shape was so similar to the tasks I had to do on a daily basis to build a strong data science culture. Even more, I came to realize that data science today is much like preparing for and running a marathon (something few do, and even less do well)"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"Despite these accolades, over the years there has been a healthy skepticism about whether or not machine learning could actually deliver business value. And this criticism is not without merit. In the ten years that"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"Data science, surprisingly perhaps, is not about designing the most advanced machine learning algorithms and training them on all of the data (and then having Skynet). It’s about finding the right data, becoming a quasi-expert on the process, system, or event you are trying to model, and crafting features that will help quirky and sometimes frail statistical algorithms make accurate predictions. Very little time is actually spent on the algorithm itself. Of course, there are exceptions to this, but even in those areas most of the research today is done by a few people (and/or companies) like Google, Facebook, etc"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"When you walk into a company that has never thought about using machine learning, some challenges immediately become apparent. After asking a few questions, you quickly find out that it will take a herculean effort to get the right information in the right place so it can be fed to a machine learning algorithm. This was especially true ten years ago, but is still very common today"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"With disparate databases managed by the traditional silos of an organization and no ability to link data points across the organization, you start to hear things like, “oh… You need to talk to Jeff about the sales pipeline data” or “Beth, in DevOps… she’s the only one who has that information”. Once you start to learn more about the business, inevitably you discover that some critical pieces of information are not being tracked or stored. Or even worse, you need to develop a way to gather data that previously wasn’t available. This may sound dramatic, but this is the reality of the experience of many data scientists when they start at a company that's just getting into machine learning. It’s no coincidence that the same study mentioned earlier shows that"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"By understanding the environment and constraints that most data scientists work under, it becomes more and more evident that machine learning isn’t actually the innovation that is driving companies using machine learning to be successful. Of course, there are industries that can and are benefiting greatly from machine learning (think self-driving cars, natural language processing, voice recognition, and the like). However, for the vast majority of companies, machine learning is an attractive hammer, and everyone thinks they have a shiny nail. Those companies might perceive their success as coming from machine learning magic, but it could be derived from another source"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"For the simple fact that machine learning doesn’t work until all (or most) of the company’s data “ducks” are in a row, these individuals lead a company to take action that would otherwise not be taken. In addition, data scientists begin thinking critically about how both products and business decisions can be informed in a data-driven way. This is a dramatic shift in the way most companies operate, and we haven’t even mentioned machine learning yet! I believe that this shift (organizing a company’s data, thinking critically about what data they have or could have, and making that data actionable and available) is the real innovation that machine learning has catalyzed"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"Most people don’t realize, but the actual “fancy” machine learning algorithm is like the last mile of the marathon. There is so much that must be done before you get there! All of the miles of training and preparing, and then you have to run the first 25.2 miles of the race. No big deal… right?"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"At CloudZero, the team is putting forth a ton of effort to make sense of disparate data sources from Amazon Web Services (AWS) to provide a holistic view of our clients’ web deployments. Being able to visualize and predict website reliability, our core value, isn’t an easy task and requires a lot of preparation. We are still in the training phase of our marathon, but fortunately we are investing heavily in getting the right data in the right place, a task that is cited as most"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"Machine learning truly is a revolutionary concept that is permeating our society, from the way we interact with our devices, communicate with one another, and conduct business around the world. But when you dissect it a little bit more, you begin to see all of the things that enable its success. It may sound blasphemous to say, but it’s important for companies to recognize that with some basic statistics and meaningful organization of their data, they're already 95% of the way there. Machine learning undeniably adds a little extra flair (and accuracy) to the final product, but here's the dirty little secret, it's not necessary to be successful"
The Dirty Little Secret Every Data Scientist Knows (but won’t admit) - KDnuggets,"In no way does this trivialize the jobs of data scientists or minimize the efforts of machine learning researchers. In fact, I believe understanding these realities increases the importance of their role as change agents within an organization. Without their constant thirst for clean and meaningful data, and the vision to do something groundbreaking with it, companies would likely never make moves to organize their data beyond the status quo"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"Netflix has released hundreds of Originals and plans to spend $8 billion over the next year on content. Creators of these stories pour their hearts and souls into turning ideas into joy for our viewers. The sublime art of doing this well is hard to describe, but it necessitates a careful orchestration of creative, business and technical decisions. Here we will focus on the latter two — business & technical decisions like planning budgets, finding locations, building sets, and scheduling guest actors that enable the creative act of connecting with viewers"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"Each production is a mountain of operational and logistical challenges that consumes and produces tremendous amounts of data. At Netflix’s scale, this is further amplified to levels seldom encountered before in the history of entertainment. This has created opportunities to organize, analyze and model this data that are equally singular in history. This is where data science can aid the art of producing entertainment"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"From the moment a show is pitched and before it shows up on our service, it goes through a few broad stages that are depicted in Figure 1. Studio Production refers to Pre-Production (planning, budgeting, etc. In the rest of this blog, we will follow a title’s journey through these stages and examine some questions that data science can help answer"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"During Pre-Production, producers and executives are tasked with critical decisions such as: do we shoot in Georgia or in Gibraltar? Do we hire a thousand extras or lean on VFX? Do we keep a 10-hour workday or a 12-hour workday? Each of these choices can have massive impact on cost, timeline and creative outcome of the project. Traditionally, these decisions are rooted in human experience and intuition. Let us see how one can supplement these with data derived insights"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"The problem we are describing is one of cost estimation: given various attributes about a production, estimate how much it will cost. These attributes should characterize both the content (genre, similarity to other titles, etc. A production team could use this model as a sandbox to answer the central question of prep: which combination of production decisions stays most true to the creative vision while also staying under budget?"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"The core challenge with building any such model is data sparsity. A production executive may want to evaluate Atlanta, Georgia as a shooting location for a big budget fantasy epic, but we may not have much historical data about Atlanta. Furthermore, the mechanisms by which location impacts cost may be complex and difficult to infer from data, even in popular shooting locations"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"For instance, suppose our goal is to model the ratio R_XY in certain production costs between geographic locations X (Atlanta) and Y (New Orleans). There are hundreds of such costs for any given production, and some vary more with location than others. Despite the sparsity in the data, one can efficiently model the fractional change in each of these costs by organizing them hierarchically (illustrated in Figure 2), placing them in a model that reflects this hierarchy, and, finally, putting priors on these ratios that reflect domain expertise. Such a model allows production executives to play around with ‘what-if’ scenarios and make informed decisions about critical aspects of a production"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"Once the pre-production minefield of decisions has been navigated, next we get to tackle the challenge of putting a plan into action. This marks the start of the Production stage (Figure 1). This is where it takes sorting through a mountain of logistical and operational challenges to enable the creative act of"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"At its core, a schedule is an ordering of scenes for each day of principal photography. A 1st AD’s job is to create such an ordering that respects various constraints and objectives: e. In particular, mathematical optimization can help generate rough schedules to inform early-stage production planning"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"Let’s describe a simplistic model that attempts to roughly capture the most basic scheduling considerations. Suppose our production consists of N scenes being shot by a single unit, over the course of D days, in L locations, and suppose we are given a rough estimate for the time each scene will take to shoot. To formulate this as an optimization problem, we need to specify variables, constraints, and an objective"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"Even with a simplistic model like this, one can generate reasonable-looking schedules within minutes. Such schedules are also useful for early stage planning (e. For expert users, models of this sort can even be used as interactive tools to selectively adjust portions of a schedule"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"After principal photography is complete, for a typical show, there could still be 100 to 200 tasks like editing, sound mixing, color correction etc. Coordination and tracking of these tasks is tackled in Post Production (Figure 1). For many TV shows and films, Post Production easily ends up consuming far more time than principal photography. For instance, ‘Apocalypse Now’ famously took over two years of post production before it was ready for the screens. Let us examine how data science can help Post Production next"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"This captures the progression of assets — like VFX shots, daily film clips, or final cuts — through a review process. In this example things flow pretty nicely, but we see drop-offs(red boxes) at the Blocked, In Progress, and Received stages. With this detail, teams can dive in to understand what is blocked or undelivered, using additional tools to evaluate these gaps on specific productions"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"Giving users the ability to choose different filters (production team, geography, facility, etc. Continuing our Sankey example, as we move toward Q4 it looks like the gaps in Blocked, In Progress, and Received even out and we’ve added more assets to the workflow. However, we might want to review the Q4 gap following the ‘In Review’ stage. This could lead teams to increase their staff, focus on technical improvements, or rebalance workload toward the end of the year"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"Locations with larger circles and a high number of edges will experience a high influx of deliveries and may need extra planning to ensure all supporting resources are in place. For Assets with complex delivery patterns (e. Asset Type 4 above), we may have an opportunity to further optimize"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"At the end of Post Production, if things go just right, we have a show or a movie ready for the screen. Quite importantly, at Netflix, this metaphorical screen is truly global. Millions of members, across 190 countries consume our content across over 20 languages. Thus localizing content to make it ‘travel’ across the globe is an important part of Studio Production. We examine it next"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"Our localization teams craft an in-depth plan to create a seamless localized experience for our content. This includes developing a relationship with the content, assessing localization complexities, and providing specific creative guidance. For example, we try to cast voice actors in each language that sound like the original cast"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"We strive to make all our content accessible to as many viewers as possible. Not surprisingly, time, talent availability, and technical constraints force us to sequence the available localization resources carefully. To make matters more complex, these decisions often need to be made many months before content is released on our service"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"Historical viewing trends inform us how our content is consumed across a range of languages and markets. If a piece of content is more popular in a language A than language B, we may sequence our efforts for A before B. For upcoming shows, this turns into the following data science problem: predict the per-language consumption for each show"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"The typical next stop in a show’s journey towards launch (Figure 1) is Quality Control (QC). Managing QC workflows for multimodal (audio, video & text) creative products like scripted or unscripted shows, films, documentaries, etc. Data Science plays an important role here by optimizing QC workflows using predictive modeling. You may read more about the specifics in our"
Data Science and the Art of Producing Entertainment at Netflix - KDnuggets,"It is not often that one gets to witness transformation of an entire industry. Opportunity to be an agent for that change is even rarer. Netflix has been that agent on a few different occasions over its short history. We believe we are at the cusp of another such transformation in the world of content creation. Working with Netflix Studio’s business, technical and creative partners to transform a century-old industry with data science is challenging, but truly invigorating. If you are interested in being part of this refreshingly new endeavor with data, please contact"
Doing Real-Time Data Analysis With Db2 Event Store - KDnuggets,"In many cases, the promise of advanced speed and performance won’t make a difference until you get your hands on the actual tool. A developer or programmer, for instance, can’t exactly gauge how much of a performance boost they’ll see until they have hands-on time. That’s exactly why IBM is offering a free developer version anyone can use to get a feel for the new system"
Doing Real-Time Data Analysis With Db2 Event Store - KDnuggets,In today’s world speed to value and simplicity are critical to get maximal value from your event data. With Db2 event store you have a “batteries included” system which combines the Db2 Event Store high speed data engine with integrated modern data science tooling through the embedded data science experience packaging. This accelerates the journey from data source to actionable business insights
Doing Real-Time Data Analysis With Db2 Event Store - KDnuggets,"That’s important as you become more reliant on real-time data and processing techniques for contextual and personalized development. Customers don’t just desire it in today’s market, they outright demand it. Experiences devoid of personalization and relevancy fail more often than those without"
"Hear From Data Science Luminaries Nate Silver and Cathy O’Neil at Rev, May 30-31, SF - KDnuggets","Rev is a 2-day summit that fills this need: Built for data science leaders and practitioners, Rev offers interactive sessions, stimulating conversations, and tutorials about how to run, manage, and accelerate data science as an organizational capability. You'll hear from luminaries Nate Silver, Cathy O'Neil, and Wes McKinney as well as speakers from Airbnb, United Health Group, BlackRock, Deloitte, Ancestry and more. Rev is happening in San Francisco, May 30-31"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"Being a data scientist involves much more than the technical side of the job. As data scientists we are expected to solve complex business problems and make sure that the models we develop provide value to the business. For this, it is crucial to understand what is important to the business and stay in constant contact with business units to avoid divergence between what we develop and what they need"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,My day starts around 9:00 A.M. I meet with people involved in the projects I work on and provide 3 pieces of information:
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"A typical day for me starts the moment I get in my car. With a cup of coffee in hand and a two hour commute (thank you, Los Angeles), I am (usually) excited to start learning and doing something productive. I use this time to catch up on my favorite podcasts (Masters of Scale, Recode Decode, or Data Skeptic), listen to YouTube videos on ML/DL, prep for my next teaching lesson, brainstorm new product ideas/approaches, or put on some good music to get me pumped for the day"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"Once at work, I settle in with a cappuccino and a Spotify playlist. I then use the morning to get caught up on emails and Slack. The first thing I look for are any urgent requests from clients. If there isn’t anything on fire, I read the Business Insider updates that come in daily. These updates help me keep current on trends and new product features on the platforms with which we work. These updates are enormously helpful when it comes time to make high-impact decisions on campaign strategy"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"The middle of the day could bring anything! Administrative requests, ad hoc reporting needs, internal meetings, client meetings, refinement of existing models, building out new tools, and many more things I cannot fully list here. Each of these tasks play a key role for the business and for my personal development. Administrative tasks like sprint planning and hiring challenges and improves my time-management skills and ability to make quick decisions. Client meetings challenges and improves my communication skills and business acumen. Refining and building out new models challenges and improves my technical skills and creativity. As a data scientist, I find cross-functional roles that allow you to develop holistically to be the most exciting and rewarding"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"Once I have put in a full day at work, I keep thinking about and learning data science. Depending on the day of the week, I keep my evenings full with other activities like: teaching a bootcamp on Data Visualization, appearing on a panel for Data Science Office Hours with other passionate data scientists in the field, mentoring women in engineering through Society of Women Engineers, or hosting my own Machine Learning Meetups. If you can’t tell, I can’t get enough of this field"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"To recap: I start my day around 7am and end up back home around 10-11pm. The days are long but I love what I do. The key to making each day a success is happiness. I try to incorporate things in my day that bring me joy and help me grow, individually and within my community"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"No alarm needed. One of my two kids will surely wake up by 6am at the latest. I’ve held this early morning schedule for close to 4 years now and I love it! I make a cup of coffee, sit down, and check my phone (guilty of doing this first thing in the morning). After reviewing my emails/ messages for work, as well as my personal messages, it’s time to start getting my older daughter (age 3) to her school. We pack her lunch together, pick out her outfit and then spend some time playing before I drop her off at 8am. The best part of my routine is that I work from home (it is super cool that my job provides this flexibility). If the weather is nice and the mood strikes, I go for a run (about 2-3 miles) before starting work"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"9am – typically log into my work computer and check my calendar to see what calls I have for the day; then I start work. My responsibilities include working on dashboards, developing KPIs, ensuring data quality and most importantly providing data transparency for leadership. I’m there to help people connect the dots using data. Mainly I do all of this via my favorite data visualization tool – Tableau! I love the tool so much that I have a YouTube channel where I provide tutorials on how to use Tableau –"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"Around 12pm I have lunch (usually something I prepare that same day). Then continue working, touch base with my team members and make sure we are all on track to accomplish what we set out to accomplish for the day/ week. At 2pm I go and pick up my daughter from her school and bring her home (it’s walking distance and takes only a few minutes; but I love the energy that I come back with after this short walk). The rest of the afternoon is spent on work and thinking about how to automate some of the work using technology"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"I first took an interest in Data Science when I attended Machine and Deep Learning classes at Dartmouth as a computer science major undergraduate; I thought the material was intriguing, exciting, and new. After realizing the potential of this field, I was certain this was what I wanted to get into. I also knew I wanted to work at a startup"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"From there, I taught myself how to use some data science APIs properly through online resources. I had already developed a good mathematical and theoretical machine learning foundation from my classes at school, but still needed to learn many of the basics of data visualization and engineering, such as how to efficiently index Pandas Dataframes, create Sci-Kit learn pipelines, visualize data with Seaborn/MatPlotlib, clean data properly with lambda functions, and more. I found"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"Dataiku is a high growth international startup that makes a platform for Data Science; the company was founded in Paris and raised a 28 million series B this past September. From day one, I knew I’d made the right decision in both field and career path. Not only does Dataiku have a phenomenal company startup culture, but everyday I also get to work on something new and exciting. No day is the same here, which makes my job a lot of more fun"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"When we onboard new customers, many of them want us to be onsite for 1-3 days to teach their data engineers, analysts, and scientists how to leverage Dataiku to enable collaboration between their teams, easily put data pipelines/ models into production, and teach them how to use many more of the features our platform offers. I typically need to do this two to three times a month while traveling all over the country and being exposed to different data science processes and teams. It’s very interesting to observe how different industries and companies are organized and what they are doing with their data"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"Many companies also purchase professional services from Dataiku, and I get to work with a lot of different firms on their data projects. I have been exposed to a wide range of data science toolbox stacks, use cases, teams, and organizational structures. I’ve worked on NLP projects, time series forecasting, database deduplication, visualization, deep learning, production pipelines, Spark pipelines, and much more with our customers. One of my favorite parts about working at Dataiku, is that I’m not confined to working within one industry, or using one tool stack; every week I’m working with different companies, learning new things, and helping them truly get the most out of their data with Dataiku. I’m lucky to be working at a startup like this, and I really look forward to seeing it continually improve and grow"
A Day in the Life of a Data Scientist: Part 4 - KDnuggets,"When I am not in the office or traveling to meet with customers, I enjoy spending my free time reading tech/ AI news, coding, working on data science projects, playing basketball, brainstorming startup ideas, hanging out with friends, and reading the latest deep learning research papers. I am able to stay updated with news/ tools in AI and Data Science by browsing my LinkedIn feed; I actually like it more than any other social media platform Some great people to follow I recommend - Matthew Mayo, Florian Douetteau, Andriy Burkov, Rudy Agovic, Gregory Renard. I typically get up around 8AM and then take the subway to downtown Manhattan to work. I get home around 7PM. I try to work out as much as possible also, since I am an ex-D1 basketball athlete"
Ranking Popular Distributed Computing Packages for Data Science - KDnuggets,"Below is a ranking of the top 20 of 140 distributed computing packages that are useful for Data Science, based on Github and Stack Overflow activity, as well as Google Search results. The table shows standardized scores, where a value of 1 means one standard deviation above average (average = score of 0). For example,"
Ranking Popular Distributed Computing Packages for Data Science - KDnuggets,"The package ranking is based on equally weighing its three components: Github (stars and forks), Stack Overflow (tags and questions), and number of Google search results. These were obtained using available APIs. Coming up with a comprehensive list of distributed computing packages was tricky - in the end, we scraped three different lists that we thought were representative. We chose to focus on 140 frameworks and distributed programing packages (see methods below for details). Computing standardized scores for each metric allows us to see which packages stand out in each category. The"
Ranking Popular Distributed Computing Packages for Data Science - KDnuggets,"Apache Spark (1) is an incredibly popular open source distributed computing framework. Apache Spark dominated the Github activity metric with its numbers of forks and stars more than eight standard deviations above the mean. Apache Spark utilizes in-memory data processing, which makes it faster than its predecessors and capable of machine learning. It also offers an interactive console in either Scala or, more popular among data scientists, Python. Although Apache Spark was initially designed for the Hadoop ecosystem, it can run on its own using one of many different file management systems. Apache Hadoop (2) outperformed Apache Spark in Stack Overflow activity. The disconnect between Hadoop's Stack Overflow activity and the other two metrics is likely due to the fact that the meaning of Apache Hadoop has evolved over time. Rather than referring to just the framework, the term ""Hadoop"" can also mean all Hadoop-related projects that make up the ecosystem. This results in a somewhat inflated Stack Overflow score. Nevertheless, most of the frameworks and engines on our list have Apache Hadoop integrations. And it measured at least two standard deviations above the mean on all our metrics, solidifying its number two spot"
Ranking Popular Distributed Computing Packages for Data Science - KDnuggets,"Apache Storm (4), initially touted as the Apache Hadoop of real-time, is a stream-only framework best for near real-time distributed computing. It performed above average on all of our metrics. While Apache Storm processes stream data at scale, it is frequently used with Apache Kafka (3), a platform that processes the raw messages from real-time data feeds at scale. Similar to Apache Spark, Apache Flink (8) is also a framework capable of both batch and stream processing. However, Apache Spark bills itself as a batch-processor that can handle streaming, while Apache Flink is suited for heavy stream processing with some batch tasks"
Ranking Popular Distributed Computing Packages for Data Science - KDnuggets,"The most popular of the two Twitter projects on our list, Apache Storm (4), was donated to the Apache Software Foundation by Twitter in 2011. Twitter Heron (9) is a direct successor to Apache Storm released in June 2016. Twitter Heron offers improved real-time, fault-tolerant stream processing with higher throughput than Storm. Twitter Heron had the fifth largest quarterly growth rate with an increase of 180%. It will be interesting to see if Twitter Heron can climb farther up the ranks with time"
Ranking Popular Distributed Computing Packages for Data Science - KDnuggets,"The Hadoop Ecosystem projects are the most prevalent and widely adopted distributed computing frameworks and interfaces. 17 of the top 20 packages we ranked are part of the Hadoop Ecosystem or designed to integrate with Apache Spark or Apache Hadoop (including HDFS). Outside of the Hadoop Ecosystem Hazelcast (10), an in-memory data grid, Google BigQuery (12), cloud-based big data analytics web service using a SQL-like syntax, and Metamarkets Druid (15) a framework for real-time analysis of large datasets performed well on our metrics"
Ranking Popular Distributed Computing Packages for Data Science - KDnuggets,"He worked as a data scientist (Foursquare), quant (D.E. Shaw, J.P. Morgan), and a rocket scientist (NASA). He did his PhD at Princeton as a Hertz fellow and read Part III Maths at Cambridge as a Marshall scholar. At Foursquare, Michael discovered that his favorite part of the job was teaching and mentoring smart people about data science. He decided to build a startup that lets him focus on what he really loves"
Top 12 Essential Command Line Tools for Data Scientists - KDnuggets,"And there you have a simple introduction to 12 handy command line tools. This is only a taste of what is possible at the command line for data science (or any other goal, for that matter). Free yourself from the mouse and watch your productivity increase"
Descriptive Statistics: The Mighty Dwarf of Data Science - KDnuggets,Nowadays fair part of the community (often influenced by the pressure from the business) seems to show a tendency of applying somewhat complex and rather computationally expensive algorithms to applications that would have been easily accommodated in the past by much simpler (hence faster) and much more interpretable (hence of greater business value) techniques. In the series of texts to come I will try to introduce the power and beauty of descriptive statistics as an approach for quantitatively describing the nature of data and creating solid foundations for any subsequent data investigations. In this post I will introduce one of the weapons of the mighty dwarf – the “kurtosis”
Descriptive Statistics: The Mighty Dwarf of Data Science - KDnuggets,"Consider a case where a monitoring system is to detect anomalies within the data. Typically, one may turn into the classic means of outlier analysis like the DBSCAN-based approaches or LOF. Nothing wrong with these, they may perfectly well point towards the directions where the outliers may be present. However, these techniques may require substantial computational resources to complete the task on high volumes of data in reasonably acceptable amount of time. A much faster alternative may come from considering the given case as a time series analysis problem. Such data coming from a system operating in ‘healthy’ conditions would have a typical, acceptable amplitude distribution and, in such scenario, any deviation from the expected shape may be considered a potential threat, worth detecting"
Descriptive Statistics: The Mighty Dwarf of Data Science - KDnuggets,"Often kurtosis is thought of as a measure of the “peakidness” of the amplitude distribution of a signal. What does it mean? We are all pretty well accustomed to the bell-shape of the Gaussian distribution. Should a Gaussian signal accumulate some additional samples of significantly greater amplitudes (impulses), its distribution will suffer from widening the its tails. As a result, the entire distribution would look more sharp i. I will present this concept in the following case study"
Descriptive Statistics: The Mighty Dwarf of Data Science - KDnuggets,"Some implementations of kurtosis use the raw output of the Eq. 1, but often a value of 3 is subtracted from the kurtosis value, so that the value of kurtosis for the Gaussian signal is around 0. Such implementation is often referred to as the “excess kurtosis”. The later version is used in SciPy by default. Since I’m in favour of the classic “output of 3 for the Gaussian”, I am setting the “fisher” parameter of the kurtosis function to False"
Descriptive Statistics: The Mighty Dwarf of Data Science - KDnuggets,A very clear increase. A very clear change. The value of kurtosis is directly proportional to the amplitude of the impulses which constitutes another very helpful property of this metric
Descriptive Statistics: The Mighty Dwarf of Data Science - KDnuggets,"Coming back to the informal definition of kurtosis as a measure of the peakidness of the distribution. Compare histograms of ‘sales’ and ‘sales_spike’ shown in Figure 3 and 4 respectively. Even though the two histograms are being generated from virtually identical signals, apart from the ‘sales_spike’ having the two impulses, the amplitude distributions look very different and one of them clearly appears as more sharp (the one with the higher kurtosis value) compared to the other"
Descriptive Statistics: The Mighty Dwarf of Data Science - KDnuggets,He subsequently moved to The University of Manchester where he obtained PhD on project sponsored by QinetiQ related to data analytics for helicopter gearbox diagnostics. Upon returning to Poland he worked as a Senior Scientist at ABB’s Corporate Research Center and a Senior Risk Modeler in Strategic Analytics at HSBC. Currently he is a Data Scientist at Codewise
Multiscale Methods and Machine Learning - KDnuggets,"Multiscale methods, in which a dataset is viewed and analyzed at different scales,are becoming more commonplace in machine learning recently and are proving to be valuable tools. At their core, multiscale methods capture the local geometry of neighborhoods defined by a series of distances between points or sets of nearest neighbors. This is a bit like viewing a part of a slide through a series of microscope resolutions. At high resolutions, very small features are captured in a small space within the sample. At lower resolutions, more of the slide is visible, and a person can investigate bigger features.Main advantages of multiscale methods include improved performance relative to state-of-the-art methods and dramatic reductions in necessary sample size to achieve these results"
Multiscale Methods and Machine Learning - KDnuggets,"As a simple example, consider taking the mean of nearest neighbors within a number line or two-dimensional space. When k increases, the mean of point sets on the number line becomes more spread out, with farther neighbors pulling the mean more than nearer neighbors. Nearest neighbors that are farther away from an existing set of points tend to drag the mean further than added points that are closer to the existing set of points. In the number line example, say the points sit at 1, 2, 4, 5, 9, 11, and 14. Consider the set of the left most point (at 1) and its 3 nearest neighbors: 2, 4, and 5. The mean for this set is 3, which has 2 points with higher values and 2 points with lower values. When 9 is added to the set as a 4"
Multiscale Methods and Machine Learning - KDnuggets,"2, which is skewed towards point 9 rather than the 4 points whose values are close to one another. Examining the mean of point 1’s 3 nearest neighbors reveals a set of points with fairly similar values and variance among values. However, examining the 4 nearest neighbors of point 1 captures the presence of an outlier. The KNN mean function with a single value of k fails to capture some of these important local properties that emerge at different scales, and important information is lost"
Multiscale Methods and Machine Learning - KDnuggets,"Not all multiscale methods rely on KNN-defined scales, but they do employ similar principles that allow the method to learn multiscale features within a dataset. Grouping points within a series of distances, providing a series of convolutions of different scales, and employing hierarchies of clusters are common choices in multiscale methods. A recent deep learning paper by Pelt & Sethian (2017) based on multiscale methods was able to learn image segmentation and image classification on training sets as small as 6 images with high accuracy (>90%); this algorithm significantly reduced tuning requirements of the deep learning framework, as well. Another deep learning paper (Xiao et al. This algorithm consistently outperformed state-of-the-art deep learning algorithms on several benchmark datasets, suggesting that multiscale methods can improve even state-of-the-art convolution networks"
Multiscale Methods and Machine Learning - KDnuggets,"Recent work on k-nearest neighbor (KNN) regression and classification ensembles using varied neighborhood size have shown dramatic improvement over not only the KNN algorithms with a single value for k but also other machine learning methods. The multiscale tools of topological data analysis (TDA), including persistent homology, have aided analysis of small datasets, images, videos, and other types of data, providing data scientists and machine learning researchers with a robust, general unsupervised learning tool. A recently-added multiscale component on the TDA tool Mapper improved theoretical stability of the algorithm and yielded consistent results"
Multiscale Methods and Machine Learning - KDnuggets,"Alternatives to multiscale methods exist, but they often come with a cost. Rather than improve performance and compute times with a small training sample, many provide either improved performance or reduction in necessary sample size. Bayesian methods, the main competitor of multiscale methods in recent years, have reduced the necessary sample size for good performance of deep learning algorithms, but they come with a high computational cost that may not be appealing for online algorithm use or quick turnaround of analyses in industry. These methods also require a high degree of statistical expertise that many practitioners may not have. Deep learning has become a ubiquitous, general tool in recent years, but its power seems to lie in the asymptotic properties. Rather than level-off in performance when a large enough sample size is reached for the algorithm to converge, deep learning algorithms continue to improve with further increases in sample size. This is desirable when a lot of data is available, but many industry use cases and research problems involve small datasets or limited examples for a given class"
Multiscale Methods and Machine Learning - KDnuggets,"It seems that local geometry matters. What is relevant and important for one neighborhood may not be relevant or important for a larger neighborhood surrounding the original one, and examining multiple scales can help capture all the important features, rather than the neighborhood yielding the best subset of features. Given the success of multiscale methods on a variety of learning tasks and their enhancement many types of algorithms, it is likely that machine learning will see more algorithms incorporating multiscale subsets in the coming years"
Multiscale Methods and Machine Learning - KDnuggets,"Dey, T. K. Multiscale mapper: Topological summarization via codomain covers. In"
Multiscale Methods and Machine Learning - KDnuggets,"Farrelly, C. M. KNN Ensembles for Tweedie Regression: The Power of Multiscale Neighborhoods"
Multiscale Methods and Machine Learning - KDnuggets,"Pelt, D. M. A. A mixed-scale dense convolutional neural network for image analysis"
Multiscale Methods and Machine Learning - KDnuggets,"Xia, K. W. Persistent homology analysis of protein structure, flexibility, and folding"
A Beginner’s Guide to Data Engineering – Part II - KDnuggets,"The discussion in part I was somewhat high level. In Part II (this post), I will share more technical details on how to build good data pipelines and highlight ETL best practices. Primarily, I will use Python, Airflow, and SQL for our discussion"
A Beginner’s Guide to Data Engineering – Part II - KDnuggets,"On the other hand, it is often much easier to query from a denormalized table (aka a wide table), because all of the metrics and dimensions are already pre-joined. Given their larger sizes, however, data processing for wide tables is slower and involves more upstream dependencies. This makes maintenance of ETL pipelines more difficult because the unit of work is not as modular"
A Beginner’s Guide to Data Engineering – Part II - KDnuggets,"The name arose because tables organized in star schema can be visualized with a star-like pattern. This design focuses on building normalized tables, specifically fact and dimension tables. When needed, denormalized tables can be built from these smaller normalized tables. This design strives for a balance between ETL maintainability and ease of analytics"
A Beginner’s Guide to Data Engineering – Part II - KDnuggets,"Furthermore, the unit of work for a batch ETL job is typically one day, which means new date partitions are created for each daily run. Finally, many analytical questions involve counting events that occurred in a specified time range, so querying by datestamp is a very common pattern. It is no wonder that datestamp is a popular choice for data partitioning!"
A Beginner’s Guide to Data Engineering – Part II - KDnuggets,"Another important advantage of using datestamp as the partition key is the ease of data backfilling. When a ETL pipeline is built, it computes metrics and dimensions forward, not backward. Often, we might desire to revisit the historical trends and movements. In such cases, we would need to compute metric and dimensions in the past — We called this process"
A Beginner’s Guide to Data Engineering – Part II - KDnuggets,"The operation above is rather tedious, since we are running the same query many times but on different partitions. If the time range is large, this work can become quickly repetitive. When dynamic partitions are used, however, we can greatly simplify this work into just one query:"
5 Things to Know Before Rushing to Start in Data Science - KDnuggets,"Matrix calculations, derivatives, eigenvalues, Set Theory, functions, vectors, linear transformations, etc. Therefore, before starting your next MOOC or Machine Learning book it’s crucial to review all those concepts again. Most schools request students to be proficient at these methods in order to graduate, but the silver lining is that it won’t require too much of your time to refresh or obtain this knowledge"
5 Things to Know Before Rushing to Start in Data Science - KDnuggets,"Almost every job/profession in industry is directly impacted by some program that enables the input, transform and print process of information. Learning about programming and how code works is not only to make software, apps or create a great website. Learning how to program will give you the advantage to understand how technology impacts our lives. Instead of blaming the computer program for “not working”, you will now think systematically and understand where the problem may be. And who knows, maybe you’ll come with better ideas to improve technology from a user perspective"
5 Things to Know Before Rushing to Start in Data Science - KDnuggets,"The highlight of these type of shows, apart from all the action, jokes, and hero-scenes, is the “Critical Thinking” used by the characters to find the solution to different kinds of problems. This is one thing that is not mentioned in most of the Data Science resources. The ability to find the correct angle to approach a problem will lead you to identify not only which tools to use for any problem, but will sometimes lead you to the most efficient solution"
5 Things to Know Before Rushing to Start in Data Science - KDnuggets,"There are many visualization packages (seaborn, ggplot, matplotlib) and software (tableau, excel) that can help create wonderful crisp charts. So, avoid getting saturated with too many options. The most important thing is how the message is delivered. Sometimes the simplest tools will generate a clear, relevant outcome"
"18 Inspiring Women In AI, Big Data, Data Science, Machine Learning - KDnuggets","Women are underrepresented in STEM fields - science, technology, engineering, and math. For instance, women made up 27% of people employed in computer and mathematical occupations in 1960. But instead of growing over several decades, as many more women participated in the workforce overall, that number had declined to 26% by 2013, according to a"
"Great Data Scientists Don’t Just Think Outside the Box, They Redefine the Box - KDnuggets","It’s a very cool application of Deep Learning. But let’s assume there “might” be an even better way to estimate solar energy savings. For example, you want to use Deep Learning to estimate how much solar energy we could generate with solar panels on the Golden Gate Bridge (that probably wouldn’t be a very popular decision in San Francisco). The obvious application would be to analyze several photos of the Golden Gate Bridge and estimate clear skies based upon cloud coverage"
"Great Data Scientists Don’t Just Think Outside the Box, They Redefine the Box - KDnuggets","A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. RNNs can use their internal memory to process arbitrary sequences of inputs, which typically makes RNNs ideal for handwriting or speech recognition. Except in this case, instead of trying to decipher handwriting into words, the data science team used the RNN to"
"Great Data Scientists Don’t Just Think Outside the Box, They Redefine the Box - KDnuggets","I love this example because the team didn’t feel constrained to try to fit the square peg into the round “Machine Learning” hole. Instead, they used Deep Learning in a different context to decipher seemingly random pixels into a prediction of the health of a device. The data scientists didn’t wait until someone developed a better Machine Learning algorithm. Instead, they looked at the wide variety of Machine Learning and Deep Learning tools and algorithms available to them, and applied them to a different, but related use case. If we can predict the health of a device and the potential problems that could occur with that device, then we can also help customers prevent those problems, significantly enhancing their support experience and positively impacting their environment"
"Great Data Scientists Don’t Just Think Outside the Box, They Redefine the Box - KDnuggets","By the way, I included this image just because I thought it was cool. This graphic measures the activity between different IT systems. Just like with data science, this image shows there’s no lack of variables to consider when building your Machine Learning and Deep Learning models!"
"Great Data Scientists Don’t Just Think Outside the Box, They Redefine the Box - KDnuggets","I would like to thank my co-author Michael Shepherd, AI Research Strategist, Dell EMC Services. Michael holds U.S. With experience in supply chain, manufacturing and services, he enjoys demonstrating real scenarios with the SupportAssist Intelligence Engine showing how predictive and proactive AI platforms running at the “speed of thought” are feasible in every industry"
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,"Learn from each application, each rejection. When I started applying for jobs I had to deal with a lot of rejection. Something I was actually not prepared to. I think no one prepares you for rejection, but if you get something from this is, it’s ok! it’s normal and not personal!"
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,"If you are lucky enough you will get an amazing recruiter that will let you know what happened and how you can improve for future interviews or processes. Recognizing your flaws and weaknesses is the beginning of getting better. This frustration you feel now, or you may feel use it to improve and get better every time"
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,"If you have something you want the reader or listener [interviewer] to know, you’d better put that up front in your message. For resumes, that means you lead with your strongest aspect. Maybe that’s your education. Maybe it’s your job experience.Don’t feel that you have to follow the order in that resume template you downloaded"
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,"You want to communicate your passion for the field? Do some personal projects. Contribute to open source. Start a blog. Heck, be active […] on LinkedIn"
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,"My advice? Consider alternative approaches to finding a job. In the past 10 years, every job I have taken has come from networking. The best jobs often do"
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,"[…] when you’re in the mode of answering questions, it’s tough to start asking them. When you’re in the mode to impress, it’s tough to expect the same in return. Remember that"
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,Leave an interview with the team wanting more but also expect to leave the interview with the same desire yourself. Were YOU impressed? What did they do to make YOU feel welcome? You’ve put in work to get to where you are now. Gravitate towards those businesses that lift you up rather than diminish all you’ve achieved
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,"Great companies put in work to leave every candidate blown away, even the ones they don’t hire. Amazon is an excellent example of a company that has impressed me with their hiring process. I’ve had multiple dealings with their recruiters; always professional, quick to respond, & bringing roles that are good fits for my capabilities"
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,Most […] peole who get your resume have absolutely no idea what we really do; they just have a list to check. They’re looking for keywords — not concepts. Most of the them are not going to be bothered with having to pan for gold. They’re going to give it less than a minute (literally) and move on to the next one on their pile
The Two Sides of Getting a Job as a Data Scientist - KDnuggets,"Read all of that advices, and look for more. They are great. Some a little be hard to read, but they are true"
Should You Ever Volunteer Your Data Skills for Free? - KDnuggets,"What you’ve read here may spur you toward working for free — at least in some cases. If you decide to take that route, always do so in practical ways. For example, don’t let volunteering take priority over work that generates income"
Datadeck Makes Data Sexy! - KDnuggets,"Soon everyone in your company will be like “Hey, who came up with those stunning reports and insights. They must be a real winner. This will allow you to (for once) focus on what’s really important: cracking data, being awesome, and getting with the prom queen!"
Datadeck Makes Data Sexy! - KDnuggets,"Do you spend a lot of time writing internal reports or reports for clients? Datadeck makes traditional reporting practices redundant by allowing its users to share individual, automatically updating dashboards in the place of regular reports. Annotate your data and click “share”. Send the secure link to whoever you want to keep in the loop, and spend the time you’ve saved on more important things"
Datadeck Makes Data Sexy! - KDnuggets,"He started playing with computers and nerdy stuff back in the days when he was a little kid. He grew up with tech, IT, software and marketing. He is living in Taipei and working for Datadeck, enjoying data every single day"
Time Series for Dummies – The 3 Step Process - KDnuggets,"After a satisfying meal of Chinese takeout, you absentmindedly crack open the complimentary fortune cookie. Glancing at the fortune inside, you read, “A dream you have will come true. Being the intelligent, well-reasoned person you are, you know the fortune is insignificant—no one can predict the future. However, that thought may be incomplete. There is a way to predict the future with great accuracy: time series modeling"
Time Series for Dummies – The 3 Step Process - KDnuggets,"Time series involves the use of data that are indexed by equally spaced increments of time (minutes, hours, days, weeks, etc. Due to the discrete nature of time series data, many time series data sets have a seasonal and/or trend element built into the data. The first step in time series modeling is to account for existing seasons (a recurring pattern over a fixed period of time) and/or trends (upward or downward movement in the data). Accounting for these embedded patterns is what we call making the data stationary.  Examples of trending and seasonal data can be seen if figures 1 and 2 below"
Time Series for Dummies – The 3 Step Process - KDnuggets,"A stationary series is one where the mean of the series is no longer a function of time. With trending data, as time increase the mean of the series either increases or decreases with time (think of the steady increase in housing prices over time). For seasonal data, the mean of the series fluctuates in accordance with the season (think of the increase and decrease in temperature every 24 hours)"
Time Series for Dummies – The 3 Step Process - KDnuggets,"There are two methods that can be applied to achieve stationarity, difference the data or linear regression. To take a difference, you calculate the difference between consecutive observations. To use linear regression, you include binary indicator variables for your seasonal component in the model. Before we decide which of these methods to apply, let’s explore our data. We plotted the historical daily page views using SAS Visual Analytics"
Time Series for Dummies – The 3 Step Process - KDnuggets,"The initial pattern seems to repeat itself every seven days indicating a weekly season. The prolonged increase in the number of page views over time indicates that there is a slightly upward trend. With a general idea of the data we then applied a statistical test of stationarity, the Augmented Dickey-Fuller (ADF) test. The ADF test is a unit-root test of stationarity. We won’t get into the details here, but a unit-root indicates if the series is nonstationary so we use this test to determine the appropriate method to handle the trend or season (differencing or regression). Based on the ADF test for the data above we removed the seven-day season by regressing on dummy variables for day of the week and removed the trend by differencing the data. The resulting stationary data can be seen in the figure below"
Time Series for Dummies – The 3 Step Process - KDnuggets,"Now that the data is stationary, the second step in time series modeling is to establish a base level forecast. We should also note that most base level forecasts do not require the first step of making your data stationary. This is only required for more advanced models such as ARIMA modeling which we will discuss momentarily"
Time Series for Dummies – The 3 Step Process - KDnuggets,"A second type of model is the average model. In this model, all observations in the data set are given equal weight. Future forecasts of y are calculated as the average of the observed data. The forecast generated could be quite accurate if the data is level, but would provide a very poor forecast if the data is trending or has a seasonal component. The forecasted values for the page views data using the average model can be seen below"
Time Series for Dummies – The 3 Step Process - KDnuggets,"If the data has either a seasonal or trend element, then a better option for a base level model is to implement an exponential smoothing model (ESM). ESMs strike a happy medium between the naïve and average models mentioned above, where the most recent observation is given the greatest weight and the weight of all previous observations decrease exponentially into the past. ESMs also allow for a seasonal and/or trending component to be incorporated into the model. The following table provides an example of an initial weight of 0.7 decreasing exponentially at a rate of 0.3"
Time Series for Dummies – The 3 Step Process - KDnuggets,There are various types of ESMs that can be implemented in time series forecasting. The ideal model to use will depend on the type of data you have. The table below provides a quick guide as to what type of ESM to use depending on the combination of trend and season in the data
Time Series for Dummies – The 3 Step Process - KDnuggets,"Because of the strong seven-day season and upward trend in the data, we selected an additive winters ESM as the new base level model. The forecast generated does a decent job of continuing the slight upward trend and captures the seven-day season. However, there is still more pattern in the data that can be removed"
Time Series for Dummies – The 3 Step Process - KDnuggets,"After identifying the model that best accounts for the trend and season in the data, you ultimately have enough information to generate a decent forecast, as we see in Figure 2 above. However, these models are still limited in that they do not account for the correlation that the variable of interest has with itself over previous periods of time. We refer to this correlation as autocorrelation, which is commonly found in time series data. If the data has autocorrelation, as ours does, then there may be additional modeling that can be done to further improve upon the baseline forecast"
Time Series for Dummies – The 3 Step Process - KDnuggets,"To capture the effects of autocorrelation in a time series model, it is necessary to implement an Autoregressive Integrated Moving Average (or ARIMA) model. ARIMA models include parameters to account for season and trend (like using dummy variables for days of the week and differencing), but also allow for the inclusion of autoregressive and/or moving average terms to deal with the autocorrelation imbedded in the data. By using the appropriate ARIMA model, we can further increase the accuracy of the page views forecast as seen in Figure 3 below"
Time Series for Dummies – The 3 Step Process - KDnuggets,"While you can see the improved accuracy of each of the models presented, visually identifying which model has the best accuracy is not always reliable. Calculating the MAPE (Mean Absolute Percent Error) is a quick and easy way to compare the overall forecast accuracy of a proposed model – the lower the MAPE the better the forecast accuracy. Comparing the MAPE of each of the models previously discussed, it is easy to see that the seasonal ARIMA model provides the best forecast accuracy. Note that there are several other types of comparison statistics that can be used for model comparison"
Time Series for Dummies – The 3 Step Process - KDnuggets,"In summary, the trick to building a powerful time series forecasting model is to remove as much of the noise (trend, season, and autocorrelation) as possible so that the only remaining movement unaccounted for in the data is pure randomness. For our data we found that a seasonal ARIMA model with regression variables for day of the week provided the most accurate forecast. The ARIMA model forecast was more accurate when compared to the naïve, average, and ESM models mentioned above"
Time Series for Dummies – The 3 Step Process - KDnuggets,"While working as an Economist for the Idaho Department of Labor, Chris produced labor forecasts and economic impact analyses for technical stakeholders. Chris has a BS in Economics and an MS in Analytics from the Institute for Advanced Analytics at NC State University. Chris’s enthusiasm for machine learning and predictive modeling is only rivaled by his enthusiasm for his family. Whether hiking trails or exploring a new museum, he is constantly on the move with his wife and two kids"
Time Series for Dummies – The 3 Step Process - KDnuggets,"He has a passion for code and enjoys the opportunity to use his skills to solve complex problems. When not working on an analytical project, Sean spends his time contributing to the Zencos blog where he uses data to tell an interesting story. A North Carolina native, Sean received a BS in Business and a MS in Analytics from NC State University"
Data Science in Fashion - KDnuggets,Fashion industry is an extremely competitive and dynamic market. Trends and styles change with the blink of an eye. Data Science can be used here on historical data to predict the trends which will be “Hot” hence potentially saving a lot of time and money
Data Science in Fashion - KDnuggets,"How many stores that you grew up visiting aren’t around anymore? Remember Sears, Zellers, American Apparel, Wet Seal, The Limited, etc. Many other prominent retailers like Aeropostale, Bebe, A&F, Guess, J.C Penney, Payless, Rue2, etc. Forbes estimates that in last sector of 2017, 21 retailers are closing 3,591 stores. They can’t compete with the e-commerce sector as more and more customers resort to shopping online as reduced prices from the comfort of their homes in lieu of physically going to retail stores. Retailers are forced to leverage data in order to upgrade their infrastructure and services to give their customers a better experience. Many job postings at Macy’s, Coach, Kate Spade, Nordstrom, etc"
Data Science in Fashion - KDnuggets,"Fashion industry is an extremely competitive and dynamic market. Trends and styles change with the blink of an eye. A collection or trend takes thousands of man-hours from the most creative minds and the lucrative outcome depends on the simple Hot or Not judgements from fashion pundits, bloggers and celebrities. Data Science can be used here on historical data to predict the trends which will be “Hot” hence potentially saving a lot of time and money. For example, training good models on previous sales data can help us predict whether Kanye West’s new Yeezy Season 6 collection will be a success or not. Data Scientists can use concepts from predictive algorithms, visual search, capturing structured data from photographs, natural language processing and many more"
Data Science in Fashion - KDnuggets,"Data is abundant in the fashion and retail industry. The vast historical data from retailers and department stores about the spending habits of customers is a traditional source. With the advent of social media, engagement on posts, Instagram trends, Twitter hashtags, clothing style of the most popular fashion bloggers, celebrity fashion styles, the “likes/reactions” on popular celebrities, etc. A new technique to test the reaction before the deployment of the collection is to release photos on Social Media (Facebook, Twitter, Instagram, Pinterest) and study the comments to make changes to the collection before launching. Sentiment analysis is used here to get insights from the public opinion. The public API of these sites are open and easy to use. Data can be scrapped in real-time through these API’s and converted to usable form. Online data sources are raw and uncensored and voice the public opinion. This data has a lot of potential if harnessed properly. Most data you find online is unstructured : texts, images, audio and YouTube videos. Unstructured data can be a challenge to use in its native form and needs to be cleaned and transformed"
Data Science in Fashion - KDnuggets,"An interesting source of data are the wifi signals of the customers in a store. The pattern of the customers is tracked to see how long they stay, which sections they visit sequentially, how often they come back and how long they stay in each section. This kind of data can be useful to arrange the collection in stores and to place closely the items frequently purchased together"
Data Science in Fashion - KDnuggets,"Zara is one of the most popular and successful stores in the fashion world. They adopt the concept of “fast fashion”, where the whole process of designing a collection to shipping it to stores takes a maximum of three weeks. The success of this brand is attributed to this dynamic concept where the retailer studies the choices and preference of the customer to create a collection catering their tastes. They create what the customer craves instead of selling what they design. The customer themselves may not know what they are particularly looking for, but the smart business analysts and data scientists at Zara make use of the data to create a collection which the customers will automatically want because it’s their “taste”. Moreover Zara has stores all over the world and their customers have different demographics which means something as simple as size, body shape, colour preferences and quantity will vary greatly. Producing the right quantity of right products helps minimize wastage"
Data Science in Fashion - KDnuggets,"Using Big data, we can find the colors preferred by the customers to curate a best-selling collection. The range of colors for a particular style, the combination of colors purchased together, etc. Many times customers purchase a piece of clothing in one color and then exchange it for another. The data from the returns/exchange can be used to create more items of the preferred color"
Data Science in Fashion - KDnuggets,"Each designer targets a different demographic or gender to increase their popularity or sales. Designers need to decide how much items in each collection and the kind of variety they need to create. They have a fixed set of resources like budget and display space and they need data-backed guidelines to decide how much to allot to each category. It is common in many stores like Forever21, H&M, etc. These retailers know to provide more options for a particular category of customers to increase their sales. These insights are derived from historic sales data"
Data Science in Fashion - KDnuggets,"Many styles featured on the runway are not “wearable” in real-life. Trends on the runway are exaggerated and too over-the-top for retail. The outfits need to be altered before they can be curated for sale in stores. Training algorithms to suggest which features to change like color, fabric, cut, length, combination, etc. Moreover each country/region has a different taste. Hence each product must be tweaked to suit the local preferences"
Data Science in Fashion - KDnuggets,"For each garment, the designers need to understand the prices the customer would be willing to pay given the quality, style, popularity and the brand value. Big Data should be used to average previous sales data to generate suggested pricing. Data from competing brands can also be used to set prices which aren’t too high but still contribute to good revenues"
Data Science in Fashion - KDnuggets,A brand needs to find new products that will be successful in the market and which product aren’t very lucratively promising. Designers need to think whether making a unique new product will be accepted or rejects by customers. For example a creative bright print may work for yoga pants but may be deemed too gaudy for sneakers. Big Data can be used to decide which category to venture into and whether to continue selling a particular previous product
Data Science in Fashion - KDnuggets,"Customers exhibit a particular behavior while shopping which can be studied to arrange merchandise in a manner which increase the chances of sales of majority of the pieces. Associative data mining can help us decide where to group products together so customer is likely to pick up most of them. You may have noticed that in many clothing stores, the accessories are placed when we stand near the billing area which causes the customers to pick them up. Wifi data can be used to track the customer movement in the stores to arrange the stock in an optimal manner"
Data Science in Fashion - KDnuggets,"Without a doubt we can say that data powered decisions will give you an edge in the competitive world of fashion. Before creating any product, data needs to be consulted to see it is economically feasible and promising. Selectively using your data to create and convert product lines your customers are sure to buy in the future would help the retails houses survive in the wake of e-commerce. Some may say that AI can dull the creativity of the collections by just creating what the customer want. But that is why the outcomes should be used only to supplement the human creative insight instead of entirely replacing it. But it doesn’t hurt to create the right product at the right price at the right time"
How data science can improve retail - KDnuggets,"The strides in ecommerce represent an entire paradigm shift in retail. Despite only making up around 10% of all retail purchases, ecommerce accounts for more than $2 trillion dollars in sales. Mapping the interactions between the offline and online world seems like an arduous task, but when we focus on each customer and their purchasing paths, it becomes something that can be broken up into a few different paths. We’re going to take a look at a few surprising ways that data science can increase your sales, both offline and online"
How data science can improve retail - KDnuggets,"Do you know who you are selling to? You have quite a few different systems for gathering information about your client. They scattered about and do not take them into consideration. You have loyalty information from in-store purchases because your front line is methodical asking but your online purchase history does not take this information into account. More precisely, groceries and big-box stores optimize separately for online and offline. We lose value of our marketing endeavors if we don’t take a wider look at our data and search for insights"
How data science can improve retail - KDnuggets,"Amazon is a great example of this, with its anticipatory shipping. Products are shipped before customers even order them. Past behavior and predicted purchases are used to plan on decreasing next day shipping to next hour shipping. Is it crazy? Of course it is! Products are being shipped without having anyone to receive them. But that doesn’t matter. Once these products are out of the warehouse and in a given area, they can be marketed to others at a discounted rate or kept at the final hub. This is more of a logistic miracle than an e commerce one, but shows how forward thinking you have to be if you want to lead the future. I would wager that you are not Amazon, but with such a behemoth so far ahead, it makes sense to try your hand at staying competitive in your niche. It is certainly working for Amazon, with over $2 billion in profits last year"
How data science can improve retail - KDnuggets,"But how does this actually work? There is some machine learning that goes into predicting client behavior. Machine learning takes in data to train a model. Training is the process of feeding data into a model for it to apply statistical weights allowing the model to automatically recognise future purchases. For example, John purchases a new book every two or three weeks. Based on this behavior, we know what to expect from him. We do not use all of our data, but divide it into train and test data. If we use too much of our data as test set, then we could end up overfitting; a situation when our model identifies artifacts in the data that do not exist"
How data science can improve retail - KDnuggets,"This a simplified example that isn’t representative of the insights pulled out of millions of clients’ purchase history. These behaviors are then juxtaposed with each other to segment clients into various cohorts that overlap and vary. Machine learning methods can be used for a variety different use cases such as product recommendations, churn predictions, logistics planning and automatic personalized marketing. There are quite a few more use cases that I’ll let your imagination come up with"
How data science can improve retail - KDnuggets,"Let’s get started and build some interesting applications of data science. First off, we need to agglomerate all of the relevant data we have on our clients. Certain regions have legal restrictions on what sensitive information you can store. There is a way around this; I am not advocating for breaking the law but rather, to anonymize your data. Many times this can be done with a common user ID across every system you use. I also want to be clear and point out that there are certain types of data that we simply can not collect. Always ask a lawyer about your options when dealing with sensitive information"
How data science can improve retail - KDnuggets,"We should also consider a few large steps that need executing, namely, creating a data lake and preferably stream processing as well. You may be wondering why we need a data lake as opposed to a data warehouse. There are a few ways in how they differ but the biggest advantages of data lakes over data warehouses are"
How data science can improve retail - KDnuggets,"Flexibility is exactly what we need when doing data science; the entire structure of a data warehouse would need changing whenever we want to try something new. This may seem amusing, but is only the tip of what insights lie below the surface. Stream processing lets us react near instantaneously to changes in our client’s behavior. For example, they walk into our store and purchase an item on our site at a lower rate. We lost money here because our dynamic pricing did not take into account the client’s location. This may not always be possible but should get those gears spinning as to when else we can achieve"
How data science can improve retail - KDnuggets,"Retail and e-commerce both generate a surprisingly large amount of data in recent years. But without the proper tools in place, most of this data illustrates what you already know. I want to point out a few common uses of data science in retail. There are quite a few more that we could discuss, but not every company is ready for them. What I mean is that they are trying to skip steps that need to be done before we start completely automating processes. At Appsilon, we help companies at any level of data science maturity take the next step"
How data science can improve retail - KDnuggets,"Traditionally, retailers would have consultants in store that would use their own judgement to help customers find what they need. There are also endcaps that present the highest bidder to customers without taking into consideration their interests or needs. Online, this would be based on the past revenue and would try and recommend the highest selling products. This didn’t really make any sense, as these products were doing quite well selling themselves and skewed the rest of the catalogue"
How data science can improve retail - KDnuggets,There are a few ways for recommendations to reach your clients. The first most basic example is where we do not know our client at all. They are literally a ghost and we don’t even know where they are browsing from. Without using data science we can recommend a high margin item or the most common purchase our clients make as the first thing our new visitor sees
How data science can improve retail - KDnuggets,"We then take a client who we recognize and can tailor our offering. Based on their activity, in combination with our other client data, we know what we expect our client to purchase. Not only that, but we have information about what items go hand in hand with our client. Let’s say that they have a grill in their shopping cart and are proceeding to checkout. This would be a perfect time to remind our client about the need for a spatula, grill scrubber, lighter fluid or a thermometer. This helps increase sales but also makes sure that our client is not unpleasantly surprised when his item doesn’t include batteries. We can always get data as well"
How data science can improve retail - KDnuggets,"The most interesting and complex way to help our customers is by taking their aesthetics into account. Taking into consideration not only historical purchases and recent activity, but also online behavior and social media gives us a much deeper purview of their interests, preferences and the kinds of designs that could interest. It is at the level of abstraction where we truly appeal to our client individually. Furniture is an segment that aptly highlights where this can provide immense value. We know that our client is looking for a chair. Based on their activity, they are quite interested in tech and sci fi. Therefore, a cushy leather chair should not be our first product recommendation, but rather, an ergonomic chair with sharp lines"
How data science can improve retail - KDnuggets,"Long tails: the bane of inventory and catalogue optimization; what if we got rid of them? There are obviously items we need to keep to satisfy our niche customers, but overall, we should take a deep look at what we keep in stock. By analysing our product demand and the types of products our clients usually by, we can more accurately ascertain the need for certain products to be in stock. There may be items that are very niche, but that pull in a large cart because this item has a lot of associated cross products. The margins on other long tail products may be so high, that even with only a few annual purchases, they are still worth including"
How data science can improve retail - KDnuggets,"In relation to the personalized product recommendations above, this works when creating new products or updating our seasonal line. We take the aggregate aesthetics of our proposed segments and can see the types of designs that could bring in the highest number of new clients. It may turn out that a neon purple carpet with pterodactyls are exactly what expecting parents want in their bedroom. I don’t think this is the case, but that is the issue we are addressing. Our intuition pales in comparison to the types of insights we can pull from our data. If we are diligent in our data science approach and streamline our inventory to a near optimal level, savings start popping up all over the board. Be it from decreased production times, to minimizing the amount of maintenance that is needed. Customer service and sales can also focus on increasing their product expertise so as to better serve customers"
How data science can improve retail - KDnuggets,"The market will pay whatever the market can support. The price of fruit obviously change depending on their country of origin, the season and weather conditions. Those are supply side changes, but what about demand? Various customers are prepared to purchase different items at drastically different prices. A taxi driver needs oil changes or car filters much more than a student who only drives home every other weekend. Scale also plays into pricing. Buying in bulk takes this into consideration but does not include recurring purchases. It makes sense to discount a given item if we expect our client to purchase the same item every month. Also, adaptive pricing that reacts to deals that may be lost could benefit from a discount at the point of purchase. The best pricing decisions also take into account a larger scope of data that includes the weather, location, time and day of purchase and other economic factors"
How data science can improve retail - KDnuggets,"This is especially beneficial for large retailers. Let’s take a look at an example that holds quite a bit of promise for routine advertising. This can apply to digital and offline equally, it is easier for digital. A monthly newsletter with information about new products, discounts, and promotions going on is often sent our entire list of customers without any regard for who they are. But they should be personalized, at least to the cohort level. Here, we can take into advantage of the product recommendations, assortment optimization and dynamic pricing, as they all have an effect on what we include in our communications"
How data science can improve retail - KDnuggets,"A few years ago, there was quite a bit of buzz around Target, in particular, detecting pregnant customers. These customers are proven to be extremely tired and not have any desire to do their groceries in multiple places. As such, it was in Target’s interest to get these customers through the door for formula or diapers because they would then do their entire grocery shopping for the week. The life time value of these clients is very high, as there is low churn among them for a few years following pregnancy. But the hype was over glorified. The story itself revolves around a 16 year old girl, her father, a Target manager and a flier. The father had received fliers and coupons for maternal vitamins, diapers, etc. The manager apologized and proceeded to call the father a few weeks later, but it turned out that the daughter was in fact pregnant. While the exact story"
How data science can improve retail - KDnuggets,It turns out that there are very specific items purchased when an individual is in their second trimester. Over 20 items taken into consideration were enough to give every customer a pregnancy chance score. This example illustrates the kinds of insights that can be discovered but only scratched the surface of what is possible
How data science can improve retail - KDnuggets,"We’ve looked at four examples of how data science and machine learning can be used in online and offline retail. While the number of possibilities is almost unlimited, the most important points I want to reiterate are the benefits of personalization and the true impact of trusting in data. It is wasteful to collect data and not take advantage of it"
How to Survive Your Data Science Interview - KDnuggets,There are many wonderful things about data science. It’s extreme breadth is not one of them. The title of data scientist means something different at every company
How to Survive Your Data Science Interview - KDnuggets,"Interviews are scary as shit. You sit across the table from someone who has the power to grant you income and measure of security. They hold your future in their hands. You have to make them like you, trust you, think you are smart. There are fewer situations in life that induce greater anxiety. Luckily, there are some things you can do to make it a little gentler on you"
How to Survive Your Data Science Interview - KDnuggets,"There are many wonderful things about data science. It’s extreme breadth is not one of them. The title of data scientist means something different at every company. To some it means PhD statistician. To some it means proficiency in Excel. To some it means machine learning generalist. To some it means being handy with Spark and Hadoop. Read job postings carefully for specific skills, tools and languages. A well written posting will give you a lot of insight into what they are looking for"
How to Survive Your Data Science Interview - KDnuggets,"You have a limited amount of time to prepare. This is your budget, and you want to spend it so as to get the biggest bang for your buck. You have a few options:"
How to Survive Your Data Science Interview - KDnuggets,This is only enough time to dust off things you already know. Compare your current level of comfort to the job posting. Are there some skills you haven’t used in five years? Some terms you don’t recognize? Where are your biggest gaps? Spend more of your time on these
How to Survive Your Data Science Interview - KDnuggets,"You will get the biggest benefit if you do all of your practicing OUT LOUD. Explain your answers to your cat, or to an empty chair. Use a pen and paper, or better yet, a whiteboard. These help recreate common interviewing environments in small ways so that when you get there it feels a bit more familiar"
How to Survive Your Data Science Interview - KDnuggets,Don’t panic if you encounter topics and tools you’ve never heard of before. Many job postings are written as wishlists. They sound like a fifteen year old describing their perfect mate—a billionaire celebrity-lookalike winner of the Nobel Prize in physics
How to Survive Your Data Science Interview - KDnuggets,"Those are all fine attributes in a partner, but most of us would be pretty excited about finding one or two of them. No one has them all. In my experience, the candidates hired are strong in some of the points listed in the post, but not necessarily all of them"
How to Survive Your Data Science Interview - KDnuggets,"Visit the their web page. Get a sense of how they make their money and who their customers are. Read their engineering blog to learn what tools they use and how their infrastructure is built. Learn the name of the CEO. If you are lucky enough to get your interviewers’ names, Google their professional activities. Learn about their research interests. Get a feeling for what matters to them"
How to Survive Your Data Science Interview - KDnuggets,"If Pocket Plan preparation is a single pancake, Standard Plan is the tall stack - the same process, repeated, each pass going a little deeper than the last. You work more of the same example problems and read through more of the same references. Where the Pocket Plan only gives you enough to time to dust off your skills, the Standard lets you put a sharp edge on them. You can learn about things that you’ve heard of but never absorbed"
How to Survive Your Data Science Interview - KDnuggets,"After your first pass through the Pocket Plan, take a step back and look at where you feel the least prepared. For instance, if the job posting says “SQL required” and the recruiter told you that you will be asked a couple of SQL questions, but you don’t know any SQL, that’s a gap. Make a wishlist of things you would like to dig into deeper and start with the biggest gap. Then go back to the Pocket Plan and allocate your time to practice problems and reference reading accordingly"
How to Survive Your Data Science Interview - KDnuggets,"Unfortunately, the best way to learn what a company wants in a data scientist is to interview with them. It’s not uncommon to interview with the same employer several times over the course of a few years. This is admittedly an expensive data gathering process, but it pays handsome rewards to the patient candidate"
How to Survive Your Data Science Interview - KDnuggets,"I wish I could guarantee that if you prepare thoroughly enough, you’ll nail your interview and get an offer. Unfortunately the truth is bleaker than that. But don’t let it get you down! Having your eyes wide open will help you survive your data science interview with style and grace"
How to Survive Your Data Science Interview - KDnuggets,"If every interview is practice for the next one, you never have to deal with the additional pressure of “I must land this,” and it’s no big deal if you don’t get an offer. When you approach it this way, a strange thing can happen. Unexpectedly one of your practice interviews goes so well that you find yourself holding an offer to work with a bunch of great people on a fun problem"
How to Survive Your Data Science Interview - KDnuggets,"It’s because they’re humans. There are common biases like gender, race, age and sexual orientation. There are also idiosyncratic biases such as voice timbre, alma mater and which text editor you use. When they are conscious and intentional, biases are a moral failure, but unconscious biases are unavoidable. We are hard wired for them"
How to Survive Your Data Science Interview - KDnuggets,"Just notice them. They are not telling you the truth or helping you, but, for better or worse, they are part of you. Let them clamor in the background and go about your business"
How to Survive Your Data Science Interview - KDnuggets,"Any group of people, whether it is a football team, a company, a rock band or a family, is quirky (not to say dysfunctional) in its own way. The trick is to find a company whose quirks are compatible with our own. Unfortunately, quirks are by definition unusual, and finding a complementary set is tough. It’s like finding a climbing partner or a good hat. You have try on a few to find one that works"
How to Survive Your Data Science Interview - KDnuggets,"Listen to how you feel. If you look around and find it easy to picture yourself in this place, working with these people, that’s a good sign. If you find yourself thinking “I could make this work”, proceed with caution. If you can’t wait to get out, that’s a strong no"
How to Survive Your Data Science Interview - KDnuggets,"After your interview, you should feel like you just completed a workout where a personal trainer pushed you to your limits. You should not feel like you were made to dance and grovel for someone else’s pleasure. If this was the case, that is a clear signal. You do not want to be connected to a culture that permits small abuses like this. They fester and foster large abuses as well. Luckily, this isn’t common, but it is worth keeping an eye out for"
How to Survive Your Data Science Interview - KDnuggets,"On the bright side, by the time you get to your interview, most of your work is done. All you need to do is take a shower and show up on time. Get some sleep (if you can), eat a good breakfast (if you can keep it down) and you are good to go"
How to Survive Your Data Science Interview - KDnuggets,"Forget about checklists of things to say. Don't bother with tricks like mirroring and power poses. Don't pretend to be hyper enthusiastic. Just be you. If you are thirsty, ask for a drink. If you are curious about something, ask about it. If something is funny to you, laugh. If you are confused, say so. You will be asked questions you don't know how to answer. That's on purpose. Feel free to admit it and to ask for clarification. Once you let go of trying to remember all the things you are supposed to be, you'll have more mental energy for working out code on the whiteboard"
How to Survive Your Data Science Interview - KDnuggets,"It's in your interest to give your potential employer an accurate impression of who you are. Truth in advertising means you don't have to keep up false pretenses. If you are a passionate Yankees fan, it's OK to mention it. If you have an abiding fondness for image processing, share that. If you are an unrepentant asshole, let your interviewer see it. You will want to be with people who know how to deal with that. If you would rather have your fingernails pulled off than pair program, tell your interviewer. If you've only written two lines of C++ code in your life, it's OK to say so. Avoid trash talking anyone, including your past self, but be straightforward. Pretending to be anything other than you are doesn't help you and it doesn't help your interviewer"
How to Survive Your Data Science Interview - KDnuggets,"It's hard to overstate to power of being genuinely you. It is magnetic. It gives you an air of strength and makes you interesting. It's kind of scary, so few of us do it. As a result, it tends to make you stand out in the minds of your interviewers. They may or may not make you an offer, but whatever the case, it will be the best outcome for you"
How to Survive Your Data Science Interview - KDnuggets,This is not a guide to the data science interview process at my current or any former employer. It is built from a collection of my experiences interviewing across the industry. It's just my own opinions. Don't blame anyone else for them
Data Science for Javascript Developers  - KDnuggets,"I love Javascript. I think it is a great programming language. It is a versatile, constantly evolving, ever growing language. It is practically the only language that runs both on the client and server. NPM, node’s packaging system, is great. For many, Javascript is their first (and sometimes only) programming language. But when it comes to data science, it is all about Python (or R). While Python is a great programming language, I don’t see any reason Python should be more suitable for data science than Javascript. They are both interpreted, non-typed programming languages. They both can wrap around C libraries for computationally intensive code. Yet, today, data science is done in Python"
Data Science for Javascript Developers  - KDnuggets,The goal of this tutorial is to open the door to data science programming using Javascript. This tutorial is intended for Javascript programmers without any data science experience. My next post is about Text Analysis. You can read it
Data Science for Javascript Developers  - KDnuggets,"I like to think of it as a really sophisticated console combined with REPL programming. A notebook is a sequence of cells. Each cell may contain code, that can be executed and its output displayed right after the code. The context is kept from cell to cell i. The magic is in that while consoles can only display character based output, notebooks can display visuals, such as HTML, SVG and images. This is extremely important with data science where visualization is key to understanding the data. Notebooks, when used in data science, typically display tables, graphs and other data visualizations. It cannot be done using plain old command line"
Data Science for Javascript Developers  - KDnuggets,"It contains great notebooks that teach data science and hosts public datasets that can be used to demonstrate data science concepts. Unfortunately, Kaggle does not support Javascript notebooks (yet). This tutorial is loosely based on the Python notebook"
Data Science for Javascript Developers  - KDnuggets,"First, we will need to install the Jupyter Notebook. This is a Python based web server used to manage, create and use Jupyter notebooks. The notebook’s interface is web based. You can find installation instructions over"
Data Science for Javascript Developers  - KDnuggets,"You will see the Jupyter logo and the ‘Files’ tab open with a file system view. On the right of the screen you will see a “new” button. Clicking on it will show a menu of possible notebook formats. You will probably see in the list “Python” but not “Javascript”. For that, you will need to install the IJavascript kernel"
Data Science for Javascript Developers  - KDnuggets,"After installing the IJavascript kernel, clicking the “New” button in the Jupyter server home page, will show the option for creating a “Javascript (Node. Choose that option and create the Javascript notebook. It will open an empty notebook with a single empty code cell"
Data Science for Javascript Developers  - KDnuggets,"Editing a notebook is pretty straight forward. You can use buttons, menus or shortcuts. Executing the selected cell is done using Ctrl+Enter. If for some reason the kernel is stuck or you just need to restart the kernel, use the Kernel menu item for restart"
Data Science for Javascript Developers  - KDnuggets,Data Science doesn’t mean much without the data. In this tutorial we are using the House Prices dataset hosted by Kaggle. Download the train
Data Science for Javascript Developers  - KDnuggets,"Each row is a data point representing a house sale. There are quite a few fields (features) for each data point. The most important field is the last one, SalePrice. A description of the data fields can be found in the “data description. Some fields are quantitative, i. Others represents categories like PavedDrive which can be one of Y (yes), P (partial) or N (no). A description of the possible categories for each field can be found in the “description"
Data Science for Javascript Developers  - KDnuggets,"For instance, if the random variable X is used to denote the outcome of a coin toss (“the experiment”), then the probability distribution of X would take the value 0.5 for X = heads, and 0.5 for X = tails (assuming the coin is fair)"
Data Science for Javascript Developers  - KDnuggets,"We can see the mean value is about 180,921 with standard deviation of 79,415. The quartiles gives us a better understanding of the prices the bulk of the houses were sold for. 50% of the houses were sold for prices between 129,900 and 214,000 (between the 25% and 75% quartiles)"
Data Science for Javascript Developers  - KDnuggets,"The describe function gave us a general idea of the distribution, but we would really like to visualize the distribution. Seeing is believing is understanding. We use"
Data Science for Javascript Developers  - KDnuggets,The histogram sorts all house prices and puts them in bins. Each bin represents a range of prices. The Y axis shows how many data points are in each bin
Data Science for Javascript Developers  - KDnuggets,"From viewing the histogram, we can see that the sale price distribution is centered around 126K. We can also see that the distribution is skewed, or asymmetrical. Here’s what Wikipedia has to say about"
Data Science for Javascript Developers  - KDnuggets,"It is possible, of course, to show histograms of categorical fields such as PavedDrive. The histogram will show the number of data points with each PavedDrive category. Not surprisingly, almost all sales were of houses with paved drives"
Data Science for Javascript Developers  - KDnuggets,"In statistics, dependence or association is any statistical relationship, whether causal or not, between two random variables or bivariate data. Correlation is any of a broad class of statistical relationships involving dependence, though in common usage it most often refers to how close two variables are to having a linear relationship with each other. Familiar examples of dependent phenomena include the correlation between the physical statures of parents and their offspring, and the correlation between the demand for a product and its price"
Data Science for Javascript Developers  - KDnuggets,"Correlation can be in the range of -1 and 1. A value of 1 means the features are positively correlated i. Zero correlation means the featurs are not related. Negative correlation means when the value of one feature rises, the value of the other feature falls. Positive correlations are in red. Negative correlations are in blue. The correlation between two identical features is always 1. You can see that in the map with the diagonal red line"
Data Science for Javascript Developers  - KDnuggets,"From the correlation map we can see that “Total basement area” and “first floor area” are positively correlated. This is because the basement is usually on first floor. The features “total room above ground”, “living area” and “overall quality” are positively correlated with “sale price”. The largest the living area, the more expensive the house is. The feature “year built” is negatively correlated with the feature “enclosed porch area”. This means, newer houses have smaller porches"
Data Science for Javascript Developers  - KDnuggets,"The X axis represents SalePrice. The Y axis, above ground living area. The correlation is now clear. You can image a line representing the relationship between sale price and area. You can also see the specific points that are distant from the line, like the two points between 100k and 200k at the top. These data points do not follow the general rule"
Data Science for Javascript Developers  - KDnuggets,"In this example, each box represents all data points with a particular quality score, between 1 and 10. The line in the center of the box is the median. The bottom box border is the 25% percentile and the top box border is the 75% percentile. This gives us a good idea of where 50% of the data points are (inside the box). The whiskers represent the maximum and minimum values, excluding outliers. Outliers are data point that are significantly distant from the rest of the data (more than 1.5 X interquartile range above or underneath the box). They are rendered separately. You can see, data points with overall quality of 8 have 4 outliers, 3 above the top whisker and 1 underneath the bottom whisker. Read"
Data Science for Javascript Developers  - KDnuggets,"Notice that connecting the boxes creates an imaginary line, but the line is not straight. It is more like a log line. This indicates that the relationship between the two features is not linear but more like log. We can plot the log of the sale price instead of the actual sale price replacing the string"
Data Science for Javascript Developers  - KDnuggets,In this tutorial we merely scratched the ground. The important lesson is that data science is possible using Javascript. Visualization can be done using front-end libraries such as plotly and Highchart. There are many packages for statistical analysis and machine learning on NPM. My next post discusses
The Great Big Data Science Glossary - KDnuggets,"The back end is all of the code and technology that works behind the scenes to populate the front end with useful information. This includes databases, servers, authentication procedures, and much more. You can think of the back end as the frame, the plumbing, and the wiring of an apartment"
The Great Big Data Science Glossary - KDnuggets,Classification is a supervised machine learning problem. It deals with categorizing a data point based on its similarity to other data points. You take a set of data where every item already has a category and look at common traits between each item. You then use those common traits as a guide for what category the new item might have
The Great Big Data Science Glossary - KDnuggets,"As simply as possible, this is a storage space for data. We mostly use databases with a Database Management System (DBMS), like PostgreSQL or MySQL. These are computer applications that allow us to interact with a database to collect and analyze the information inside"
The Great Big Data Science Glossary - KDnuggets,A data warehouse is a system used to do quick analysis of business trends using data from many sources. They're designed to make it easy for people to answer important statistical questions without a Ph.D
The Great Big Data Science Glossary - KDnuggets,"Algorithms that use fuzzy logic to decrease the runtime of a script. Fuzzy algorithms tend to be less precise than those that use Boolean logic. They also tend to be faster, and computational speed sometimes outweighs the loss in precision"
The Great Big Data Science Glossary - KDnuggets,"A greedy algorithm will break a problem down into a series of steps. It will then look for the best possible solution at each step, aiming to find the best overall solution available. A good example is"
The Great Big Data Science Glossary - KDnuggets,Overfitting happens when a model considers too much information. It’s like asking a person to read a sentence while looking at a page through a microscope. The patterns that enable understanding get lost in the noise
The Great Big Data Science Glossary - KDnuggets,"Regression is another supervised machine learning problem. It focuses on how a target value changes as other values within a data set change. Regression problems generally deal with continuous variables, like how square footage and location affect the price of a house"
The Great Big Data Science Glossary - KDnuggets,Statistics (plural) is the entire set of tools and methods used to analyze a set of data. A statistic (singular) is a value that we calculate or infer from data. We get the median (a statistic) of a set of numbers by using techniques from the field of statistics
The Great Big Data Science Glossary - KDnuggets,"This is part of the machine learning workflow. When making a predictive model, you first offer it a set of training data so it can build understanding. Then you pass the model a test set, where it applies its understanding and tries to predict a target value"
The Great Big Data Science Glossary - KDnuggets,"Underfitting happens when you don’t offer a model enough information. An example of underfitting would be asking someone to graph the change in temperature over a day and only giving them the high and low. Instead of the smooth curve one might expect, you only have enough information to draw a straight line"
The Great Big Data Science Glossary - KDnuggets,"A discipline involving research and development of machines that are aware of their surroundings. Most work in A.I. In case you didn’t know, A.I"
The Great Big Data Science Glossary - KDnuggets,"Similar to data analysis, but more narrowly focused on business metrics. The technical side of BI involves learning how to effectively use software to generate reports and find important trends. It’s descriptive, rather than predictive"
The Great Big Data Science Glossary - KDnuggets,This discipline is the little brother of data science. Data analysis is focused more on answering questions about the present and the past. It uses less complex statistics and generally tries to identify patterns that can improve an organization
The Great Big Data Science Glossary - KDnuggets,"Data engineering is all about the back end. These are the people that build systems to make it easy for data scientists to do their analysis. In smaller teams, a data scientist may also be a data engineer. In larger groups, engineers are able to focus solely on speeding up analysis and keeping a data well organized and easy to access"
The Great Big Data Science Glossary - KDnuggets,This discipline is all about telling interesting and important stories with a data focused approach. It has come about naturally with more information becoming available as data. A story may be about the data or informed by data. There’s a full
The Great Big Data Science Glossary - KDnuggets,"Given the rapid expansion of the field, the definition of data science can be hard to nail down. Basically, it’s the discipline of using data and advanced statistics to make predictions. Data science is also focused on creating understanding among messy and disparate data. The “what” a scientist is tackling will differ greatly by employer"
8 Common Pitfalls That Can Ruin Your Prediction - KDnuggets,"It may seem obvious, but if you have a dataset with many columns, you can be easily trapped. For example, do not predict a web-shop visitor’s chance of purchase based on the time spent on your web-shop that day. That “Total Minutes in Web Shop” column may also contain those minutes that are spent in your web-shop after purchasing"
8 Common Pitfalls That Can Ruin Your Prediction - KDnuggets,"He took his degree in maths and programming. Norbert is passionate about Data Analytics, Predictive Analytics, and Data Science. He can be reached at norbert"
Want a Job in Data? Learn This - KDnuggets,"I first heard about SQL in 1997. I was in high school, and as part of a computing class we were working with databases in Microsoft Access. The computers we used were outdated, and the class was boring. Even then, it seemed that SQL was ancient"
Want a Job in Data? Learn This - KDnuggets,"Almost all of the biggest names in tech use SQL. Uber, Netflix, Airbnb — the list goes on. Even within companies like Facebook, Google, and Amazon who have built their own high-performance database systems, data teams use SQL to query data and perform analysis"
Want a Job in Data? Learn This - KDnuggets,"He quickly found himself using SQL daily: ""SQL is so pervasive, it permeates everything here. It’s like the SQL syntax persists through time and space. Everything uses SQL or a derivative of SQL"
Want a Job in Data? Learn This - KDnuggets,"SQL was easily the most mentioned skill, being mentioned in 35.7% of ads– 1.39 times as many ads as Python, and over twice the number of ads as R. Of greater interest is what skills are required for people who want to get their first job in data. Most entry-level jobs in data are Data Analyst roles. If we look at jobs ads with 'data analyst' in the title, the numbers are even more conclusive:"
Want a Job in Data? Learn This - KDnuggets,The queries above demonstrate the complexity of the SQL taught at the end of SQL courses by three of the more popular online learning sites. The problem is that real-world SQL doesn't look like that. Real-world SQL looks like this:
Want a Job in Data? Learn This - KDnuggets,"Over the last few months, we've undertaken to rewrite and extend our SQL curriculum to equip aspiring data analysts, data scientists, and data engineers for their new careers. We've already released four new SQL courses with more on the way. In our Data Analyst and Data Scientist paths:"
Histogram 202: Tips and Tricks for Better Data Science - KDnuggets,"Compared to other summarizing methods, histograms have the richest descriptive power while being the fastest way to interpret data – the human brain prefers visual perception. However, if you are not careful, viewers will not be able to understand your histogram, or you may fail to get the most out of it. It is especially important to specify the optimal bin size"
Histogram 202: Tips and Tricks for Better Data Science - KDnuggets,"It is not so easy to decide. Now comes the trouble. If you look at the 10-15-20-25… binned histogram, are the occurrences of value “20” represented in the second column, the third column, or both? Obviously, you need to put each specific value into an exact bin"
Histogram 202: Tips and Tricks for Better Data Science - KDnuggets,"You are free to choose any of these options, but be careful! With both of these options, one value will not be included in the histogram. If you choose option #1, then value “10” will not be included in any of the bins. If you choose option #2, then value “25” will not be included in any of the bins"
Histogram 202: Tips and Tricks for Better Data Science - KDnuggets,"He took his degree in maths and programming. Norbert is passionate about Data Analytics, Predictive Analytics, and Data Science. He can be reached at norbert"
Data Science at the Command Line: Exploring Data - KDnuggets,"One of the less considered modes of attack is a strictly command line approach. Sure, you use the command line to execute your Python scripts, or run your C program, or invoke your R environment. But what about running the entire show from the terminal?"
Data Science at the Command Line: Exploring Data - KDnuggets,"Exploring your data can be done from three perspectives. The first perspective is to inspect the data and its properties. Here, we want to know, for example, what the raw data looks like, how many data points the data set has, and what kind of features the data set has"
Data Science at the Command Line: Exploring Data - KDnuggets,The second perspective from which we can explore out data is to compute descriptive statistics. This perspective is useful for learning more about the individual features. One advantage of this perspective is that the output is often brief and textual and can therefore be printed on the command line
Data Science at the Command Line: Exploring Data - KDnuggets,"The third perspective is to create visualizations of the data. From this perspective we can gain insight into how multiple features interact. We’ll discuss a way of creating visualizations that can be printed on the command line. However, visualizations are best suited to be displayed on a graphical user interfaces. An advantage of visualizations over descriptive statistics is that they are more flexible and that they can convey much more information"
Data Science at the Command Line: Exploring Data - KDnuggets,"Depending on how comfortable or reliant you are on the command line (or are willing to become), you can delve into more advanced concepts such as creating bash scripts with numerous commands working in sequence. This, however, moves from ""command line"" data science into the more familiar realm of scripting, but with bash instead of, for example, Python. You could also do something in between, such as creating a library of bash snippets as functions and adding them to your"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"We will not go into an in-depth comparison of Scala vs. Python here, but it’s important to note, that, unlike Python, Scala is a compiled language. Hence the code written in it gets executed much faster (comparing to pure Python, and not specialized libraries like NumPy)"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"In contrast to Java, writing in Scala is much more enjoyable, since the same logic can usually be expressed with the significantly smaller number of lines. Scala’s functionality is in no way inferior to that of Java and even has some properties that are more advanced. Java-old-timers could provide a lot of counter-arguments here, but there is no doubt that Scala is much better suited for data science tasks"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"For your convenience, we have prepared a comprehensive overview of the most important libraries used to perform machine learning and Data Science tasks in Scala. We will use analogies with the corresponding Python tools for better understanding of some important aspects. In fact, there is just one top-level comprehensive tool that forms the basis for the development of data science and big data solutions in Scala, known as Apache Spark, that is supplemented by a wide range of libraries and instruments written in both Scala and Java. Let's take a closer look at it"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"Breeze is known as the primary scientific computing library for Scala. It scoops up ideas from MATLAB's data structures and the NumPy classes for Python. Breeze provides fast and efficient  manipulations with data arrays, and enables the implementation of many other operations, including the following:"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"Another data manipulation toolkit for Scala is Saddle. It is a Scala analog of R and Python's pandas library. Like the dataframes in pandas or R, Saddle is based on the Frame structure (2D indexed matrix)"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"The Vec and Mat classes are at the base of Series and Frame. You can implement different manipulations on these data structures, and use them for basic data analysis. Another great thing about Saddle is its robustness to missing values"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"The main difference from the previous computation libraries is that ScalaLab uses its own domain-specific language called ScalaSci. Conveniently, Scalalab gets access to the variety of scientific Java and Scala libraries, so you can easily import your data and then use different methods to make manipulations and computations. Most of the techniques are similar to Breeze and Saddle. In addition, as in Breeze, there are plotting opportunities which allow further interpretation of the resulting data"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"Scala has some great natural language processing libraries as a part of ScalaNLP, including Epic and Puck. These libraries are mostly used as text parsers, with Puck being more convenient if you need to parse thousands of sentences due to its high-speed and GPU usage. Also, Epic is known as a prediction framework which employs structured prediction for building complex systems"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"As the name suggests, Breeze-viz is the plotting library developed by Breeze for Scala. It is based on the prominent Java charting library JFreeChart and has a MATLAB-like syntax. Although Breeze-viz has much fewer opportunities than MATLAB, matplotlib in Python, or R, it is still very helpful in the process of developing and establishing new models"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"Another Scala lib for data visualization is Vegas. It is much more functional than Breeze-viz and allows to make some plotting specifications such as filtering, transformations, and aggregations. It is similar in structure to Python’s Bokeh and Plotly"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"Statistical Machine Intelligence and Learning Engine, or shortly Smile, is a promising modern machine learning system in some ways similar to Python’s scikit-learn. It is developed in Java and offers an API for Scala too. The library will amaze you with fast and extensive applications, efficient memory usage and a large set of machine learning algorithms for Classification, Regression, Nearest Neighbor Search, Feature Selection, etc"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"Built on top of Spark, MLlib library provides a vast variety of machine learning algorithms. Being written in Scala, it also provides highly functional API for Java, Python, and R, but opportunities for Scala are more flexible. The library consists of two separate packages: MLlib and ML. Let’s look at them in more detail one by one"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"DeepLearning. It utilizes mathematical formulas to create complex dynamic neural networks through a combination of object-oriented and functional programming. The library uses a wide range of types, as well as applicative type classes. The latter allows commencing multiple calculations simultaneously, which we consider crucial to have in a data scientist’s disposal. It’s worth mentioning that the library’s neural networks are programs and support all of Scala features"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"Of course, we can not ignore a machine learning server for constructing and deploying predictive engines called PredictionIO. It is built on Apache Spark, MLlib, and HBase and was even ranked on Github as the most popular Apache Spark-based machine learning product. It enables you to easily and efficiently build, evaluate and deploy engines, implement your own machine learning models, and incorporate them into your engine"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"Developed by the Scala’s creator company, Akka is a concurrent framework for building distributed applications on a JVM. It uses an actor-based model, where an actor represents an object that receives messages and takes appropriate actions. Akka replaces the functionality of the Actor class that was available in the previous Scala versions"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"Last but not least on our list is Slick, which stands for Scala Language-Integrated Connection Kit. It is a library for creating and executing database queries that offer a variety of supported databases such as H2, MySQL, PostgreSQL, etc. Some databases are available via slick-extensions"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"To build queries, Slick provides a powerful DSL, which makes the code look as if you were using the Scala collections. Slick supports both simple SQL queries, and strongly-typed joins of several tables. Moreover, simple subqueries can be used to construct more complex ones"
Top 15 Scala Libraries for Data Science in 2018 - KDnuggets,"In this article, we have outlined some of the Scala libraries that can be very useful while performing major data scientific tasks. They have proved to be highly helpful and effective for achieving the best results. You can also view the activity statistics taken from GitHub on every provided library below"
Why Data Scientists Must Know About Change Management - KDnuggets,This week I was invited to give a guest lecture on data science for a group of change managers. We discussed the social effects of predictive maintenance on the workforce and how to deal with the implementation of this concept from a change management perspective. At the end of the lecture I came to realize that it's the data scientist who should know about change management rather than the other way around. This might make the implementation of the improvements found by data scientists much more efficient since this tackles behavioral change: One of the biggest hurdles in implementation of our ideas and realizing our goals
Why Data Scientists Must Know About Change Management - KDnuggets,"Change management may be seen as a domain opposed to data science. Data science is hard, change is soft. Change management is not about the solution of messy problems but about the process of change, data science is all about the solution after getting our the messy data. One of the core ideas of change management is that the change comes as part of a process and not at a sudden, let alone predicted, time. Change comes from within peoples' acceptance to change"
Why Data Scientists Must Know About Change Management - KDnuggets,Let me illustrate my ideas by elaborating on a specific case. Let us imagine a factory producing a not-too-complex product. Maintenance and overhaul is performed by the technical department. Every day coworkers on production line and employees from maintenance department work with the machinery and as such they
Why Data Scientists Must Know About Change Management - KDnuggets,"Take James, a 45 year old maintenance engineer who has worked at this company for more than 25 years. He feels whether maintenance of a machine is needed simply by twisting bolts and arms. Of course machinery sometimes fails, but hey - it happens"
Why Data Scientists Must Know About Change Management - KDnuggets,Now predictive maintenance comes in. Data scientists take their seats and start working on prediction of upcoming maintenance before anyone could sense that the machinery would fail. Note that the intention to start doing data science within the organization
Why Data Scientists Must Know About Change Management - KDnuggets,"Now this is where change management comes in. What will change after implementation? Behavior may need to change, but there is a bigger issue: Predictive maintenance is a change of reality. In the old reality machinery was put to maintenance at the point that something was bound to fail, or indeed in case of preventive maintenance, machinery was put to maintenance periodically. In the new reality, machinery is potentially put up for servicing by an algorithm even before human sense skills could do so. In the new reality things are fixed that aren't broken yet"
Why Data Scientists Must Know About Change Management - KDnuggets,Successful implementation of predictive maintenance thus is the result of a change in behavior due to the reaction on a new reality. The challenge is to not only focus on the core data science project but get the new reality accepted. This requires a range of interventions parallel or even before data science kicks in
Why Data Scientists Must Know About Change Management - KDnuggets,"So what are best practices of change management and how should they be used? The first thing to acknowledge is that the change starts even before the data scientists start their analysis. Any project that contains smart predictive algorithms, intervenes with smart and skilled employees. Therefore, engage with these employees before showing any results. Project managers may assume that people want to contribute to something bigger than themselves. In case of predictive maintenance, striving for zero production failures due may inspire more than it repels"
Why Data Scientists Must Know About Change Management - KDnuggets,"Next thing is to realize that domain knowledge needs to be part of a data science project. Therefore, engage employees from the work floor in the project, let them share what they know about the machinery or any subject under analysis. In doing so, your employees' skills get acknowledged. They know that you know that they are skilled. The importance of being heard cannot be underestimated"
Why Data Scientists Must Know About Change Management - KDnuggets,"Also, during the project at and the end of the project test with your skilled employees. Challenge your employees to get acquainted with the prediction algorithms. Moreover, let them compete men versus machine: Who knows which machines needs service the most efficient"
Deep Feature Synthesis: How Automated Feature Engineering Works - KDnuggets,"The single biggest technical hurdle that machine learning algorithms must overcome is their need for processed data in order to work — they can only make predictions from numeric data. This data is composed of relevant variables, known as “features. The process for extracting these numeric features is called “feature engineering"
Deep Feature Synthesis: How Automated Feature Engineering Works - KDnuggets,"To understand this, let’s consider a dataset of customers and all of their purchases. For each customer, we may want to calculate a feature representing their most expensive purchase. To do this, we would collect all the transactions related to a customer and find the Maximum of the purchase amount field. However, imagine a user wanted to extract “the longest flight delay” from a dataset of airplane flights to predict future delays. She would would use the same Maximum function"
Deep Feature Synthesis: How Automated Feature Engineering Works - KDnuggets,"Even though the natural language descriptions differ completely, the underlying math remains the same. In both of these cases, we applied the same operation to a list of numeric values to produce a new numeric feature that was specific to the dataset. These dataset-agnostic operations are called “primitives"
Deep Feature Synthesis: How Automated Feature Engineering Works - KDnuggets,"This means that a community of people can join together to contribute primitives from which everyone can benefit. Since primitives are defined independently of a specific dataset, any new primitive added to Featuretools can be incorporated into any other dataset that contains the same variable data types. In some cases, this might be a dataset in the same domain, but it could also be for a completely different use case. As an example, here is"
Deep Feature Synthesis: How Automated Feature Engineering Works - KDnuggets,"It’s easy to accidentally leak information about what you’re trying to predict into a model. One of our previous retail enterprise customer’s applications is a great example: production models didn’t match the company’s development results. They were trying to predict who would become a customer, and the most important feature in their model was the number of emails that their prospects had opened. It showed high accuracy during training but unfortunately didn’t work when it was deployed into production"
Deep Feature Synthesis: How Automated Feature Engineering Works - KDnuggets,"For many problems, a baseline score is enough for a human to decide if an approach is valid. In one case, we ran an experiment against 1,257 human competitors on Kaggle. We produced feature matrices using DFS and then utilized a regressor in order to create a machine learning model"
Deep Feature Synthesis: How Automated Feature Engineering Works - KDnuggets,"We found that with almost no human input, DFS outperforms both baseline models in this prediction problem. In a real-world setting, this is valuable supporting evidence for leveraging machine learning in this use case. Next, we showed how adding custom primitives can be used to outperform more than 80% of our competitors and get close to the best overall score"
Deep Feature Synthesis: How Automated Feature Engineering Works - KDnuggets,"The features that DFS generates are more explainable to humans because they are based on combinations of primitives that are easily described in natural language. The transformations in deep learning must be possible through matrix multiplication, while the primitives in DFS can be mapped to any function that a domain expert can describe. This increases the accessibility of the technology and offers more opportunities for those who are not experienced machine learning professionals to contribute their own expertise"
Deep Feature Synthesis: How Automated Feature Engineering Works - KDnuggets,"Additionally, while deep learning often requires many training examples to train the complex architectures it needs to work, DFS can start creating potential features based only on the schema of a dataset. For many enterprise use cases, enough training examples for deep learning are not available. DFS offers a way to begin creating interpretable features for smaller datasets that humans can manually validate"
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"These are tech education institutions where the students are not in charge of the cost of their tuition but the corporates who need the talent they breed are. This changes the game in terms of accessibility tremendously. Ecole42 and we think code_ in particular have no pre-requisites in terms of qualifications whatsoever. That’s right. Someone can have dropped out of school at 15, and still get in, as long as they pass the online problem-solving skill test. It is not a test of knowledge, but a test of how people approach problems. Something that most schools are not teaching"
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"The problem comes when there is a breach. While those large companies may own the data, they don’t bear the risk that comes with a breach. That risk — of identity theft, of credit card abuse, and more — is still"
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"The first image is a screen grab of a google image search for “face front” with the additional filters of colour images and creative commons licensing allowing me to show the images elsewhere. Looking for a face, I got one statue, one drawing, one building, TWO animals, but only one black person. This essentially means that when searching for an image of a face, a black face appears no more frequently than errors like animals and buildings! This is not just a poor representation of the world’s faces, but an utterly biased signal-to-noise ratio"
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"The answer is of course no. Therefore, this is clearly not working. And if we are going to trust our AI, we both need to improve it and to bear in mind the edge cases that the AI is going to miss. We have to stay smarter than AI ("
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"Again, this is not an exhaustive list. I have just identified four interesting trends that I’m eager to see develop. There is a bias in this list, as this is mainly technology that is visible to the consumer, if not direct consumer technology. There are fascinating tech developments in industrial and research areas that are not directly visible to us, and that’s an entire other world to explore"
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"We will also see robots entering our physical space more, I think. Beyond robots at the end of phone numbers, or regulating our social media feeds, we will start seeing them. Any autonomous vehicle is in fact a robot, whether a"
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"Artificial Inteligence is unavoidable. It will penerate most of what we do. There are some very intersting creative applications of AI, such as optimising chances of"
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"The first point is why cryptocurrencies are so attractive in countries where the central economic authority is in difficulty or untrusted. It can have applications in contracts, supply chain, etc. The second point is unprecedented. The ability to tell an original from a copy has not before been possible with digital artefacts. This can guarantee authenticity of pieces of art, limit copying of copyrighted material, etc"
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"There are two examples of win-win situations that have nothing charitable about them in this article. The first one is the business model of we think code_ (and Ecole 42). By funding the school, the companies that hire their graduates can be seen as selfishly looking after their interests, i. In South Africa, someone with a job often supports an entire family, often an extended family too, especially if they entered the coding school without major qualifications, and this contributes to alleviating the dreadful effects of poverty. So beyond the win-win of the model itself, there’s an additional positive side-effect"
Four Broken Systems & Four Tech Trends for 2018 - KDnuggets,"The second example is that of AI applied to relocate refugees to increase their chances of finding employment. While this example is not from the private sector, the reward/currency of academia is scientific publications and this clearly led to at least one. But it also applied cutting-edge technology for the benefit of people whose interests often come last. Moreover, the outcome, higher chance of employment, directly results in better integration, and seeing that integration goes both ways, leads to more tolerance too. So beyond the win-win of this project, there is an additional positive side-effect"
5 Key Data Science Job Market Trends - KDnuggets,"You’d be forgiven for thinking automation would make data scientists obsolete. A lot of negative talk surrounds automation, particularly about uprooting jobs and careers. That’s"
5 Key Data Science Job Market Trends - KDnuggets,"Data empowerment is another important movement to keep your eye on. In some ways, “empowerment” sounds bold — even a little ominous. It’s just a buzzword though, used to explain a"
5 Key Data Science Job Market Trends - KDnuggets,"This process results in what many like to call a data swamp or even a dump. It’s a mass void of raw data, information and potential insights. The problem is, it needs to be cleaned up, skimmed and organized"
Learn why the future of data science gathers here + save 50% - KDnuggets,"Open Data Science Conference just released 80% of their schedule and the first round of speakers for ODSC East 2018 in Boston, May 1-4. Learn, train and engage with 200+ world class experts. Save 50% with code KDNUGGETS"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 2 - KDnuggets,In business discover phase we talked on how the business problem i. In the data discovery phase we discussed data sufficiency and other considerations like variety and velocity of data and how these considerations affect the data science problem formulation. In the last phase we touched upon how the data points and its various constituents drive the predictive problem formulation. In this post we will discuss further on how exploratory analysis can be used for getting insights for feature engineering
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 2 - KDnuggets,"This phase entails digging deep to get a feel of the data and extract intuitions for feature engineering. When embarking upon exploratory analysis, it would be a good idea to get inputs from domain team on the relation between variables and the business problem. Such inputs are often the starting point for this phase"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 2 - KDnuggets,"Let us now get to the context of our preventive maintenance problem and evolve a philosophy for exploratory analysis.In the case of industrial batteries, a key variable which affects the state of health of a battery is its conductance. It turns out that an indicator of failing health of  battery is the precipitous drop in conductance. Armed with this information our next task should be to  identify, from our available data set,batteries that have higher probability to fail. Since precipitous fall in conductance is an indicator of failing health,the conductance data of  unhealthy batteries will have more variance than the normal ones. So the best way to identify failing batteries from the normal ones would be to apply some consolidating metric like standard deviation or variance on the conductance data and further drill deep on samples which stand apart from the normal population"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 2 - KDnuggets,The above is a plot depicting standard deviation of conductance for all batteries. Now what might be of interest to us is the red zone which we can call the “Potential failure Zone”. The potential failure zone consists of those batteries whose conductance values show high standard deviation. Batteries with failing health are likely to exhibit large fall in conductance and as a corollary their values will also show higher standard deviation. This implies that the samples of batteries which have higher probability of failure will in all likelihood be from this failure zone. However to ascertain this hypothesis we will have to dig deep into batteries in the failure zone and look for patterns which might differentiate them from normal batteries. Another objective to dig deep is also to elicit clues from the underlying patterns on what features to include in the predictive model. We will discuss more on the feature extraction when we discuss about feature engineering. Now let us come back to our discussion on digging deep into the failure zone and ferreting out significant patterns. It has to be noted that in addition to the samples in the failure zone we will also have to observe patterns from the normal zone to help separate wheat from the chaff . Intuitions derived by observing different patterns would become vital during feature engineering stage
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 2 - KDnuggets,"The above figure is a comparison of patterns from either zones. The figure on the left is from the failure zone and the one on the right is from the other. We can clearly see how the precipitous fall is manifested in the sample from the failure zone. The other aspect to note is also the magnitude of the fall. Every battery will have degrading conductance over time. However the magnitude of  degradation is what differentiates the unhealthy  battery from a normal one. We can observe from the plot on the left that the fall in conductance is more than 50%, however for the battery to the right the drop is more muted.  Another aspect we can observe is the slope of conductance. As evident from the two plots, the slope of  conductance profile for the battery on the left is much more steeper over time than the one on the right. These intuitions which we have derived so far might become critical from the overall scheme of feature engineering and modelling. Similar to the intuitions which we have disinterred so far, more could be extracted by observing more samples. The philosophy behind exploratory analysis entails visualizing more and more samples, observing patterns and extracting clues for feature engineering. The more time we spend on doing this more ammunition we get for feature engineering"
Applied Data Science: Solving a Predictive Maintenance Business Problem Part 2 - KDnuggets,The discussions so far were centered on exploratory analysis on a single variable. Next we have to connect other variables to the one which we already observed and identify trends in unison. When we combine trends from multiple variables we will be able to unravel more insights for feature engineering. We will continue our discussions on combining more variables and subsequent feature engineering in our next post. Watch out this space for more
Governance in Data Science - KDnuggets,"In order to build predictive models, data scientists need accurate data for training and validation. While a lot of work usually goes into cleaning up data sources for modeling, such as dealing with missing attributes, there’s often larger issues with the underlying data set that need to be correctly in order for the trained models to actually be representative. One of the goals of data governance is data integrity, which involves validating that your underlying assumptions about the data set match reality. An example showing why this aspect of data science is so important is the recent"
Governance in Data Science - KDnuggets,"Governance roles for data science and analytics teams are becoming more common, because companies are using large and complex data sets from a variety of internal and external sources. One of the key functions of this role is to perform analysis and validation of data sets in order to build confidence in the underlying data sets. We want to build trust in our data sets before we use them as input to our models, where the outputs are visible to customers. At"
Governance in Data Science - KDnuggets,"One of the key challenges when using data sets is determining the validity of the data. Often data is stale or sampled in a way that is not representative of the overall population.If you’re using a data source that is several years old, many conclusions that could be drawn from the data may no longer hold true. For example, using data about broadband connectivity in 2010 would be problematic when determining the impact of repealing net neutrality on US households today. In the case of the FiveThirtyEight article, a sampled data set was used where the distribution of broadband subscribers significantly varied from other data sources analyzed"
Governance in Data Science - KDnuggets,"In order to question underlying assumptions about data, it’s often necessary to audit the data against different sources. For example, transaction-level data provided by the FEC about political contributions can be compared with aggregate amounts reported from campaigns, and estimates of housing values can be compared to estimates from Zillow and Redfin. A governance role will prioritize which data points to manually inspect, in order to build more confidence in the data sets, and make sure that conclusions reached from a sample data set can be applied to a wider population"
Governance in Data Science - KDnuggets,"Another aspect of this role is determining how to resolve issues with data sets when they are discovered. In the case of incorrect findings being published, a postmortem should be published explaining how the findings change based on the newly discovered information, and the FiveThirtyEight article is a great example of this. But if the input data is instead used for modeling, then the role should work with an engineering team to resolve these issues in the data pipeline"
Governance in Data Science - KDnuggets,"One of the non-trivial situations we encountered at Windfall is handling multiple-property transactions, where properties at multiple addresses are purchased as part of the same transaction. Handling these types of transactions required adding new rules to our automated valuation model (AVM) calculations. Much like productizing a model, a governance data scientist should be capable of putting data quality fixes into production. This can involve handing off a script, or submitting PRs with code changes"
Governance in Data Science - KDnuggets,"An additional function that we are defining for a governance role is to evaluate if new data sources are worth using for modeling purposes. At Windfall, this means determining if adding a new data source will improve the accuracy of our net worth models. A data scientist in this role should be able to work with third party data in a variety of data formats and types of sources, and perform exploratory analysis on the data. Often the goal of exploring a new data set is to test for correlations between attributes in different data sets, and data scientists need to be able to work effectively with disparate data sources"
Governance in Data Science - KDnuggets,"This role differs from a machine learning role, because the focus is not on predictive modeling, but instead focused on improving data quality and integrity. It also differs form product analytics roles, because the goal is to identify discrepancies in the underlying data rather than business metrics. Despite these differences, the role still requires the statistical knowledge, domain expertise, and hacking skills commonly associated with data science"
How Docker Can Help You Become A More Effective Data Scientist - KDnuggets,"For the past 5 years, I have heard lots of buzz about docker containers. It seemed like all my software engineering friends are using them for developing applications. I wanted to figure out how this technology could make me more effective but I found tutorials online either too detailed: elucidating features I would never use as a data scientist, or too shallow: not giving me enough information to help me understand how to be effective with Docker quickly"
How Docker Can Help You Become A More Effective Data Scientist - KDnuggets,"You can think of Docker as lightweight virtual machines — that contain everything you need to run an application. A docker container can capture a snapshot of the state of your system so that someone else can quickly re-create your compute environment. That’s all you need to know for this tutorial, but for more detail you can head"
How Docker Can Help You Become A More Effective Data Scientist - KDnuggets,"04 and are all aliases for the same image. Furthermore, the links provided in this repository link you to the corresponding Dockerfiles that were used to build the images for each version. You will not always find Dockerfiles on DockerHub repositories as it is optional for the maintainer to include the Dockerfile on how they made the image. I personally found it useful to look at several of these Dockerfiles to understand Dockerfiles more (but wait until you are finished with this tutorial!)"
How Docker Can Help You Become A More Effective Data Scientist - KDnuggets,"Then you will just end up pulling the ubuntu:16.04 image. Why? — If you look closely at the above screenshot, you will see the :latest tag is associated with 16.04"
How Docker Can Help You Become A More Effective Data Scientist - KDnuggets,One thing you may notice about the RUN statement is the formatting. Each library or package is neatly indented and arranged in alphabetical order for readability. This is a prevalent convention for Dockerfiles so I suggest you adopt this as it will ease collaboration
How Docker Can Help You Become A More Effective Data Scientist - KDnuggets,"This statement allows you to share data between your docker container and the host computer. The VOLUME statements allow you to mount externally mounted volumes. The host directory is declared only when a container is run (because you might run this container on different computers), not when the image is being defined*. For right now, you only specify the name of the folder within the docker container that you would like to share with the host container"
How Docker Can Help You Become A More Effective Data Scientist - KDnuggets,"This is to preserve image portability.For this reason, you can’t mount a host directory from within the Dockerfile. The"
How Docker Can Help You Become A More Effective Data Scientist - KDnuggets,"Docker containers are designed with the idea that they are ephemeral and will only stay up long enough to finish running the application you want to run. However, for data science we often wish to keep these containers running even though there is nothing actively running in them. One way that many people accomplish this by simply running a bash shell (which doesn’t terminate unless you kill it)"
Where Will Data Science and Marketing Analytics Take You in 2018? - KDnuggets,"MADS Can Help You Achieve Your 2018 Goals in San Francisco, April 11-13, 2018. Hear from speakers like DJ Patil, Former U.S. Chief Data Scientist, as he reveals the secrets to navigating the digital transformation. Save 20% with VIP Code MADS18KDN"
The Art of Learning Data Science - KDnuggets,"If you blinked on these acronyms perhaps you need to google a bit and then continue reading the rest of this post. This post has 2 goals. First, it attempts to put all the fellow Data Science learners at ease. Second, if you have just begun on the Data Science, this may serve you as a guide to the next step"
The Art of Learning Data Science - KDnuggets,"I started the journey of Data Science in the beginning of October, 2017. First 15 days spent in just trying to answer a single question, “What is Data Science” in a manner that is convincing to me. After browsing a variety of resources on the internet, Quora, Medium, Springboard blogs and e-books, Udacity blogs, Forbes, datascience. Yes, it is that simple. No it is not? yes it is. Well, there are two broad types of Data Science jobs, here I am talking about Data science for business. The other one’s end product is not a story, but a data driven product. Let us not get into that because then we digress into Machine Learning Engineering. Typically Google, Facebook etc have data driven roles, which fall into second category. Much of the academic research is also of second type too"
The Art of Learning Data Science - KDnuggets,"Back to the first type, let me make it a little more advanced definition. Data Science is the process of coming up with answers to business questions with the help of historical data, by cleaning and analysing it first, then fitting it into one (or combination) of the machine learning model(s) and often forecasting and suggesting measures to prevent possible future issues. Ah"
The Art of Learning Data Science - KDnuggets,"Again I spent a few days looking up this phrase and lo, I found countless advice. This time there was no other option but to try a few of them. I have Bachelors and Masters degrees in Electronics and Communications Engineering and a decade of experience of programming in languages like C/C++, Octave/Matlab, Verilog/SystemVerilog, Perl. Mathematics was my favorite subject since my childhood, Probability was the favorite in Masters. Experience with programming and Probability is a definite plus in my case"
The Art of Learning Data Science - KDnuggets,"I was a bit afraid of the term “Machine Learning”, and I am the type who likes to face the fear head on, so ended up enrolling in Prof Andrew’s Coursera course. That is the first one I took and I am glad it worked well for me. I was literally scared of both Python the snake and the language, but fortunately Andrew’s course exercises are in Octave. I tried learning Python fundamentals from Coursera, Udacity, Edx and Datacamp and chose Coursera and Datacamp. I knew that as a beginner in data science, R would perhaps be the better start. However, at that point of time, I was not too confident of taking only the data science path. Python is more general. Took several courses across several platform at the same time. Tried Intro to Machine Learning, Statistics, CS Fundamentals, Intro to Data Science etc on Udacity. Could not continue with them for a long time, as I do not like interrupts when a concept is being absorbed by my neurons"
The Art of Learning Data Science - KDnuggets,"Meanwhile, it has been a long time since I had taken any coursework. I found a nice Coursera one on “Learning how to learn” by UC, San Diego. It helped me to reassure that the learning techniques I apply are good enough. In addition, it removed my doubts on whether I am not young enough to take up something new, as the recent research has proven that certain activities like exercise, meditation or just walking in nature (which anyway I do) give birth to new neurons in the brain and form new connections. The"
The Art of Learning Data Science - KDnuggets,"I also found it the right time to take up a course titled “A Life of Happiness and Fulfillment” by Indian School of Business. The content of this course has made me dispassionately pursue my passion of Data Science. It reminds me to learn it for the pure joy of learning and focus on the process, rather than the end product. Although these are non-technical, I find them super-helpful in fast and effective learning"
The Art of Learning Data Science - KDnuggets,"I attended a meetup in mid October and found that it was organized by a local DS Consulting company that also conducts training. I was not too impressed by their model. They train you with your own money and if you do well, recruit you as employee. The takeaway from the meetup was: “MOOCs will not get you a job, real projects, Kaggle competitions, having your own blogs will. Masters from reputed institutes will matter, but MOOC certificates from same institutes carry no value”"
The Art of Learning Data Science - KDnuggets,"Here’s my take on that: “It doesn’t matter which path you take to learn. All that matters is, you can do a real world DS project”. If you have a way to prove that in an interview, why can’t you get a job? You don’t need to shell out thousands of dollars on bootcamps or get MOOC certificates either. You need to have a set of qualities/talents/skills to be a Data Scientist: good understanding of fundamentals in high-school math-probability-statistics, a lot of curiosity, inquisitiveness, inclination to learn new things, familiarity with programming, ability to document and present, and above all, you must know that you possess these. [If you have self-doubts, you need to get them out of the way first.] Learning the rest (like ML) follows. Companies, especially in smaller towns like mine are in so much need for a Data Scientist that they are looking for means to hire a good one. Keep in mind though, it is essential that you do a few real DS projects and showcase them to prospective employers through reports/presentations or github repo. If you are unsure how to do a real project, one way is to seek help from a mentor-an industry expert for technical stuff. Then the job search process calls for another post, out of scope of this one"
The Art of Learning Data Science - KDnuggets,"In summary, what is the best path to take to fill the gaps in essentials? There are no shortcuts. Try out a few platforms and see what suits best for you. Start with MOOCs followed by going hands on. Be sure to be organized and document your journey. First learn a topic which is not in your comfort zone. For example, if you already know C++, don’t jump into learning Python, instead know that you can do it eventually. It makes sense to try ML to see if you like it, because that is what separates a data scientist from a data analyst or data engineer. If you do not enjoy learning in any of them on your own, chances are that you won’t be able to learn even with a bootcamp. Data Science is a field which requires learning everyday: new tools, new concepts/algorithms, new business/domains, could be anything on earth, there are only steps, and no end"
The Doing Part of Learning Data Science - KDnuggets,"If you are not satisfied with generic advice above, read on. Here is an attempt to address  project related questions I received regarding what to do after crossing the initial study phase. I might publish a detailed one once I am done with my current project. My views are based on the path I am taking and some other aspirant would have succeeded by taking an entirely different approach. Find out what works best for you and just do that"
The Doing Part of Learning Data Science - KDnuggets,"Once done with a few books and MOOCs, it is important to take action. However, don’t fall for “Go for messy data, because that is what real projects look like”. It can easily demotivate you. For a beginner what is more important is, to know if one can visualize, draw insights, make inferences from the exploratory data analysis and write a story. Once you know that you are too familiar with the data you are working with, try some predictive modeling. Then it becomes fun, otherwise it is pain"
The Doing Part of Learning Data Science - KDnuggets,"I know initially this type of advice from pros, “If you want to impress the hiring manager, don’t say what you know or how you know, but demonstrate by showcasing projects” sounds too overwhelming! Trust me, I struggled for one whole month thinking of ‘ideas that no one has done before’. Here’s a good news. Recently, datacamp announced a few beginner level projects. I have not worked on them, but had a quick look. I think it is a good place to try, as it splits a project into several subtasks and gives instructions, without hand-holding too much. I think it is a good value for money with low monthly fee of $29"
The Doing Part of Learning Data Science - KDnuggets,"When I did what I wrote in the previous article, I was pretty sure that being data scientist is not easy. I sort of understood what makes potential employers hesitant to hire people without any ‘data science industry experience’. I think realizing these two by reflecting on what you have read and learned is very important. I know the value of paid advice, be it on technical or career tips. I don’t mind paying a couple of thousands of dollars if it can get me a job at least 2 weeks earlier than, had I done everything on my own. That is not it. Doing projects with guidance or getting tips from a person who had trodden the same path on getting the first job has added advantages like,"
The Doing Part of Learning Data Science - KDnuggets,"Therefore, I did pay and get two separate paid advice: one for technical stuff and one for getting that first job. I can not recommend them right now, because I have not fully gone through the experience. Side by side, I download  a few intermediate size datasets (A few thousands of rows/records and a few 10s of columns) and play with them. An example using IBM telecom Dataset:"
The Doing Part of Learning Data Science - KDnuggets,"The paid advice do not work the same way for everyone. It depends on the approach taken by the individual. Stated in electronic communication language,"
The Doing Part of Learning Data Science - KDnuggets,"Then go for paid advice if you like. Channel of communication between the student and the guide gets better along the way. While looking for a guide, beware of the people on LinkedIn who claim to be data science mentors. Look at their profiles and see if they are just educators or practicing data scientists"
The Doing Part of Learning Data Science - KDnuggets,"I was about to publish this post, and my predictive model suggested that the next question from readers would be, “Okay, I’ve done toy datasets, but at this time I really can’t afford to pay, and I don’t mind the delay or not having those added factors. Can you please this one time tell me what I can do?”. Well, the only option is, you really evaluate your work so far and put yourself in a potential hiring manager’s shoes. Would he/she say “Wow! Awesome!”? I think you would want to take a dataset with few MBs of data, with 50+ features which contain rich information. You might have to scrape websites for this. The real world projects will be largely specific to the business. Normally there will be a business question you need to answer. That would mean you should demonstrate your skills at collecting relevant data by defining a problem first. Try doing that and you will never need anyone’s advice, because you will eventually land in a data scientist job or you will know if it is not for you. Either way, the learning is yours to cherish for lifetime"
How Nonprofits Can Benefit from the Power of Data Science - KDnuggets,"This is not surprising, given the fact that most nonprofit bodies do not have a dedicated data analysis team. However, the wave of big data is now reaching NGOs and government bodies as well, who can use data science to make a bigger impact. Data analysis can help these organizations make sure that they're putting their time, effort, money and energy into the right channels. Nonprofit organizations need funds to fulfill their missions but also need to prove the results of their work in order to attract donors. Data science has immense potential in optimizing this cycle, and also helps organizations make well-informed, sound decisions. While not every organization can afford to hire full-time data analysts, some fill the resource gap by seeking help on platforms like"
How Nonprofits Can Benefit from the Power of Data Science - KDnuggets,"Data insights help NGOs identify and segregate donors based on various factors, using which they can drive their marketing and fundraising efforts more effectively. Data analysis also helps charity organizations discover relationships, that can help them develop specific incentives. After studying their data and finding that they could provide something for their donors, rather than the other way around, Macmillan's Cancer Support introduced the ""World's Biggest Coffee"" initiative, which saw a leap in the income generated from"
How Nonprofits Can Benefit from the Power of Data Science - KDnuggets,"Data science can help nonprofits accurately measure of the performance of their activities, and help them tailor their processes towards better results. Data analysis and visualization can also play a huge role in real-time tracking during crises and optimizing relief efforts. Qlik, a software company based in the USA, provides data science tools that organizations can use to discover insights, patterns and power crucial decision-making processes. In the past, they've tied up with nonprofits to help leverage the power of data for social causes: facilitating"
How Nonprofits Can Benefit from the Power of Data Science - KDnuggets,"The Fellows work with government bodies and nonprofits to tackle issues related to education, healthcare, public safety and the environment. One of their projects, in partnership with the Chicago Department of Public Health, uses predictive analytics to help prevent lead poisoning in children. Lead-based paint was used before the ban in 1978, and children who live in older homes are still likely to be exposed to it. The models identify houses that are likely to have lead-based paint, taking into account various factors, which then allows for mitigating the dangers of exposure"
How Nonprofits Can Benefit from the Power of Data Science - KDnuggets,"That data science bears enormous potential for government organizations and nonprofits, big and small, is quite clear. But accessing experts is a common problem that these organizations face. Processes are often bureaucratic and funds may be inadequate to employ a full-fledged team. And so many organizations don't see the power of data science manifest. Organizations like Datakind help connect data scientists to social organizations, however, scientists offer their services on a voluntary basis. One solution is for organizations is to"
How Nonprofits Can Benefit from the Power of Data Science - KDnuggets,"Ramya Sriram is Digital Content Manager at Kolabtree (kolabtree. Kolabtree's pool of global experts are from the likes of NASA, Harvard, Stanford, Oxford, MIT, Cambridge, and more. Organizations benefit from consulting the on-demand platform's data scientists, machine learning and artificial intelligence experts"
Docker for Data Science - KDnuggets,Docker is a tool that simplifies the installation process for software engineers. Coming from a statistics background I used to care very little about how to install software and would occasionally spend a few days trying to resolve system configuration issues. Enter the god-send Docker almighty
Docker for Data Science - KDnuggets,Think of Docker as a light virtual machine (I apologise to the Docker gurus for using that term). Generally someone writes a *Dockerfile* that builds a *Docker Image* which contains most of the tools and libraries that you need for a project. You can use this as a base and add any other dependencies that are required for your project. Its
Docker for Data Science - KDnuggets,Lastly Dockerhub deserves a special mention. Personally Dockerhub is what makes Docker truly powerful. It’s what
Docker for Data Science - KDnuggets,Docker is one of the tools that as a software engineer (and now data scientists/ analysts) should have in their repertoire (with almost the same regard and respect as git). For a long time statisticians and Data Scientists have ignored the software aspect of data analysis. Considering how simple and intuitive it has become to use Docker there really is no excuse for not making it part of your software development pipeline
Generalists Dominate Data Science - KDnuggets,"Large companies often fill each role with a pair of shoes, resulting in a twelve person team like the one below. The problem with this setup is that it becomes much more difficult to achieve consensus and to perform any task that spans roles. And in data science, most tasks span roles"
Generalists Dominate Data Science - KDnuggets,"Charts take iteration, so this cycle of communication could happen repeatedly for each chart. You can see how communication overhead starts to predominate. A meeting of six people is a full-blown, formal meeting. It is hard to get things done in formal meetings"
Generalists Dominate Data Science - KDnuggets,"Revisiting the chart example, creating a chart becomes a collaboration between the product manager, a designer who codes, and a data scientist. This is the kind of ad hoc meeting of 2–3 people where “shit gets done” efficiently. This group will be more efficient than the six man group. Put another way: this small team will kick the large team’s ass"
Generalists Dominate Data Science - KDnuggets,"We’ve shown that small teams generalists outperform large teams of specialists. In fact, generalist skills are something every data scientists should work to develop. That doesn’t mean you can’t specialize, but should combine specialization with generalization in order to develop “"
5 things that will be important in data science in 2018 - KDnuggets,"But that doesn’t mean data professionals shouldn’t take an interest and put forward their own point of view. Far from it; in fact, it’s essential that data scientists, analysts, and engineers all contribute to the ongoing discussions about the field. You are the people developing that field, making decisions every day that will shape the way it is used and viewed by the wider world - so make sure you play your part!"
5 things that will be important in data science in 2018 - KDnuggets,"For too long businesses haven’t been getting the most out of their data. And that’s infuriating for data scientists and analysts - it means they’re not being allowed to do their job properly because of mismanagement and a lack of strategic direction. But now is the time for change. It’s time for organizations to start being more intelligent with their data, to ensure there is greater alignment between business goals and needs and the way data is handled, processed, visualized and shared"
5 things that will be important in data science in 2018 - KDnuggets,"Data empowerment sounds big and bold, but it comes down to one thing - better BI tools that more people can use. Data doesn’t, after all, belong in one corner of a room - it belongs everywhere, to all stakeholders. Yes, their needs will be different, but they all need different types of data, processed and analyzed in different ways. Ultimately data empowerment feeds into the previous point about alignment - it helps to ensure everyone is working together towards clearly defined goals, with the same tools and resources at their disposal"
5 things that will be important in data science in 2018 - KDnuggets,"Automation means many different things in the data world. And all of them are important. But most interestingly is how data science itself might start to be automated. Thanks to tools like auto-sklearn it’s something we’ve started to see already, and its likely to change the nature of data science roles. As algorithm selection and optimization becomes less intensive and time-consuming, focus will move elsewhere to the next thing that data professionals can add value to"
5 things that will be important in data science in 2018 - KDnuggets,"Okay, so cloud has been part of discussions around software and infrastructure for the good part of a decade. But it’s only very recently that it’s starting to have an impact on the data world. As cloud solutions become more popular, it’s going to open up new options for a diverse range of organizations, providing new ways of storing, capturing and processing data. No doubt we’ll see cloud offerings develop in a way that accommodates the data needs of their customers, but more than that, we’ll also hear more from data scientists and data architects about how cloud is revolutionizing the way they build intelligent systems"
Data Science for Laymen: 5 Writers Who Speak Your Language - KDnuggets,"Comfort with uncertainty, ability to identify use cases, and participation in online competitions were some of the key takeaways.  But one that really stood out was the importance of personality—a candidate’s willingness to engage with the community and learn from the meaningful connections he makes.  That can be intimidating when the core concepts are quite complicated.  Where do you start when it seems your peers know everything?"
Data Science for Laymen: 5 Writers Who Speak Your Language - KDnuggets,"For the most part, these connections begin online; a candidate reaches out in hopes his questions might be answered, an introduction might be made.  As a result, we all know the influencers—Evan Kirstel, Andrew Ng, Meta S. Brown, et cetera—but for someone just getting their feet wet, even someone who interacts with data scientists daily, it can be refreshing to brush up on key principles with people who don’t already get hundreds of InMail.  That’s why we’re sharing some trusted resources—influencers in their own right—who engage candidates from different corners of our space, in terms we all can understand"
Data Science for Laymen: 5 Writers Who Speak Your Language - KDnuggets,"I haven’t read all of Hinton’s papers.  I don’t use TensorFlow on a daily basis.  I still prefer R to Python despite using both in my work,”"
Data Science for Laymen: 5 Writers Who Speak Your Language - KDnuggets,"Currently Karlijn leads the data journalism efforts at DataCamp, leveraging her master’s in information management and experience as an analyst at Deloitte.  As with most people on this list, she emphasizes the value of experience—real life solutions over certificates— but Karlijn is a real gem when it comes to citing her sources.  In this"
15 Minute Guide to Choose Effective Courses for Machine Learning and Data Science - KDnuggets,I am a semiconductor professional with 8+ years of post-PhD experience in a top technology company. I take pride in the fact that I work in the cross-section of physical electronics which directly contributes to the energy sector. I develop
15 Minute Guide to Choose Effective Courses for Machine Learning and Data Science - KDnuggets,"In this picture, I just want to show the possibilities and impossibilities of this process i. Having said that, however, these circles broadly encompass the core skills that one can study to venture into the field of data science/machine learning from a non-CS background. Please note that even if you are in information technology (IT) sector, you may have a steep learning curve ahead because traditional IT is being disrupted by these new fields and the core skills and good practices are often different"
15 Minute Guide to Choose Effective Courses for Machine Learning and Data Science - KDnuggets,"I, for one, view the data science field as more democratic than many other professional domains (e. For me personally, I have no burning desire to ‘break in’ this field, rather I just have a passion to borrow the fruits to apply to my own area of expertise. However, that end goal does not impact the initial learning curve that one has to traverse. So, you could be aiming to be either data engineer, or business analyst, or machine learning scientist, or a visualization expert — the field and choices are wide open. And if your aim is like mine - stay in the current domain of expertise and apply the newly learned techniques — you are fine too"
15 Minute Guide to Choose Effective Courses for Machine Learning and Data Science - KDnuggets,"In all likelihood, you cannot go more basic than this :-). It worked though. I had this aversion towards coding but the simple and fun interface and the right pace of Codeacademy’s free course was appropriate to excite me enough to keep going. I could have picked a Java or C++ course on Coursera or Datacamp or Udacity but some reading and research told me that Python is the optimal choice balancing learning complexity and utility (especially for data science) and I decided to trust the insight"
15 Minute Guide to Choose Effective Courses for Machine Learning and Data Science - KDnuggets,"Codeacademy’s introduction was a fine base to start with. I had choices from so many online MOOC platforms and predictably enough, I signed up for multiple courses at the same time. However, after dabbling with a Coursera class for few days, I realized I was not ready enough to learn Python from a professor! I was looking for a course taught by some enthusiastic instructor who will take time to go over the concepts in great detail, teach me other essential tools like Git and Jupyter notebook system, and maintain a right balance between basic concepts and advanced topics in the curriculum. And I found the right man for the job:"
15 Minute Guide to Choose Effective Courses for Machine Learning and Data Science - KDnuggets,"The next step proved crucial for me. I could have gone astray and try to study anything and everything I could on Python. Especially, the object-oriented and class definition part which easily can suck you in for a long and arduous journey. Now, taking nothing away from that key sphere of Python universe, one can safely say that you can practice deep learning and good data science without being able to define your own class and methods in Python. One of the fundamental reasons of"
"Data Science in 30 Minutes: A Conversation with Gregory Piatetsky-Shapiro, President of KDnuggets - KDnuggets","Previously, he worked as a data scientist (Foursquare), Wall Street quant (D.E. Shaw, J.P. Morgan), and a rocket scientist (NASA). He completed his PhD at Princeton as a Hertz fellow and read Part III Maths at Cambridge as a Marshall Scholar. At Foursquare, Michael discovered that his favorite part of the job was teaching and mentoring smart people about data science. He decided to build a startup to focus on what he really loves"
Back to the Future: 2018 Big Data and Data Science Prognostications - KDnuggets,"This is the time of year when everyone makes his or her predictions for 2018.  I have my predictions as well, but wanted to do something a bit more fun.  So I thought I’d look backwards to the state of technology 50 years ago to gain some insights that we can use to make projections about 2018. That is, what “predictions” made in the 1950’s might tell us about 2018"
Back to the Future: 2018 Big Data and Data Science Prognostications - KDnuggets,"However, it’s really hard to find predictions about the future made in the 1950’s.  There was no Internet or Social Media or Reality TV, so I found the next best proxy…sci-fi movies!  I decided to review the most popular sci-fi movies from 1950’s, and provide my perspective as to what these movies might tell us about 2018.  Maybe drink a Tab or Fresca as you read this"
Back to the Future: 2018 Big Data and Data Science Prognostications - KDnuggets,"In the movie “The Day the Earth Stood Still”, a humanoid alien visitor named Klaatu comes to Earth, accompanied by a powerful eight-foot tall robot, Gort, to deliver an important message about how we are destroying planet Earth.  And let’s not forget the important chant “Klaatu barada nikto” that stops Gort from destroying Earth when Klaatu gets shot.  I’m sure that catchy phrase is something you chant every morning while showering.  I do!"
Back to the Future: 2018 Big Data and Data Science Prognostications - KDnuggets,"The unusualness of open source project naming (Linux, Hadoop, Ubuntu, Pidgin, GNU) might be able to exploit names from this movie.  I can already imagine a new Machine Learning framework called “Klaatu barada nikto” popping on the 2018 landscape. By the way, have some fun creating your own open source project names:"
Back to the Future: 2018 Big Data and Data Science Prognostications - KDnuggets,"A starship crew goes to investigate the silence of a planet’s colony only to find two survivors and a deadly secret.  Forbidden Planet was the first science fiction film to depict humans traveling in a faster-than-light starship. But more importantly, “Robby the Robot” is one of the first “real” movie robots (where real means more than a “tin can” on legs). Robby displays a distinct personality and plays an integral role in the film"
Back to the Future: 2018 Big Data and Data Science Prognostications - KDnuggets,"A doctor returns to his small town only to find several of his patients suffering the paranoid delusion that their friends or relatives are impostors. He is initially skeptical, especially when the alleged doppelgängers (digital twins?) are able to answer detailed questions about their victim’s lives. He eventually determines that something odd has happened and searches for the cause of this phenomenon"
Back to the Future: 2018 Big Data and Data Science Prognostications - KDnuggets,H.G. Wells’ classic novel is brought to life in this tale of alien invasion. The residents of a small town in California are excited when a flaming meteor lands in the hills. Their joy turns to H-O-R-R-O-R when they discover that the meteor has passengers who are not very friendly (the understatement of 1953)
How Much Mathematics Does an IT Engineer Need to Learn to Get Into Data Science? - KDnuggets,"However, I have many friends in IT industry and observed a great many traditional IT engineers enthusiastic about learning/contributing to the exciting field of data science and machine learning/artificial intelligence. I am dabbling myself in this field to learn some tricks of the trade which I can apply to the domain of semiconductor device or process design. But when I started diving deep into these exciting subjects (by self-study), I discovered quickly that I don’t know/only have a rudimentary idea about/ forgot mostly what I studied in my undergraduate study some essential mathematics"
How Much Mathematics Does an IT Engineer Need to Learn to Get Into Data Science? - KDnuggets,"Now, I have a Ph.D. Meaning no disrespect to an IT engineer, I must say that the very nature of his/her job and long training generally leave him/her distanced from the world of applied mathematics. Often, there is immense time pressure, and the emphasis is on ‘"
How Much Mathematics Does an IT Engineer Need to Learn to Get Into Data Science? - KDnuggets,"They are often taught at advanced graduate level courses in an applied science/engineering discipline. Or, one can imbibe them through high-quality graduate-level research work in similar field. Unfortunately, even a decade long career in traditional IT (devOps, database, or QA/testing) will fall short of rigorously imparting this kind of training. There is, simply, no need"
How Much Mathematics Does an IT Engineer Need to Learn to Get Into Data Science? - KDnuggets,"Always a good idea to start at the root. Edifice of modern mathematics is built upon some key foundations — set theory, functional analysis, number theory etc. From an applied mathematics learning point of view, we can simplify studying these topics through some concise modules (in no particular order):"
How Much Mathematics Does an IT Engineer Need to Learn to Get Into Data Science? - KDnuggets,Sir Issac Newton wanted to explain the behavior of heavenly bodies. But he did not have a good enough mathematical tool to describe his physical concepts. So he invented this (or a certain modern form) branch of mathematics when
How Much Mathematics Does an IT Engineer Need to Learn to Get Into Data Science? - KDnuggets,The importance of having a solid grasp over essential concepts of statistics and probability cannot be overstated in a discussion about data science. Many practitioners in the field actually call machine learning nothing but statistical learning. I followed the widely known “
How Much Mathematics Does an IT Engineer Need to Learn to Get Into Data Science? - KDnuggets,"To plug those gaps, I started taking other MOOCs focused on basic statistics and probability and reading up/watching videos on related topics. The subject is vast and endless, and therefore focused planning is critical to cover most essential concepts. I am trying to list them as best as I can but I fear this is the area where I will fall short by most amount"
How Much Mathematics Does an IT Engineer Need to Learn to Get Into Data Science? - KDnuggets,"On the other hand, it is always deeply satisfying and insightful experience to understand a computer algorithm’s time complexity as it becomes extremely important when the algorithm is applied to a large data set. In this era of big data, where a data scientist is routinely expected to extract, transform, and analyze billions of records, (s)he must be extremely careful about choosing the right algorithm as it can make all the difference between amazing performance or abject failure. General theory and properties of algorithms are best studied in a formal computer science course but to understand how their time complexity (i"
"Transitioning to Data Science: How to become a data scientist, and how to create a data science team - KDnuggets","It is difficult to define data science these days: every company claims to be doing data science and everyone claims to be a data scientist. Practitioners are puzzled by their fuzzy job descriptions, and people who are trying to become data scientists are frustrated by the lack of standard definitions. In this conversation, at Toronto Machine Learning Summit 2017, we have tried to demystify data science and clarify what it means to be a data scientist"
"Transitioning to Data Science: How to become a data scientist, and how to create a data science team - KDnuggets","Once that core is formed and depending on the priority, timeframe, and the complexity of the problem you might end up hiring people with various skill sets. Most of the time the best solution is to have a data engineer to build the infrastructure and then a data scientist on top of it in addition to the business domain experts. In fact, “"
"Transitioning to Data Science: How to become a data scientist, and how to create a data science team - KDnuggets","There are many nuances in creating a data science team that taking it loosely will certainly lead into failure. So far, it should be clear how different technical roles and expertise along with soft skills intertwine to create a team that can achieve great objectives in data science. Selecting individuals to join such a team is a great challenge and needs to be done with care"
"Transitioning to Data Science: How to become a data scientist, and how to create a data science team - KDnuggets",Some of these characteristics come from personality but experience is also very important. That causes a big contradiction as there is almost no experienced data scientist out in the market. Majority of people looking for data science jobs are right out of school and therefore don't have the business experience needed
"Transitioning to Data Science: How to become a data scientist, and how to create a data science team - KDnuggets","One of the best ways to find your best next job opportunity or hire is to network with like-minded people. Keep an open mind, and be respectful of everyone, and try to help others as much as you can, and they will come back and help you when they can. It is very crucial to establish good professional relationships and maintain them"
Why understanding of truth is important in Data Science? - KDnuggets,"Data science involves hypothesizing (theory-driven or top-down) or discovering (data-driven or bottom up) systematically observable properties of a phenomenon, i.[2] Belief in an absolute truth may suggest that there is only one model, one set of properties. As Plato’s allegory of the cave taught us 2,400 years ago, we cannot observe the “real” thing, we can observe only an image of the thing (a model) from our perspective. In science, as in life, understanding of a phenomenon may be enriched by observing the phenomenon from multiple perspectives (models). A recent scientific trend, e"
Why understanding of truth is important in Data Science? - KDnuggets,"These are called correlations amongst variables. Ideally, data science methods will help us identify highly probably (plausible) hypotheses (correlations) that will be proven causal by other means. Data Science involves accelerated methods of discovering THAT correlations occur under certain conditions and with certain probabilities; it cannot discover"
Why understanding of truth is important in Data Science? - KDnuggets,"Banks must maintain a single version of truth for your bank account, not multiple versions, since you want the bank to make sure that every euro you put in is credited to you and every euro taken out is credited to the person you are paying. Databases were first developed for banking and business; hence they claim to support a single version of truth. While this is critical for some problems, e. Hence, database products do not support multiple models, i. For over 40 years, researchers have tried but failed to develop databases that support multiple perspectives or multiple semantic models"
Why understanding of truth is important in Data Science? - KDnuggets,"98% of what people say are opinions that are impossible to prove as “true”. The previous sentence is an opinion, hence unprovable. However, it suggests that almost all assertions are mere opinions and should be considered as opinions"
Why understanding of truth is important in Data Science? - KDnuggets,"Following the above discussion of truth, our knowledge – ideally verifiable, systematic observations under specific conditions – is relative to the data we have and the models (perspectives) that we have used to establish the knowledge (informally truths of the phenomenon). Recently, it has been observed that algorithms used in many areas (mortgage and loan approvals, hiring and promotion, parole and sentencing) are biased. To be biased means to be prejudiced in favor of or against one thing, person, or group compared with another, usually in a way considered to be unfair. For example, automated parole systems have been shown by"
Why understanding of truth is important in Data Science? - KDnuggets,"Specifically, ProPublica showed that automated parole systems systematically made parole decisions that disadvantaged minorities, i. In the terms used above, the automated parole system is based on a model that is inconsistent with a community model for fairness towards minorities. This could be that the firms that designed the systems believe based on some evidence (i. Even though the political disposition of the community is that minorities need to be treated fairly just like non-minorities, i. When is knowledge biased? When the knowledge used to produce a model is in conflict with another model, then the two models are biased with respect to each other. Assuming the knowledge on which the parole system model is based is verifiable under the conditions in which it is applied, it is biased with respect to a model based on fairness to minorities that may be a political aspiration rather than a reality. How do you prove veracity of a model? In this case what is the recidivism rate for the automated parole system versus a parole system based on a model that supports politically fairness to minorities? If the original parole system model has a better recidivism rate than that of the fairness model, does society select better recidivism over fairness? This is a modelling question that is outside the realm of data science. A deeper question is how do you detect bias in algorithms? You need to evaluate and compare the models underlying the algorithm versus some other model. Only models can be biased with respect to each other"
Why understanding of truth is important in Data Science? - KDnuggets,"Hamlet: Why, then, 'tis none to you, for there is nothing either good or bad, but thinking makes it so. To me it is a prison. [A reference to Hamlet’s earlier “Denmark's a prison"
Why understanding of truth is important in Data Science? - KDnuggets,"[1]	M. Braschler, T. Stadelmann, K. Stockinger (Eds"
Why understanding of truth is important in Data Science? - KDnuggets,"[2]	M.L. Brodie, Necessity is the Mother of Invention: On Developing Data Science, to appear in [1]"
Why understanding of truth is important in Data Science? - KDnuggets,"[3]	M.L. Brodie, What is Data Science? to appear in[1]"
Why understanding of truth is important in Data Science? - KDnuggets,"He is concerned with the Big Picture aspects of information ecosystems including business, economic, social, application, and technical. Dr. Brodie is a Research Scientist at CSAIL, MIT; advisor at Tamr"
"Why Use Data Analytics to Prevent, Not Just Report - KDnuggets","I recently had another client conversation about optimizing their data warehouse and Business Intelligence (BI) environment. The client had lots of pride in their existing data warehouse and business intelligence accomplishments, and rightfully so. The heart of the conversation was about taking costs out of their reporting environments by consolidating runaway data marts and “spreadmarts [1],” and improving business analyst BI self-sufficiency"
"Why Use Data Analytics to Prevent, Not Just Report - KDnuggets","We did a project for a hospital to predict which patients are likely to catch a staph infection (what hospitals call Hospital Acquired Infections or HAI). Staph infections are costly to hospitals due to increased levels of care plus the potential financial and legal liabilities if a patient becomes sick or dies from the staph infection. In order to meet the business use case of “Reducing HAI Infections,” we created a “HAI Score” for every patient (based upon personal data such as their health care history, demographics, current health readings, and family health history; diet, coupled with clustering of “similar” patient situations). Think of it as a FICO score [2] that measures the likelihood of catching a Hospital Acquired Infection while in the hospital"
"Why Use Data Analytics to Prevent, Not Just Report - KDnuggets","Online returns are a big issue in the rapidly growing world of eCommerce. In 2016, e-retail sales accounted for 8.7 percent of all retail sales worldwide. This figure is expected to reach 15.5 percent in 2021"
"Why Use Data Analytics to Prevent, Not Just Report - KDnuggets","The opportunities to reduce costs by preventing them require a different frame of thinking – to think like a data scientist. While optimizing business and operational processes is good, one must be careful about “paving the cow path” – of optimizing a business or operational process that is out dated. As I challenged a recent client:"
Another Day in the Life of a Data Scientist - KDnuggets,"First, I would like to thank Matthew for giving me this opportunity to share my experience. I recently started my career as a data analyst after doing my Masters in Analytics. I have my undergraduate in Computer Science with more than 5 years of experience in software development, working closely with business and understanding their requirements and managing small teams"
Another Day in the Life of a Data Scientist - KDnuggets,"I typically start my day with reading articles and blogs on machine learning, AI and listening to podcasts (best way to utilize my commute time). For me one of the most important role of a data scientist is to figure out the business requirement to deliver the best use of the end-product. I work as a data analyst at the City of Dallas and my role spans in several areas starting from gathering requirements, coordinating with multiple teams, cleaning the data, building machine learning models, and visualization. At my workplace we follow the principle that a comprehensive smart city solution encompasses technology, data, intelligence and application"
Another Day in the Life of a Data Scientist - KDnuggets,"In my job a clear understanding of the data and it’s cleaning is crucial to be able to use it for real purpose. I frequently meet the business for defining and refining the requirements that can help them be more efficient and work resourcefully. Quite a part of my day is spent in doing research and exploring different machine learning techniques, collaborating and communicating with the team and brainstorming on what can be added to improve business insights. I enjoy my work as it is quite dynamic. It is not just working on one tool or following a set of steps, it gives you a chance to experiment, explore and be creative. Occasionally I need to build a visual straight from excel or Cognos or Tableau and sometimes build models and give insights to the business"
Another Day in the Life of a Data Scientist - KDnuggets,"I often start at 9.30 by writing down my objectives for the day as per the iteration plan. This is followed by a team stand-up at 9:45 AM. Our team is currently comprised of a product manager, interns and myself. We go over our iteration progress, discuss any potential blockers and ask for help if needed. I usually keep the time between 10:00 AM to 12:00 PM reserved for meetings, as I find the 2-hour period before lunch too short and too distracting for time-consuming tasks. The meetings serve as an interface with both internal and external stakeholders (sales, customer success, clients, etc. I use any extra time I have within that period to catch up on emails"
Another Day in the Life of a Data Scientist - KDnuggets,"After lunch, I look back at my list of objectives and prioritize any work that needs a deep dive. This includes your usual data extraction, cleaning, transforming, followed by statistical analysis and the training the testing of machine learning models. I find that this usually takes around 3 to 4 hours. My primary tools of choice are R, SQL and python, and are used in that particular sequence. As the time nears 5:00 PM, I shift focus to research and development. I take an hour to read up on recently published work, or to build a working MVP for a data product or solution that aligns with our long-term strategy. All in all, anything I do has to positively impact Nulogy’s bottom line. A few things left out include people management, communicating results and progress to executive leadership and automating certain tasks when possible"
Another Day in the Life of a Data Scientist - KDnuggets,"Training days. I am an educator, and at least 25% of my days is spent in classrooms and webinars teaching professionals about data. I get attendees from all walks of life, but the top three professions that I meet are digital marketers, financial analysts and accountants, and human resource managers - who for me represent the top three functional areas analytics can make a difference in any company. Best of all: there's is always a potential client or collaborator in every class"
Another Day in the Life of a Data Scientist - KDnuggets,"Client days. I run my own data consultancy and easily 30% of my time is spent with proposals, meeting prospects, and doing client workshops. The paperwork is the worst part, but it is a necessary evil to get to the data science work. The best part is being with a client, finding their pain points, and I always live for the moment a data-driven solution presents itself that solves real life problems"
Another Day in the Life of a Data Scientist - KDnuggets,"Deep tech days. 30% of my days and all of my nights are in development. Given my business, I'm now closer to software development than just data science, but all of the applications we develop are data products - whether it's a scoring app that can improve lead conversion, to a customer segmentation and sentiment analysis dashboard that feeds off social media. Lately, we've been getting a few more esoteric requests such as chatbots, imaging lead generation, and document classification - and this part really excites me because it means the data science is maturing to concrete applications. Data engineering is more than just moving tables from one place to another, it's creating data products and crystalizing solutions for people"
Another Day in the Life of a Data Scientist - KDnuggets,"Speaking days. I get invites to do a keynote here and there, and lately that's 5% of my days. The topics that I enjoy most are about disruptive innovation from data and how data and technology always triggers cultural change in people. I take this time to build networks. Tip to data speakers: try to avoid powerpoints and show people real work like an app or website and just talk from it. The audience is bored enough with every other speakre already and they will thank you for showing them something real"
Another Day in the Life of a Data Scientist - KDnuggets,"How did I get into Data Science? I went to engineering school where I specialized in finance and then worked for 3 years in a trading room. Algorithmic trading was the area I was most interested in and it turns out to be very similar with AI. After following Andrew Ng's MOOC, I took a training course in Data Science at École Polytechnique. At this time, I met Gilles Moyse and decided to join his NLP startup,"
Another Day in the Life of a Data Scientist - KDnuggets,"No working day is the same at Recital except for the pleasure of working there and solving new problems. Therefore, the best way I can tell about my daily basis is to describe the typical way to solve a problem. First you must think from the client/business point of view, understand what the goal is so you can ask the right questions as what are the best metrics for instance. Sometimes, you will maximize the recall, sometimes the precision. Once the problem is well posed, I read research papers on the topic to be aware of the state of the art. It can be frustrating but a good old TFDIF often works better than the hippest seq2seq. It is my job to pick in my toolbox the one that fits the problem the best. Then I implement and adapt the algorithms. Preprocessing is often the largest part of the code. And finally, I evaluate the results. Most of the time, I must iterate over those steps because of something I didn’t think about till I find a good solution. I don’t say “the solution”: there are always different approaches that all works well!"
Another Day in the Life of a Data Scientist - KDnuggets,"So I would say that my typical day is “read / think / write / think / read again / think / write”. And by write, I mean write code or research papers. By the way, we are writing a paper about question generation and are expecting to release the biggest French dataset of QA in first quarter 18’. What I love the most in my job is that I keep learning every day while contributing to solve real world problems"
Another Day in the Life of a Data Scientist - KDnuggets,"Hi! First I'll take this paragraph to tell you something about myself. I'm a Physicist and computer engineer. I hold a Master's Degree in Physical Sciences from UNAM. I've been working for several years in Big Data, Data Science, Machine Learning and Computational Cosmology. I have a passion for science, philosophy, programming and data science. Since 2015, I've collaborated with Apache Spark in the Core and the MLlib library. I'm the Chief Data Scientist at Iron performing distributed processing, data analysis, machine learning and directing data projects for the company. In addition, I work at BBVA Data & Analytics as a data scientist, performing automatic learning, data analysis, maintaining the life cycles of projects and production models with Apache Spark"
Another Day in the Life of a Data Scientist - KDnuggets,"So let's get into business. I believe that this is not the first time that someone will write that some job has no typical day. But in the data science world, this is really accurate"
Demystifying Data Science - KDnuggets,"Statistics is the scientific collection, organization, analysis, and interpretation of data. IT involves the development, maintenance, and use of computer systems, software, and networks. Based upon the colloquial usage, I would define Data Science as a catch-all term for everything to do with data. DS = IT + Stat. AI is software to support and make decisions"
Demystifying Data Science - KDnuggets,"We need to define the fields of application by their problems, rather than types of tools. There are two broad and distinct sets of data problems: managing the data (data management; IT Data Science) and extracting information from it (data analysis; Statistical Data Science). Supplying one name for two completely different fields has led to the misunderstanding that the skills, thinking, and software are compatible, even transferable. Instead and contrary to claims by talking heads, these fields are contradictory, yet complementary"
Demystifying Data Science - KDnuggets,"An Applied Statistician/Statistical Data Scientist collects, organizes, analyzes, and interprets data in the field. IT Data Scientists develop, maintain, and leverage computer systems, software, and networks. A Data Scientist does one or the other, and not both"
Demystifying Data Science - KDnuggets,"The best way to become a Statistical Data Scientist is to earn a quantitative degree and then work with applied statisticians in the field; read the applied books; learn the field of application; master the software; work toward a PSTAT; and build the required professional skills. Right now, we are seeing a flood of statistical malfeasance in the field because the software has become user friendly without corresponding protections. One tell-tale sign that there is a massive problem is the increase in"
Demystifying Data Science - KDnuggets,"The ability to recognize and solve advanced statistics problems varies by country and by industry. Based on biased observations from my workshops, it is difficult to confirm the expectation that countries with more technically trained staff have an advantage. It is clear that the third world is watching this technology closely and is capable of learning quickly. Industries tend to master a few advanced statistical problems and not others. Manufacturing is strong at Quality Control, Process Control, and Design of Experiments. They do not use predictive modeling very often. Banking is great at predictive modeling and does not use Quality Control, Process Control, or Design of Experiments very often. For the most part though, management is too political to make fact-based decision making the overriding priority"
Demystifying Data Science - KDnuggets,"It seems corporations are held back by two things: culture and integration. My book addresses how to change your culture and I spent a chapter (CH 6) explaining how to plan for and integrate data science/business analytics/applied statistics into the business. When I discuss this in workshops, I see that organizations have far to go"
Demystifying Data Science - KDnuggets,"I see AI as decision-making software. Chess computers are examples of early AI. Automation is the replacement of human work with machines or software. The evolution of automation has developed within factories for a very long time. The IoT is the networking of machines working cooperatively. This is the next phase in automation.  The internet is, perhaps, the first example; now many different types of machines will be added to ‘the collective"
Demystifying Data Science - KDnuggets,"All three have tremendous promise, both utopian and dystopian. On the utopian side, they can replace human toil, end hunger, etc. On the dystopian side, they might undermine equality, freedom, etc. AI should have much greater ramifications. Combining AI with automation and IoT gives it legs. All three should help grow both the IT and Statistical sides of decision science initially. In the distant future, AI will replace much of IT Data Science and then Statistical Data Science. However, by this time AI is set to replace just about everything else too"
Demystifying Data Science - KDnuggets,"I believe the current trajectory is for AI to be used to advance private interests ahead of the greater public ones, and to oppress and manipulate large populations. One hopeful approach toward greater equality is to insist that AI be developed and transparently managed in the public sector. That said, technology can beget serendipity. Triremes and muskets unexpectedly enabled greater equality. By educating the world population, AI could discourage selfishness and conflict"
Demystifying Data Science - KDnuggets,"Automation is replacing jobs faster than new ones can be created. The pace of this is accelerating. Our current consumer-driven crony capitalism depends on people buying things, often things that they do not need and cannot afford. Past consumption was facilitated by loaning money to consumers. Possible solutions include Guaranteed Minimum Income and a massive jobs program, as in FDR’s New Deal. Alternatively, there will be massive unemployment; a great deal of social unrest with a possible violent revolution; and a deepening of the current economic depression"
Demystifying Data Science - KDnuggets,IoT is set to remove human intervention from many activities. E. The combine pays with its credit card. Initially there will be grave dangers. We will employ IoT as soon as it is profitable and before we can anticipate many downstream ramifications—just like every technology before it
Yet Another Day in the Life of a Data Scientist - KDnuggets,"Having the ability to adapt and embrace the unexpected with an open mind is key to being a Data Scientist. You should anticipate projects not working as expected, being put on hold, or being dropped all together as business needs change. Rather than viewing these events as failures, a good Data Scientist embraces these as opportunities to learn and developed new skills. In this field, opportunities like these are endless.  Here is what a typical day looks like"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"Today was a typical day, I arrived in anticipation of my weekly meeting with the Vice President of Data Science. I provide a brief overview of the progress on current projects and then address pending issues. Today, I highlighted a few forecasting issues with times series models intended to impute metric values when there are issues tracking traffic on our site or when the site is down"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"Mornings are the fun part of the day where my time is spent on my projects. The initial stages of most projects involve a good amount of cleaning and transforming data, the rest being research and development primarily focused in statistics and software development. Today I spent the rest of my morning taking insights from our meeting and additional research on time series analysis to recognize the revisions required to resolve forecasting issues and proceed with next steps"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"I allocate sometime in afternoon to meet with product teams and other departments to understand current challenges and seek out new projects where I can apply my expertise. This can involve evaluating the effectiveness of products, problems with data, and suggestions on next steps. Being involved in these conversations allows me to do my job more effectively by minimizing the number and complexity of issues down the road"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"I try to devote the final hours of the day to keeping myself current and learning new skills in the field of Data Science, though in today’s case I had to make some time for a last-minute team meeting, where we were notified of changes in business needs, resulting in one of my projects being place on an indefinite hold. As a Data Scientist, it is essential to have the ability to adapt and embrace the unexpected, be ready to take on the next task with an open mind. You should anticipate projects not working as expected, having your projects be put on hold, or the projects being dropped all together as business needs change. Rather than viewing these events as failures, a good Data Scientist embraces these as opportunities to learn and develop new skills. In this field, opportunities like these are endless"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"I have a rather peculiar daily schedule, but here’s some insight into what I do and why I do it. Every day starts the same -  I wake up, eat a banana, drink a glass of water, and get a cup of coffee (Gevalia or Starbucks). The banana and water started years ago to prevent cramping during sports and has since turned into a habit over time. The coffee is my jet fuel that helps me get in the “zone”"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"As soon as I’m ready, which can be as early as 4:30AM or as late as 8AM, I begin to work. My start is always the same. I create my to-do list. I review my to-do list against my goals. I ask myself which items help me achieve my goals. Those that do get prioritized to the top of the list. Those that don’t are either removed or pushed to the bottom. Then I begin"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"My day can go in a number of different directions, but generally it’s split by my three main areas of interest: consulting, educating, and creating. Consulting runs the business, so it usually gets priority. I’ll schedule calls with clients, work on projects, do preparation for meetings, and so on. The preparation is not super thrilling, but the projects become very exciting when we help a client solve a complex problem"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"Education is my second focus. I spend a lot of time in two areas: writing and building our University. Our blog is where Business Science got its start, identifying cool topics that matter and showing how data scientists can use their skills to solve these challenges. I love the process - finding an interesting topic, researching it to expose details that often go overlooked, and implementing cutting-edge technology. People are often surprised when they find out an article can take 2 or 3 weeks to put together, but I spend that much time off-and-on because of the research and my own personal learning"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"I work with my team creating the storyline, developing the problem, implementing the data science, and creating interactive web applications. My team is great. They are super talented in many different skills, and they are all striving toward one mission: to educate. The culmination of our hard work is coming soon. We are preparing our first two courses as we speak, and I couldn’t be more impressed with the direction we are going"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"The third and final area of focus is creating. We create tools and software to help data scientists do great things, and it’s our way of giving back to the community. My software manager, Davis, has since taken over responsibilities, and he’s taking this area to a new level. Together we’ve created four amazing, open source software packages for R: tidyquant, timetk, sweep and tibbletime. They are all packages that we built to help us perform analytics, and we open sourced them so you can use the same tools!"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"I hope this gives you a better understanding on what I do and why I do it. I’m really passionate about data science. The crazy thing is there’s a lot of untapped potential just waiting be found. If you’re on the fence, jump in. Last, I love feedback and feel free to connect with/contact me"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"I am happy to be a part of the Big Data Program here at the Bank. It is a small, tightly knit team with distinct cross-cutting functionality. One day we may be working on monitoring and predicting electricity outages, the next one would be about sourcing data from our partners to measure employment. Things are developing rapidly, and it is important to be able to handle a wide variety of work"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"It is the weekly cycles that define our work, that is why it feels very non-linear to me. I arrive at 9 am to either follow up on important emails or dedicate time to technical projects. In either case, this lasts until noon. Depending on the scheduling, I proceed to meetings with other teams, our internal clients. We serve them to enable data science capabilities within their units. Many are economists and have worked with data their entire careers. We present new methods and sources of data to add to the traditional survey and other toolboxes. The meetings sometimes require physical presence, travel, or using Webex to connect with country offices. Then is the time to grab lunch in our cafeteria with most of the world’s cuisines to choose from"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"The late afternoons are for the technical work. Ultimately, this is when I get most of my R&D done. Given the nature of our projects, we have to be able to build prototypes in different domains and using a variety of tools. A successful pilot project in country A may be scaled up to a few more next year, and potentially globally too. My background has been in text analytics and GIS. However, many other fields have come up since I started. A particularly interesting one was network science and graph theory"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"I like to end the day by going to our fitness center. So far I have been on a streak going to instructor-led classes. Before a week’s end, I try to go to the sauna. This is the best part of the gym, especially during the winter"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"My day at Kpler, Paris starts at 9:30, and at first, I usually go over all the systems (especially some upstream data collection and filtering pipeline) and ensure that everything is working properly. Although we have automated alerts and monitoring in place, checking manually time to time and looking into the granular stats helps a lot to identify potential bottleneck and issues and creating game plans to tackle them. We also collect and analyze logs from all our systems to a great detail, and I often look into them to find out some odd corner cases that we never expected and thus did not cover while writing the unit tests. Mornings also involve meeting some days. Although, we have a light meeting culture normally"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"We are in the energy market, and we track thousands of seafaring large vessels containing expensive cargoes with precision and in real time, and as a senior data engineer and a member of the core team, I have already worked on creating a scalable and easily accessible data pipeline using Amazon S3, Redis, Python and AWS Lambda (the upstream system mentioned earlier) to make this process smooth and easy. Now I am actively involved in introducing some new business objects in the system. This means I usually analyze the business problems we are facing presently and suggest approaches/algorithms to overcoming them. A good part of my day is usually spent writing complex SQL queries and trying to make sense of the data returned. I also do some quick POC/Data exploration using Python and Jupyter. And often you may find me writing 100s of lines of mission-critical Python code in our production systems as well"
Yet Another Day in the Life of a Data Scientist - KDnuggets,"We have a good code review system in place for all the PR we open in Github across all our repos. And sometimes I spend time reviewing PR from other teammates and providing my feedback to them, and reading comments others have left while reviewing any of my PR and then either incorporate the changes that they suggested or commenting to explain or asking for more details. Task management in JIRA and other issue tracking system is also a part of a working day as well"
"Data Science in 30 Minutes: A Conversation with Gregory Piatetsky-Shapiro, President of KDnuggets, Jan 11 - KDnuggets","Previously, he worked as a data scientist (Foursquare), Wall Street quant (D.E. Shaw, J.P. Morgan), and a rocket scientist (NASA). He completed his PhD at Princeton as a Hertz fellow and read Part III Maths at Cambridge as a Marshall Scholar. At Foursquare, Michael discovered that his favorite part of the job was teaching and mentoring smart people about data science. He decided to build a startup to focus on what he really loves"
8 Ways to Improve Your Data Science Skills in 2 Years - KDnuggets,"Two years. Two years is the maximum amount of time you should spend focused on your learning, education and training. That’s exactly why this guide is focused on honing the most beneficial skills in two years"
8 Ways to Improve Your Data Science Skills in 2 Years - KDnuggets,"But that doesn’t mean you have to know or be involved with everything. Therein lies the rub. Step one is admitting to yourself that you won’t be able to specialize in every skill a data scientist can use. Yes, you need the core skills — analytics and data processing, mostly — but everything else is more of an aside"
8 Ways to Improve Your Data Science Skills in 2 Years - KDnuggets,"So? Narrow down your focus. Choose the skills you use most and the ones which will be most beneficial to your career. If your plan is to work with Python or Hadoop-based systems, for instance, learning other languages can improve your knowledge base but won’t really help you improve your core skills"
8 Ways to Improve Your Data Science Skills in 2 Years - KDnuggets,"Two years. Two years is the maximum amount of time you should spend focused on your learning, education and training. Yes, you can enter the workforce before then if you’re new to the industry, but the point is you never want to go more than two years without actual field experience"
8 Ways to Improve Your Data Science Skills in 2 Years - KDnuggets,That’s exactly why this guide is focused on honing the most beneficial skills in two years. The goal is to dedicate about fifteen to twenty hours per week to developing your knowledge and skills. There are several ways to do this:
8 Ways to Improve Your Data Science Skills in 2 Years - KDnuggets,It’s important to understand that you will never become an expert in two to three years no matter how hard you work and how much you learn. It’s just not a feasible goal. But that doesn’t mean you shouldn’t strive to continue growing and improving your skills. Quite the opposite!
The 10 Statistical Techniques Data Scientists Need to Master - KDnuggets,"It is important to understand the ideas behind the various techniques, in order to know how and when to use them. One has to understand the simpler methods first, in order to grasp the more sophisticated ones. It is important to accurately assess the performance of a method, to know how well or how badly it is working. Additionally, this is an exciting research area, having important applications in science, industry, and finance. Ultimately, statistical learning is a fundamental ingredient in the training of a modern data scientist. Examples of Statistical Learning problems include:"
The 10 Statistical Techniques Data Scientists Need to Master - KDnuggets,"We did a lot of exercises on Bayesian Analysis, Markov Chain Monte Carlo, Hierarchical Modeling, Supervised and Unsupervised Learning. This experience deepens my interest in the Data Mining academic field and convinces me to specialize further in it. Recently, I completed the"
The 10 Statistical Techniques Data Scientists Need to Master - KDnuggets,"Pick any 2 things that you use in your daily life and that are related. Like, I have data of my monthly spending, monthly income and the number of trips per month for the last 3 years. Now I need to answer the following questions:"
The 10 Statistical Techniques Data Scientists Need to Master - KDnuggets,"Classification is a data mining technique that assigns categories to a collection of data in order to aid in more accurate predictions and analysis. Also sometimes called a Decision Tree, classification is one of several methods intended to make the analysis of very large datasets effective. 2 major Classification techniques stand out:"
The 10 Statistical Techniques Data Scientists Need to Master - KDnuggets,"Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Types of questions that a logistic regression can examine:"
The 10 Statistical Techniques Data Scientists Need to Master - KDnuggets,"Resampling is the method that consists of drawing repeated samples from the original data samples. It is a non-parametric method of statistical inference. In other words, the method of resampling does not involve the utilization of the generic distribution tables in order to compute approximate p probability values"
The 10 Statistical Techniques Data Scientists Need to Master - KDnuggets,"Resampling generates a unique sampling distribution on the basis of the actual data. It uses experimental methods, rather than analytical methods, to generate the unique sampling distribution. It yields unbiased estimates as it is based on the unbiased samples of all the possible results of the data studied by the researcher. In order to understand the concept of resampling, you should understand the terms"
The 10 Statistical Techniques Data Scientists Need to Master - KDnuggets,"Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Thus this method also performs variable selection. The two best-known techniques for shrinking the coefficient estimates towards zero are the"
"AnacondaCON – Harness the Power of Data Science, Austin, April 8-11 - KDnuggets","AnacondaCON is coming to Austin, TX April 8-11. Register now to take advantage of our Early Bird offer of two tickets for the price of one. We’re also offering a bonus 10% off your ticket price if you book a hotel room at the conference site"
Watch the Best of Open Data Science Talks of 2017 for Free - KDnuggets,Here is a selection of some of the highest rated ODSC talks of 2017 as voted by our attendees. Also check out our series of bi-weekly data science and AI webinars. Attend ODSC East 2018 in person and save 70% with code KDNUGGETS!
You have created your first Linear Regression Model. Have you validated the assumptions? - KDnuggets,"For validity of a LR model, the VIF (Variance Inflationary Factor) should not be too high. How high is too high? Well, usually a VIF of 5 is used as threshold. Some even use a 2.5. You can do your own research on this, but almost always a VIF as high as 10 is way too high. A high VIF indicates presence of multicollinearity, that is, high correlation between IVs. You can test your model by removing one or more correlated IVs and re-running the model. You can also use other dimension reduction techniques such as PCA (Principal Component Analysis)"
You have created your first Linear Regression Model. Have you validated the assumptions? - KDnuggets,"Thus, homoscedasticity means that the error terms are similarly (read randomly) distributed. If there is any non-random behaviour in the error terms, such as in the residual Vs fit plot below, then the model is said to be suffering from an ailment called Heteroscedasticity. This can also be detected by the methods such as the ncv test. Transformation of variables using methods such as Box-Cox, or trying out different variables altogether can resolve this issue in a model"
You have created your first Linear Regression Model. Have you validated the assumptions? - KDnuggets,"This is the difference between observed value of the DV(y) and the predicted value (ŷ). The error term is something that keeps us from perfect prediction. Remember that in calculating standard deviation we use {x-x̄}, and in Chi-Square tests we use the {Observed- Expected} value in calculations. Residual analysis is an integral part of several statistical methods, including LR. The assumption for LR is that the residuals must be independent and randomly distributed with a mean of zero, which technically means that the residuals should pass the normality test. This can be tested by extracting the residuals and subjecting to normality tests such as Anderson-Darling Test or Shapiro test, or graphically, with the help of a quantile plot or probability plot. Presence of non-random behaviour indicates that there is some adjustment that needs to be made to the model. Outliers could be the culprit, or that higher order terms are required. Transformations can also come handy in some cases"
You have created your first Linear Regression Model. Have you validated the assumptions? - KDnuggets,"In the previous point, I have stated that residuals must be Independent and normally distributed. When the independence of residuals is violated, we can see patterns such as the one below, or cyclical trends upwards or downwards for a residuals Vs order plot. This can also be detected by applying a Durbin-Watson test on the data. Solutions, again, can be of several types such as using a different model or transforming some of the variables"
You have created your first Linear Regression Model. Have you validated the assumptions? - KDnuggets,"Simple Linear Regression, as the name suggests, is easy to understand and easy to apply. There are several cases, that I have found in my projects, that Linear Regression works quite well in modelling the data. It comes as no surprise that Linear Regression is still one of the most commonly used algorithms for data modelling"
You have created your first Linear Regression Model. Have you validated the assumptions? - KDnuggets,"Ltd. He has 15+ years of experience in Business Analytics in domains such as software, market research, education and supply chain. He is an experienced Six Sigma Master Black Belt and project management professional (PMP) with an educational background in Mathematics and Statistics. He has an active interest in the Data Sciences"
6 Business Trends Benefiting Data Scientists - KDnuggets,Company leaders in various industries are increasingly concerned with tracking the business trends that could impact their profits. They often engage in more activities that need data scientists’ expertise. Here are six trends making data scientists even more in-demand
6 Business Trends Benefiting Data Scientists - KDnuggets,"Data scientists can help business leaders embrace this trend. Making progress requires knowing how a company currently uses data and where weaknesses exist. Once data professionals have those insights, they can help company leaders make the most of the information they have now and monitor its quality"
6 Business Trends Benefiting Data Scientists - KDnuggets,"Data scientists also have welcome expertise about showing the material in ways that don’t mislead people. It’s not enough to present the content in a visually appealing way. Viewers also must interpret the data correctly, even if they only look at it briefly"
6 Business Trends Benefiting Data Scientists - KDnuggets,"It’s no longer effective to show people content intended for the masses. They want more personalization — even when seeing online product advertisements. In one 2020 study, 31% of people"
6 Business Trends Benefiting Data Scientists - KDnuggets,"Real-time data is also vital in manufacturing, especially since unexpected downtime is prohibitively costly. Many brands use predictive analytics tools that give real-time data about machine performance and possible issues to address. Data scientists can help company leaders determine the best ways to use real-time data to meet business goals"
6 Business Trends Benefiting Data Scientists - KDnuggets,Many company leaders no longer see data as an optional part of their operations. They realize that ongoing access to accurate information will aid decision-making and result in a stronger business. Data scientists can evaluate the best ways for a company to become more dependent on data and scale those efforts
6 Business Trends Benefiting Data Scientists - KDnuggets,"However, they cautioned that business leaders often get overwhelmed by data and miss related opportunities. Gartner’s experts suggest involving chief data officers to set goals and strategies. Doing that could result in the consistent production of business value that increases by a factor of 2.6 times, they said"
6 Business Trends Benefiting Data Scientists - KDnuggets,"Data can help business leaders in all industries unlock insights they’d not otherwise have. However, making sense of that information isn’t always straightforward. Data scientists have the knowledge and skills needed to help company decision-makers take advantage of the information at their disposal and use it to succeed"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","The data architect is concerned with managing data and engineering the infrastructure which stores and supports this data. There is generally little to no data analysis needing to take place in such a role (beyond data store analysis for performance tuning), and the use of languages such as Python and R is likely not necessary. An expert level knowledge of relational and non-relational databases, however, will undoubtedly be necessary for such a role. Selecting data stores for the appropriate types of data being stored, as well as transforming and loading the data, will be necessary. Databases, data warehouses, and data lakes; these are among the storage landscapes that will be in the data architect's wheelhouse. This role is likely the one which will have the greatest understanding of and closest relationship with hardware, primarily that related to storage, and will probably have the best understanding of cloud computing architectures of anyone else in this article as well"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","SQL and other data query languages — such as Jaql, Hive, Pig, etc. Verifying the consistency of this data as well as optimizing access to it are also important tasks for this role. A data architect will have the know-how to maintain appropriate data access rights, ensure the infrastructure's stability, and guarantee the availability of the housed data"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","What is the data infrastructure? It's the collection of software and storage solutions that allow for the retrieval of data from a data store, the processing of data in some specified manner (or series of manners), the movement of data between tasks (as well as the tasks themselves), as data is on its way to analysis or modeling, as well as the tasks which come after this analysis or modeling. It's the pathway that the data takes as it moves along its journey from its home to its ultimate location of usefulness, and beyond. The data engineer is certainly familiar with"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","From where does the data infrastructure come? Well, it needs to be designed and implemented, and the data engineer does this. If the data architect is the automobile mechanic, keeping the car running optimally, then data engineering can be thought of as designing the roadway and service centers that the automobile requires to both get around and to make the changes needed to continue on the next section of its journey. The pair of these roles are crucial to both the functioning and movement of your automobile, and are of equal importance when you are driving from point A to point B"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","Truth be told, some the technologies and skills required for data engineering and data management are similar; however, the practitioners of these disciplines use and understand these concepts at different levels. The data engineer may have a foundational knowledge of securing data access in a relational database, while the data architect has expert level knowledge; the data architect may have some understanding of the transformation process that an organization requires its stored data to undergo prior to a data scientist performing modeling with that data, while a data engineer knows this transformation process intimately. These roles speak their own languages, but these languages are more or less mutually intelligible"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","While the previous pair of roles were related to designing the infrastructure to manage and facilitate the movement of the data, as well managing the data itself, data analysts are chiefly concerned with pulling from the data and working with it as it currently exists. This can be contrasted with the following 2 roles, machine learning engineers and data scientists, both of which focus on eliciting insights from data above and beyond what it already tells us at face value. If we can draw parallels between data scientists and inferential statisticians, then data analysts are descriptive statisticians; here is the current data, here is what it looks like, and here is what we know from it"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","Data analysts require a unique set of skills among the roles presented. Data analysts need to have an understanding of a variety of different technologies, including SQL & relational databases, NoSQL databases, data warehousing, and commercial and open-source reporting and dashboard packages. Along with having an understanding of some of the aforementioned technologies, just as important is an understanding of the"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","Machine learning engineers are those crafting and using the predictive and correlative tools used to leverage data. Machine learning algorithms allow for the application of statistical analysis at high speeds, and those who wield these algorithms are not content with letting the data speak for itself in its current form. Interrogation of the data is the"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","Statistics and programming are some of the biggest assets to the machine learning researcher and practitioner. Maths such as linear algebra and intermediate calculus are useful for those employing more complex algorithms and techniques, such as neural networks, or working in computer vision, while an understanding of learning theory is also useful. And, of course, a machine learning engineer must have an understanding of the inner workings of an arsenal of machine learning algorithms (the more algorithms the better, and the deeper the understanding the better!)"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","The data architect and data engineer are concerned with the infrastructure which houses and transports the data. The data analyst is concerned with pulling descriptive facts from the data as it exists. The machine learning engineer is concerned with advancing and employing the tools available to leverage data for predictive and correlative capabilities, as well as making the resulting models widely-available. The data scientist is concerned primarily with the data, the insights which can be extracted from it, and the stories that it can tell, regardless of what technologies or tools are needed to carry out that task"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","The data scientist may use any of the technologies listed in any of the roles above, depending on their exact role. And this is one of the biggest problems related to ""data science""; the term means nothing specific, but everything in general. This role is the Jack Of All Trades of the data world, knowing (perhaps) how to get a Spark ecosystem up and running; how to execute queries against the data stored within; how to extract data and house in a non-relational database; how to take that non-relational data and extract it to a flat file; how to wrangle that data in R or Python; how to engineer features after some initial exploratory descriptive analysis; how to select an appropriate machine learning algorithm to perform some predictive analytics on the data; how to statistically analyze the results of said predictive task; how to visualize the results for easy consumption by non-technical folks; and how to tell a compelling story to executives with the end result of the data processing pipeline just described"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","And this is but one possible set of skills a data scientist may possess. Regardless, however, the emphasis in this role is on the data, and what can be gleaned from it. Domain knowledge is often a very large component of such a role as well, which is obviously not something that can be taught here. Key technologies and skills for a data scientist to focus on are statistics (!!!), programming languages (particularly Python, R, and SQL), data visualization, and communication skills — along with everything else noted in the above archetypes"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","There can be a lot of overlap between the data scientist and the machine learning engineer, at least in the realm of data modeling (read: the process of employing predictive analytics) and everything that comes along with that. However, there is often confusion as to what the differences are between these roles as well. For a very solid discussion of the relationship between data engineers and data scientists, a pair of roles which also can also have significant overlap, have a look at"
"Data Scientist, Data Engineer & Other Data Careers, Explained - KDnuggets","Remember that these are simply archetypes of five major data profession roles, and these can vary between organizations. The flowchart in the image from the beginning of the article can be useful in helping you navigate the landscape and where you might find your role within it. Enjoy the ride to your ideal data profession!"
Unleash a faster Python on your data - KDnuggets,"Use Intel Python with existing code, and you’re all set for a significant performance boost. Easy. Imagine that!"
A checklist to track your Data Science progress - KDnuggets,"Whether you are just starting out in data science or already a gainfully-employed professional, always learning more to advance through state-of-the-art techniques is part of the adventure. But, it can be challenging to track of your progress and keep an eye on what's next. Follow this checklist to help you scale your expertise from entry-level to advanced"
A checklist to track your Data Science progress - KDnuggets,"The following checklist can help you get an overview of your progress. It’s intended for users looking for a general outline of where they are on their journey. Rather than emphasizing specific guides, courses, and software packages, it focuses on general concepts:"
A checklist to track your Data Science progress - KDnuggets,"Classic machine learning techniques include Support Vector Machines, Linear Regression, and clustering algorithms. Even though the more sophisticated relatives, the neural networks, seem to dominate the recent research, they come in handy for small problems — or as a baseline. Knowing how to use them is also handy when it comes to analyzing the data"
A checklist to track your Data Science progress - KDnuggets,"The second commonly used network type is the Convolutional Neural Network, which uses convolution operation at its core. It’s hard to think about any successful research that has not used and benefited from this simple operation: You slide a kernel over your input and then calculate the inner product between the kernel with the patch that it covers. The convolution operation extracts a small set of features from every possible location of the input data. It’s no wonder that they are very successful in classifying images, where important characteristics are scattered all around"
A checklist to track your Data Science progress - KDnuggets,"The previous two network types are mainly used for static inputs, where we know the data’s shape in advance. When you have data of varying shapes, take sentences as an example, you might look for more flexible approaches. That is where Recurrent Neural Networks come in: They can retain information over long periods, which enables connecting the “I” in “I, after getting up this morning and dressing, took a walk with my dog” and the “took a walk with my dog” to answer the question “Who took a walk with the dog?”"
A checklist to track your Data Science progress - KDnuggets,An important step is to get to know the data itself. This is not limited to image data or text data alone but also covers time-series and audio data and any further data type. The term exploratory data analysis (thanks to
A checklist to track your Data Science progress - KDnuggets,"When you begin training your basic networks on those small datasets, with those architectures above, you’ll gradually uncover the helpfulness of callbacks. Callbacks are code that gets executed during training, and they implement many functionalities: Periodically saving your model to make it fail-safe, stopping the training, or changing parameters. The most common ones are already built-in with most libraries, and a short function call is all it takes to use them!"
A checklist to track your Data Science progress - KDnuggets,"At this stage, the datasets tend to become larger and might no longer fit into your RAM. Efficient pre-processing becomes more important, as you won’t let your hardware idle around. Also, you might have to write your own generators when you handle samples of unequal shapes, fetch samples from a database, or do custom pre-processing"
A checklist to track your Data Science progress - KDnuggets,"For both dataset types, custom pipelines become more important. When you quickly want to iterate settings, for example, cropping an image to 32x32 rather than 50x50, you rarely want to start long-running scripts. Incorporating such possibilities into your pipeline enables quick experimentation"
A checklist to track your Data Science progress - KDnuggets,"The first level might have led you through training a network on the MNIST datasets. On this level, you’ll naturally focus more on your own data and how to parse it. I have only listed the three major domains audio, images, and text, but there are many more"
A checklist to track your Data Science progress - KDnuggets,"A first step to customizing training is using transfer learning and fine-tuning. Before, you will usually have used the standard methods to train and test your models, but now you might require something more powerful. That’s where it comes in handy to simply re-use models that other practitioners trained. You load their models and carefully train them on your own datasets"
A checklist to track your Data Science progress - KDnuggets,"In case you miss some feature, this is the point where you begin to write custom training loops and custom callbacks. Want to print some statistics every 20 batches? Write a custom callback. Want to accumulate gradients over 20 batches? Write a custom training loop"
A checklist to track your Data Science progress - KDnuggets,"More complex loops might need more resources: multi-GPU training is coming! However, it’s not simply adding a second, a third, or even more resources; you have to fetch the data fast enough. Keep in mind that multi-device training increases the complexity. Most of the background work is already done by PyTorch and TensorFlow, which handle this case with a few minor code changes only, but the front-end part is left to you"
A checklist to track your Data Science progress - KDnuggets,"If you are like me and do not have access to multiple GPUs, then training on the cloud is a possible next step. All major cloud services provide you with the requested resources. At first, there will be some (mental) overhead while transitioning from local setups to cloud computing. After some attempts, this gets way easier"
A checklist to track your Data Science progress - KDnuggets,"When you are already in the cloud, why not try TPU training? TPUs are Google’s custom-made chips. And they are fast: A year ago, I worked with a team to classify a large text corpus, around 20,000 documents, and each document had about 10 pages of text. We ran our first experiments on a CPU only, and one epoch took 8 hours. In the next step, we improved pre-processing, caching, and used a single GPU: The time went down to 15 minutes, which was a huge leap. After reading TensorFlow’s documentation and playing around with our code, I then managed to make TPU training possible, and one epoch went down to"
A checklist to track your Data Science progress - KDnuggets,"Now that you are dealing with custom datasets, it becomes important to get a grasp on problem thinking. Let me explain it this way: Say you are working with an audio dataset. You have done your initial data analysis, and it turns out that your data is not only imbalanced, but the samples are of different lengths. Some are only three seconds long, others 20 seconds and more. Further, you want to classify only segments of the audio, not the whole clip as is. And lastly, this is for an industry project, so restrictions apply. Bringing all this together is what’s required from you"
A checklist to track your Data Science progress - KDnuggets,"Luckily, you are not alone at this. Data analysis is ticked already, and pre-processing custom data is ticked too. It’s the industrial part that gets the main attention here. Check what other folks have done (and published on GitHub), ask your people, and experiment with different techniques"
A checklist to track your Data Science progress - KDnuggets,"Say you want a network layer that normalizes your data following a custom scheme. Looking into your library’s documentation, you don’t find anything similar already implemented. Now is the time to implement such functionality yourself. As before,"
A checklist to track your Data Science progress - KDnuggets,"For a project a while ago, I wanted to write a custom embedding layer. After finding no implementation, I decided to tackle this by writing a custom one. Now, that was a challenge! After many trial and error loops, I finally came up with a working TensorFlow layer"
A checklist to track your Data Science progress - KDnuggets,"Another large field in Deep Learning opens up now: Generative networks. Before, you mainly worked with classifying existing data, but nothing is holding you back from also generating it. That’s exactly what generative networks do, and a large part of their recent success comes from one single paper:"
A checklist to track your Data Science progress - KDnuggets,"Besides only using a wider variety of models, you’ll also want to track your experiments. This is helpful when you want to comprehend how your model’s metrics are influenced by its parameters. Speaking of which, you might also start a parameter search, which aims to find the best set of parameter which optimizes a target metric"
A checklist to track your Data Science progress - KDnuggets,"As in the last part, you will definitely use more advanced models. Besides those generative networks, there are also huge language models. Have you ever heard of Transformers (not the movie)? I bet you have, and now comes the time to use them for your own projects. Or try getting access to GPT-3, and discuss AI with it"
A checklist to track your Data Science progress - KDnuggets,"When working with such enormous datasets, it might be required to run a multi-worker training setup. The GPUs are no longer installed on a single machine but distributed across multiple machines, the workers. Managing the distribution of your workload and your data becomes necessary. Thankfully,"
A checklist to track your Data Science progress - KDnuggets,"When you have also done that, get into Reinforcement Learning. Placing it in the advanced section might not be 100 percent correct (the basics can be grasped quite early), but it’s mostly distinct from the fields you have worked on so far. As always, there are"
A checklist to track your Data Science progress - KDnuggets,"Now that you have gained some vast experience and knowledge, it’s time to do research. Have you noticed anything on your journey that could be improved? That could be the first thing to work on. Or contribute to other research projects by providing code to their repositories. Collaborate with other enthusiasts and keep learning"
A checklist to track your Data Science progress - KDnuggets,"That might be the last thing that’s left here: Staying up-to-date. Data Science is a fast-moving field, with many new exciting things coming up day after day. Keeping track of what matters is hard, so choose some newsletters (I read Andrew Ng and deeplearning"
A checklist to track your Data Science progress - KDnuggets,"Yes, definitely. There are no distinct dividing lines between some items from different categories. For example, when you work with images, you might have stored them on disk, and a CSV file holds all the file paths and labels. You can thus tick two items in one go"
A checklist to track your Data Science progress - KDnuggets,"On the other days, you can do your studies. Take a course on Wednesday, on Thursday, and Friday. Studying on the weekends is a bonus"
Animated Bar Chart Races in Python - KDnuggets,You can save the bar graph by downloading from the download option at the bottom of the image. Hope you have enjoyed this fun tutorial. Enjoy and Keep Learning :)
Awesome list of datasets in 100+ categories - KDnuggets,"With an estimated 44 zettabytes of data in existence in our digital world today and approximately 2.5 quintillion bytes of new data generated daily, there is a lot of data out there you could tap into for your data science projects. It's pretty hard to curate through such a massive universe of data, but this collection is a great start. Here, you can find data from cancer genomes to UFO reports, as well as years of air quality data to 200,000 jokes. Dive into this ocean of data to explore as you learn how to apply data science techniques or leverage your expertise to discover something new"
Awesome list of datasets in 100+ categories - KDnuggets,"A corpus of web crawl data composed of over 50 billion web pages. The Common Crawl corpus contains petabytes of data collected since 2008. It contains raw web page data, extracted metadata and text extractions"
Awesome list of datasets in 100+ categories - KDnuggets,"The Therapeutically Applicable Research to Generate Effective Treatments (TARGET) program applies a comprehensive genomic approach to determine molecular changes that drive childhood cancers. The goal of the program is to use data to guide the development of effective, less toxic therapies. TARGET is organized into a collaborative network of disease-specific project teams. TARGET projects provide comprehensive molecular characterization to determine the genetic changes that drive the initiation and progression of childhood cancers. The dataset contains open Clinical Supplement, Biospecimen Supplement, RNA-Seq Gene Expression Quantification, miRNA-Seq Isoform Expression Quantification, miRNA-Seq miRNA Expression Quantification data from Genomic Data Commons (GDC), and open data from GDC Legacy Archive"
Awesome list of datasets in 100+ categories - KDnuggets,The Pubmed Diabetes dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words. The README file in the dataset provides more details
Awesome list of datasets in 100+ categories - KDnuggets,"It contains 315 drugs, 250 targets, 1,306 drug-target interactions, 5 types of drug-drug similarities, and 3 types of target-target similarities. Drug-drug similarities include Chemical-based, Ligand-based, Expression-based, Side-effect-based, and Annotation-based similarities. Target-target similarities include Sequence-based, Protein-protein interaction network-based, and Gene Ontology-based similarities. The original task on the dataset is to predict new interactions between drugs and targets based on different types of similarities in the network"
Awesome list of datasets in 100+ categories - KDnuggets,"The data available through NatureServe Explorer represents data managed in the NatureServe Central Databases. These databases are dynamic, being continually enhanced and refined through the input of hundreds of natural heritage program scientists and other collaborators. NatureServe Explorer is updated from these central databases to reflect information from new field surveys, the latest taxonomic treatments and other scientific publications, and new conservation status assessments"
Awesome list of datasets in 100+ categories - KDnuggets,"The U.S. Department of Transportation’s (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled and diverted flights appears in DOT’s monthly"
Awesome list of datasets in 100+ categories - KDnuggets,"Yahoo is shutting down in 2021. This is Yahoo Answers datasets (300MB gzip) that is fairly extensive from 2015 with about 1.4m rows. This dataset has the best questions answers, I mean all the answers, including the most insane awful answers and the worst questions people put together"
Awesome list of datasets in 100+ categories - KDnuggets,"The AQS Data Mart is a database containing all of the information from AQS. It has every measured value the EPA has collected via the national ambient air monitoring program. It also includes the associated aggregate values calculated by EPA (8-hour, daily, annual, etc. The AQS Data Mart is a copy of AQS made once per week and made accessible to the public through web-based applications. The intended users of the Data Mart are air quality data analysts in the regulatory, academic, and health research communities. It is intended for those who need to download large volumes of detailed technical data stored at EPA and does not provide any interactive analytical tools. It serves as the back-end database for several Agency interactive tools that could not fully function without it: AirData, AirCompare, The Remote Sensing Information Gateway, the Map Monitoring Sites KML page, etc"
Awesome list of datasets in 100+ categories - KDnuggets,"This list of a topic-centric public data sources in high quality. They are collected and tidied from blogs, answers, and user responses. Most of the data sets listed below are free, however, some are not"
Awesome list of datasets in 100+ categories - KDnuggets,"The US Department of Ed has a dataset called the CRDC that collects data from all the public schools in the US and has demographic, academic, financial and all sorts of other fun data points. They also have corollary datasets that use the same identifier—an expansion pack if you may. It comes out every 2-3 years"
Awesome list of datasets in 100+ categories - KDnuggets,Each attack is described by a 0/1-valued vector of attributes whose entries indicate the absence/presence of a feature. There are a total of 106 distinct features. The files in the dataset can be used to create two distinct graphs. The README file in the dataset provides more details
Awesome list of datasets in 100+ categories - KDnuggets,"This dataset contains information about terrorists and their relationships. This dataset was designed for classification experiments aimed at classifying the relationships among terrorists. The dataset contains 851 relationships, each described by a 0/1-valued vector of attributes where each entry indicates the absence/presence of a feature. There are a total of 1224 distinct features. Each relationship can be assigned one or more labels out of a maximum of four labels making this dataset suitable for multi-label classification tasks. The README file provides more details"
Awesome list of datasets in 100+ categories - KDnuggets,"This network dataset is in the category of Social Networks. A social network of bottlenose dolphins. The dataset contains a list of all of links, where a link represents frequent associations between dolphins"
Best Python Books for Beginners and Advanced Programmers - KDnuggets,"The internet is filled with loads of convenient mediums to learn everything about Python. Although we discussed the more widely adopted mediums earlier, books still remain an all-time hot favorite among learners. The thing that makes books so much popular among Python learners is that they allow the reader to absorb and practice the content at their own time and pace"
Best Python Books for Beginners and Advanced Programmers - KDnuggets,"The latest edition of the book updates all the included code examples with their Python 3 counterpart to offer learners the most up-to-date learning experience. The book also comes with a number of exercises, case studies, and detailed explanations about the topics. Some key highlights of “Think Python” are listed below:"
Best Python Books for Beginners and Advanced Programmers - KDnuggets,"The author targeted this book to be used as a college-level textbook, and for that, the author has taken more of a traditional approach to teaching Python by focussing on problem-solving, program design, and programming as core skills. So, if you are a college student and want to learn Python, we recommend giving this book a try. Key highlights of this book are listed below:"
Best Python Books for Beginners and Advanced Programmers - KDnuggets,"The latest edition of the book covers all the need-to-know content from Python 3.4 and 2.7 while also covering the difference between the two versions. Mark’s pocket reference is targeted towards developers with some experience with Python programming. The book covers several topics, such as:"
Best Python Books for Beginners and Advanced Programmers - KDnuggets,"Learning Python can open a slew of lucrative opportunities for you. If you are eager to snag one of the most trending and high-paying job profiles of the century, we recommend getting a solid understanding of the concepts in Python. The books we covered in this article do an exceptional job explaining even the most intricate Python topics in a reader-friendly manner"
"Data Preparation in SQL, with Cheat Sheet! - KDnuggets","For many organizations, the answers to those questions are going to lead to SQL. Not only is SQL widely known and used in most organizations, but it also leverages existing database resources, security, and pipelines. If your raw data is in a SQL-based data lake, why spend the time and money to export the data into a new platform for data prep?"
"Data Preparation in SQL, with Cheat Sheet! - KDnuggets","A final word on creating an interface to your model. SQL views allow you to wrap up the complexity of many of the data prep steps in a clean, secure, modular format. Rather than embedding long, complex queries in your Python or R code, you can create a view that allows access to that code in a simple, reusable format. Views are also a great way to apply security on private data elements by masking or hiding those from model access"
A Day in the Life of a Data Scientist - KDnuggets,"Then I read my emails received during the night and react if necessary. After that I work on my current project, which currently is a salary extractor from job announcements. I need to create a separate pair of models for each pair country-language we support (around 30 of country-language pairs). The process consists of dumping the job announcements for a certain part country-language, clustering them, then getting the subset of training examples. Then I annotate these examples manually and build the model. I iterate build/test/add data/rebuild until the test error is low enough (~98%)"
A Day in the Life of a Data Scientist - KDnuggets,"In the afternoon, I help my team members to improve their models by testing the current model on the real data, identifying the false positives/negatives and creating new training examples to fix the problem. The decision when to stop improving the model and deploy in production depends on the project. For some cases, especially user-facing, we want a very low level of false positives (less than 1%): the user always see that the extraction of some element from their text was wrong, but not always remark the lack of extraction"
A Day in the Life of a Data Scientist - KDnuggets,"I switched into data science and machine learning during an MD/PhD program after a joint humanities and sciences undergraduate degree, and my day-to-day projects are highly interdisciplinary as a rule. Some projects include simulating epidemic spread, leveraging industrial psychology to create better HR models, and dissecting data to obtain risk groups for low socioeconomic status students. The best part of my job is the variety of projects and a new challenge every day"
A Day in the Life of a Data Scientist - KDnuggets,"A typical day for me starts around 8:00 am, when I catch up on my social media accounts related to machine learning and data science. I switch into work projects around 8:30 am and finish around 4:30 pm to 5:00 pm with a break for lunch. About 40% of my time is spent on research and development, with a strong focus in mathematics (topology, in particular)--involving anything from developing and testing new algorithms to writing mathematical proofs to simplify data problems. Sometimes, the results are confidential and stay within the company (shared through monthly Lunch & Learn presentations within the company); other times, I'm allowed to publish or present at external conferences"
A Day in the Life of a Data Scientist - KDnuggets,"Another 30% of my time is spent building relationships across departments at my company and seeking new projects, which often identify problems related operating procedures, problems related to data capture, or connections between previous projects that provide a more comprehensive view of operations. This is probably one of the most crucial aspects of the job. People I meet often bring up problems they are seeing or mention how neat it would be to have a predictive model for sales/student outcomes/operations, and I've found it opens the door to conversations and best practice suggestions down the road. As a data scientist, it's important to communicate with a wide range of stakeholders, and it's helped me simplify my explanations of machine learning algorithms to a layman's level"
A Day in the Life of a Data Scientist - KDnuggets,"The remaining 30% of my time is typically spent on data analysis and writing up results. This includes forecast models, predictive models of key metrics, and data mining for subgroups and trends within a given dataset. Each project is unique, and I try to let the project and its initial findings guide me to next steps. I mainly use R and Tableau for projects, though Python, Matlab, and SAS are occasionally helpful with specific packages or R&D requests. I can usually recycle the code, but each problem has its own assumptions and data limitations with respect to the mathematics. Projects can usually be simplified using tools from topology, real analysis, and graph theory, which speeds up the project and allows for the use of extant packages, rather than a need to code from scratch. As the only data scientist at a large company, this allows me to cover more projects and uncover more insight for our internal customers"
A Day in the Life of a Data Scientist - KDnuggets,"A bit of context is necessary to explain why we do so. My current role is a data scientist in a team implementing Big Data Analytics in a southeast Asian Bank. We have data engineers, admin/ infrastructure people, data scientists and of course customer engagement managers in the team catering to each specific need of the project. My current organization is an AI startup named"
A Day in the Life of a Data Scientist - KDnuggets,AMP is quite cool and I will come to it soon. This leads to the focus of my startup to get as many clients as possible as well as test and implement out our Big Data Product. This means demonstrating success in our client engagements- one of our client was shortlisted for an award last month. Am I sounding too marketing oriented- you bet I am. The work a data scientist does is usually of a strategic consequence to the client
A Day in the Life of a Data Scientist - KDnuggets,"What do I do on a daily basis? It could be many things - including not just emails and meetings. I could be using Hive to pull data, using it to merge data (or using Impala), I could be using PySpark (Mllib) to make churn models or do k means clustering. I could be pulling data in an excel file to make summaries and I could be making data visualizations. Some days I prototype in R using some machine learning packages. I also help with testing of AMP, our Big Data Analytics product and work with that team for feature enhancement of the product (if you forgive the pun- since the product is used for feature enhancement). When I code Big Data, I could be using the GUI for Hadoop HUE or I could be using command line programming including batch submitting of code"
A Day in the Life of a Data Scientist - KDnuggets,"Our client was India’s Ministry of Finance (the arm that deals with taxes). Junior data scientists pulled data using SQL from an RDBMS (due to legacy issues), and I validated the results.The reports were then sent to the various clients. On an ad-hoc basis we also used SAS Enterprise Miner as a concept test to show time series forecasts of imports and exports for India. Timelines are quite slow and bureacratic when working for a federal government vis a vis working for the private sector. I remembered one presentation when the bureaucrat in charge was astonished we were executing machine learning and why the government did not use it earlier. But SAS/VA (for Dashboards),SAS Fraud Analytics (which I trained on and which was in process of implementation) and Base SAS (the analytics workhorse) are amazing software and I doubt how anything resembling SAS Domain Specific Bundles can be made soon"
A Day in the Life of a Data Scientist - KDnuggets,"Prior to this for ten years I ran Decisionstats. I blogged, sold ads (not very good), wrote 3 books in data science, scores of articles for Programmable Web, StatisticsViews and did some data consulting. I even wrote a few articles for KDnuggets. You can see my profile here"
A Day in the Life of a Data Scientist - KDnuggets,"A day in the life at LinkedIn. Well, I think I can say there is no “typical” day. Keep that in mind as you read!"
A Day in the Life of a Data Scientist - KDnuggets,"First, a little bit about me and my major responsibilities. I’m fortunate to work on our LinkedIn Learning team, which is the newest data science group in the organization. Specifically, I support Enterprise level sales for LinkedIn Learning. What does that mean? Think about it like this: we use data, models and analytics to make decisions on how to sell effectively. Of course, the details on how we do that are internal but you can imagine that we want to answer questions like: which accounts do we try to sell into? We work to understand what makes certain accounts stand out from the rest"
A Day in the Life of a Data Scientist - KDnuggets,"Second, a key aspect of everyday is communication. I’ve written about this extensively on LinkedIn but I believe that effective communication with teammates and business partners is a defining characteristic of a great data scientist. On a typical day, this involves providing updates on key projects to both immediate team members, managers and senior leaders, as appropriate. One thing I find fascinating about this aspect of the job is the need for brevity. A company like LinkedIn has tons of internal communication happening so everything that goes out must be distilled into clear and concise results/talking points"
A Day in the Life of a Data Scientist - KDnuggets,"Finally, an important part of each day is failure. I’m a big believer that if you are not failing, you are not learning. This does not mean catastrophic failure of course. It means that each day I work on things that expand my understanding of analytics, data science and the organization itself. I learn from my mistakes and watch how others do things more efficiently or in different ways from me. When I wake up each day, I seek failure as part of the job because it makes me better the next day. Analytics and the rapid pace of expansion of data science sure provides plenty of these opportunities!"
Rebuilding My 7 Python Projects - KDnuggets,"Python is an all-time favorite language for programming enthusiasts like me. I have a keen interest in this language and have been using it for over 2 years. This year I had a lot of spare time to work on my programming skills and developed a lot of projects centered on web development, android app, and Data Science. In this article, I will explain what was the purpose of each project, how I made it, my associated article with that project, and the GitHub repository link. Maybe this can seed a similar project idea in your mind too! Let’s explore these projects"
Rebuilding My 7 Python Projects - KDnuggets,"As a developer, we create hundreds of repositories and hardly few of them actually make it to the final project we showcase on social media/LinkedIn. This GitHub action allows you to generate a self-updating portfolio with Projects, Hackathons, and the latest Blogs. An index file is generated by this action, which with the help of GitHub pages gets deployed as soon as it is committed to the repository. The main purpose of this project was to serve as a helping hand in portraying the skills you have as projects developed by you"
Rebuilding My 7 Python Projects - KDnuggets,"Being a Data Science follower, I am always curious about discovering trends in Data. I always try to look up real-world scenarios where data can be obtained easily, and when I discovered that WhatsApp has the functionality to export group chats, I couldn't resist analyzing the Data. I did it for my college group, was happy with that, but then got the idea that why not develop a generalized web application where anybody can upload their chat file and gather some interesting insights! This is exactly what this project does"
Rebuilding My 7 Python Projects - KDnuggets,"It takes up the exported chats file (without media), cleans it, runs all the stats generation custom functions, and displays it to the user on the go! The file uploaded is deleted as soon as the stats are generated for privacy. This web app displays the total emoji count, its usage over the group members, the activity of members per day, overall and on designated holidays, and some more features too! This is an excellent project for Data cleaning and visualization, or you can come up with a prediction model to predict the next chat of the person based on this data. There are endless possibilities!"
Rebuilding My 7 Python Projects - KDnuggets,"I am currently enrolled in college (3rd year) and pursuing my bachelor's from India. My university releases the semester exam results in form of long PDF’s which are usually cluttered with a lot of irrelevant information for a student. Even the subject names are in coded form, and it becomes difficult to calculate the credit score obtained. Also, predicting the rank of a candidate manually is next to impossible as there are around 6k students in every batch every year. To ease this process, I developed a parsing script that reads these long pdfs (some are close to 400 pages!), stores them in a readable format, applies all the data transformation techniques to obtain grade points, percentage, and ranks at college and university-level!"
Rebuilding My 7 Python Projects - KDnuggets,"Assume an average of 5k records per semester, I have records for the 2017 batch onwards and there are two semesters in a year here. So I probably have 60k records till now! The website also offers a generate profile functionality that pulls up all the previous semester's results. This is one of my biggest projects I have ever made and took around 2 months for complete satisfaction"
Rebuilding My 7 Python Projects - KDnuggets,"As I was successful in developing the website, it was time to expand this functionality to other platforms, and discovered how to make android apps in Python. It was possible because of the amazing library called Kivy and the material design of Kivymd. The has the ability to make requests to the backend API and displays as results in tabular form. It took a lot of time to have a good grasp of this library but it was worth it"
Rebuilding My 7 Python Projects - KDnuggets,"I think, I went too far with this results project and made a telegram bot too! While developing the first version of the bot, I made a huge mistake to run an infinite loop that checks the new message and this process consumes a lot of resources. When I deployed it for the first time on Heroku, the very next day I got the mail that all my current hours have been consumed and that time I realized that I made a huge mistake. To resolve this issue, I adapted the webhook concept of Telegram that allows messages to be directly redirected to my link whenever it happens"
Rebuilding My 7 Python Projects - KDnuggets,"This project is special for me because, with this project, I won a competition! This is built using Brython that allows you to run Python code on front-end websites without any Flask, Django, or any other server. I made a song lyrics fetcher that makes an API call based on the artist's name and album passed to the website. This was so easy project that I hosted this project on GitHub pages that ensured longer uptime and no recurring costs!"
Rebuilding My 7 Python Projects - KDnuggets,"While discovering the Kivy & Kivymd library, I discovered that I can deploy a machine learning model on android. This was an indirect method as the python-to-android doesn't support sklearn for now. I had to deploy the model as an API on Heroku and then make GET requests to fetch the predictions and display them on the user screen. Though this was a naive approach and many other things could be implemented here but this was all I could think of when I build this project"
Rebuilding My 7 Python Projects - KDnuggets,"This was all about how I upgraded and managed my Python projects this year. I made a lot of other projects too but, these are the top 7 I choose for this article. Making a project is a whole systematic process from idea, to design, code and if applicable, deploying it on the internet. I think I provided some sort of motivation to you, the readers, to come up with your own ideas and show the world your skills"
"The secret to analysing large, complex datasets quickly and productively? - KDnuggets","Data is beautiful, and lots of data is simply sublime, but be wary of the pitfalls. Sometimes you have so much data you can waste hours exploring without answering the important questions. These 5 tips will show you how to analyse large complex datasets productively by constraining yourself"
"The secret to analysing large, complex datasets quickly and productively? - KDnuggets","But seriously, sometimes we have more data than we know what to do with. There are worse problems, I’ll admit, but this has been a problem for me during my PhD. I’ve lost entire days digging around in big datasets. Leaving aside the problem of p-hacking (I generally try to validate any results found on a new dataset, and so should you!), this can really eat into your time"
"The secret to analysing large, complex datasets quickly and productively? - KDnuggets","You can also get suckered into this when your goal is to “get to know the data inside and out. This is valuable, but it’s not necessary to know every corner of the European Social Survey. It’s often not productive to see how a model is influenced by 10 different covariates that are barely related to the dependent variable. You have to draw the line somewhere and say “enough”!"
"The secret to analysing large, complex datasets quickly and productively? - KDnuggets","These are the questions you will set out to answer, and after, you will stop (at least until you’ve written the paper/chapter). What are the questions that, once answered, could be publishable? Not to be too cynical, but in academia, papers are the name of the game. Papers aren’t usually a collection of interesting observations stapled together. They are focused, and you should be too"
"The secret to analysing large, complex datasets quickly and productively? - KDnuggets","For example, you run model A, and predictor A1 shows an effect that you don’t understand. So you see if A1 correlates with a few other things, and that might explain it. You build model B and eureka!"
"The secret to analysing large, complex datasets quickly and productively? - KDnuggets","Set yourself an amount of time to do it in. Deadlines, even self-imposed ones, are great for clarifying the mind. Having years to do a PhD is not an excuse to waste time. Any data stuff not done after this time will have to wait for another day. It’s not a big deal if you miss something important, as your supervisor/colleagues/reviewers on the eventual paper will find them. You can even come back to the dataset and take a few more days on it later when you’ve got some more time. This also has the advantage of returning to the data refreshed. Give yourself a time limit. Don’t just wander leisurely through the data"
"Feature Engineering of DateTime Variables for Data Science, Machine Learning - KDnuggets","At the outset, this date field gives us nothing more than a specific point on a timeline. But these DateTime fields are potential treasure troves of data. These fields are immensely powerful ‘if used rightly’ for uncovering patterns"
Analyzing the Migration of Scientific Researchers - KDnuggets,"Thanks for reading. If you like it, please share it. Feedbacks are always welcomed"
Why You Should Consider Being a Data Engineer Instead of a Data Scientist - KDnuggets,"We’ve all heard the saying “garbage in, garbage out”, but only now are companies starting to truly understand the meaning of this. Machine learning and deep learning can be powerful but only in very special circumstances. Aside from the fact that there needs to be a substantial amount of data and a practical use for ML and DL, companies need to satisfy the"
Why You Should Consider Being a Data Engineer Instead of a Data Scientist - KDnuggets,"And so, if you’re starting out your career, I personally think it’s more worthwhile investing your time learning data engineering than data science because you have more time to invest. When you’re working a full time job and a couple of years into your career, you might find that you don’t have the capacity or energy to invest as much time in learning. So from that perspective, I think it’s better to learn the harder realm first"
Why You Should Consider Being a Data Engineer Instead of a Data Scientist - KDnuggets,"While this is an opinionated article, I hope that this sheds a bit of light as to why you may want to be a data engineer. I want to reiterate that whether you choose data science or data engineering should ultimately depend on your interests and where your passion lies. As always, I wish you the best of luck in your endeavors!"
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,People in various industries are increasingly interested in using data science to learn more about notable trends that improve their decision-making. It’s also instrumental in predicting events and preventing unwanted consequences. Here are some thought-provoking examples
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,Prevention also becomes possible when data science helps spot outliers. Suppose a system sees that an unknown person with an IP address in a foreign country tried to access a workplace network in the middle of the night. It could automatically deny that individual until a human takes a closer look
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,An organization called Campaign Zero uses a data-driven approach to suggest new policy frameworks for the future of policing. Researchers there reviewed 600 police union contracts. The results showed that 84% of the
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,Data also shows which locations have police departments associated with the deaths of Black men that are higher than the national homicide rate. Another way to apply data analytics is to assess which officers have above-average incidents of misconduct or complaints lodged against them. Police chiefs could then make departmental decisions about interventions that could predict and prevent unnecessary violence
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,"Difficulties in finding new, high-quality mineral deposits mean companies often engage in costly exploration efforts that fall short of expectations. Data science could help with that, as well as monitor ventilation systems at established mines. Workers would stay safer while companies avoid excessive costs"
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,Many people can relate to the scenario of joining a gym and making good on promises to get fit but becoming less enthusiastic over time. Data science can predict what causes a loss of motivation and prevent it from happening. It can also assist in making personalized recommendations for members
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,"Records may indicate that a particular member has only attended yoga sessions, and their attendance has gradually become less consistent. The system could recommend that gym staff inform that person about a class that combines yoga with brief periods of intensive cardio. That suggestion could raise interest by presenting a different opportunity"
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,"Such a tool might also show broader trends, such as which months most members end their subscriptions. If the data indicates it usually happens in March, a club could offer extra incentives to reduce the chances of people leaving. It could also predict the likely interest in a new, specific type of class, such as chair workouts for older or injured people"
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,"That approach can also send the appropriate quantities of merchandise to the right locations, such as chain stores with national outlets. The data may show a rapid increase in people wanting workout clothes in Colorado, while such sales decline or remain flat in Arkansas. Retailers could use that information to keep stores adequately stocked"
Using Data Science to Predict and Prevent Real World Problems - KDnuggets,"Some large-scale festivals, such as the United Kingdom’s Glastonbury, have also relied on data science to predict outcomes and prevent disappointment. Researchers examined data to see whether a specific year was likely to have more or less rainfall than usual. That information helped retailers know what kind of merchandise to bring with them. The same approach enabled vendors to determine the most popular food and beverages so they could be better prepared"
Data science is not about data – applying Dijkstra principle to data science - KDnuggets,"He had deep insight into what computer science is and a well-founded notion of how it should be taught in academics. In this post, we extrapolate his ideas into data science. We develop something called the"
Data science is not about data – applying Dijkstra principle to data science - KDnuggets,"Indeed, it is about how the universe works and how its constituent parts are interacting. Telescopes, either being optical or radio observations or similar detection techniques, are merely tools to practice and do investigation for astronomy. A formed analogy goes into computer science as well, and this is the quote from Dijkstra:"
Data science is not about data – applying Dijkstra principle to data science - KDnuggets,"This sounds absurd. If data science is not about data, then what is it about? Apart from the definition of data science as an emergent field, as an amalgamation of multiple fields from statistics to high-performance computing,  the idea that data not being the core tenant of data science implies the practice does not aim at data itself rather a higher purpose. Data is used similar to a telescope in astronomy, and the purpose is to reveal the empirical truths about the"
"KDD-2021, The premier Data Science Conference, Aug 14-18, Virtual - KDnuggets","14-18. The premier interdisciplinary data science conference, KDD 2021 will bring together researchers and practitioners from data science, machine learning, big data and artificial intelligence to fuel the innovation of tomorrow. Nominations for the annual ACM SIGKDD awards, which will be announced at the conference, are being accepted now through May 1. Based out of Singapore this year, conference organizers are focusing on equity across time zones – offering key content twice, on eight-hour intervals, to cover a multitude of attendee locations"
"KDD-2021, The premier Data Science Conference, Aug 14-18, Virtual - KDnuggets","Each year, outstanding data science practitioners are honored during the conference with SIGKDD Innovation, Service and Rising Star Awards. The Innovation and Service Awards recognize an individual or group of collaborators whose technical innovations in the field of knowledge discovery and data mining have had a lasting impact, and an individual or group whose professional services contributions to the field are remarkable. Launched in 2020, the Rising Star Award aims to promote current SIGKDD researchers as the build their careers, and the honoree is selected based on their whole body of work in the first five years following completion of the Ph.D"
"KDD-2021, The premier Data Science Conference, Aug 14-18, Virtual - KDnuggets","The largest and longest running data mining conference, KDD is managed by a team of volunteers who serve as chairs for the various committees. 2021 general co-chairs are Feida Zhu, associate professor at Singapore Management University; Beng Chin Ooi, distinguished professor in the department of computer science, School of Computing at National University of Singapore; and Miao Chun Yan, chair and professor of the School of Computer Science and Engineering at Nanyang Technology University. Zhu is a founding director of two major data analytics joint labs with industrial giants at the School of Computing and Information Systems at Singapore Management University. His research in data mining, machine learning, blockchain and data asset, with emphasis on their application to business, financial and consumer innovation lends itself well to his leadership role on the KDD 2021 organizing committee"
"KDD-2021, The premier Data Science Conference, Aug 14-18, Virtual - KDnuggets","ACM is the premier global professional organization for researchers and professionals dedicated to the advancement of the science and practice of knowledge discovery and data mining. SIG is ACM's Special Interest Group on Knowledge Discovery and Data Mining. The annual KDD International Conference on Knowledge Discovery and Data Mining is the premier interdisciplinary conference for data mining, data science and analytics"
Data Science Books You Should Start Reading in 2021 - KDnuggets,Data science is undoubtedly one of the hottest career choices right now. Companies (many of whom have data science departments) are hiring data scientists around the board. It is a considerable thing to become a data scientist. It is also a fantastic opportunity to hone your expertise if you are already a statistician and want to step through the ranks
Data Science Books You Should Start Reading in 2021 - KDnuggets,"This detailed workbook will provide data scientists and data miners with active methods for dealing with data. Data scientists will enjoy the inclusion of multiple illustrations, the concise clarification of the algorithms behind each process, and the tools available on the companion website. This is hands down, the only detailed, up-to-date resource for scientific computation in Python"
Data Science Books You Should Start Reading in 2021 - KDnuggets,It would cater to all the individuals who are specialists in the area and others who are not. It starts with a gentle introduction to machine learning and deep learning and then moves to more advanced ways. A fantastic book!
Data Science Books You Should Start Reading in 2021 - KDnuggets,The book begins with a practical approach because you can learn several helpful techniques straight away. It is often incredibly realistic because you will adopt it right away to activities right after the read. This is an utter must-read in deep learning
How to ace A/B Testing Data Science Interviews - KDnuggets,"A/B Testing is an in-demand skill that is often tested in data science interviews. At the same time, there are very few resources out there to help prepare for A/B testing interviews. In my 15 year career and as a hiring manager in Data Science, I have found that most candidates perform poorly in these interviews. In fact, the field of experimentation has been evolving, and there are new concepts and approaches that are becoming more relevant each year. This means that even seasoned data scientists who may have done A/B testing some years back often find themselves stumped in interviews"
How to ace A/B Testing Data Science Interviews - KDnuggets,"In this post, we will go through a mock interview that will help you understand what the interviewer is looking for and how to approach these interviews. Why mock interview, you might ask? Well, as data scientists, we sometimes struggle with communication, and having a template in mind helps tremendously. Personally, I have also found that when I can visualize a high stake situation and how it may play out, it helps me be better prepared mentally, handle pressure well and perform better overall"
How to ace A/B Testing Data Science Interviews - KDnuggets,"INTERVIEWEE — Before we begin with the experiment details, I would like to make sure my understanding of the background is clear. There could be multiple goals with a feature like this one — such as increasing new user acquisition, increase conversion for this category, increasing the number of orders in the category, or increasing total order value. Can you help me understand what the"
How to ace A/B Testing Data Science Interviews - KDnuggets,"INTERVIEWER — We are not offering any discount at this point. The messaging is simply going to be to let them know we have a new category that they can start ordering from. If the experiment is successful, we intend to roll out the notification to all users"
How to ace A/B Testing Data Science Interviews - KDnuggets,INTERVIEWEE — OK. Thanks for that background. I am now ready to dive into the experiment details
How to ace A/B Testing Data Science Interviews - KDnuggets,Now let me state the different metrics that we will want to include in the experiment. Since the goal of the notifications is to increase the conversion rate in the new category. That will be our
How to ace A/B Testing Data Science Interviews - KDnuggets,"INTERVIEWER — I agree with your choice of primary metric, but you can ignore the secondary metrics for this exercise. And you are spot on in terms of guardrail metrics. Doordash wants to be judicious about any features or releases when it comes to their app because we know that the LTV of a customer who has installed the app is much higher. We want to be careful so as not to drive users to uninstall the app"
How to ace A/B Testing Data Science Interviews - KDnuggets,"Treatment will receive notifications, while control will not receive any notifications. Next, I would like to calculate the sample size and duration. For this, I need a few inputs"
How to ace A/B Testing Data Science Interviews - KDnuggets,"A 5% significance level and power of 80% are usually chosen, and I will assume these unless you say otherwise. Also, I will assume a 50–50 split between the control and treatment. Once I have these inputs finalized, I will use power analysis to calculate the sample size. I would use a programming language for this. For example, in R, there is a package called ‘pwr’ that can be used for this"
Top 10 Must-Know Machine Learning Algorithms for Data Scientists – Part 1 - KDnuggets,"And, so, this is the source we will use to identify the top 10 machine learning algorithms being used, and, as such, the top 10 must-know algorithms for data scientists. The first 5 of these top 10 must-know algorithms are introduced below, with a brief overview of what the algorithms are and how they work. We will followup with part 2 in the coming weeks"
Top 10 Must-Know Machine Learning Algorithms for Data Scientists – Part 1 - KDnuggets,"Over the years, C4.5 has become a benchmark against which the performance of newer classification algorithms are often measured. Quinlan’s original implementation contains proprietary code; however, various open-source versions have been coded over the years, including the (at one time very popular) Weka Machine Learning Toolkit's J48 decision tree algorithm"
Top 10 Must-Know Machine Learning Algorithms for Data Scientists – Part 1 - KDnuggets,"One of the most important takeaways from this discussion should be that decision tree is a classification strategy as opposed to some single, well-defined classification algorithm. While we have had a brief look at 3 separate decision tree algorithm implementations, there are a variety of ways to configure different aspects of each. Indeed, any algorithm which seeks to classify data, and takes a top-down, recursive, divide-and-conquer approach to crafting a tree-based graph for subsequent instance classification, regardless of any other particulars (including attribution split selection methods and optional tree-pruning approach) would be considered a decision tree"
Top 10 Must-Know Machine Learning Algorithms for Data Scientists – Part 1 - KDnuggets,"In a given scenario, it may prove more useful than not to to chain or group classifiers together, using the techniques of voting, weighting, or combination to pursue the most accurate classifier possible. Ensemble learners are classifiers which provide this functionality in a variety of ways. Bagging is an example of an ensemble learner"
Top 10 Must-Know Machine Learning Algorithms for Data Scientists – Part 1 - KDnuggets,"Bagging operates by simple concept: build a number of models, observe the results of these models, and settle on the majority result. I recently had an issue with the rear axle assembly in my car: I wasn't sold on the diagnosis of the dealership, and so I took it to 2 other garages, both of which agreed the issue was something different than the dealership suggested. Voila. Bagging in action"
Top 10 Must-Know Machine Learning Algorithms for Data Scientists – Part 1 - KDnuggets,"I only visited 3 garages in my example, but you could imagine that accuracy would likely increase if I had visited tens or hundreds of garages, especially if my car's problem was one of a more complex nature. This holds true for bagging, and the bagged classifier often is significantly more accurate than single constituent classifiers. Also note that the type of constituent classifier used are inconsequential; the resulting model can be made up of any single classifier type"
Top 10 Must-Know Machine Learning Algorithms for Data Scientists – Part 1 - KDnuggets,"As mentioned, Support Vector Machines (SVMs) are a particular classification strategy. SMVs work by transforming the training dataset into a higher dimension, which is then inspected for the optimal separation boundary, or boundaries, between classes. In SVMs, these boundaries are referred to as hyperplanes, which are identified by locating support vectors, or the instances that most essentially define classes, and their margins, which are the lines parallel to the hyperplane defined by the shortest distance between a hyperplane and its support vectors. Consequently, SVMs are able to classify both linear and nonlinear data"
How to organize your data science project in 2021 - KDnuggets,"For illustrative purposes, we will use the cruise ship data set. We assume that we would like to build a machine learning model for recommending cruise ship crew size based on predictor variables such as age, tonnage, passengers, length, cabins, etc. In section I, we describe how the project should be organized locally. Then in section I, we describe how to create a Github repository for the project. It is always recommended that you maintain two versions of your project, one locally and the other on Github. The advantage of this is that you can access the Github version of your project from anywhere in the world and at any time, as long as you have an internet connection. Another advantage is that if something were to happen with your local computer that could impact your computer adversely, such as viruses in the computer, then you can always be confident that you still have your project files on Github that can serve as a backup"
How to organize your data science project in 2021 - KDnuggets,"This could be a world document where you describe what your project is all about. You may start by providing a brief synopsis followed by step by step plan of what you would like to accomplish. For example, before building a model, you may ask yourself:"
How to organize your data science project in 2021 - KDnuggets,"Once you’ve figured out what your project plans and objectives are, it is time to start coding. Depending on the type of problem you are solving, you may decide to use a jupyter notebook or an R script for writing your code. Let’s just assume we are going to be using a jupyter notebook"
How to organize your data science project in 2021 - KDnuggets,"You may also store key project outputs in your local directory. Some key project outputs could be data visualizations, graphs illustrating model error as a function of different parameters, or tables containing key outputs such as R2 values, mean square errors, or regression coefficients. Project outputs are very handy because they can be used to prepare project reports or PowerPoint presentation slides to be presented to your data science team or to the business administrators in your company"
How to organize your data science project in 2021 - KDnuggets,"In this case, you need to put together a project report using MS word. When writing a project report, you can make good use of some visualizations produced from your main code. You want to add these to the report. Your main code may be added as an appendix to the project report"
How to organize your data science project in 2021 - KDnuggets,"Once you’ve solved the problem of interest, you then have to create a project repository on GitHub and upload project files such as datasets, jupyter notebooks, R program scripts, and sample outputs. Creating a GitHub repository for any data science project is extremely important. It enables you to have access to your code at all times. You get to share your code with a community of programmers and other data scientists. Also, it is a means for you to showcase your data science skills"
How to organize your data science project in 2021 - KDnuggets,"In summary, we’ve discussed how a data science project has to be organized. Good organization leads to better productivity and efficiency. When next you have to work on a new project, please take the time to organize your project. This will not only help with increasing efficiency and productivity, but it will also help to minimize errors. Also, keeping good records of all your current and completed projects enables you to create a repository where you can save all your projects for future use"
Essential Math for Data Science: Linear Transformation with Matrices - KDnuggets,"The goal of this chapter is to get you to the next level of understanding of vectors and matrices. You’ll start seeing matrices, not only as operations on numbers, but also as a way to transform vector spaces. This conception will give you the foundations needed to understand more complex linear algebra concepts like matrix decomposition. You’ll build up on what you learned about vector addition and scalar multiplication to understand linear combinations of vectors"
Essential Math for Data Science: Linear Transformation with Matrices - KDnuggets,The linear transformation associated with a singular matrix (that is a non invertible matrix) can’t be reversed. It can occur when there is a loss of information with the transformation. Take the following matrix:
Essential Math for Data Science: Linear Transformation with Matrices - KDnuggets,"You can see in Figure 6 that the transformed vectors are on a line. There are points that land on the same place after the transformation. Thus, it is not possible to go back. In this case, the matrix"
Essential Math for Data Science: Linear Transformation with Matrices - KDnuggets,"He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog ("
Top 3 Statistical Paradoxes in Data Science - KDnuggets,"To see how this is possible, let’s consider the following dataset with the two departments Dept. A and Dept. B"
Top 3 Statistical Paradoxes in Data Science - KDnuggets,"A: 80–30, Dept. B: 20–70). Indeed,"
Models of Data Science teams: Chess vs Checkers - KDnuggets,"In a checkers-team, everyone works possibly on every piece of the development lifecycle. In this sense, the team is more similar to checkers pieces. There is no move that a piece can take and another piece cannot. Similarly, there is no activity that any team member cannot do. For example, everyone can contribute to building devops pipelines and automation"
Models of Data Science teams: Chess vs Checkers - KDnuggets,The balance of types of activities is not stable over time in a team. There can be times when there is a peak of work items in data engineering and little or no work items in ML model development. These peaks can be due to different phases of the data product development cycle or due to varying business requirements. A checkers-team is flexible and can adapt quickly to these peaks. A checkers-team could for example dedicate the entire team to develop data engineering pipelines in a Scrum sprint if needed. The same flexibility is not as easy in a chess-team model where you have constraints due to different skills and different responsibilities
Models of Data Science teams: Chess vs Checkers - KDnuggets,"Not every data science team is facing the same level of complexity in their projects. Imagine a team that is building an AI model for self-driving cars. It is a complex problem to solve that requires advanced skills in computer vision and AI. These skills cannot be learned quickly but usually need a specific education or career path. When facing such problems, you need team members which are specialists in area like vision or AI. A chess-team is designed to host specialists in certain fields and is designed to grow vertically such skills. In a checkers-team, there are not such specialists"
Models of Data Science teams: Chess vs Checkers - KDnuggets,"A member of a checkers-team knows in details every phase of the development cycle. While he is designing a ML model, he is aware at the same time of how the release pipeline and the operations of the model work. He may take decisions during model selection that take into consideration where the model will be hosted and possible constraints of the production platform. On the other hand, a data scientist of a chess-team knows less details (because he has not being working on it by himself) of how the model will be deployed and run. This minor awareness may lead to assumptions taken during model development, and these assumptions can bring to more complexity to those in charge of deploying such model"
Models of Data Science teams: Chess vs Checkers - KDnuggets,A checkers-team requires the same person to work on very different tasks. This is viable only if the complexity of such tasks is small. Adopting
Models of Data Science teams: Chess vs Checkers - KDnuggets,"They let you focus on your goal. For example, building an API endpoint hosted by a function-as-a-service is something feasible by a data scientist with a mathematical background. Doing the same from scratch on an on-premise server is not as feasible"
Why Automated Feature Selection Has Its Risks - KDnuggets,"In this regard, any customers who have cancelled their booking will be assigned to the “Cancelled” category. In this regard, the ReservationStatus variable will show near perfect collinearity with the IsCanceled outcome variable. No wonder the ExtraTreesClassifier shows such a high value for this feature — it is effectively describing the exact same thing as the outcome variable!"
Why Automated Feature Selection Has Its Risks - KDnuggets,"Instead, a more reasonable approach would be to assume that the features which indicate whether a customer will cancel their booking or not will also be of relevance in determining that customer’s ADR. Customers that follow through with the booking have already demonstrated greater customer loyalty than those who cancelled. In this regard, even if a customer who cancelled originally had a high ADR value — this is now redundant since the booking will not go ahead"
Why Automated Feature Selection Has Its Risks - KDnuggets,"The phrase “garbage in, garbage out” also applies to feature selection. Should the features in the dataset be nonsensical, then feature selection tools will not be able to interpret those features in a meaningful way. Proper understanding of the data is paramount, and automating of feature selection needs to be balanced with domain knowledge to be able to properly judge whether a feature is appropriate for use in a model"
Best Podcasts for Machine Learning - KDnuggets,"Lukas hosts his podcasts in his own style. He asks really meaningful questions and is well-informed on the topics. The podcast episodes are very informative, crisp, to-the-point, and engaging"
Best Podcasts for Machine Learning - KDnuggets,"Hannah Fry is a superb host, as good as any professional podcast host. That sets this podcast apart from the others. Hanna Fry is a Mathematician and has had experience in hosting podcasts. She is passionate and knowledgeable about AI and passionate about this podcast. This podcast likely has a team working behind it, and the sound effects and the music tells us so. They also make the experience absolutely entertaining"
Best Podcasts for Machine Learning - KDnuggets,"This podcast focuses on works being done on DeepMind and hosts people working on those problems within DeepMind. The problems and solutions are described in plain words. The engaging, passionate host also gives you a big picture view surrounding the problem and what the future holds. This podcast is really well-structured, well-designed, and is really informative and edifying"
Best Podcasts for Machine Learning - KDnuggets,"Lex Fridman is an AI researcher working on autonomous vehicles and human-robot interaction. He also teaches at MIT. The way he is unique as a podcast host is that he is very focused on long-term goals and far-reaching implications of the works of the guests. He is interested in deep things such as consciousness and the idea of AGI. He asks deep, fundamental questions and lets us know about the guests’ views on these questions"
Best Podcasts for Machine Learning - KDnuggets,"The topics on his podcast transcend the AI universe. He has had guests like Roger Penrose, Richard Dawkins, and Noam Chomsky. A significant portion of his interviews"
Best Podcasts for Machine Learning - KDnuggets,Sanyam Bhutani is a community man. Period. As a Data Scientist at h2o. He is also very active in the
Best Podcasts for Machine Learning - KDnuggets,"This podcast hosts a lot of Kaggle Competition winners and Grandmasters. He talks about the guests’ journey, career, and current research, projects, and interests. And then there is Kaggle! He often interviews winners of particular competitions and asks about their thinking and approach to those particular competitions. These are really informative for people competing on Kaggle or interested in developing full pipelines for any project. It is very interesting to see the diverse set of people from diverse backgrounds win Kaggle competitions, and talk about their careers, lives, and Kaggle on this podcast"
Best Podcasts for Machine Learning - KDnuggets,This is the only entry in the list which has multiple hosts. Yannic is famous for his paper-explaining videos and his sunglasses. Dr. Scarfe and Keith also maintain an online presence and are involved in Deep Learning
Best Podcasts for Machine Learning - KDnuggets,"This podcast deliberately maintains a hacker, open-source sort of aura about them. That is evident from their choice of backgrounds, style of presentation, and many more things. They delve deep into research topics and interview the person(s) attached to them. The fact that there is more than one host adds value to the conversations rather than being a liability. The listeners get to hear from multiple perspectives and get acquainted with multiple views. The guests are also much more likely to shed light on different aspects of the research"
Best Podcasts for Machine Learning - KDnuggets,"Of course, you have noticed that “+1” in the title. I include these resources together as one because all of the content present at the first two resources are not focused on ML or AI. And although TED has a few talks based on this topic and broader topics, it has some really good talks on ML, Data Science, and AI"
Best Podcasts for Machine Learning - KDnuggets,"TED has quite a few talks on Data Science, AI, and its impact. And TED has been covering Data Analysis, AI, etc. I remember listening to the talk,"
Best Podcasts for Machine Learning - KDnuggets,Podcasts about and around Machine Learning have seen a boom in recent times. It is important not to get overwhelmed in this overflow. I have listened to many podcasts about ML/Data Science and decided on these as the bests
How to Build an Impressive Data Science Resume - KDnuggets,Every one of us needs a resume to showcase our skills and experience but how much effort are we putting into it to make it impactful. It is undeniable that resumes play a key role in our job application process. This article will explore some simple strategies to significantly improve the presentation as well as the content of data science resumes
How to Build an Impressive Data Science Resume - KDnuggets,Every one of us needs a resume to showcase our skills and experience but how much effort are we putting into it to make it impactful. It is undeniable that resumes play a key role in our job application process. This article will explore some simple strategies to significantly improve the presentation as well as the content of data science resumes
How to Build an Impressive Data Science Resume - KDnuggets,"Profile summary is key to a resume, consider it as an elevator pitch. It should be persuasive and should cover information like who you are, what are your skills and strengths. This part of the resume will be the main driver for the first impression also in influencing the recruiter’s decision hence spend enough time to ensure it includes the key details about you"
How to Build an Impressive Data Science Resume - KDnuggets,"My people include career objectives at the beginning of the resume. I personally advocate removing the career objective from the resume and instead use that space for a better profile summary. Because most recruitment happens based on your achievements, strengths, and skill and not based on your aspirations. So make an intelligent decision and efficiently use the real-estate of your resume especially the start"
How to Build an Impressive Data Science Resume - KDnuggets,Your contact details are important for the recruiter to contact you hence ensure that you double-check your details. Many people start editing their resumes based on their colleague’s or friend’s resumes in those cases ensure the hyperlinks are also edited when you edit the text. Like when you edit the email id ensure the email in the hyperlink is edited as well
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets","When there’s a change in the product, people react to it differently. Some are used to the way a product works and are reluctant to change. This is called the"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets","However, both effects will not last long as people’s behavior will stabilize after a certain amount of time. If an A/B test has a larger or smaller initial effect, it’s probably due to novel or primacy effects. This is a common problem in practice, and many interview questions are about this topic. A sample interview question is:"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets","We ran an A/B test on a new feature and the test won, so we launched the change to all users. However, after launching the feature for a week, we found that the treatment effect quickly declined. What is happening?"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets","One way to deal with such effects is to completely rule out the possibility of those effects. We could run tests only on first-time users because the novelty effect and primacy effect obviously doesn’t affect such users. If we already have a test running and we want to analyze if there’s a novelty or primacy effect, we could 1) compare new users’ results in the control group to those in the treatment group to evaluate novelty effect 2) compare first-time users’ results with existing users’ results in the treatment group to get an actual estimate of the impact of the novelty or primacy effect"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets","In the simplest form of an A/B test, there are two variants: Control (A) and treatment (B). Sometimes, we run a test with multiple variants to see which one is the best amongst all the features. It can happen when we want to test multiple colors of a button or test different home pages. Then we’ll have more than one treatment group. In this case, we should not simply use the same significance level of 0.05 to decide whether the test is significant because we are dealing with more than 2 variants, and the probability of false discoveries increases. For example, if we have 3 treatment groups to compare with the control group, what is the chance of observing at least 1 false positive (assume our significance level is 0.05)?"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets","We are running a test with 10 variants, trying different versions of our landing page. One treatment wins and the p-value is less than .05. Would you make the change?"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets",The answer is no because of the multiple testing problem. There are several ways to approach it. One commonly used method is
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets","It divides the significance level 0.05 by the number of tests. For the interview question, since we are measuring 10 tests, then the significance level for the test should be 0.05 divided by 10 which is 0.005. Basically, we only claim a test if significant if it shows a p-value of less than 0.005. The drawback of Bonferroni correction is that it tends to be too conservative"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets","It measures out of all of the rejections of the null hypothesis, that is, all the metrics that you declare to have a statistically significant difference. How many of them had a real difference as opposed to how many were false positives. This only makes sense if you have"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 2 - KDnuggets",Suppose we have 200 metrics and cap FDR at 0.05. This means we’re okay with seeing false positives 5 of the time. We will observe at least 10 false positive in those 200 metrics every time
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"Kaggle is a wonderful place. It is a gold mine of knowledge for data scientists and ML engineers. There are not many platforms where you can find high-quality, efficient, reproducible, awesome codes brought by experts in the field all in the same place"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"It has hosted 164+ competitions since its launch. These competitions attract experts and professionals from around the world to the platform. As a result, there are many high-quality notebooks and scripts on each competition and for the massive amount of open-source datasets Kaggle provides"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"But now, I find myself spending a considerable amount of time reading other’s notebooks and making submissions to competitions. Sometimes, there are pieces that are worth spending your entire weekend on. And sometimes, I find simple but deadly effective code tricks and best practices that can only be learned by watching other pros"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"And the rest is simple, my OCD practically forces me to spill out every single piece of data science knowledge I have. So here I am, writing the first edition of my ‘Weekly Awesome Tricks And Best Practices From Kaggle’. Throughout the series, you will find me writing about anything that can be useful during a typical data science workflow including code shortcuts related to common libraries, best practices that are followed by top industry experts on Kaggle, and so on, all learned by me during the past week. Enjoy!"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"A good correlation matrix can say a lot about your dataset. It is common to plot it to see the pairwise correlation between your features and the target variable. According to your needs, you can decide which features to keep and feed into your ML algorithm"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"However nice, there is just too much information to take in. Correlation matrices are mostly symmetrical along the main diagonal, so they contain duplicate data. Also, the diagonal itself is useless. Let’s see how we can plot only the useful half:"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"The resulting plot is much easier to interpret and free of distractions. First, we build the correlation matrix using the . Then, we use"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"Many of us never realize the vast, untapped potential of pandas. An underrated and often overlooked feature of pandas is its ability to style its DataFrames. Using the"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"This time, we are aggregating diamond prices for each cut and clarity combination. From the styled DataFrame, we can see that the most expensive diamonds have ‘VS2’ clarity or premium cut. But it would be better if we could display the aggregated prices by rounding them. We can change that with"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"Just like Matplotlib, pandas has global settings you can play around with. Of course, most of them are related to displaying options. The official user guide says that the entire options system of pandas can be controlled with 5 functions available directly from pandas namespace:"
Awesome Tricks And Best Practices From Kaggle - KDnuggets,"The temporary behavior change will only be applied to the code block that follows the statement. For example, if there are large numbers, pandas has an annoying habit of converting them to standard notation. You can avoid this temporarily by using:"
What did COVID do to all our models? - KDnuggets,"Machine Learning (ML) algorithms assume consistency between past and future. When things change, the models fail. COVID has changed our habits, and therefore our data. Pre-COVID models struggle to deal with the new situation"
What did COVID do to all our models? - KDnuggets,"A simple example would be the Traffic layer on Google Maps. After lockdowns hit country after country in 2020, Google Maps traffic estimates were very inaccurate for a while. It had been built on fairly stable training data but now that system was thrown completely out of whack"
What did COVID do to all our models? - KDnuggets,"Here’s a little trick I use: I partition my data by time and label records as “before” and “after”. I then build a classification model to discriminate the “after” vs. If the discrimination is possible, then the “after” is different from the “before”, the world has changed, the data has changed, and the models must be retrained"
What did COVID do to all our models? - KDnuggets,"It can vary of course, by complexity. Most of our projects get functioning prototypes at least in a few months. But for all, I cannot stress enough the importance of feedback: You have to talk to people much more often than you want to. And listen! We learn new things about the business problem, the data, or constraints, each time. Not all us quantitative people are skilled at speaking with humans, so it often takes a team. But the whole team of stakeholders has to learn to speak the same language"
What did COVID do to all our models? - KDnuggets,"It is important to talk to our business counterpart. People fear change and don’t want to change the current status. One key problem really is psychological. The analysts are often seen as an annoyance. So, we have to build the trust between the business counterpart and the analytics geeks. The start of a project should always include the following step: Sync up domain experts / project managers, the analysts, and the IT and infrastructure (DevOps) team so everyone is clear on the objectives of the project and how it will be executed. Analysts are number 11 on the top 10 list of people they have to see every day! Let’s avoid embodying data scientist arrogance: “The business can’t understand us/our techniques, but we know what works best”. What we don’t understand, however, are the domains experts are actually experts in the domain we are working in! Translation of data science assumptions and approaches into language that is understood by the domain experts is key!"
What did COVID do to all our models? - KDnuggets,"Deep learning sucked a lot of the oxygen out of the room. It feels so much like the early 1990s when neural networks ascended with similar optimism! Deep Learning is a set of powerful techniques for sure, but they are hard to implement and optimize. XGBoost, Ensembles of trees, are also powerful but currently more mainstream. The vast majority of problems we need to solve using advanced analytics really don’t require complex solutions, so start simple; deep learning is overkill in these situations. It is best to use the Occam’s razor principle: if two models perform the same, adopt the simplest"
What did COVID do to all our models? - KDnuggets,"I often find myself fighting interpretability. It is nice, sure, but often comes at too high a cost of the most important model property: reliable accuracy. But many stakeholders believe interpretability is essential, so it becomes a barrier for acceptance. Thus, it is essential to discover what kind of interpretability is needed. Perhaps it is just knowing what the most important variables are? That’s doable with many nonlinear models. Maybe, as with explaining to credit applicants why they were turned down, one just needs to interpret outputs for one case at a time? We can build a linear approximation for a given point. Or, we can generate data from our black box model and build an “interpretable” model of any complexity to fit that data"
What did COVID do to all our models? - KDnuggets,"Maybe full explainability is too hard to obtain, but we can achieve progress by performing a grid search on model inputs to create something like a score card describing what the model does. This is something like regression testing in hardware and software QA. If a formal proof what models are doing is not possible, then let’s test and test and test! Input Shuffling and Target Shuffling can help to achieve a rough representation of the model behavior"
What did COVID do to all our models? - KDnuggets,"Talking about understanding what a model does, I would like to raise the problem of reproducibility in science. A huge proportion of journal articles in all fields -- 65 to 90% -- is believed to be unreplicable. This is a true crisis in science. Medical papers try to tell you how to reproduce their results. ML papers don’t yet seem to care about reproducibility. A recent study showed that only 15% of AI papers share their code"
What did COVID do to all our models? - KDnuggets,"To “discriminate” in the ML world word is your very goal: to make a distinction between two classes. The computer knows nothing about the world except for what’s in the data in front of it. So the analyst has to curate the data -- take responsibility for those cases reflecting reality. If certain types of people, for example, are under-represented then the model will pay less attention to them and won’t be as accurate on them going forward. I ask, “What did the data have to go through to get here?” (to get in this dataset) to think of how other cases might have dropped out along the way through the process (that is survivor bias). A skilled data scientist can look for such problems and think of ways to adjust/correct for them"
What did COVID do to all our models? - KDnuggets,"The bias is not in the algorithms. The bias is in the data. If the data is biased, we’re working with a biased view of the world. Math is just math, it is not biased"
What did COVID do to all our models? - KDnuggets,"I believe AI is just good engineering. Will AI exceed human intelligence? In my experience anyone under 40 believes yes, this is inevitable, and most over 40 (like me, obviously): no! AI models are fast, loyal, and obedient. Like a good German Shepherd dog, an AI model will go and get that ball, but it knows nothing about the world other than the data it has been shown. It has no common sense. It is a great assistant for specific tasks, but actually quite dimwitted"
What did COVID do to all our models? - KDnuggets,"These ideas have been around for a long time. Here is one reason why AI will not solve all the problems: We’re judging its behavior based on one number, one number only! (Model error. We all know that RMSE is too coarse of a measure. Deep Learning algorithms will continue to get better, but we also need to get better at judging how good a model really is. So, no! I do not think that AI will take over humanity"
What did COVID do to all our models? - KDnuggets,"Initially on the Event Team, her background is actually in translation & proofreading, so by moving to the blog in 2019 she has returned to her real passion of working with texts. P.S. She is always interested to hear your ideas for new articles"
What’s ETL? - KDnuggets,"You could certainly use an ML model to power a recommendation engine to achieve this goal. But the challenge is that the data you need is sitting in two different systems. The solution in our case is to use an ETL process to extract, transform and combine them into a data warehouse:"
What’s ETL? - KDnuggets,"You might have also come across the term ‘ELT’. Extract, load, and transform (ELT) differs from ETL solely in where the transformation takes place. In the ELT process, the data transformation occurs in the destination data store"
What’s ETL? - KDnuggets,"The amount of data businesses produce is only expected to grow — 175 Zettabytes by 2025 according to a report by IDC[2]. So you should ensure that the ETL tool you choose has the ability to scale to not just your current but also future needs. You may move data in batches now, but will that always be the case? How many jobs can you run in parallel?"
What’s ETL? - KDnuggets,"Data is growing in volume. But more importantly, it’s growing in complexity. One enterprise could be handling diverse data from hundreds — or even thousands — of data sources. These can include structured and semi-structured sources, real-time sources, flat files, CSVs, object buckets, streaming sources, and whatever new comes along"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","A/B tests, a. It allows tech companies to evaluate a product/feature with a subset of users to infer how the product may be received by all users. Data scientists are at the forefront of the A/B testing process, and A/B testing is considered one of a data scientist’s core competencies"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","Thus, we want to first select which ideas are worth testing, particularly when people have different opinions and ideas on improving a product, and there are many ideas from which to choose. For example, a UX designer may suggest changing some UI elements, a product manager may propose simplifying the checkout flow, an engineer may recommend optimizing a back-end algorithm, etc. In times like this, stakeholders rely on data scientists to drive data-informed decision making. A sample interview question is:"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","For example, before investing in multiple items checkout for an e-commerce website, obtain an upper-bound sizing of the impact by analyzing the number of multiple-items purchased per user. If only a very small percentage of users purchased more than one item, it may not be worth the effort to develop this feature. It’s more important to investigate users’ purchasing behaviors to understand why users do not purchase multiple items together. Is it because the selection of items is too small? Are the items too pricey, and they can only afford one? Is the checkout process too complicated, and they do not want to go through it again?"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","This kind of analysis provides directional insights on which idea is a good candidate for A/B testing. However, historical data only tells us how we’ve done in the past. It’s not able to predict the future accurately"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","It is the smallest difference that would matter in practice. For example, we may consider a 0.1% increase in revenue as the minimum detectable effect. In reality, this value is discussed and decided by multiple stakeholders"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","Once we know the sample size, we can obtain the number of days to run the experiment by dividing the sample size by the number of users in each group. If the number is less than a week, we should run the experiment for at least seven days to capture the weekly pattern. It is typically recommended to run it for two weeks. When it comes to collecting data for a test,"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","Normally we split the control and treatment groups by randomly selecting users and assigning each user to either the control or treatment group. We expect each user is independent and no interference between control and treatment groups. However, sometimes"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","Company X has tested a new feature with the goal to increase the number of posts created per user. They assigned each user randomly to either the control or treatment group. The Test won by 1% in terms of the number of posts. What do you expect to happen after the new feature is launched to all users? Will it be the same as 1%, if not, would it be more or less? (assume there’s no novelty effect)"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","Facebook, Linkedin, and Twitter), users’ behavior is likely impacted by that of people in their social circles. A user tends to use a feature or a product if people in their network, such as friends and family, use it. That is called a"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets","Uber, Lyft, ebay, and Airbnb): interference between control and treatment groups can also lead to biased estimates of treatment effect. It is mainly because resources are shared among control and treatment groups, meaning that control and treatment groups will compete for the same resources. For example, if we have a new product that attracts more drivers in the treatment group, fewer drivers will be available in the control group. Thus, we’ll not be able to estimate the treatment effect accurately. Unlike social networks where the treatment effect underestimates the real benefit of a new product, in two-sided markets, the treatment effect"
"A/B Testing: 7 Common Questions and Answers in Data Science Interviews, Part 1 - KDnuggets",We are launching a new feature that provides coupons to our riders. The goal is to increase the number of rides by decreasing the price for each ride. Outline a testing strategy to evaluate the effect of the new feature
Data vault: new weaponry in your data science toolkit - KDnuggets,"Picnic is an online grocer, where we aim to bring grocery shopping and the traditional Milkman model into the 21st century. Easy and frictionless ordering, a focus on personal service, home delivery with electric vehicles, and an efficient, highly optimised just-in-time supply chain. In short, we are tech’s answer to groceries"
Data vault: new weaponry in your data science toolkit - KDnuggets,"We do not just run an app and outsource the actual work to third-party companies. Instead, we also run a full supply chain and a delivery fleet with dedicated systems and have the tech stack to support all this. We are a full-fledged technology company. As a Data Scientist, the challenge is to solve complex (business) problems and make relevant predictions based on all the data that is tracked and produced. Thankfully, we have an amazing Data Engineering team in place. As detailed"
Data vault: new weaponry in your data science toolkit - KDnuggets,"But first, let me introduce myself. My name is Bas Vlaming, and I am one of the lucky Data Scientists at Picnic who gets to interact with this treasure trove of data on a daily basis. We are currently a team of 7 full-time Data Scientists, and we like to think of ourselves as full-stack data scientists: we do data exploration, model and pipeline design, all the way to actually implementing our solutions in production. As a team, we work on a"
Data vault: new weaponry in your data science toolkit - KDnuggets,We work with the Payments team to combat fraud. We personalise the app experience. We
Data vault: new weaponry in your data science toolkit - KDnuggets,"In all these topics, and in Data Science in general, quality data is the magic dust that is required to build reliable models. Thankfully, our data is (almost always) readily available, cleanly structured, and in an immaculate state. As described in"
Data vault: new weaponry in your data science toolkit - KDnuggets,"In most businesses, the concepts used in both the backend and on the business side will change over time. Picnic is no exception. When we expanded from the Netherlands into Germany, we required a completely different setup with regards to how we define ‘delivery regions’. Some two years ago, we added the much-desired functionality to add additional products to an already existing order, which meant that the concept of an order had to be redesigned. When we started taking back bottles and recyclables, we needed to be able to handle deposits. These kinds of changes often have wide-ranging knock-on effects"
Data vault: new weaponry in your data science toolkit - KDnuggets,"However, with a Data Vault architecture, we track “all the data, all the time. Likewise, we can accommodate the corresponding changes in the backend systems. Sure, this requires modifications to the contracts that are made with the backend, work from both sides, and quite possibly additional hubs, satellites, and links. But in the end, we still have a set of logical and reliable building blocks and links between them in place and clean recipes on how to distil the relevant business concepts out of it — both the new and old versions. These can then be exposed to the business using a Kimball-style dimensional model"
Data vault: new weaponry in your data science toolkit - KDnuggets,"So far, this is an important but mostly generic advantage of our data architecture. For the Data Science team specifically, this brings additional value to the table. When new or changing business concepts come to the fore, we often want to build some machine learning model that involves these new definitions. In other architectures, building a historical data set might involve complex and error-prone queries that try to reconstruct these quantities in the past. But now, the Data Engineering team has done the heavy lifting through their Data Vault set up, and the new concepts are directly accessible for past data too — also before these new concepts were defined in the first place!"
Data vault: new weaponry in your data science toolkit - KDnuggets,"Say you are a Data Scientist in charge of flagging potential fraud cases. You want to build features and reconstruct what happened with historical payments. A dimensional model might give you some of these answers, but most likely, not the complete picture. For many analysts and business users at Picnic (we have a 50% SQL adoption rate among our business users), it is probably interesting to know what has been bought and that an order has been paid. But here, as in many cases, a Data Scientist has different requirements: we are not only interested in the final state or the current state, but we need to be able to reconstruct the full business reality at some historical point in time. For example, we might be interested in a customer’s number of outstanding unpaid orders at a given time, how many times a payment attempt has bounced, or what the outstanding debt was at the time of ordering. Very relevant to this specific Data Science use case, even if such quantities are not necessarily of interest to anyone outside of this particular business problem"
Data vault: new weaponry in your data science toolkit - KDnuggets,"Data Vault to the rescue. DV makes this task quite straightforward: we can at any given point in time easily find what states the hubs, satellites, and links were in. Moreover, we have a known recipe for recombining these into the business concepts we care about: these are simply the queries that are used to construct the dimensional model, as exposed to the business. By adapting these, we can straightforwardly rebuild the historical values of any relevant business quantity, and in this way, build historical features for our models that really reflect the values they would have had at the time"
Data vault: new weaponry in your data science toolkit - KDnuggets,"The possibility of time travelling also allows us to align our training and production queries as much as possible. The queries that generate features in production use the most recently available data. By using time travelling in Data Vault, we can use very similarly structured queries to generate historical features for training purposes. Ideally, all we have to do is change the reference point-in-time, and the historical truth (from the data perspective) rolls out. Keeping such queries aligned as much as possible reduces code and logic duplication and also reduces the scope for mistakes. In contrast, in a different architecture, reproducing such a past historical state might involve complicated and error-prone queries full of filtering and join logic"
Data vault: new weaponry in your data science toolkit - KDnuggets,"Of course, you could say: sure, but I could also just store these historical snapshots in my regular dimensional model. And of course, with 20/20 hindsight, maybe you could have — but this would require an almost prophetic ability on the Data Engineering side to predict what kind of snapshots (and what kind of business concepts and definitions) would become relevant in the future. With the unwanted side effect of requiring all kinds of complications to the data structure, making it harder for any business users to use and interpret the data correctly. Our current setup gets the best of both worlds by maintaining a strict separation between a Data Vault approach on the backend side and a dimensional Kimball-style model that exposes the data to business users. This allows for tractability, accountability, and resilience on the backend side and a clean and structured dimensional model on the front end"
Data vault: new weaponry in your data science toolkit - KDnuggets,"Data warehouse solutions and the provided structures are great for historical analysis, but data is not received in real-time. Typically, the Data Vault pulls in data every few hours; the dimensional model is usually refreshed once a day. However, the Data Vault architecture also allows for a pretty neat extension beyond the typical data warehouse use case, where we use real-time data to augment the available structured data. This allows us to use the structure as provided by the Data Warehouse (either the Data Vault backend or the Kimball dimensional front end) but allowing for features that are as up-to-date as possible"
Data vault: new weaponry in your data science toolkit - KDnuggets,"In those cases, we start out from the most recent version of the relevant tables of the dimensional model that is available. At the time of prediction, we can on-demand construct an enriched version of our relevant features. That is, we pull data from the backend ourselves. By using slightly modified versions of the queries that are used by Data Engineering to build the Data Vault and subsequently the dimensional model, we can construct our own little job-specific Data Warehouse that is real-time augmented. Now that we have a fully up-to-date version of the relevant tables, we can make our predictions as accurately as possible while again ensuring that we can use very similar queries to build features in training and in production"
Data vault: new weaponry in your data science toolkit - KDnuggets,"Still, in the end, only those concepts that are relevant to the business are exposed. However, as Data Scientists, we are often trailblazing from the data perspective — in particular, when exploring a new business problem, we often need to construct niche quantities that could be relevant to our model, but not too much else. And maybe they turn out to be not that relevant at all, and we just do not know"
Data vault: new weaponry in your data science toolkit - KDnuggets,"Exploring such novel quantities might be quite tricky in many other architectures. It might require Data Engineers or Backend Engineers to create new concepts or expose other data, which takes a lot of valuable time and effort. And all that for a feature that might not even yield anything useful! Our Data Vault architecture directly unblocks us: we can simply build such quantities ourselves, do our analysis and exploration without needing to bother anyone else. And if a certain feature is indeed relevant for a wider audience, this can always be exposed structurally in the dimensional model at a later stage"
Data vault: new weaponry in your data science toolkit - KDnuggets,"In daily life, the dimensional model is still the go-to source of data for any quick analysis. It is clean, well-structured, reliable, and will contain the commonly used business concepts and quantities. But the choice to use the Data Vault philosophy adds a number of very useful tools to our Data Science arsenal:"
Data vault: new weaponry in your data science toolkit - KDnuggets,"This is absolutely crucial in a number of Data Science projects that we run in production. And more generally, it is a multiplier on our productivity — being unblocked when we want to experiment with new features and directly having access to historical data under changing definitions is extremely helpful. The majority of our twenty-ish production Data Science jobs and services have directly touched various backend Data Vault structures at some stage, and several crucial ones still do so structurally whenever they are being run"
Software Engineering Best Practices for Data Scientists - KDnuggets,"The trick is to try to be as descriptive as the situation allows. Of course, there will be instances when the computations are so complex that it wouldn’t make sense to write such descriptive variables. However, until that time comes, try to be descriptive"
Software Engineering Best Practices for Data Scientists - KDnuggets,"However, the best programmers are said to be the laziest programmers. Why? Because they write the least amount of code and often write the cleanest code to produce a solution. They work as little as possible to get their code to work and they often produce the most concise solutions. This involves using functions"
Software Engineering Best Practices for Data Scientists - KDnuggets,"Don’t be like me. Instead, take the time to learn the syntax conventions of each new language, and force yourself to use those conventions properly. This will not only help you immerse yourself in the language, but it will also help you write cleaner code, and will help you communicate with other developers using the same language"
Software Engineering Best Practices for Data Scientists - KDnuggets,"Using pre-existing libraries is a huge time saver, especially when it comes to coding data analyses. Python is chock full of libraries that can handle every request a data scientist could throw at it. Not only are these libraries already coded for you, but they’ve already been debugged and are production-ready"
Software Engineering Best Practices for Data Scientists - KDnuggets,"The goal is to not repeat any code. What this means, is that if you’re noticing that you’re writing the same lines of code over and over, you need to turn that code into a function that you only write once. This function can then be called multiple times"
Software Engineering Best Practices for Data Scientists - KDnuggets,"Unit tests are written to ensure that each piece of code performs as expected. The tests are written so that a small section of code executing a particular functionality is tested to ensure that it is completing its job accurately. The “units” that are tested could be individual functions, procedures, or entire objects"
Overview of MLOps - KDnuggets,"Considerable data science expertise is usually required to create a dataset and build a model for a particular application.  But building a good model is usually not enough.  In fact, it is not nearly enough.  As illustrated below, developing and testing a model is just the first step"
Overview of MLOps - KDnuggets,"Creating a production ML system requires multiple steps:  First, the data must undergo a series of transformations.  Then, the model is trained.  Usually, this requires experimentation with different network architectures and hyperparameters.  Often, it is necessary to go back to the data and try different features.  Next, the model must be validated with unit tests and integration tests.  It needs to pass tests for data and model bias and explainability.  Finally, it is deployed into a public cloud, an on-premise environment, or a hybrid environment.  Additionally, some steps in the process might require an approval workflow"
Overview of MLOps - KDnuggets,"If each of these steps is performed manually, the development process tends to be slow and brittle.  Fortunately, many MLOps tools exist to automate these steps from data transformation to deployment end-to-end.  When retraining is necessary, it is an automated, reliable, and reproducible process"
Overview of MLOps - KDnuggets,"ML models tend to work well when first deployed and then work less well over time.  As Forrester analyst, Dr. Kjell Carlsson"
The question that makes your data project more valuable - KDnuggets,"Decisions are so core to our work that we sometimes forget to acknowledge them before starting a data project. We assume it's baked into everything we do, so when the product team asks a question like: ""Which products are commonly purchased together?"" it's natural to jump straight into figuring out what data we need, which approach is best, etc, etc. It's a straightforward ask, and the data work could be fun. How should we visualize the results? What if the patterns have changed over time? We dive right into the data without taking the time to really (I mean really) understand the decision we're trying to inform"
The question that makes your data project more valuable - KDnuggets,"These responses are specific and have a clear action in mind. It's clear that the action will change based on what the data says. Unfortunately, not all responses start out as good responses"
The question that makes your data project more valuable - KDnuggets,"Next time someone comes to you with a data request, remember the magic question. And remember, a bad response doesn't mean game over. It should be a starting point to get you to the final, better data question. A skilled data analyst will always tease out the real problem and understand the real decision before starting any project. This makes the difference between a data project that is just ""interesting"" and a data project that drives an impact"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"However, what should be kept in the back of your mind at all times is that to remain relevant in the workforce, you need to stay up to date with technology. So, if you’ve been doing data analysis with MATLAB your whole career, try learning to code in Python. If you’ve been creating your visualizations with Matplotlib, try using Plotly for something fresh"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Take an hour every week (or as much time as you can spare), and experiment with new technologies. Figure out which technologies are relevant by reading blog posts, and pick a couple you would like to add to your stack. Then, create some personal projects to learn how to use the new technologies to the best of their abilities"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Take some time to either read good code documentation or articles on how to write good code documentation. To practice, write documentation for old personal projects, or take some time to revamp the documentation of your current projects. Since a good portion of the data science world runs on Python, check out this really well-written article on how to document Python code:"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"I find that when I initially write code to solve data science problems, I usually throw good coding practices out of the door in favor of writing code that works when I need it to. In other words, a lot of spaghetti code happens. Then, after I get my solution to work, I’ll go back and clean up my code"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Take a look at old code and ask if the same code could be written more efficiently. If so, take some time to educate yourself on best coding practices and look for ways where you can shorten, optimize, and clarify your code. Check out this great article that outlines best practices for code refactoring:"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Once a year (or more often if that works better for you), take stock of your overall effectiveness and efficiency, and determine where you could improve. Perhaps this means working on your machine learning algorithms first thing in the morning, or sitting on an exercise ball instead of a chair, or adding a new extension to your IDE that will lint your code for you. Experiment with different workspaces, tools, and workflows until you enter your optimal form"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Coding, algorithms, and mathematics are the easy part. Understanding how to implement them so they can solve a specific business problem, not so much. By taking more time to understand the business problem and the objectives you’re trying to solve, the rest of the process will be much smoother"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Take some time to research the specific company you’re working for and the industry that they’re in. Write a cheat sheet that you can refer to, containing the major goals of the company, and the issues it may face within its specific industry. Don’t forget to include algorithms that you may want to use to solve business problems or ideas for machine learning models that could be useful in the future. Add to this cheat sheet whenever you discover something useful and soon you’ll have a treasure trove of industry-related tidbits"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Though by minimalist I don’t immediately want you to assume scarcity. Often when someone discusses the importance of minimalism in code that leads people to try to develop outrageous solutions that use only a few lines of code. Stop that. Yes, it’s impressive, but is that really the best use of your time?"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,I’ll be the first to admit that I severely neglect functions when I’m writing data analysis code for the first time. Spaghetti code fills my IDE as I struggle to reason my way through different analyses. If you looked at my code you would probably deem it too far gone and volunteer to take it out behind the barn to put it out of its misery
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Once I’ve managed to cobble together a half-decent result, I’ll then go back to try to fix the equivalent of a bad accident. By packaging my code into functions, I quickly remove unnecessary complexities and redundancies. If that’s the only thing I do to my code, then I will already have simplified it to a point that I can revisit the solution and understand how I got to that point"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Don’t forget the importance of functions when writing code. It’s often said that the best developers are lazy developers because they figure out how to create solutions that don’t require much work. After you’ve written a solution, go back and bundle redundant or complex code into functions to help organize and simplify your code"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Study up on test-driven development, and determine whether or not this technique can add something to your workflow. TDD isn’t the perfect answer for every problem, but it can be useful if implemented thoughtfully. Check out this article that gives a great description of TDD and offers an example of how to implement it into data science projects:"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Regardless of whether the industry is becoming saturated or arid, you will be competing with tons of highly qualified, and often over-qualified, candidates for a single job. This means that in the lead-up to applying for jobs, you need to have already developed the habit of self-improvement. Today, everyone is obsessed with upskilling, and for good reason. This trend should be no exception to data scientists"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"To effectively relate this to data science projects, you need to ask yourself in the planning phase of a project what the desired outcome of the project is. This will help shape the path of the project and will give you a roadmap of outcomes that need to be met to reach the final goal. Not only that but determining the outcome of the project will give you an idea of the feasibility and sustainability of the project as a whole"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Begin each project with a planning session that lays out exactly what you hope to achieve at the end of the development period. Determine which problem you will be attempting to solve, or which piece of evidence you are trying to gather. Then, you can begin to answer feasibility and sustainability questions that will shape the milestones and outcomes of your project. From there, you can start writing code and machine learning models with a clear plan in place to guide you to the end of your project"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Data science, the art of using data to tell a compelling story, is only successful if the storyteller understands the story they are trying to tell. In other words, it’s your task to understand so that you can be understood. Developing this habit early on of understanding what you’re trying to accomplish, such that you can share it with someone else to a fair level of comprehension, will make you the most effective data scientist in the room"
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,Pick one or two research papers to read every week that are relevant to your current work or to technologies that you’re interested in pursuing or studying. Try to set aside time for this literature review every week to make this a priority. Become familiar with the
15 Habits I Learned from Highly Effective Data Scientists - KDnuggets,"Whenever a new technology or practice makes the news, take a test-drive and see what that new technology or practice brings to the table. Even if you just read the documentation, you can keep yourself up-to-date on the changing trends of the industry. Furthermore, you can bring a perspective on the technology to your company and help them navigate technological changes and advances. Being that person in the office with your ear to the ground can help you stay ahead of the curve, and can also help you guide your team and company to better, more efficient solutions"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,Let’s say you trained a prediction model. And set it up for regular use. Welcome to machine learning in production!
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"We trained a random forest model using data for the four weeks from January. Let’s imagine that in practice, we just started the data collection, and that was all the data available. The performance of the trained model looked acceptable, so we decided to give it a go"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,That is a realistic assumption in real-world machine learning. Integrating and updating different data sources is not always straightforward. Even after the actual event has occurred! Maybe the daily usage data is stored locally and is only sent and merged in the database once per week
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"Since the actuals are available only once per week, we decide to run a regular model analysis every time. There is no real-time monitoring. Instead, we schedule a job that generates a standard weekly report for the data scientist to look at"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"By default, Evidently uses the index as an x-axis in plots. In this case, it is datetime, so we do not add anything explicitly. Otherwise, we would have to specify it in our column mapping"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"Observing the model in production has straightforward goals. We want to detect if something goes wrong. Ideally, in advance"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"We can see that the distribution of the actual number of bikes rented remains sufficiently similar. To be more precise, the similarity hypothesis is not rejected. No drift is detected"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"MAE remains almost the same. But, the skew towards under-estimation continues to grow. It seems that the error is not random! On average, we underestimate by ten bikes"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"To know more, we move to the plots. We can see that the model catches overall daily trends just fine. So it learned something useful! But, at peak hours, the actual demand tends to be higher than predicted"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"In the error distribution plot, we can see how it became “wider,” as we have more predictions with a high error. The shift to the left is visible, too. In some extreme instances, we have errors between 80 and 40 bikes that were unseen previously"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"We can see that the target distribution is now different: the similarity hypothesis is rejected. Literally, people are renting more bikes. And this is a statistically different change from our training period"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"Evidently generates a few more plots to display the error. In the current context, these are different ways to paint the same story. We have a high error, and it has a clear skew towards underestimation"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"The model faces new, unusually high demand. Given how it was trained, it tends to underestimate it. On top of it, these errors are not at all random. At the very least, they are related to the temperature we observe. The higher it is, the larger the underestimation"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"We can, of course, look at all our features. But we can also conclude that categorical features (like “season,” “holiday” and “workingday”) are not likely to change. Let’s look at numerical features only!"
How to break a model in 20 days — a tutorial on production model analytics - KDnuggets,"Earlier she co-founded an industrial AI startup and led business development at Yandex Data Factory. Since 2014, she has worked with companies from manufacturing to retail to deliver ML-based solutions. In 2018, Elena was named 50 Women in Product Europe by Product Management Festival"
Top 10 Python Libraries Data Scientists should know in 2021 - KDnuggets,"Pandas is primarily used for data analysis, and it is one of the most commonly used Python libraries. It provides you with some of the most useful set of tools to explore, clean, and analyze your data. With Pandas, you can load, prepare, manipulate, and analyze all kinds of structured data. Machine learning libraries also revolve around Pandas DataFrames as an input"
Top 10 Python Libraries Data Scientists should know in 2021 - KDnuggets,"NumPy is also used by other libraries such as TensorFlow for their internal computation on tensors. NumPy also provides fast precompiled functions for numerical routines, which can be hard to manually solve. To achieve better efficiency, NumPy uses array-oriented computations, so working with multiple classes becomes easy"
Top 10 Python Libraries Data Scientists should know in 2021 - KDnuggets,"Keras is mainly used for creating deep learning models, specifically neural networks. It’s built on top of TensorFlow and Theano and allows you to build neural networks very simply. Since Keras generates a computational graph using back-end infrastructure, it is relatively slow compared to other libraries"
Top 10 Python Libraries Data Scientists should know in 2021 - KDnuggets,"As the name suggests, SciPy is mainly used for its scientific functions and mathematical functions derived from NumPy. Some useful functions which this library provides are stats functions, optimization functions, and signal processing functions. To solve differential equations and provide optimization, it includes functions for computing integrals numerically. Some of the applications which make SciPy important are:"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"Data science is an ever-expanding field. More industries continue to rely on technology for gathering and acting upon vital information, and data scientists are in high demand. However, finding a job that fits your needs can sometimes be a challenge. In these instances, you can turn to freelance work"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"Use your website to show off your skills. Market yourself with your best foot forward. Put a portfolio of relevant work experience you have, and update it as you successfully help more clients. Remember, a site must be easy to navigate or else people will click away. Make it easy to contact you and get the information they need"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"Data science changes with the times. New skills come into play, and you must keep up with the demands of the job. Fortunately, data science is a broad profession — you can apply skills you learned for cybersecurity to machine learning algorithms"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"You can also turn to online courses to gain skills you don’t already have. Lynda has plenty of options that will help you develop new skills like data management in blockchain technology. You can add the certificates to your website, boosting your appeal and qualifications for any data science-related job"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"As a freelance data scientist, you should know the basics of the job, statistics, programming, data visualization, machine and deep learning, and software engineering. You’ll also need a good understanding of big data. These skills give you a well-rounded approach to all freelance work"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"The abundant need for data science professionals is another benefit of this job. You’ll find that tech jobs are a necessity across industries, no matter their focus. For instance, travel and banking require data scientists to protect and monitor sensitive information"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"This idea applies across all sectors because every institution needs to gather and protect data. The key is keeping your options open. Look into every opportunity and remember that you can work with established businesses, startups and individual customers"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"While you can use the internet to get certifications, you can take it a step further. Other resources help you network and create the connections you need to take off as a freelancer. Here’s where you should start:"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"If you divide that down to an hourly rate, it comes out to a little over $54 per hour. However, if you have skills like mastery of Scala and Spark, you can increase your fee. It depends on your background. Remember, don’t sell yourself short — know your worth"
How to Succeed in Becoming a Freelance Data Scientist - KDnuggets,"With these five steps, you can begin your career as a successful freelance data scientist. Everyone’s path will look different, but you should start by building yourself up. Then, with a strong foundation, you can accept any opportunity that comes your way"
"Metric Matters, Part 2: Evaluating Regression Models - KDnuggets","We have different options for summarizing a model’s error across all its predictions. That’s where things can get a little fuzzy because there are a lot of different approaches. As in our discussion of classification metrics, you have to decide which strategy works best for your particular situation: your data, your preferred kind of prediction error, and your need for"
"Metric Matters, Part 2: Evaluating Regression Models - KDnuggets","First, we’ll check out some different ways of looking at your model’s blunders — specifically, the gaps between its predictions and our observed values. Let’s make up a dataset for the cost of holiday gifts. We’ll assume there were some other features used for prediction here, but to keep it simple, let’s just look at the actual cost, your model’s predicted cost, and the error"
"Metric Matters, Part 2: Evaluating Regression Models - KDnuggets","Values range from 0 to infinity, and lower values reflect a better-performing model. The MSE for our tiny dataset is 1633.33"
More Data Science Cheatsheets - KDnuggets,"At 10 pages, you should be able to imagine the breadth of probability topics being covered herein. But don't let that deter you; Chen's ability to boil concepts down to their essential bullet points and explain in plain English while not sacrificing on essentials is noteworthy. It is also rich in explanatory visualizations, something quite useful when space is limited and the desire to be concise is strong"
How to frame the right questions to be answered using data - KDnuggets,"Data science is an interdisciplinary field that uses the scientific method, processes, and algorithms to extract knowledge and insights from data. The field of data science has several subdivisions, such as data mining, data transformation, data visualization, machine learning, deep learning, etc. In this article, we will focus on how to ask meaningful questions that could be answered using data. The quality of any analysis performed on data depends on your ability to frame the right questions. The following tips will help you to frame the right questions to be addressed with data"
How to frame the right questions to be answered using data - KDnuggets,"In descriptive analytics, you are interested in studying relationships between features in your dataset. Data visualization plays an essential role here. You need to decide the type of data visualization that is suitable for the project at hand. It could be a scatter plot, barplot, line graph, density plot, heat map, etc. Some examples of data visualization projects are presented below"
How to frame the right questions to be answered using data - KDnuggets,"In predictive analysis, the goal is to build a model using available data that can then be used for making predictions on unseen data. Here, the type of model to build will depend on the type of target variable. If the target variable is continuous, then one can use linear regression, and if the target variable is discrete, then classification could be used. The framework for prescriptive data analysis is illustrated in Figure 6 below"
How to frame the right questions to be answered using data - KDnuggets,"This is where you select the model that you would like to use, e. The dataset has to be divided into training, validation, and test sets. Hyperparameter tuning is used to fine-tune the model in order to prevent overfitting. Cross-validation is performed to ensure the model performs well on the validation set. After fine-tuning model parameters, the model is applied to the test dataset. The model’s performance on the test dataset is approximately equal to what would be expected when the model is used for making predictions on unseen data"
How to frame the right questions to be answered using data - KDnuggets,"In this stage, the final machine learning model is put into production to start improving the customer experience or increasing productivity or deciding if a bank should approve credit to a borrower, etc. The model is evaluated in a production setting in order to assess its performance. This can be done by comparing the performance of the machine learning solution against a baseline or control solution using methods such as A/B testing. Any mistakes encountered when transforming from an experimental model to its actual performance on the production line has to be analyzed. This can then be fed back into the original model and used in fine-tuning the model to increase its predictive power"
How to frame the right questions to be answered using data - KDnuggets,"Sometimes, the available data may serve only as sample data that can be used for generating more data. The data generated could then be used in prescriptive analysis for recommending a course of action. An example of this is the loan status forecasting problem:"
How to frame the right questions to be answered using data - KDnuggets,"In summary, we’ve discussed some tips for framing the right questions to be answered using available data. The quality of any analysis performed on data depends on your ability to frame the right questions. It is, therefore, important that the right questions be framed before using data to perform any type of analysis"
Forget Telling Stories; Help People Navigate - KDnuggets,These are the lead visualizations that you would place on your executive dashboards. They give people a sense for where the key metrics for the business in one place. What are the KPIs that tell the current story of the organization?
Forget Telling Stories; Help People Navigate - KDnuggets,These are supplemental visualizations. The path to do drill downs needs to be simple and clearly linked. Ideally each department or business process would have its own drill-down area or folder
Forget Telling Stories; Help People Navigate - KDnuggets,"Not every change in every chart will have an automatic recommended best route. But many do. If inventory is low in a particular SKU, there are just a few routes to follow, so why not note them right on the report? If costs are increasing faster than planned in one department, do you have standard ways to address the issue? If so, then list out those standard actions right where the information is needed and useful as the information is consumed"
Kedro-Airflow: Orchestrating Kedro Pipelines with Airflow - KDnuggets,"The Kedro team and Astronomer have released Kedro-Airflow 0.4.0 to help you develop modular, maintainable & reproducible code with orchestration superpowers!"
Kedro-Airflow: Orchestrating Kedro Pipelines with Airflow - KDnuggets,You can deploy your Kedro projects virtually anywhere with minimal effort as long as you can run Python. Our users have the freedom to choose their deployment targets. The future of deploying Kedro pipelines is in designing a deployment process with a great developer experience in mind
Kedro-Airflow: Orchestrating Kedro Pipelines with Airflow - KDnuggets,"Airflow has evolved quite a lot since its inception in 2014; it now has over 20,000 stars on Github, 600k downloads/month, and tens of thousands of users worldwide. Airflow 1. Airflow 2.0 meets the needs of users with a handful of much anticipated features. These include:"
Kedro-Airflow: Orchestrating Kedro Pipelines with Airflow - KDnuggets,"I had chatted with a few data scientists who were using Kedro to author their pipelines and looking for a good way to deploy those pipelines to Airflow. Kedro does an outstanding job of allowing data scientists to apply good software engineering principles to their code and make it modular, but Kedro pipelines need a separate scheduling and execution environment to run at scale. Given this need, there was a natural bond between Kedro pipeline and Airflow: we wanted to do everything we could to build a great developer experience at the intersection of the two tools"
Kedro-Airflow: Orchestrating Kedro Pipelines with Airflow - KDnuggets,"Airflow 2.0 extends and upgrades the Airflow REST API, allowing it to be robust in the coming years. As the API develops, there will be new opportunities for specific abstraction layers to assist with DAG authoring and deployment, leading to a richer plugin ecosystem. There will be extra opportunity to integrate the"
Kedro-Airflow: Orchestrating Kedro Pipelines with Airflow - KDnuggets,"As we look towards Airflow 3.0 and beyond, building upon developer love and trust is inevitable. But it won’t stop there. As data orchestration becomes critical to a growing number of business units, we want Airflow to become a medium for making data engineering more approachable. We seek to democratise access such that product owners and data scientists alike can leverage Airflow’s distributed execution and scheduling power without being a master in Python or Kubernetes. Empowering users to author and deploy data pipelines from a framework of their choice will become increasingly important in that journey"
Kedro-Airflow: Orchestrating Kedro Pipelines with Airflow - KDnuggets,"Airflow’s inception kicked off a “data pipelines as code” movement that changed the way enterprises thought about workflow orchestration. For many years, job scheduling was handled by a combination of legacy drag-and-drop frameworks and complex networks of cron jobs. As we transitioned into the “big data” era and companies began building dedicated teams to operationalise their siloed data, the need for additional flexibility, control, and governance became apparent"
Kedro-Airflow: Orchestrating Kedro Pipelines with Airflow - KDnuggets,"Airflow solved many first-order problems for data engineers, which explains its explosive adoption. But with that early adoption came some pitfalls; since Airflow is highly configurable by design, users began applying it to use cases it was not necessarily designed for. This imposed evolutionary stress on the project, pushing the community to add additional configuration options to “mould” Airflow to various use cases"
Kedro-Airflow: Orchestrating Kedro Pipelines with Airflow - KDnuggets,"While the added configuration options helped Airflow extend to accommodate these additional use cases, they introduced a new class of user needs. Data platform owners and administrators now need a way to deliver standard patterns to their pipeline authors to abate business risk. Likewise, pipeline authors need additional guardrails to be sure they don’t “use Airflow wrong”. Finally, engineers with a pythonic background now need to learn how to operationalise big data infrastructure for stable & reliable orchestration at scale"
Must Know for Data Scientists and Data Analysts: Causal Design Patterns - KDnuggets,"Similarly, statistics has many prototypical analyses, but too often, these frameworks are siloed within specific disciplines and clouded by domain-specific language. This makes methods harder to discover and hides their general applicability. Consequently, it can be hard for practitioners outside of these fields to recognize when the problem they are facing fits one of these paradigms"
Must Know for Data Scientists and Data Analysts: Causal Design Patterns - KDnuggets,"However, these advantages don’t make causal inference unnecessary; if anything, they simply make it more possible and more relevant. Good businesses don’t act at random. For example, we market to customers who are likely to be interested in our company and who, therefore, might have been interested even without marketing. When it comes to measuring effectiveness, good business is bad science. Because our"
Must Know for Data Scientists and Data Analysts: Causal Design Patterns - KDnuggets,"Beyond these specific challenges, perhaps the best reason is that there are so many questions that you can answer. As we’ll see, almost all of these methods rely on exploiting some arbitrary amount of randomness in whether or not a specific individual or group received a certain treatment. Industry (and life in general) is full of non-random but well-defined (and somewhat arbitrary) policies, which make it fertile ground for observational causal inference. Analysts can embark on data search-and-rescue missions and find new uses in reams of historical data that might be otherwise discounted as hopelessly biased or outdated"
Must Know for Data Scientists and Data Analysts: Causal Design Patterns - KDnuggets,"Often, in real life and particularly in industry, we violate the “positive probability of treatment throughout the covariate space” assumption required by stratification and propensity score weighting. Business and public policy often use strategies that have sharp cut-offs (e. In such cases, we have no relevant observations to re-weight. However, we can apply a regression discontinuity design to understand the local effect of a treatment at the point of discontinuity"
Must Know for Data Scientists and Data Analysts: Causal Design Patterns - KDnuggets,"One way we can evaluate these methods is to see if, as we would expect, their values of the variable of interest before treatment are similar (e. Instead of looking for similarity in the absolute level, difference-in-differences helps us more flexibly settle for similarity in trajectories over time. That is, instead of comparing treatment and control groups within the same population at the same time, we can compare the"
Must Know for Data Scientists and Data Analysts: Causal Design Patterns - KDnuggets,"Data management is needed to ensure that data on past treatments are preserved, discoverable, and sufficiently detailed. All of these methods require rich data with measures of baseline characteristics of each individual being studied and a solid understanding of the treatment they received. This may seem obvious, but it’s easy to neglect to preserve data of sufficient granularity. For example, we might have a database that maps every customer’s account to a system-generated campaign_id denoting some marketing campaign that they participated in; however, unless information about that specific campaign (the specific treatment, the targeting, the timing, etc. Additionally, as in our stratification example, some of the best opportunities for causal inference come from execution errors (or, more gently, “natural experiments”). We may be inclined to forget about these errors and move on, but information on events that did not go as intended can be powerful fodder for future analysis"
Must Know for Data Scientists and Data Analysts: Causal Design Patterns - KDnuggets,"Domain knowledge is essential to validate the assumptions. Unlike other forms of inference (e. Instead, assumptions rely largely on and careful attention to detail combined with intuition and background knowledge from one’s domain. This means causal inference should necessarily be a human-in-the-loop activity"
Must Know for Data Scientists and Data Analysts: Causal Design Patterns - KDnuggets,"Finally, a solid grasp of probabilistic reasoning and understanding of these methods is also critical. As many of the resources I link below discuss at length, it’s easy to do causal inference wrong. For example, attempting to control for the wrong variables can sometimes"
Must Know for Data Scientists and Data Analysts: Causal Design Patterns - KDnuggets,"The point of this post is not to teach any one method of causal inference but to help raise awareness for basic causal questions, data requirements, and analysis designs which one might be able to use in the wild. There are a plethora of fantastic resources available to learn more about the specific implementation of these or other methods. Please check out my companion"
Data careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space - KDnuggets,"I moved from the U.S. And let me tell you, has it ever been FUN!"
Data careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space - KDnuggets,"First, let’s analyze your current skills. I find most data professionals tend to have serious chops in one main area. Those main skillsets tend to be:"
Data careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space - KDnuggets,"But once we progress in our career, our needs change. We begin to want the recognition, the accolades, the promotions - in other words, our esteem needs. Finally, once we’ve gotten the money and the praise, we often find ourselves searching for MORE. This is the stage of seeking true fulfillment and greater impact as a data professional"
Data careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space - KDnuggets,"If it’s coding, you’ll definitely want to look into a data implementation role. But if that's managing programs, and projects and products or consulting with the business, then consider a data leadership role. And if innovation is more your jam, then you may have an entrepreneurial bone!"
Data careers are NOT one-size fits all! Tips for uncovering your ideal role in the data space - KDnuggets,"The world is your oyster with a data skillset. There’s no need to limit yourself to data science simply because it’s one of the most talked-about tech careers. By diving deeper into your personality, passions, goals, and skillsets, you’ll be able to land a job that not only pays well but brings you true fulfillment in the long run"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"It is not easy to build a machine learning model. It is even harder to deploy a service in production. But even if you managed to stick all the pipelines together, things do not stop here"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"Machine learning service is still a service. Your company probably has some established process of software monitoring that you can reuse. If the model runs in real-time, it needs proper alerting and responsible people on-call"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"Upstream pipelines and models break. Users make an unannounced schema change. The data can disappear at the source, the physical sensors fail. The list goes on"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,Things change. Even when we deal with very stable processes. Almost every machine learning model has this inconvenient trait: it will degrade with time
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"If the real-world patterns change, the Concept Drift kicks in. Think of something casual like a global pandemic affecting all customer behavior. Or a new competing product on the market offering a generous free tier. It changes how users respond to your marketing campaigns"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"The ultimate measure of both drifts is the degradation of model quality. But sometimes, the actual values are not yet known, and we cannot calculate it directly. In this case, there are leading indicators to track. We can monitor if the properties of the input data or target function have changed"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"The most direct way to know if your model works well is to contrast your predictions against the actual values. You can use the same metrics from the model training phase, be it Precision/Recall for classification, RMSE for regression, and so on. If something happens with the data quality or the real-world patterns, we will see the metrics creep down"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"For example, if you make your forecasts for a long horizon or there is a lag in data delivery. Sometimes you need an extra effort to label new data to check if your predictions are correct. In this case, it makes sense first to track data and target drift as an early warning"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"For example, the accuracy metric is far from ideal if you have unbalanced classes. With regression problems, you might care about the error sign. Thus, you should track not just the absolute values but the error distribution, too. It is also critical to distinguish between occasional outliers and real decay"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"In other cases, it would make sense to search for segments of low performance proactively. Imagine that your real estate pricing model consistently suggests higher-than-actual quotes in a particular geographic area. That is something you want to notice!"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"For that, we need to track suitable metrics such as parity in the accuracy rate. It applies both to the model validation and ongoing production monitoring. So, a few more metrics to the dashboard!"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"We know that models make errors. In some use cases, like ad targeting, we probably do not care if individual inputs appear weird or usual. As long as they do not constitute a meaningful segment the model fails on!"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"In other applications, we might want to know about each such case. To minimize the errors, we can design a set of rules to handle outliers. For example, send them for manual review instead of making an automated decision. In this case, we need a way to detect and flag them accordingly"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"Monitoring might sound boring. But, it is essential to make machine learning work in the real world. Don’t wait for the model to fail to create your first dashboard!"
A Machine Learning Model Monitoring Checklist: 7 Things to Track - KDnuggets,"Earlier she co-founded an industrial AI startup and led business development at Yandex Data Factory. Since 2014, she has worked with companies from manufacturing to retail to deliver ML-based solutions. In 2018, Elena was named 50 Women in Product Europe by Product Management Festival"
How to Speed Up Pandas with Modin - KDnuggets,The pandas library provides easy-to-use data structures like pandas DataFrames as well as tools for data analysis. One issue with pandas is that it can be slow with large amounts of data. It
How to Speed Up Pandas with Modin - KDnuggets,"Modin’s coverage of the pandas API is over 90% with a focus on the most commonly used pandas methods like pd.DataFrame, df. This means if you have a lot of data, you can perform most of the same operations as the pandas library faster. This section highlights some commonly used operations"
How to Speed Up Pandas with Modin - KDnuggets,"Modin allows you to use the same Pandas script for a 10KB dataset on a laptop as well as a 10TB dataset on a cluster. This is possible due to Modin’s easy to use API and system architecture. This architecture can utilize Ray as an execution engine to make scaling Modin easier. If you have any questions or thoughts about Ray, please feel free to join our community through"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"When I started my career as a Data Analyst, a Data Science engineer position was not widely available in Ukraine. And mostly, it wasn’t available for female Maths graduates without special skills and experience. Self-education and getting acquainted with ML algorithms took me some time and a lot of effort. Nowadays, I work as an AI engineer at"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"ImageNet was considered the birth of AI-dataset training, with the first tests achieving 71.8% accuracy. Since then, the annual ImageNet challenge is a contest to see which algorithm identifies objects with the lowest error rate. The last competition was held in 2017 when the winning algorithm hit 97.3% accuracy, which is better than human abilities"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"As the co-founder and CEO of qAIRa, Monica Abarca develops technological solutions to deal with air contamination. Dangerous levels of air contamination are responsible for over four million deaths each year. The qAIRa company combines drone technology with air-quality monitoring, using data analytics to identify air contamination, and produces real-time maps showing critical areas"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"Professor Barzilay is a strong advocate for having standards that ensure equity and fairness in applying AI technology, especially in medicine. As an example, she points to the Tyrer-Cuzick model used to determine a patient’s risk of getting breast cancer by analyzing imaging data. This system has a modest accuracy with white women but fails miserably with women of African or Asian descent"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,This failure happens because the software has not been adequately machine-trained on enough sets of images from racially-diverse groups. This problem is due to undesirable researcher-bias. It is readily proven and unconscionable. Professor Barzilay insists that software developers eliminate bias by validating AI software on different groups of people or by allowing it to be open source to compare system accuracy using diverse models
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"As the Director of Data Science at Fitbit R&D, Dr. Emir-Farinas works with AI applications using machine learning combined with behavioral science and healthcare. She is working to answer questions that include:"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"At the Nelson Mandela African Institution of Science and Technology, Dina Machuve is a lecturer and researcher. She focuses on creating data-driven solutions to improve agriculture. One application is a diagnostics system that helps identify diseases in poultry using bioinformatics and computer vision technology"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"There are over 380 million household farms that create food for 70% of the people living in developing countries. Manchuve’s solution, now deployed in Tanzania, uses systematic data collection and analysis in small to medium sized farms. The project demonstrates the value of using AI methods of deep learning for disease diagnostics to improve livestock health. It works by collecting data for analysis in low-resourced settings from the 3.7 million households that raise chickens"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,Deborah Raji was given the 2020 Barlow award at the Electronic Frontier Foundation Pioneer Award Ceremony for her work on racial-bias in AI. Her concentration is the negative impact AI has on minorities when dealing with the American justice system. She advocates eradicating and replacing the seriously-flawed facial recognition and surveillance systems used by law enforcement in many American cities
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"At Google, Lucy Vasserman works as a staff software engineer on innovative AI projects. Google partnered with conservation groups to use AI to study wildlife. Since the 1990s, the groups have collected 4.5 million photos of animals using camera traps. That database of images formed the initial image library for"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"Vasserman worked on the program to train it to identify around 100 species. It can process and categorize 3,600 photos per hour. The machine learning software analyses the images to discover trends such as the population size of individual species, predator/prey interactions, and how wild animals respond to human hunting and encroachment on their habitat"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"Some of her projects show wind currents, collaboration patterns of Wikipedia editorial activities, and dynamic maps of world-news events. Dr. Viégas produces highly-visual, data-driven artwork that is part of a permanent collection at the Museum of Modern Art in New York"
8 Women in AI Who Are Striving to Humanize the World - KDnuggets,"Women are encouraged to get involved in AI due to its world-changing qualities and to reduce the gender-bias that exists. Take inspiration from the late Justice Ruth Bader Ginsburg of the U.S. Supreme Court. When asked, “How many justices would she like to be women on the Supreme Court?” She said, “All of them"
3 Mathematical Laws Data Scientists Need To Know - KDnuggets,"The mathematics behind machine learning is not just a random notation thrown here and there, but it consists of many theories and thoughts. This thought creates a lot of mathematical laws that contribute to the machine learning we can use right now. Although you could use the mathematics in any way you want to solve the problem, mathematical laws are not limited to machine learning after all"
3 Mathematical Laws Data Scientists Need To Know - KDnuggets,"When we think about the first digit of the numbers, it should be distributed uniformly when we randomly took a number. Intuitively, the random number leading digit 1 should have the same probability as leading digit 9, which is ~11.1%. Surprisingly, this is not what happens"
3 Mathematical Laws Data Scientists Need To Know - KDnuggets,"For example, when rolling dice, the possibilities for a 6-sided die are 1, 2, 3, 4, 5, and 6. The mean for the 6-side dice would be 3.5. As we roll the die, the number we get is random from 1 to 6, but as we keep rolling the dice, the results' average get closer to the expected value, which is 3.5. This is what the Law of Large Numbers denote"
3 Mathematical Laws Data Scientists Need To Know - KDnuggets,"When I sum all the words that exist in the Spotify corpus, the total is 759,389. We can see if Zipf’s law applies to this dataset by counting the probabilities when they occur. The first most occurring word or punctuation is ‘-’ with 32,258, which has the probability of ~4%, followed by ‘the,’ which has the probability of ~2%"
Top YouTube Channels for Data Science - KDnuggets,"I'm not telling you anything you don't know, but I am here to (hopefully) help alleviate some of the trouble of finding quality data science video content on YouTube. The idea here was to take a qualitative approach to identifying those channels of value on the platform. Our approach was to start with the intuition that quality content should lead to a channel being popular, which could be measured in the number of subscribers"
Top YouTube Channels for Data Science - KDnuggets,"Statistics, Machine Learning and Data Science can sometimes seem like very scary topics, but since each technique is really just a combination of small and simple steps, they are actually quite simple. My goal with StatQuest is to break down the major methodologies into easy to understand pieces. That said, I don't dumb down the material. Instead, I build up your understanding so that you are smarter"
Top YouTube Channels for Data Science - KDnuggets,"Great Learning aims to make quality education accessible to anyone who wants to learn. Great Learning Academy (http://greatlearning. In addition to videos you can earn certificates of completion, do assignments and projects, attend live sessions and interact with top faculty & industry experts. All for free"
Top YouTube Channels for Data Science - KDnuggets,"I work as a Lead Data Scientist, pioneering in machine learning, deep learning, and computer vision,an educator, and a mentor, with over 8 years' experience in the industry. This is my YouTube channel where I explain various topics on machine learning, deep learning, and AI with many real-world problem scenarios. I have delivered over 30 tech talks on data science, machine learning, and AI at various meet-ups, technical institutions, and community-arranged forums. My main aim is to make everyone familiar of ML and AI.Please subscribe and support the channel. As i love new technology, all these videos are free and I promise to make more interesting content as we go ahead"
Top YouTube Channels for Data Science - KDnuggets,Data Science and Sports Analytics are my passions. My name is Ken Jee and I have been working in the data science field doing sports analytics for the last 5 years. I have held data science positions in companies ranging from startups to fortune 100 organizations.  I transitioned into data science from a business and consulting background. When I was first starting out on my data science journey I was extremely lost; there were very few resources for me to learn about this field from. I decided to start making YouTube videos to share my experiences and to hopefully help others get break into the data science and sports analytics fields
Top YouTube Channels for Data Science - KDnuggets,"Subscribers: 67.1 K, Videos: 246, Views: 3.6 M, Start date: Aug 6, 2014"
Top YouTube Channels for Data Science - KDnuggets,"At Data Science Dojo, we believe data science is for everyone. Our in-person data science bootcamp has been attended by more than 4,000+ professionals from over 1,500+ companies globally. Our channel is the perfect showcase of our commitment to teaching data science for all skill levels! You will find tutorials, community talks, and courses on data science and data engineering"
Top YouTube Channels for Data Science - KDnuggets,"Subscribers: 54.7 K, Videos: 252, Views: 3.5 M, Start date: Aug 15, 2016"
Top YouTube Channels for Data Science - KDnuggets,"Subscribers: 43.2 K, Videos: 229, Views: 0.6 M, Start date: Jul 18, 2011"
Top YouTube Channels for Data Science - KDnuggets,"As part of a Cognitive strategy, organizations can transform their industry and professions with data. IBM Analytics enables anyone to engage with data to answer the toughest business questions, uncover patterns and pursue breakthrough ideas. At IBM, we offer a holistic approach to data analytics, including the expertise to deliver immediate business value at the point of impact. Here you can find presentations, demos, interviews, technical tutorials and more"
Top YouTube Channels for Data Science - KDnuggets,"Subscribers: 25.9 K, Videos: 659, Views: 2.5 M, Start date: Nov 1, 2015"
Top YouTube Channels for Data Science - KDnuggets,"Subscribers: 19.8 K, Videos: 306, Views: 0.5 M, Start date: Apr 18, 2017"
Top YouTube Channels for Data Science - KDnuggets,"I help you get into data engineering, the plumbing of data science. Building up big data platforms. Home of the Plumbers of Data Science Podcast Figuring out how to ingest, process and store data to enable the Data Scientist to do awesome stuff for customers. Using tools like Hadoop, Spark and Kafka. That's what data engineering, the plumbing of data science is all about. An awesome job I am doing for almost eight years now :)"
Top YouTube Channels for Data Science - KDnuggets,"Subscribers: 19.0 K, Videos: 77, Views: 0.5 M, Start date: Feb 26, 2016"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"When you start pursuing a job in data science, some skills to acquire will be obvious. You know you’ll need experience in coding, analytics and mathematics, but you should also cultivate some soft skills. While these may not come to mind immediately when you think of data science, they’ll play a critical role in your career"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"Many jobs will look for applicants with strong critical thinking skills, especially in data science. You should be able to see a problem from multiple perspectives, understand how to approach it and analyze your results. This process is the foundation for many data science applications, even if it isn’t unique to the industry"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"As a data scientist, you need to know how to frame a question correctly, not just answer it. You have to analyze an issue from multiple angles to find the root of the problem. After resolving something, you should reflect on the process and understand why it proceeded the way it did"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"To develop critical thinking skills, take on various problem-solving projects in your spare time. Try approaching them from multiple perspectives and demonstrating a diversity of methods in resolving them. Cultivating a portfolio of these projects can show potential employers your knack for critical thinking"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"You may not think of data science as a communication-heavy field, but that’s far from the truth. While analysis may be the core of your work, you have to communicate your results. Data science involves a lot of collaboration and reporting, so you should know how to do so effectively"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"Thankfully, developing and showcasing communication skills is relatively straightforward. As you go throughout your work and personal life, seek team projects. The more you work in a group, the better you’ll become at communicating, and you’ll have evidence of it"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"A good data scientist will look for solutions to problems, but a great one will seek things to fix. Data science is a potentially disruptive field, so you should be able to think outside of traditional frameworks. Intellectual curiosity drives data scientists to find under-the-radar issues and solve them creatively"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"Employers want a data scientist with a drive to learn more. This frame of mind helps find solutions and can lead to company expansion. Curiosity drives growth, so any business will be happy to find an intellectually curious candidate"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"To cultivate intellectual curiosity, start asking questions. Pursue independent projects and ask why and how at every step of the process. Over time, you’ll develop a portfolio full of unique problem-solving approaches and a history of curiosity"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"Data science influences many aspects of business today, so you’ll have to apply yourself to various situations. As a tech-centric field, data science is also always evolving. New technologies and methods emerge regularly, and you’ll have to be able to adapt to them"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,You can develop adaptability by intentionally putting yourself in unfamiliar situations. Start some projects in an area you’re less comfortable or knowledgeable in. Volunteer to be part of new projects or processes in your current job or at school. You’ll learn how to evolve in the process
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"You should have reliable time management skills as a data scientist. It can be a demanding field, and it’s easy to feel overwhelmed in today’s fast-paced work environment. If you can effectively manage your time, you’ll be more productive and avoid burnout"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"You can start applying time management techniques in your current position or with schoolwork. Test different strategies, like setting timers and prioritizing essential tasks, and find a method or combination that works best for you. You can then explain to potential employers how you use these to manage your time effectively"
5 Supporting Skills That Can Help You Get a Data Science Job - KDnuggets,"As you work to acquire the experience and technical skills you need for data science, remember these support skills. If you can cultivate these abilities, you’ll become a more valuable candidate. Even if you already work in a related field, you can start applying these to maximize your potential"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"Data science (DS) is a relatively new profession compared to other types of roles in the tech industry, such as software engineering and product management. Initially, DS interviews had a limited coding component, including only SQL or applied data manipulation sessions using Python or R. In recent years, however, DS interviews have shown an increased emphasis on computer science (CS) fundamentals (data structures, algorithms, and programming best practices)"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"For someone looking to enter the data science profession, this trend towards more CS in interviews can be daunting. In this post, we hope to increase your understanding of the coding interview and teach you how to prepare for it. We will categorize different coding questions and provide tips to crack them so that you can have a stellar interview. You can reach out to us"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"In contrast, if you are interviewing for a DS role with a Product Analytics emphasis, there is a lower likelihood of encountering coding questions. Interviews for these roles do not often go beyond evaluating SQL proficiency, but general programming may still be tested from time to time. Candidates who do not possess a basic level of coding knowledge can be easily caught off guard during the interview and may fail to move forward in the process. Do not let that be you! Make sure you are prepared. You can start your preparation by learning what to expect with a coding interview"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"A coding interview can appear during the technical phone screen (TPS), onsite, or both. There could even be multiple rounds of coding interviews during the onsite portion, depending on the coding proficiency expected. In general, you should expect coding interviews in at least one stage of an overall DS interview loop"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"During the TPS, the delivery of the coding interview will typically be through online integrated development environments (IDEs) such as CoderPad, HackerRank, and CodeSignal. During onsite sessions, either an online IDE or a whiteboard can be used. In the current remote interview environment, the former is used by default"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"Typically, multiple questions will be asked about a single scenario, ranging from simple to hard. Each question may cover a unique data structure or algorithm. Here is an example of a classic problem that revolves around finding the median of a list of numbers:"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"For such questions, the candidate is expected to code up a solution to a hypothetical applied problem, which is usually related to the company’s business model. These questions are easy to medium in the level of difficulty (based on the categorization of Leetcode). The key here is to understand the business scenario and exact requirements before coding"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"For each of the four major question themes outlined above, begin by reviewing the fundamentals. These descriptions can be found in various online sources as well as books. Specifically:"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"You can save the problem statements in an organized manner, ideally grouped by theme using tools such as Notion or Jupyter notebooks. For each of the topics, practice a lot of easy questions and a few medium ones. Taking the time to create a categorized collection of coding problems will not only benefit your current job search, but it will also prove helpful for future job searches"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"The effectiveness of your communication matters significantly. Before coding, clearly communicate your thought process. If the interviewer asks questions at any point during the interview, you need to be able to explain the reasoning of your assumptions and choices"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"Just as with software engineering coding interviews, for DS coding interviews, it is reasonable to expect multi-part questions and sometimes multiple questions. In other words, speed is also important. Being able to solve more questions within a limited amount of time is a signal of overall proficiency"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"Before the interview, it is worth clarifying with recruiters what kinds of coding questions will be asked, as well as the approximate difficulty level. Lots of data science interviews do not require heavy programming, but that does not mean interviewers will not expect basic coding proficiency at your fingertips. Always ask your recruiter what to expect. If you make incorrect assumptions on the types of questions that can appear during interviews, you may end up preparing inadequately"
The Ultimate Guide to Acing Coding Interviews for Data Scientists - KDnuggets,"Coding interviews, like other technical interviews, require systematic and effective preparation. Hopefully, our article has given you some insights into both what to expect in a coding interview for DS related positions and how to prepare for them. Remember: Enhancing your coding skills will be extremely rewarding not only for landing your dream job, but also for excelling in the job!"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"By reading papers, we were able to learn what others (e. We can then adapt their approach and not have to reinvent the rocket. This helps us deliver a working solution with lesser time and effort"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"This suggestion was the most useful in our discussion. Following up on it led to our product classifier’s eventual accuracy of 95%. How was she able to contribute that critical insight, I asked. To be specific, she tries to read 1–2 papers weekly, usually around topics that the team was working on"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"By reading papers, we were able to learn what others (e. We can then adapt their approach and not have to reinvent the rocket. This helps us"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"The field of NLP has made large advances in the past decade. Nonetheless, by reading the most crucial 10 or so papers, we can quickly get up to speed. By being up-to-date, we become more effective at work, thus requiring less time and effort. We then have more time to read and learn, leading to a virtuous cycle"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"What papers do I read? Out of practicality, I mostly read papers related to work. This allows me to immediately apply what I’ve read and thus reinforce my learning. Outside of work, I have an interest in"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"If it does, I skim through the headings to identify the problem statement, methods, and results. In this example, I’m specifically looking for formula on how to calculate the various metrics. I give all papers on my list a first pass (and resist starting on a second pass until I’ve completed the list). In this example, about"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"This helps me quickly spot important portions when I refer to the paper later. Then, I take notes for each paper. In this example, the notes were mostly around metrics (i. If it was a literature review for an application (e"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"For most papers, the second pass suffices. I’ve captured the key information and can refer to it in future if needed. Nonetheless, I sometimes do a third pass if I’m reading papers as part of a literature review, or if I want to cement my knowledge"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"Various papers have their own methods to measure novelty, diversity, serendipity, etc. I consolidate them into a single note and compare their pros and cons. While doing this, I often find gaps in my notes and knowledge and have to revisit the original paper"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"Lastly, if I think it’ll be useful for others, I write about what I’ve learnt and publish it online. Relative to starting from scratch, having my notes as a reference makes writing much easier. This has led to pieces such as:"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"Before jumping deep into your next project, spend a day or two scanning through a couple of relevant papers. I’m confident it’ll save you time and effort in the medium to long term. Not sure where to start? Here are some useful resources to start with:"
How Reading Papers Helps You Be a More Effective Data Scientist - KDnuggets,"He's currently an Applied Scientist at Amazon. Previously, he led the data science teams at Lazada and uCare. He writes & speaks about data science, data/ML systems, and career growth at"
Data Science Learning Roadmap for 2021 - KDnuggets,"Just a note: I've prepared this roadmap based on my personal experience in data science. This is not the be-all and end-all learning plan. You can adapt this roadmap to better suit any specific domain or field of study that interests you. Also, this was created with Python in mind, as I personally prefer it"
Data Science Learning Roadmap for 2021 - KDnuggets,"Take it one day at a time, one video/blog/chapter a day. It is a wide spectrum to cover. Don’t overwhelm yourself!"
Can Robots and Humans Combat Extinction Together? Find Out April 17 - KDnuggets,"Our goal in designing this conference was not only to present the latest research for leading minds in tech, but also to foster connections between tech professionals at all career stages – from students to business leaders alike. Our networking features will foster discussion that we hope will extend beyond the conference to form new connections. Whether you’re more inclined to swap ideas or trade dank memes, we expect there will be a little something for everyone"
Can Robots and Humans Combat Extinction Together? Find Out April 17 - KDnuggets,"Join us to find out how robots and humans can combat extinction or for a host of great presentations at the DataYap Virtual Conference, designed for technology leaders and enthusiasts at all career stages. This one-day virtual event will feature a full day of panels presenting the latest developments in data science, artificial intelligence, cybersecurity, and more. We are excited to welcome Dr. Kirk Borne (Data Scientist & Top Worldwide Data Science Influencer) as our keynote speaker. We will also feature presentations by Fermilab, a panel dedicated to diversity/equity/inclusion in tech, and high performance data analytics, among others. Attendees will have the chance to ask questions and network with tech professionals as part of the conference. Treat yourself to a full day of the hottest tech topics and opportunities to network with industry professionals. View the full DataYap Virtual Conference schedule and register today for early bird pricing:"
People Skills for Analytical Thinkers - KDnuggets,"You’re not alone. Many data professionals have this experience. Luckily, there is a way to prevent the frustration: people skills"
People Skills for Analytical Thinkers - KDnuggets,"This is not just another book on communication: it’s written in an analytical language. Through data and algorithm metaphors, you’ll deepen your knowledge of human behavior. The writing style makes it easy to read"
People Skills for Analytical Thinkers - KDnuggets,"He loves to help data professionals make a bigger impact with their data skills. Next to training, Gilbert enjoys writing. Last year, he published the bestselling book """
10 resources for data science self-study - KDnuggets,"Several top universities offer traditional graduate-level programs in data science. Because these are graduate-level programs, most will require an undergraduate degree in an analytical field such as physics, mathematics, accounting, business, computer science, or engineering. These programs typically have a duration of 3 to 4 semesters for those who pursue full-time enrollment. Traditional programs come in different flavors, such as: Data Science Master’s, Data Analytics Master’s, or Business Analytics Masters. The cost of tuition for traditional face-to-face programs could be anywhere in the range of $15,000 to $40,000, not including living expenses. For online data science master’s programs, the cost could be anywhere from $12,000 to $40,000.  The links below examine top MS degrees, online, in Europe, and US/Canada, with ranking, tuition, and more"
10 resources for data science self-study - KDnuggets,"If you want, put in four years at a college (or more at a graduate school). This will give you a deeper understanding of the field of data science, but if your circumstances don’t allow you to pursue a college degree, you can (with some passion and dedication) teach yourself data science through self-study. There are so many excellent online data science courses on platforms such as edX, Coursera, DataCamp, Udacity, and Udemy. By dedicating some time, you can learn the fundamentals of data science from these courses. The self-study pathway is thus very affordable compared to the college degree pathway"
10 resources for data science self-study - KDnuggets,"If you are going to be taking one of these courses, keep in mind that some MOOCs are 100% free, while some do require you to pay a subscription fee (which could range anywhere from $50 to $200 per course or more, varies from platforms to platforms). Keep in mind that gaining expertise in any discipline requires an enormous amount of time and energy. So, do not be in a rush. Make sure that if you decide to enroll in a course, you should be ready to complete the entire course, including all assignments and homework problems. Some of the quizzes and homework assignments will be quite challenging. However, keep in mind that if you don’t challenge yourself, you won’t be able to grow in your knowledge and skills"
10 resources for data science self-study - KDnuggets,"The author explains fundamental concepts in machine learning in a way that is very easy to follow. Also, the code is included, so you can actually use the code provided to practice and build your own models. I have personally found this book to be very useful in my journey as a data scientist. I would recommend this book to any data science aspirant. All that you need is basic linear algebra and programming skills to be able to understand the book"
10 resources for data science self-study - KDnuggets,"YouTube contains several educational videos and tutorials that can teach you the essential math and programming skills required in data science, as well as several data science tutorials for beginners. A simple search would generate several video tutorials and lectures. Three of my favorite courses on YouTube are:"
10 resources for data science self-study - KDnuggets,"As data science is a practical field, academic knowledge obtained from coursework alone will not make you a data scientist. You need to apply your knowledge to real-world data science projects in order to qualify as a data science practitioner. The following platforms will enable you to hone your data science skills by applying your knowledge to practical problems"
10 resources for data science self-study - KDnuggets,Internships provide an excellent opportunity to work on data science projects. Several companies provide opportunities for students to intern for a period that can range from few months to a year. Data science internships are typically advertised via platforms such as indeed
10 resources for data science self-study - KDnuggets,"As data science is a field that is continuously evolving due to technological developments in the field, continuous study is essential in data science. Creating a network of collaboration with other data scientists would enable you to always be on top of the game. The following platforms are great resources for networking and continuous studies"
10 resources for data science self-study - KDnuggets,"If you are interested in using this platform for data science self-study, the first step would be to create a medium account. You can create a free account or a member account. With a free account, there are limitations on the number of member articles that you can access per month. A member account requires a monthly subscription fee of $5 or $50/year. Find out more about becoming a medium member from here:"
10 resources for data science self-study - KDnuggets,"With a member account, you will have unlimited access to medium articles and publications. Medium has several data science publications that can help you to learn about new developments in the field as well as network with other data scientists or aspirants. The 2 top data science publications on the medium are"
10 resources for data science self-study - KDnuggets,"LinkedIn is an excellent platform for networking. There are several data science groups and organizations on LinkedIn that one can join, such as Towards AI, DataScienceHub, Towards data science, KDnuggets, etc. You can also follow top leaders in the field on this platform"
Data Observability: Building Data Quality Monitors Using SQL - KDnuggets,"Next, we want to assess the field-level, distributional health of our data. Distribution tells us all of the expected values of our data, as well as how frequently each value occurs. One of the simplest questions is, “how often is my data"
Data Observability: Building Data Quality Monitors Using SQL - KDnuggets,"As detection algorithms go, this approach is something of a blunt instrument. Sometimes, patterns in our data will be simple enough for a threshold like this to do the trick. In other cases, though, data will be noisy or have other complications, like"
Telling a Great Data Story: A Visualization Decision Tree - KDnuggets,Have you ever seen a great looking dashboard or report that doesn’t do much more than just look good? You can’t really figure out the story. It’s usually because the developers haven’t picked the right visualizations and organization. Picking the right visualization will tell the story that you may never have the time or opportunity to tell
Telling a Great Data Story: A Visualization Decision Tree - KDnuggets,"Imagine presenting a chart in which you hope to highlight an important trend over the past twelve months. After observing it, the executives come out of the meeting conversing about this month’s record high measure. Your point has been missed. You didn’t tell a story. This year when virtual meetings are creating a bigger challenge to capture and hold the attention of attendees, no one can afford to waste time trying to decode the meaning of a visualization"
Telling a Great Data Story: A Visualization Decision Tree - KDnuggets,"Great stories have compelling characters, a looming conflict, and well-organized narrative arc. Each of your key metrics is a character in the story. Conflict arises when one metric threatens to undermine another. In our banking case study, a great presentation could begin at the climax of conflict, drawing attention to the pressing problems with liquidity and looming loan defaults with scorecard visualizations. Then the narrative arc could loop back to the beginning of the trouble showing time trend visualizations of how individual metrics began to change as the pandemic unfolded. Next you would compare categories to add nuance to the metrics, showing, for example, that loan risk has not increased evenly across industries.  This type of character development helps build more anticipation to the looming conflict as multiple metrics interact, as shown in a scatter plot type visualization. Finally you arrive back at the climax of the conflict, with the audience in heavy anticipation to see what you will propose to resolve the situation. The overall presentation will leave a powerful affect on the audience due to the combined impact of well-crafted narration backed by visual pictures"
Telling a Great Data Story: A Visualization Decision Tree - KDnuggets,"A visualization should speak for itself. You should not need to spend time trying to tell its story or do its job. Stephen Few writes that “An effective [visualization] is the product not of cute gauges, meters, and traffic lights, but rather of informed design: more science than art, more simplicity than dazzle. It is above all else, communication"
Telling a Great Data Story: A Visualization Decision Tree - KDnuggets,"Some data stories involve finding trends in a collection of measurements. For example, our bank may want to look for red flags that a borrower is a risk of defaulting on a loan. Those red flags may include financial ratios, account balances, and missed payments"
Telling a Great Data Story: A Visualization Decision Tree - KDnuggets,"Some measures demand special attention and can be presented as a single number in extra large or colored font, such as a total revenue number or percent change in deposits. These are your story headlines. A"
Telling a Great Data Story: A Visualization Decision Tree - KDnuggets,"The bottom line is that there is a science behind how to select the right visualization. You can’t just hand the assignment to a graphic designer to select a visualization based on the look. To make a visualization tell your story, you need the visualization type that is built for your purposes.  Learning the concepts outlined in figure 1 will make for a more powerful and effective story told"
Online MS in Data Science from Northwestern - KDnuggets,"Advance your data science career with Northwestern. Build the essential technical, analytical, and leadership skills needed for careers in today's data-driven world in Northwestern's Master of Science in Data Science program. Apply now"
"Data Science vs Business Intelligence, Explained - KDnuggets","For example, hundreds of thousands of data science (DS) and business intelligence (BI) jobs will open up in the next few years. The pool of candidates can seem impossibly small or surprisingly large depending on how relevant and useful you judge the required skills of each position to be. Can a BI expert successfully transition to a DS role? Is DS important to BI positions?"
"Data Science vs Business Intelligence, Explained - KDnuggets","In 2021, business executives are going to evaluate billions of dollars of new projects. What gets the green light and what gets shelved will be affected by how executives and their teams understand and define the two terms. Project champions are quick to attach industry buzzwords to their projects to ride the latest trend, but are such projects just a rebranding of older ideas?"
"Data Science vs Business Intelligence, Explained - KDnuggets","Data management and data visualization are still at the core of any effort to understand and plan a business. These involve technologies and processes to capture, clean, standardize, integrate, visualize, and secure data in a high-performing way. Excel is not enough. A pretty dashboard is not enough. You must commit long term to preserve data as an asset, and you need discipline to build and maintain data lake and data warehouse environments"
"Data Science vs Business Intelligence, Explained - KDnuggets","The crucial point is that any DS or BI initiative that does not have a solid data foundation will be unsustainable. Any processes that are built on manual, inconsistent processes will be slow, untrustworthy, and resource intensive. Eventually they need to mature with professional IT assistance, or they will fall apart under their own weight"
"Data Science vs Business Intelligence, Explained - KDnuggets","Since the 1980s, nearly every company has tried to use computers and databases to manage and understand their historical data. However, here is the thing -- after almost 40 years, nobody has truly mastered it. Every year, companies add or replace software systems, and the IT department can rarely keep up, so enterprises end up constantly prioritizing projects to see which data gets attention and which data gets ignored (sorry, marketing department)"
"Data Science vs Business Intelligence, Explained - KDnuggets","BI has a permanent advantage over DS because it has concrete data points; few, simple assumptions; self-explanatory metrics; and automated processes. Furthermore, BI will never go away. It will always be a work in progress because you will never stop changing your business or upgrading and replacing the source systems"
"Data Science vs Business Intelligence, Explained - KDnuggets","Looking in the rearview mirror of data is important and helpful, but it's limited and will never get you where you want to go. At some point you need to look ahead. BI needs to be accompanied by data science"
"Data Science vs Business Intelligence, Explained - KDnuggets","Where traditional planning is done in discrete, human-directed sessions, DS techniques should result in planning and optimization steps that are embedded into software and run as part of automated processes. The model is trained using historical data, setting aside a subset of data to validate the accuracy of prediction. If the results are promising, then the model is deployed and monitored, often using BI reports"
"Data Science vs Business Intelligence, Explained - KDnuggets","As a result, pilot projects can be slow to start and frustrating to sustain. The results you achieve may be sporadic because of the unpredictability of your work. Predicting the future is never going to be simple"
"Data Science vs Business Intelligence, Explained - KDnuggets","Although BI projects are often completed by a single person, DS projects require extensive cooperation between employees who don't usually speak the same language, including data engineers, statisticians, business experts, and software developers. The competencies of each position require many years to master. Data scientists often have deep expertise in statistics but only elementary software development skills and limited business expertise. DS teams need to partner with IT and business departments to create truly integrated solutions"
"Data Science vs Business Intelligence, Explained - KDnuggets",The bottom line is that the difference between the terms is a matter of whether you need to look back (BI) or look forward (data science). BI collects data to understand events in the past. DS generates data to model events that have not yet occurred
"Data Science vs Business Intelligence, Explained - KDnuggets","Knowing the difference between the practices is vital to approving or rejecting a proposed project, hiring staff with skills needed for a BI or a DS project, and architecting a data management platform that can support both. We should avoid looking at them as competing initiatives or as fads that will pass. Both data science and business intelligence are here for the long term and will be major differentiators for business that harness their potential"
Who is fit to lead data science? - KDnuggets,"A few months ago, I spoke with the professional colleague who I just described above because she was on the market after years of leading teams for a Fortune 100 company in an industry that competes heavily on analytics and data science. She was lamenting that so many job descriptions want ""hands-on"" experience with the latest tools. I’ve long seen this gap between what is needed and what so many job postings demand, so I"
Who is fit to lead data science? - KDnuggets,"I’ll repeat my plea to HR and those hiring data science leaders: if you want to maximize results from your data science teams, please hire people who are good at leading people and have experience delivering results with data science. Stop insisting on years of hands-on experience with the latest tools and techniques, which they won’t touch themselves once they get hired. Instead, value the skills they will actually need, which are far more critical to a team’s success than proficiency in GPT-3"
Who is fit to lead data science? - KDnuggets,"While you don't want someone out of touch, what you DO want is someone who knows how to frame business problems, help data scientists abstract that business problem into a technical structure they can model, coach them to explain their results to business leaders, considers usability and deployment, and develop careers. I view the skills needed in data science like the legs on a stool. Programming experience, quantitative rigor, business acumen, and interpersonal skills are all essential, but it takes a while for the stool to be balanced. A junior data scientist is part of a team, so you can afford for his stool to be wobbly because others can compensate for areas where he is weaker. Your leader’s stool needs to be balanced, but that balance is the result of skills that developed into steadiness over time"
Who is fit to lead data science? - KDnuggets,"The balance is illustrated well by an exchange I had years ago with another former colleague, Annie Tjetjep, who relates development for data scientists to frozen yogurt. She argues that people start with one set of strengths, just like the first swirl of frozen yogurt added to the cup. For most junior data scientists, the first legs to develop are in programming and quantitative rigor, which they learn in university or through online courses and practice. And in spite of the fact that"
Who is fit to lead data science? - KDnuggets,"As a data scientist moves into more senior leadership roles over her career, she’ll need to develop the legs of her stool, focusing on business acumen and interpersonal skills. By the time she reaches the director level or higher, she hasn’t likely written hands-on code in years, nor should she. She will be busy"
Who is fit to lead data science? - KDnuggets,"The italics are mine because my point is that the italicized verbs are the kinds of things I expect someone at a senior leadership level to be doing, not writing code. The leader needs to understand the business value, technical strengths, and limitations of Tensorflow, Keras, and XGBoost, but she doesn’t need to gain that knowledge by hand. At the senior leadership level, she has spent at least the last 5-6 years rising in leadership ranks since the birth of those tools and is no longer in the trenches slinging code"
Who is fit to lead data science? - KDnuggets,"Those skills are the result of lessons learned, years of listening to business users complain about their problems, and translating those symptoms into a properly-framed business problem. Junior data scientists may jump into solving their first pass at the problem and throwing the fanciest math they know at a solution. Over time they will improve at abstracting the problem correctly and choosing the right methods (not always the latest) to address it, but they will get there faster if led by someone who already learned those lessons. Experience has taught me volumes, like finally succeeding at explaining operations research to health care leaders who think OR means the operating room or having my team’s models languish because we didn’t involve IT soon enough in planning for deployment"
The Best Data Science Project to Have in Your Portfolio - KDnuggets,"If you are trying to find your first path into a Data Science career, then demonstrating the quality of your skills can be the greatest hurdle. While many standard projects exist for anyone to complete, creating an original data-driven project that attempts to solve some challenge is worth so much more. A good Data Scientist is one that can solve data-related questions, and a great Data Scientist poses original data-related questions and then solves"
The Best Data Science Project to Have in Your Portfolio - KDnuggets,"A data scientist identifies a problem and comes up with a suggested solution. The projects listed in those articles lay out the problem and solution for you. Moreover, it is relatively easy to obtain the required data in a neat and clean format"
The Best Data Science Project to Have in Your Portfolio - KDnuggets,"Since most of those projects are so common, the solution or implementation can easily be found online. Thus, think of them as projects to practice your skills. Unfortunately, they are not enough to convince hiring managers that you are a good candidate for a data scientist position"
The Best Data Science Project to Have in Your Portfolio - KDnuggets,"What we should do instead is to find a problem that can be solved with data and design our solution. The problem does not have to be complex, and we do not have to provide the best and most efficient solution. We might even fail to properly solve the problem"
The Best Data Science Project to Have in Your Portfolio - KDnuggets,It helps a lot to convince your future employer to have one or two projects that you build from scratch. You might even end up having a new business idea. Another advantage of having unique projects is that they attract recruiters and hiring managers. They are likely to reach out to you instead of you applying to numerous positions
The Best Data Science Project to Have in Your Portfolio - KDnuggets,You may argue that it is a highly challenging task to come up with a new project idea. I completely agree with you. This is the reason why I call it the best project to have in your portfolio
The Best Data Science Project to Have in Your Portfolio - KDnuggets,"I’m aware that it takes a great amount of time, effort, and thinking to come up with a unique project idea. Moreover, you will be spending long hours trying to implement your idea. It is important to point out that you might end up having a failed project. However, what you learn throughout the process are likely to be some skills that you cannot learn from a MOOC course or any tutorial"
The Best Data Science Project to Have in Your Portfolio - KDnuggets,"You will also be improving your skills on how to approach an issue. You will learn how to evaluate a task from different perspectives. In some cases, the solution you have in mind does not fit the libraries or frameworks you are comfortable working with. Thus, it will also motivate you to learn new tools"
The Best Data Science Project to Have in Your Portfolio - KDnuggets,"The 10 projects to have in your portfolio sounds appealing. I did some of those projects too. However, keep in mind that most people you compete with to get a data scientist job are doing those 10 projects as well. You will not be left behind, but doing the same thing will not take you further ahead either"
The Best Data Science Project to Have in Your Portfolio - KDnuggets,"Hiring managers or recruiters will know how much you put into a project. Some of the popular commonplace projects can be completed in a day or two. Thus, you will have a hard time demonstrating your skills based on those projects"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","Getting interviews is naturally essential for many jobs and data science jobs are no exception. Although resources on this topic are certainly not lacking, practical and actionable advice is rare. Going into my job search I knew that over 70% of job seekers find employment through some form of networking. I knew that I should “"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets",The first time I looked for a data science job was when I was about to complete grad school in Feb. 2017. I tried everything I could to get interviews including…
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets",The second time I found myself searching for a data science job was when I was laid off by my then startup company in Dec. 2018. This time around I got
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","This enormous difference was not because I had job experience. In truth, I was not much more competitive on the job market the second time. This drastic change was because I did it all wrong the first time! This was not because the articles on getting a data science interview were misleading or incorrect, but they only told me what to do not how to do it"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","Raw application means simply submitting your resume to job openings. Although this is an easy method, it also tends to have low efficiency. Contacting the gatekeeper can be more effective but requires a bit more effort. Getting a referral is the most effective way, but it also takes the most amount of time and effort (assuming that you don’t know anyone willing to refer you yet). The below diagram gives a visual of the three methods in terms of effort/time and effectiveness"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","Why? The problem was that for each job post, hundreds of applicants applied within a week. If you rely solely on these popular job boards, the chances that you will get a response are slim. You are competing against a mountain of candidates!"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","Even though the websites I recommend tend to have higher response rates than big job boards, it can still take weeks for recruiters to respond since they always have a pile of resumes to review. Now you would want to try the second method — reaching out to the gatekeeper directly. I was able to get responses much faster using this method than raw applications"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","If the gatekeeper sees exactly the same email template, it will leave a negative impression. You can change things. You can make it longer or shorter. Just remember that the main idea is to show your interest and send everything that’s necessary"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","When looking for jobs the first time, I reached out to alumni, people with common friends, and even random people to get referrals. However, I ended up not getting any. In contrast, the second time I looked for jobs, people within my network told me they were willing to refer me before I even asked"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","Whether they are a product manager, software engineer, product designer, or anything else. A relationship goes a long way. Haseeb Qureshi has a"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","Depending on your goal and availability, you may choose one or more of the three methods to get interviews. But no matter which methods you use, a good resume is key. This is the thing that turns the table with recruiters"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","A resume is a summary of your achievements. It should be brief, so it’s not a place to showcase everything you know or can do. Adding too much content only overwhelms people with unnecessary information. Therefore, in your data science resume, you want to highlight the most important things related to data science, such as work experience, training, and relevant skills. When a recruiter or a hiring manager sees your resume, they should immediately feel that you have lots of experience in data science and that you are a qualified candidate"
"How to Get Data Science Interviews: Finding Jobs, Reaching Gatekeepers, and Getting Referrals - KDnuggets","At the end of the day getting data science interviews is difficult, especially for beginners. It can be discouraging when you don’t get any response even when you have been working diligently for months. Hopefully, this article has made things clearer for any aspiring data scientists out there who are in the process of landing a job. If you want more advice feel free to contact me"
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,"As vectors, matrices are data structures allowing you to organize numbers. They are square or rectangular arrays containing values organized in two dimensions: as rows and columns. You can think of them as a spreadsheet. Learn more here"
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,"They are square or rectangular arrays containing values organized in two dimensions: as rows and columns. You can think of them as a spreadsheet. Usually, you’ll see the term"
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,"Using Numpy, the indexing process is the same to that of vectors. You just need to specify two indexes. Let’s take again the following matrix"
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,Being able to manipulate matrices containing data is an essential skill for data scientists. Checking the shape of your data is important to be sure that it is organized the way you want. It is also important to know the data shape you’ll need to use libraries like Sklearn or Tensorflow
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,Figure 2 illustrates the steps of the product between a matrix and a vector. Let’s consider the first row of the matrix. You do the dot product between the vector (the values 3 and 4 in red) and the row you’re considering (the values 1 and 2 in blue). You multiply the values by pairs: the first value in the row with the first in the column vector (
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,There is another way to think about the matrix product. You can consider that the vector contains values that weight each column of the matrix. It clearly shows that the length of the vector needs to be equal to the number of columns of the matrix on which the vector is applied
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,Figure 3 might help to visualize this concept. You can consider the vector values (3 and 4) as weights applied to the columns of the matrix. The rules about scalar multiplication that you saw earlier lead to the same results as before
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,"Figure 5 shows you an example of matrix product. You can see that the resulting matrix has two columns, as the second matrix. The values of first column of the second matrix (3 and 4) weight the two columns and the result fills the first column of the resulting matrix. Similarly, the values of the second column of the second matrix (9 and 0) weight the two columns and the result fills the second column of the resulting matrix"
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,Keep this property in a corner of your mind. It explains many “cosmetic rearrangements” that you can encounter when matrices and vectors are manipulated. Trying these manipulations with code is a great way to learn
Essential Math for Data Science: Introduction to Matrices and the Matrix Product - KDnuggets,"He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog ("
Build Your First Data Science Application - KDnuggets,"When I started my journey to learn data science, those were the questions that I always had in my mind. My intention to learn data science was not only to develop models or clean data. I wanted to make applications that people can use them. I was looking for a fast way to make MVPs (minimum viable products) to test my ideas"
Build Your First Data Science Application - KDnuggets,"Data science and machine learning application are all about data. Most datasets are not clean, and they need some sort of cleaning and manipulation for your project. Pandas is a library that lets you load, clean, and manipulates your data. You may use alternatives like SQL for data manipulation and database management, but Pandas is much easier and more applicable for you as a data scientist who wants to be a developer (or at least MVP developer) as well"
Build Your First Data Science Application - KDnuggets,"In many data science projects, including computer vision, arrays are the most important data type. Numpy is a powerful Python library that lets you work with arrays, manipulate them, and efficiently apply algorithms to them. Learning Numpy is necessary to work with some other libraries that I mention later"
Build Your First Data Science Application - KDnuggets,"Neural networks, especially deep neural network models, are very popular models in data science and machine learning. Many computer vision and natural language processing methods rely on these methods. Several Python libraries provide you access to neural network tools. TensorFlow is the most famous one, but I believe it is difficult for beginners to start with TensorFlow. Instead, I suggest you learn Keras, which is an interface (API) for Tensorflow. Keras makes it easy for you as a human to test different neural network architectures and even build your own. The other option which is getting popular recently is PyTorch"
Build Your First Data Science Application - KDnuggets,"Nowadays, many data science applications are working with APIs (Application Programming Interfaces). In simple words, through an API, you can request a server application to give you access to a database or do a specific task for you. For example, Google Map API can get two locations from you and return travel time between them. Without APIs, you must reinvent wheels. Requests is a library to talk to APIs. Nowadays, it is hard to be a data scientist without using APIs"
Build Your First Data Science Application - KDnuggets,"Plotting different types of graphs is an essential part of data science projects. Although the most popular plotting library in Python is matplotlib, I found Plotly more professional, easy to use, and flexible. The types of plots and mapping tools in Plotly are enormous. The other nice thing about Plotly is its design. It looks more user-friendly compared to matplotlib graphs, which have a scientific look"
Build Your First Data Science Application - KDnuggets,"You must choose between the traditional-looking user interface and web-based user interfaces when it comes to the user interface. You can build traditional-looking user interfaces using libraries like PyQT or TkInter. But my suggestion is to make web-looking applications (if possible) that can run on browsers. To make it happen, you need to work with a library that gives you a set of widgets in the browser"
Build Your First Data Science Application - KDnuggets,"The last tools that you need to learn to make your first data science application are the easiest ones. First, ipywidgets works in Jupyter Notebook, and you need to use Jupyter to make your application. I am sure many of you already use Jupyter Notebook for your model building and exploratory analysis. Now, think about Jupyter Notebook as a tool for front-end development. Also, you need to use Voila, a third-party tool that you can launch, and it hides all the code parts from Jupyter Notebook. When you launch a Jupyter Notebook application via Voila, it is like a web application. Even you can run the Voila and Jupyter Notebook on an AWS EC2 machine and access your simple application from the internet"
Build Your First Data Science Application - KDnuggets,"Using the 7 libraries that I mentioned in this article, you can build data science applications that people use. By becoming a master in using these tools, you can build MVPs in a few hours and test your idea with real users. Later, if you decided to scale up your application, you can use more professional tools like Flask and Django in addition to HTML, CSS, and JS codes"
How to Get Your First Job in Data Science without Any Work Experience - KDnuggets,"Whether you’re a new graduate, someone looking for a career change, or a cat similar to the one above, the data science field is full of jobs that tick nearly every box on the modern worker’s checklist. Working in data science gives you the opportunity to have job security, a high-paying salary with room for advancement, and the ability to work from anywhere in the world. Basically, working in data science is a no-brainer for those interested"
How to Get Your First Job in Data Science without Any Work Experience - KDnuggets,"I got my first job in tech by maintaining a relationship with a university colleague. We met as a result of being teamed up for our final four-month-long practicum. After graduation, we kept in touch. Almost two years later, I got a message saying that the company they work for is interested in hiring me to do some work for them. Thanks to maintaining that relationship, I managed to score my first job after graduation with no work experience thanks to my colleague putting my name forward"
How to Get Your First Job in Data Science without Any Work Experience - KDnuggets,"As I mentioned above, being a storyteller and an overall solid communicator, are essential skills of data scientists that only improve when they’re being practiced. For example, by explaining the results of your data analysis to the general public, you begin to think of data in simple terms that anyone can understand and appreciate. As"
How to Get Your First Job in Data Science without Any Work Experience - KDnuggets,"As a future data scientist, articles you’ve written become part of your professional portfolio and give recruiters insight into your comprehension of particular concepts. Not only will they be able to see that you’ve been able to build a following of people who trust and value your work, but they will also be able to see that you’re willing to contribute knowledge to further the lives and careers of fellow data scientists. Furthermore, publishing on a website that pays you for your work tells recruiters that people value your knowledge so much, that you’re actually getting paid for it"
How to Get Your First Job in Data Science without Any Work Experience - KDnuggets,"If you have the skills and the confidence, why not take on some freelance clients? It’s a win-win situation. You get real-world experience without having to go through the pain and suffering of the hiring process (mind you, there can be just as much pain and suffering doing freelance work which is why it’s not for everyone). The beauty of hiring yourself is that if you finally get a job offer from one of your coveted companies thanks to the real-world experience you’ve been able to accumulate, you can walk away from freelancing at any time"
How to Get Your First Job in Data Science without Any Work Experience - KDnuggets,"Interning, volunteering, or doing pro bono work, are three of the best ways to get the necessary work experience that many companies are looking for. Not only do these “jobs” allow you to gain real-world experience using real-world data, but it also shows hiring managers that you’re a team player who earned their work experience the hard way without pay. Furthermore, you might get the opportunity to create meaningful solutions that will positively impact many individuals and communities along the way. If the company you work for is willing to compensate you with a glowing review on your LinkedIn profile or a reference letter, even better!"
How to Make Sure Your Analysis Actually Gets Used - KDnuggets,"One of the most universally demoralizing experiences is to see the results of your hard work go unseen, unappreciated, and unused. In the world of data, that is something we experience all too often. Take the following hypothetical situation:"
How to Make Sure Your Analysis Actually Gets Used - KDnuggets,"We’ve been told that data can speak for itself, that a well-crafted chart can communicate all its nuances on its own. This is simply not true. A chart alone cannot possibly convey everything, and that kind of thinking inhibits our ability to influence the business with our work"
How to Make Sure Your Analysis Actually Gets Used - KDnuggets,"So to really make sense of your analysis, your users will need to poke and prod it. Kolb’s Learning model suggests they’ll need to experiment with our analysis and take the time to reflect on its real-world implications before they can properly understand it. Let’s help them get there"
How to Make Sure Your Analysis Actually Gets Used - KDnuggets,"At a minimum, this involves setting up interactive elements for your analysis. Add filters and parameters that let the user start to interrogate the data. What if you had double the budget? Half it?"
How to Make Sure Your Analysis Actually Gets Used - KDnuggets,"One part of this process that is severely neglected is the question of turning this analysis into scalable knowledge. How do you make sure the business question you’ve just answered is shared not just with Jim or Jim’s team but with the wider company? And not just this week, but that it can be used in 6 months when the same question comes up again. The answer is unequivocally not a dashboard but something more nuanced"
How to Make Sure Your Analysis Actually Gets Used - KDnuggets,"[3] has been to implement a Knowledge Feed that takes the type of detailed analysis we’ve just outlined and publishes it for the whole company to find. The result is a collection of reports that are easily understood by all users but still have access to the raw code and notes for analysts to use as a starting point for future work. The key attributes are documented that give everyone confidence in what they’re seeing (when it was published, the limitations, etc. And they’ve made this database of knowledge easily parsable so people can quickly find the analysis related to their questions before they’ve submitted their request to the data team"
How to Make Sure Your Analysis Actually Gets Used - KDnuggets,"The benefit of this kind of way of working is it’s easy to test. The next time a request comes through from one of your more friendly business users (avoid pirates), I suggest trying this method out. Instead of materializing the chart they requested, ask to meet with them to better understand what they hope to do with this chart. What decisions is it informing? Who is the audience?"
Does Data Science Make You Happy? - KDnuggets,"You feel happier with every line of code that you have added, bringing you closer to the result. You enjoy typing. You enjoy recalling the functions from your memory effortlessly and fitting them into the processing logic you have outlined before"
Does Data Science Make You Happy? - KDnuggets,"Even if they are not at all better than the old ones. Even if they are messy and contra-intuitive, you still like learning new data science tools. Just because it makes you feel like a child in a sandbox building a sandcastle that none will live in"
Does Data Science Make You Happy? - KDnuggets,You may be a shy person and avoid public speeches or giving presentations in front of a huge audience. But you do not refrain from answering your best friends’ questions about your work. You are proud of your professional field at a semi-conscious level
Does Data Science Make You Happy? - KDnuggets,"Then, step by step, you do some data cleaning, transform it, and — voilà! — you see clear dimensions, patterns, and possible dependencies. It is like watching from the airplane window when taking off the ground. You were standing near the airport building, but then you gradually get higher and higher and all of a sudden you see the whole city at once!"
Does Data Science Make You Happy? - KDnuggets,"After I have settled inside the two fields that make me feel happy — data science and technical writing — my career started to develop more organically. I stopped fighting for the next nice title. I absorbed new knowledge without thinking about immediate rewards. And this attitude began to pay back, although a bit of patience was indeed required"
Does Data Science Make You Happy? - KDnuggets,"When you conclude your day with a good feeling, you’ll be more productive the next morning. When you radiate calm and self-confidence, you can stop worrying about job competition. Companies will hire you for being a rational and secure person: in addition to being an efficient data scientist!"
3 Ways Understanding Bayes Theorem Will Improve Your Data Science - KDnuggets,"Let’s say your friend calls to tell you that she’s very sorry, but she cannot make it to dinner tonight. She’s recently adopted a pet koala who has developed a case of the sniffles. She really needs to stay home to monitor the situation"
3 Ways Understanding Bayes Theorem Will Improve Your Data Science - KDnuggets,"But there was a problem. As my team connected data tables from disparate data sources across the enterprise, we discovered a connection between the materials from the new vendor and a 2.5% increase in scrap"
3 Ways Understanding Bayes Theorem Will Improve Your Data Science - KDnuggets,The plant manager had a very strong prior that the new supplier was a net positive for his business. We had some evidence to the contrary. We also had Bayes Theorem. And we understood this fact:
One question to make your data project 10x more valuable - KDnuggets,"Decisions are so core to our work that we sometimes forget to acknowledge them before starting a data project. We assume it's baked into everything we do, so when the product team asks a question like: ""Which products are commonly purchased together?"" it's natural to jump straight into figuring out what data we need, which approach is best, etc, etc. It's a straightforward ask, and the data work could be fun. How should we visualize the results? What if the patterns have changed over time? We dive right into the data without taking the time to really (I mean really) understand the decision we're trying to inform"
One question to make your data project 10x more valuable - KDnuggets,"These responses are specific and have a clear action in mind. It's clear that the action will change based on what the data says. Unfortunately, not all responses start out as good responses"
One question to make your data project 10x more valuable - KDnuggets,"Next time someone comes to you with a data request, remember the magic question. And remember, a bad response doesn't mean game over. It should be a starting point to get you to the final, better data question. A skilled data analyst will always tease out the real problem and understand the real decision before starting any project. This makes the difference between a data project that is just ""interesting"" and a data project that drives an impact"
Top 5 Reasons Why Machine Learning Projects Fail - KDnuggets,"There is a massive shortage of data scientists in the market. Although there are a number of engineers completing courses and tagging themselves as data scientists, the ones who are truly skilled to see through a complex ML project are extremely limited in number. According to a"
How to Get a Job as a Data Scientist - KDnuggets,"Data scientists are some of the most in-demand professionals today. As data continues to play an increasingly prominent role in modern business, the profession will only become more valuable. Considering this promising outlook, it’s an ideal time to pursue a career as a data scientist"
How to Get a Job as a Data Scientist - KDnuggets,"As with most professions, you’ll need an appropriate education before you can work as a data scientist. Ideally, you should get an undergraduate degree in a related field, such as computer science, information systems, or data analytics. Most professional data scientists also have a master’s degree, typically in a more specialized area within data science"
How to Get a Job as a Data Scientist - KDnuggets,"If you already have a degree, you don’t necessarily need to go back to school for a more relevant one. You should, however, look into online programs where you can take a few data science courses. Seeking out some extra certifications and licenses will also prove helpful"
How to Get a Job as a Data Scientist - KDnuggets,The skills you learn in your classes aren’t the only education you’ll need to become a data scientist. You should also look into learning various programming languages and seek hands-on experience. You can find plenty of books and online courses that will help you develop these skills
How to Get a Job as a Data Scientist - KDnuggets,"You’ll need more than an education to get a job as a data scientist. Most companies will also look for tangible evidence of your skills. Mohammad Shokoohi-Yekta, a former senior data scientist at Apple, says you"
How to Get a Job as a Data Scientist - KDnuggets,"The best way you can show your comfort and knowledge in this area is through a portfolio of your work. As early as you can, start getting involved in hands-on data science projects and compiling them into a portfolio. You can do this through freelance data work and pet projects in areas that interest you"
How to Get a Job as a Data Scientist - KDnuggets,"Your portfolio should feature a variety of different data science projects to showcase your versatility. You should demonstrate skills in various programming languages, industries, and project types. If you can get into any data science-related competitions, your work in those will be an excellent portfolio addition"
How to Get a Job as a Data Scientist - KDnuggets,"Remember to tailor your resume and cover letter to each potential employer. Emphasize your skills and experience that are most relevant to the specific industry and position at hand. In addition to applying to jobs through sites like Indeed, grow your network on LinkedIn and try to build a respectable online presence where employers will notice you"
How to Get a Job as a Data Scientist - KDnuggets,"You may not be able to get a data scientist position at first, and that’s okay. In fact, it may be best to apply to a related but more entry-level position like data analysis first. You can grow your career from there"
How to Get a Job as a Data Scientist - KDnuggets,"On-the-job experience is your best resource for advancing your career. In light of that, try not to be too selective about the first position you accept. If you get an offer for a steady job in a data-related field, but it isn’t your ideal position, you may still want to take it. Think of your first data job as a launching point"
How to Get a Job as a Data Scientist - KDnuggets,"The more you seek out new opportunities within your company, the more relevant experience you’ll gain. As you work, look for chances to move upward both within your current business and other companies. If you show initiative and a remarkable work ethic, you’ll advance as a data scientist before long"
How to Get a Job as a Data Scientist - KDnuggets,"It’s never too late to start a career in data science. But if you know it’s what you want to do, don’t procrastinate. You can start getting the skills and experience you need, today. Being a data scientist is far from easy, but if you follow these steps, you can enjoy a long and rewarding career in data science"
What to Learn to Become a Data Scientist in 2021 - KDnuggets,"There is an inordinate amount of data collected from all nooks and corners of a user's online movements. This data needs to be well stored, maintained and analyzed and systems need to be developed, in order to manage it well. Data Scientists are basically data and technology specialists that tend to do this job well. Job titles like data analysts, data engineers and business intelligence analysts come under the same purview"
What to Learn to Become a Data Scientist in 2021 - KDnuggets,"The above stated job description requires an array of expertise in a number of fields. Also, before taking steps towards a career in data sciences, you also need to understand that data mostly includes numbers. So, if you do not enjoy working with numbers, being a data scientist might not be a very good option for you"
What to Learn to Become a Data Scientist in 2021 - KDnuggets,The above stated job description clearly indicates that a data scientist tends to develop algorithms and systems to sieve through scores of data for business development. Only those with a solid understanding of computer programming can develop such solutions. Software specializations herein include:
What to Learn to Become a Data Scientist in 2021 - KDnuggets,"Storage and assimilation of scores of data has been referred to as big data. As stated in the job description earlier, a data scientist needs to develop models that shall help in acquiring and analysis of acquired data to develop meaningful models and solutions. This kind of"
What to Learn to Become a Data Scientist in 2021 - KDnuggets,"Collecting and analyzing data is just not enough. A data scientist needs to process out meaningful outputs from data sets and present them in a manner that is understandable and usable for the stakeholders. Thus, they need to include various storytelling techniques including data visualizations, to ensure the output is presented well. Various data visualization tools like Matplotlib, Ggplot and D3. To be an able data scientist, you should be well versed with at least one of them"
What to Learn to Become a Data Scientist in 2021 - KDnuggets,"As a data scientist, you tend to develop solutions to business problems, through user data. But, to develop these solutions effectively, you would first need to have a form hold on business requirements and the issues you tend to resolve using big data solutions. Only then would you be able to develop and present an effective solution"
What to Learn to Become a Data Scientist in 2021 - KDnuggets,"Data science is one of the most promising careers of modern times. So, if you wish to see yourself as a data scientist, try acquiring the above stated skills to some level. There are various online tutorials that can help you with python, SQL and other requisite concepts. Try going through them, for a well defined introduction to the world of data science"
"Cloud Computing, Data Science and ML Trends in 2020–2022: The battle of giants - KDnuggets","In this post, we reviewed the state of art with using Cloud computing platforms, products, and tools by the professionals in Data Science and ML industry. These are not just their preferences as of the end of 2020. These are cornerstones which are most likely to determine the trends for 2021–2022 as well"
"Cloud Computing, Data Science and ML Trends in 2020–2022: The battle of giants - KDnuggets","The next couple of years will be crucial in the battle of Cloud Computing giants for minds, arms, and budgets in the Data Science and ML industry. Although AWS’s position still looks stronger than other top rivals, the challenges from GCP could be the intrigues part of the market reshaping in the years to come. At the same time, MS Azure seems to keep its strong positions in North America (while having little chances to penetrate other continents significantly vs. AWS and GCP)"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,Making your big break into the data science profession means standing out to potential employers in a crowd of tough competition. An important way to showcase your skills and experience is through the presentation of a portfolio. Following these recommendations for developing your portfolio will help you network effectively and stay on top of an ever-changing field
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"As a matter of fact, you can even design your own data science curriculum from the innumerable amount of available resources. While knowledge acquired from course work is essential to lay a good foundation in data science, you need to remember that data science is a practical field. As such, hands-on skills are very important, especially if you are interested in working outside academia as a practicing data scientist"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"This article will discuss 4 important platforms that will enable you to build a portfolio to showcase your experience in data science. A strong portfolio will give your employer an edge over the competition in attracting the best possible talent in the workforce. Keep in mind that employers interested in hiring you are going to ask you to provide evidence of completed data science projects. This famous quote from Elon Musk summarizes the mindset of employers in any technical discipline, including data science:"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"GitHub is a very useful platform for displaying your data science projects. As a data science aspirant, GitHub should serve as the first platform that you use as a repository of completed projects throughout your data science journey. These projects could include projects from weekly assignments or capstone projects. This platform enables you to share your code with other data scientists or data science aspirants. Employers interested in hiring you would check your GitHub portfolio to assess some of the projects you’ve completed. So, it’s important for you to build a very strong and professional portfolio on GitHub"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"To establish a GitHub portfolio, the first thing to do is create a GitHub account. Once your account is created, you may go ahead to edit your profile. When editing your profile, it’s a good idea to add a short biography and a professional profile picture. You may find an example of a GitHub profile here:"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"Make sure you choose a suitable title for your repository. Then include a README file to provide a synopsis of what your project is all about. Then you may upload your project files, including the dataset, Jupyter notebook, and sample outputs"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"One of the primary purposes of joining Kaggle is to network with other data science professionals. It doesn’t matter if you are new to data science or if you are a seasoned data scientist, you can find a suitable forum on Kaggle that would allow you to discover content and engage in discussion around topics that you’re interested in. Your end goal should be to enter and participate in data science competitions launched on this platform. Because most competitions encourage teamwork, it is important to build a network with other data science aspirants who can serve as team members for Kaggle challenge competitions. As you participate in Kaggle competitions, you can showcase your completed projects, including your datasets, Jupyter notebooks, and project reports on your public profile"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,LinkedIn is a very powerful platform for showcasing your skills and for networking with other data science professionals and organizations. LinkedIn is now one of the most famous platforms for posting data science jobs and for recruiting data scientists. I’ve actually got numerous data science interviews via LinkedIn
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"Make sure your profile is up-to-date at all times. List your data science skill sets, as well as your experiences, including projects that you’ve completed. It would be worthwhile to also list awards and honors. You also want to let recruiters know that you are actively searching for a job. Also, on LinkedIn, you want to keep up-to-date by following data science influencers and publications such as"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"If you are interested in using this platform for portfolio building, the first step would be to create a Medium account. You can create a free account or a member account. With a free account, there are limitations on the number of member articles that you can actually access per month. A member account requires a monthly subscription fee of $5 or $50/year. Find out more about becoming a Medium member from here:"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"Once you’ve created an account, you can go ahead and create a profile. Make sure to include a professional picture and a short bio. Here is an example of a Medium profile:"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"On Medium, a good way to network with other data science professionals is to become a follower. You can also follow specific Medium publications that are focused on data science. The 2 top data science publications are"
Build a Data Science Portfolio that Stands Out Using These Platforms - KDnuggets,"In summary, we’ve discussed 4 important platforms that could be used for building a data science portfolio. A portfolio is a very important way for you to showcase your skills and to network with other data science professionals. A good portfolio will not only help you keep up-to-date with new developments in the field, but it will help increase your visibility to potential recruiters"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"During this unprecedented time with the pandemic, many are finding their careers affected. This includes some of the most talented data scientists with which I have ever worked. Having shared my personal experience with some close friends to help them find a new job after being laid off, I thought it worth sharing publicly. After all, this touches more than me and my friends. Any data scientist who was laid off due to the pandemic or who is actively looking for a data science position can find something here to which they can relate, and which I hope will ultimately offer hope in your job search"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"In December of 2018, I was informed by my manager that I was to be laid off in January 2019. Three months before, the VP of Engineering of my then startup company had written a letter to our head of People Success. This letter explained why I was one of the top performers in the company and advocated for an increase in my salary. This helped me get a 33% increase in my salary. I was naturally feeling motivated and eager to crack the next milestone on an important project. The company’s future and my own looked bright. It was during this moment of success that I was told that I was impacted by the company-wise cost-cutting initiative. I was let go on January 15th"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"To be forced to start looking for a new job was daunting, to say the least. After browsing the data science job openings on the market, I soon realized my knowledge gap. What I was doing at the B2B startup (a mix of entry-level data engineering and machine learning) was simply irrelevant to many of the job requirements out there, such as product sense, SQL, stats, and more. I knew the basics but was unsure how to fill the gap towards more advanced skills. However, even that issue seemed secondary to more pressing questions, such as how do I even get an interview? I had a mere 1.5 years of work experience with a startup, and I lacked any statistics or computer science-related degree. More questions soon followed. What if I cannot find a job before I lose my visa status? What if the economy takes a downturn before I can find a new job? Despite my fears, there was little choice. I had to find a new job"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"In the face of what felt like an overwhelming task, I needed some information to decide my next steps. After doing some research, I realized that more than half of the data science positions on the market were product-driven positions (‘product analytics’), and the rest were either modeling or data engineering oriented positions. I also noted that positions other than product analytics tended to have higher requirements. For example, most modeling positions required a PhD degree, and engineering positions required a computer science background. Clearly, the requirements for different tracks varied widely, so it followed that preparation for each would differ as well"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"With this knowledge in hand, I made an important decision: preparing for all tracks would be both overwhelming and most likely less effective. I would need to focus on one. I choose product analytics because, based on my background and experience, there was a higher chance that I could get interviews on this track. Of course, not everyone in data science has my exact background and experience, so below I have summarized the general requirements for three categories of data science positions at big companies. Understanding this basic breakdown saved me a lot of time, and I trust it will prove useful for others looking for a job in data science. I will add, however, that for small startups it’s possible that the interview will be less structured and require more of a mixture of all three"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Asking for referrals proved to be much more effective than applying by myself. Out of about 50 raw applications, I only got 3 interviews, but out of 18 referrals, I got 7 interviews. Overall, it was becoming obvious that I was not considered a strong candidate in this market"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Around half of the companies (4/10) that I’ve interviewed with had a take-home assignment before or instead of a TPS. Take-home assignments consumed a lot of energy. Typically, an 8-hour take-home assignment caused me to need at least half a day to rest after submission. Because of this, I did my best to schedule the interview accordingly. There were no interviews the morning after my take-home assignment. Simply being aware of the basic structure can go a long way in making you feel more at ease and able to cope with the process of finding a new job"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Going into my interviews, every opportunity was critical to me. Although I was aware that some people learn by interviewing, becoming better after many interviews, and typically obtaining offers for the last few companies with which they interview, I did not feel I could take this approach. When I graduated in 2017, I only received 4 interviews out of 500 raw applications. I was not expecting to get many more in 2019. Thus, my plan was to be fully prepared for each interview I got. I would let no opportunity go to waste"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"One benefit of being laid off was that I could study full time for the interview. Each day I structured what I studied, focusing on two or three things per day. No more. From previous interviews, I had learned that a deep understanding allows you to give more thorough answers during interviews. It especially helps to have a depth of knowledge in an interview situation when you tend to be more nervous and anxious than usual. That is not the time when you want to try faking things"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"As I describe my own experience, I can’t help thinking of a common misconception I often hear: it’s not possible to gain the knowledge on product/experimentation without real experience. I firmly disagree. I did not have any prior experience in product or A/B testing, but I believed that those skills could be gained by reading, listening, thinking, and summarizing. After all, this is the same way we were taught things in school. Actually, as I get to know more senior data scientists I continue to learn that this method is common, even for people with years of experience. What you will be interviewed on may not be related to what you were doing at all, but you can gain the knowledge you need in ways other than job experience"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Here are the basics of what you can expect. Typically, product and SQL questions were asked during a TPS. Onsite interviews included a few rounds of questions, including product sense, SQL, stats, modeling, behavior, and maybe a presentation. The next few subsections summarize the most useful resources (all freely available) I used when preparing for interviews. In general,"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Working as a data scientist at a startup, I was mainly responsible for developing and deploying machine learning models and writing spark jobs. Thus, I barely gained any product knowledge. When I saw some real interview questions on"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"To learn product sense I resorted to the basic read and summarize strategy, using the resources listed below. All this reading helped me build up my product knowledge. As a result, I came up with a structured way (my own ‘framework’) to answer any type of product questions. I then put my knowledge and framework to the test with that all essential to learning any skill: practice. I wrote out answers to questions involving product sense. I said my answers out loud (even recording myself with my phone), and used the recordings to finetune my answers. Soon I could not only fake it for an interview, I actually knew my stuff"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"The first time I took a SQL TPS I failed, and it was with a company in which I was very interested. Clearly, something needed to change. I needed to, once again, practice, and so I spent time grinding SQL questions. Eventually, I was able to complete in a day, questions that had previously taken me an entire week. Practice makes perfect!"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"To prepare for these kinds of questions, I brushed up on elementary statistics and probability and did some coding exercises. While this may seem overwhelming (there is a lot of content for both topics), the interview questions for a product data scientist were never hard. The resources below are a great way to review"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Without a CS degree, I went into the job search with limited machine knowledge. I had taken some courses during my previous job, and I reviewed my notes from these to prep for interviews. However, even though modeling questions are getting more and more frequent nowadays, the interview questions for a product data scientist mainly geared toward how to apply those models rather than the underlying math and theories. Still here are some helpful resources to bump up your machine learning skills before the interview time"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Some companies required candidates to either present the take-home assignment or a project of which they are most proud. Still, other companies asked about the most impactful project during behavioral interviews. However, no matter what the form the key is to make your presentation interesting and challenging"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"That sounds great, but how do you do that? My main recommendation is to think through all the details, such as high-level goals and success metrics to ETL to modeling implementation details, to deployment, monitoring, and improvement. The little things add up to make a great presentation rather than one big idea. Here are a few questions worth rethinking to help reach your ideal presentation:"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"When presenting a project, you want to engage the audience. To make my presentations interesting, I often share interesting findings and the biggest challenges of the project. But the best way to make sure you are engaging is practice. Practice and practice out loud. I practiced presenting to my family to ensure my grasp of the material and ease of communication. If you can engage the people you know, an interviewer, who is required to listen, doesn’t stand a chance"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"While it is easy to get caught up in preparing for the technical interview questions, don’t forget that the behavioral questions are equally important. All companies I’ve interviewed with had at least 1 round of behavior interviews during the onsite portions. These questions typically fall into these three categories:"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Behavioral questions are very important for data scientists. So be prepared! Understanding a company’s mission and core values helps answer questions in the first group. Questions like 2 and 3 can be answered by telling a story — 3 stories were enough to answer all behavioral questions. Make sure you’ve got a few good stories on hand when you walk in for an interview. Similar to product questions, I practiced a lot by saying it out loud, recording, and listening to then fine-tune my answers. Hearing a story is the best way to make sure it works"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"The night before an onsite interview was typically a stressful, hectic night. I always tried to cram in more technical knowledge while simultaneously reviewing my statistics notes and thinking of my framework to answer a product question. Of course, as we all learned in school, none of that was incredibly useful. The results were largely determined due to the amount of preparation before not a single night of cramming. So preparation is important, but there are some rules you can follow the day of to make sure your interview is a success"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"After receiving verbal offers, the next step was to work with recruiters to finalize the numbers. There’s only one rule here that I stick with - ALWAYS negotiate. But how?"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Every single rule was so true. I negotiated with all companies that gave me an offer. The average increase for offers was 15%, and the highest offer was, in total value, increased by 25%. Negotiating works, so don’t be afraid to try it!"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"After losing 11 pounds and lots of cries and screaming (job hunting is stressful and it is okay to admit that), I finally got 4 offers within 2 months of being laid off. 3 of those offers were from companies that I have never dreamed of joining: Twitter, Lyft, and Airbnb (where I ultimately joined) and another offer from a healthcare startup. By the end of two frenzied months, I had received a total of 10 interviews, 4 onsite interviews, and 4 job offers, giving me a 40% TPS-to-onsite rate and 100% onsite-to-offer rate"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"I was so lucky that I got lots of support and help from family and friends after being laid off, which was critical to landing a job at my dream company. It was difficult. Ironically looking for a job is also a lot of work, but everything was worth it"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"I wrote this blog because I know how overwhelmed I was. There is so much to prepare for interviews. I hope this post has made things clearer for other data specialists out there in need of work, and if you want more advice feel free to contact me"
How I Got 4 Data Science Offers and Doubled my Income 2 Months After Being Laid Off - KDnuggets,"Since I published this post three weeks ago, I got hundreds of questions on data science interviews. So I decided to make a series of videos to help you land your dream data science job. Check my YouTube channel if you are interested!"
The Difficulty of Graph Anonymisation - KDnuggets,"The objective of the program was to quickly identify people who might be in close contact with anyone who has tested positive for the virus. It comprises of an app or physical token which uses Bluetooth signals to store proximity records. As at the end December 2020,"
The Difficulty of Graph Anonymisation - KDnuggets,"This led to some clarifications from the ministers in-charge and a greater debate surrounding privacy concerns of the programme. The Minister of Home Affairs clarified that under the Criminal Procedure Code (CPC), police can and has used TraceTogether data to investigate ""serious crimes, like murders or terrorist incidents"". The"
The Difficulty of Graph Anonymisation - KDnuggets,"But I believe that once the pandemic has passed, that data – certainly, the specific, personalised data – those fields should be eliminated. For research purposes, I believe MOH may want to still have epidemiologic data but it should be anonymised. It should not be personalised, it should not be individualised"
The Difficulty of Graph Anonymisation - KDnuggets,"The use of anonymised data for research has either flown under the radar or there is just not enough awareness of the risk of de-anonymisation, especially of network data. The word ""anonymised data"" seems to convey a certain sense of certainty that user information cannot be back-derived. As a data scientist working in the space of network science, this is far from the truth and I wish to take this space to explain the difficulties of network anonymisation"
The Difficulty of Graph Anonymisation - KDnuggets,"Anonymisation is a trade-off. We can achieve 100% privacy by substituting every field of our dataset of interest with random values, but that would render the data to be totally useless. Hence, Statisticians have researched different data anonymisation techniques including masking, permuting, perturbation or synthetic data creation, with the goal of balancing privacy and usefulness"
The Difficulty of Graph Anonymisation - KDnuggets,"In an interesting twist to the story, Dr Sweeney sent William Weld the then−Governor of Massachusetts who assured the public that GIC had protected patient privacy by deleting identifiers his health records. Based on the 1990's census data, he discovered that 87% of American citizens were uniquely identified by their ZIP code, birth date, and sex. More recent studies suggest a lower but still significant figure of 63% (Golle"
Data Science and Analytics Career Trends for 2021 - KDnuggets,"Many of these innovations and trends impact careers related to data science and analytics. This is especially true as data science is a multidisciplinary field that also interacts with various other technologies like Artificial Intelligence, Machine Learning, Deep Learning, the Internet of Things, etc. So let's check out what are the new data science and analytics career trends for 2021 that may also shape the career options in the future"
Data Science and Analytics Career Trends for 2021 - KDnuggets,"There are many emerging trends that can shape your data science and analytics career in 2021 and in the future. Some of these are terms that you might have never heard before but may become common technologies in the future. After all, who had heard of Data Science 10 years ago?! There are also some of these trends that may die out in the future but that is not known right now.  So let's check out these popular career trends that may become popular in 2021 here:"
Data Science and Analytics Career Trends for 2021 - KDnuggets,"Now, these are some general trends in Data Science and Analytics that will be observed in 2021 and the coming years. However, there are many ways in which Data Science is changing the shape of different industries like marketing, finance, etc. Given enough time, Data Science might be a part of all the industries in the world, not just tech! So let's understand the role of this technology in different industries"
Data Science and Analytics Career Trends for 2021 - KDnuggets,"These programs will teach you right from the basics of Data Science such as Python, Business Statistica, and Data Visualization to various techniques of Machine Learning such as Supervised and Unsupervised algorithms. You will also get direct domain exposure by doing projects relating to Data Science in different industries like Marketing and Retail, Web and Social Media Analytics, Supply Chain and Logistics, and Finance and Risk Analytics. Some of these projects include Facebook Comments Prediction, Retail Sales Prediction, Insurance Data Visualization, etc"
Essential Math for Data Science: Scalars and Vectors - KDnuggets,Linear algebra is the branch of mathematics that studies vector spaces. You’ll see how vectors constitute vector spaces and how linear algebra applies linear transformations to these spaces. You’ll also learn the powerful relationship between sets of linear equations and vector equations
Essential Math for Data Science: Scalars and Vectors - KDnuggets,"Data can then be stored in vectors, matrices, and tensors. For instance, images are represented as matrices of values between 0 and 255 representing the luminosity of each color for each pixel. It is possible to leverage the tools and concepts from the field of linear algebra to manipulate these vectors, matrices and tensors"
Essential Math for Data Science: Scalars and Vectors - KDnuggets,"However, vectors refer to various concepts according to the field they are used in. In the context of data science, they are a way to store values from your data. For instance, take the height and weight of people: since they are distinct values with different meanings, you need to store them separately, for instance using two vectors. You can then do operations on vectors to manipulate these features without loosing the fact that the values correspond to different attributes"
Essential Math for Data Science: Scalars and Vectors - KDnuggets,"Python uses zero-based indexing, meaning that the first index is zero. However mathematically, the convention is to use one-based indexing. I’ll denote the component"
Essential Math for Data Science: Scalars and Vectors - KDnuggets,"Storing data in vectors allows you to leverage linear algebra tools. Note that, even if you can’t visualize vectors with a large number of components, you can still apply the same operations on them. This means that you can get insights about linear algebra using two or three dimensions, and then, use what you learn with a larger number of dimensions"
Essential Math for Data Science: Scalars and Vectors - KDnuggets,"He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog ("
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,"Clearly, Data Scientists are not industry leaders in this area since only 54.9% use Agile. The results get worse with similar roles, except for Data Engineers, who aren’t far from the best here. Also, there’s a positive correlation in terms of “engineeringness” between Data Scientists and the percentage of Agile users within that group"
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,I’m assuming that you have at least some basic system of defining tasks in your project. Perhaps you use tools like Jira or Trello for it or a to-do list in a dedicated document somewhere. The tool doesn’t really matter
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,"Let’s consider the following scenario: you’re working on a system to recognize hot dogs and were given a massive collection of pictures you can use to train your model. An impatient voice in your head tells you to quickly create a new item in your to-do list, call it “Improve model accuracy,” and… start coding. Then, you’re asked how long it will take, and you quickly respond, “three days"
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,"Now, imagine that your tests are stored and shared. They are run along with many others cyclically, e. You’ve just gained"
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,"Remember to start with a failing test checking the new requirement, then write a solution. Make sure the previous tests aren’t failing! You can additionally refactor your code so that your solution is cleaner and/or better performing. If you’re done with the requirement, you should repeat the process for a new one"
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,"Let’s compare that to web developers. Most of the time, they have very concrete tasks, like tweaking a form or adding a new filter. The outputs of such tasks are rather tangible. Users can click on the new interface items, and the value provided is fairly easy to recognize"
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,"Avoid writing scripts that just fill the void. Create something that is actually valuable for your end-user. Perhaps that’s a CLI tool, a jupyter notebook with some informative analysis, or a tiny library. Preparing a"
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,"Finally, don’t forget to apply the “divide and conquer” approach I mentioned before. Try to identify the smallest valuable deliverables. Remember: the outcome of your work should not be murky just because there’s no clickable interface!"
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,The sad fact is that most prototype code ends up in the trash. There’s no denying it. So is it worth it to care about the quality of such code? I’d argue it is
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,"Prototypes often end up being forgotten scripts living in some ancient repositories. But from time to time, somebody (including you!) may want to give them a second life. If the code is a total mess, the resurrection won’t happen, or it will be a painful one"
Can Data Science Be Agile? Implementing Best Agile Practices to Your Data Science Process - KDnuggets,"However, there are aspects of the Data Science world that make it harder to go Agile. I’d argue that the more engineering responsibilities you have, the more Agile practices it’ll be easy for you to apply with success. On the flip side, Agile for analytics and research often requires a substantial change of perspective"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"Let’s illustrate this with an example that is similar to general machine learning challenges that face every data scientist. In this example, we want to understand how to improve ride-sharing in New York City. As you would expect, there are millions of rows of data to analyze, which takes days to begin to process. We are going to use Snowflake and Saturn Cloud to get to performance fit for a business where time is money. We will use 3 years of NYC Taxi data stored in Snowflake. We do the feature engineering in Snowflake, and then load the data into a distributed GPU dataframe with dask_cudf, and then train a random forest model on GPUs using cuML. Check out the"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,Let’s look at how it works. We’re using a 20 node Dask cluster using g4dn. A Saturn Cloud cluster is provisioned with the following code:
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"Next, we’ll connect to Snowflake. In our notebook, we’re pulling in the relevant connection information and environment variables. You will need to replace these values with your own Snowflake connection information"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"We’re going to use Snowflake to compute some predictive features, and then have each of our 20 Dask workers load a piece of the resulting training set. The NYC taxi data has the pickup time of each ride, and the data is fairly evenly distributed across days, weeks, and months. Our approach here is to divide up the entire 3 year time period into 20 chunks based on the day of the ride, and load each chunk into a Dask worker. First, we write a function to calculate our date chunks"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"There’s a lot to unpack here, but we’ll walk through each step. Our load function is a Dask delayed function. This means that when you call it, nothing happens, but an object is returned that keeps track of the operations that need to happen and can be executed at the appropriate time. In the load function, we’re calling the Snowflake query defined above to pass through the date chunks that need to be pulled out of Snowflake. The results are pulled into Python using fetch_pandas_all(). This is an"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"You may also notice the meta parameter that we’re passing around. This isn’t strictly necessary, however many Pandas routines infer datatypes based on the data. A chunk that has an integer field with no missing data may return that as an integer type, but another chunk that is missing some data points for that same field may cast it as a float. As a result, it’s useful to pass the metadata through, and coerce types using metadata to ensure consistency across every chunk"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,Snowflake has 2 primary caching mechanisms that are great for ML workloads. The first is the local disk cache. The local disk cache stores the underlying data on the local disk of the machines that make up your Snowflake warehouse. The result cache stores the results of previous queries so they can be accessed quickly
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"For ML workloads, both caches are useful. The local disk cache can speed up every query, especially as data sizes grow. The result cache is useful if you’re re-executing the same query, such as iterating on the ML model without changing your feature engineering, this can be very advantageous"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"Snowflake tables can also be configured to use data clustering. In data clustering, one or more columns or expressions on the table can be designated as the clustering key. Data clustering co-locates similar rows and helps Snowflake understand what data to skip for a particular query"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"Here, taxi_train is a dask_cudf dataframe and we’re importing RandomForestClassifier from the RAPIDS cuML package. As a result, the training happens entirely on the GPU. Note the call to . If we don’t do this, the Snowflake queries would be executed each time the random forest model needed to pull some data"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"Loading data, feature engineering, and training a RandomForestClassifier on Spark took 38.5 Minutes. Loading data and feature engineering in Snowflake, and training the RandomForestClassifier in Saturn Cloud with Dask and Rapids takes 35 seconds. That’s a 60x speedup with minimal work, and has the added benefit of being able to use Python end to end"
Snowflake and Saturn Cloud Partner To Bring 100x Faster Data Science to Millions of Python Users - KDnuggets,"In terms of computational cost, the Snowflake and Saturn Cloud solutions are quite cost-effective because the runtime is so short. Snowflake has a minimum charge of 1 minute. The x-large warehouse we’re running costs us $0.53, and the 20 g4dn.4xlarge machines used in machine learning cost 12 cents for that time period. The equivalent Spark cluster costs $7.19 for the 38 minutes it takes to complete the workload"
"Data Observability, Part II: How to Build Your Own Data Quality Monitors Using SQL - KDnuggets","I’m using SQLite 3.32.3, which should make the database accessible from either the command prompt or SQL files with minimal setup. The concepts extend to really any query language, and"
"Data Observability, Part II: How to Build Your Own Data Quality Monitors Using SQL - KDnuggets","In an ideal world, we’d like a record of this change, as it represents a vector for possible issues with our pipeline. Unfortunately, our database is not naturally configured to keep track of such changes. It has no versioning history"
"Data Observability, Part II: How to Build Your Own Data Quality Monitors Using SQL - KDnuggets","Let’s add another table to our database. So far, we’ve been recording data on exoplanets. Here’s one fun question to ask: how many of these planets may harbor life?"
"Data Observability, Part II: How to Build Your Own Data Quality Monitors Using SQL - KDnuggets","Explanation (1) uses just the fact that an anomaly took place. Explanation (2) uses lineage, in terms of dependencies between both tables and fields, to put the incident in context and determine the root cause. Everything in (2) is actually correct, by the way, and I encourage you to mess around with the environment to understand for yourself what’s going on. While these are just simple examples, an engineer equipped with (2) would be faster to"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"While 2020 has been a challenging year, I was able to use the transition to remote work to explore new tools to expand my data science skill set. It was the year that I made the transition from data scientist to applied scientist, where I was responsible for not only prototyping data products, but also putting these systems into production and monitoring system health. I had prior experience with tools such as Docker for containerizing applications, but I didn’t have experience with deploying a container as a scalable, load balanced application. While many of the technologies that I learned in 2020 are more commonly associated with engineering rather than data science, it can be useful to learn these tools in order to learn to build end-to-end data products. This is especially true for data scientists working at startups. Here are the technologies I learned in 2020:"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"I’ll cover each of these topics in main detail below. The main motivation for getting hands on with all of these different tools was to build a research platform for programmatic advertising. I was responsible for building and maintaining a real-time data product, and needed to explore new tools to deliver on this project"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"The goal of the project is to provide modules that support the development, serving, and monitoring of ML models. I starting using two of these components in 2020: MLflow tracking and Model Registry. The tracking module enables data scientists to record the performance of different model pipelines and visualize the results. For example, it’s possible to try out different feature scaling approaches, regression models, and hyperparameter combinations, and review which pipeline configuration produced the best results. I used this within the Databricks environment, which provides useful visualizations for model selection. I also started using the registry module in MLflow to store models, where a training notebook trains and stores a model, and a model application notebook retrieves and applies a model. One of the useful features in the model registry is the ability stage models prior to deployment. The registry can maintain different model versions and provides the ability to revert to a prior version if an issue is detected. In 2021, I plan on exploring more of the modules in MLFlow, including model serving"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"Kubernetes is an open source platform for container orchestration. It enables data scientists to deploy containers as scalable web applications, and provides a variety of configuration options for exposing services on the web. While it can be quite involved to set up a Kubernetes deployment from scratch, cloud platforms offered managed versions of Kubernetes that make it easy to get hands-on with this platform. My recommendation for data scientists that want to learn Kubernetes is to use Google Kubernetes Engine (GKE), because it provides fast cluster start up times and has a great developer experience"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"Why is Kubernetes so useful? Because it enables teams to separate application development and application deployment concerns. A data scientists can build a model serving container and then hand this off to an engineering team that exposes the service as a scalable web application. In GCP, it also integrates seamlessly with systems for load balancing and network security. However, with managed services such as GKE, there’s less of a barrier to using Kubernetes and data scientists should get hands-on experience with this platform. Doing so enables data scientists to build end-to-end data products"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"While I’ve used a variety of databases throughout my data science career, it wasn’t until 2020 that I first explored NoSQL databases. NoSQL includes databases that implement key-value stores with low latency operations. For example, Redis is an in-memory database that provides sub-millisecond reads. This performance is useful when building real-time systems, where you need to update user profiles as data is received by a web service. For example, you may need to update the attributes of a feature vector that describes user activity, that is passed as input to a churn model and applied within the context of a HTTP post command. In order to build real-time systems, it’s essential for data scientists to get hands on with NoSQL databases. To learn technologies such as Redis, it’s useful to use"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"OpenRTB is a specification for real-time ad auctions and ad serving. The specification is used across exchanges such as Google Ad Exchange in order to connect publishers selling ad inventory with buyers that want to serve advertisements. I used this protocol to implement a research platform for programmatic user acquisition. While this specification is not broadly applicable to data science, it is useful for data scientists learn how to build systems that can implement a standardized interface. In the case of OpenRTB, this involves building a web service that receives HTTP posts with JSON payloads and returns a JSON response with pricing details. If you’re interested in getting up and running with the OpenRTB specification, Google provides a"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"I decided to author the OpenRTB research platform in Java, since I have the most experience with this language. However, Rust and Go are both great alternatives to Java for building OpenRTB systems. Since I selected Java, I needed to select a web framework for implementing the endpoints for my application. While I used Jetty library over a decade ago to build simple web applications with Java, I decided to explore new tools based on benchmarks. I started with the"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"Implementing the OpenRTB protocol now requires serving traffic over secure HTTP. Enabling HTTPS for a web service involves setting up web services as a named endpoint via DNS and using a signed certificate for establishing the identity of the endpoint. Securing endpoints on GCP hosted in GKE is relatively straight forward. Once the service is exposed using a node port and service ingress, you need to set up a DNS entry for the service’s IP address and then use a GCP managed certificate to enable HTTPS"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"It’s useful for data scientists to learn about setting up HTTPS endpoints, because of some of the subtleties in securing services. If end-to-end HTTPS is not required, as in the case of OpenRTB, where HTTP can be used internally between the load balancer and pods in the Kubernetes cluster, then deployment is easier. If end-to-end HTTPS is required, such as a web service that uses OAuth, then the Kubernetes configuration is a bit more complicated, because the pods may need to respond to health pings on a separate port from the port that serves web requests. I ended up submitting a"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"To scale to OpenRTB volumes of web traffic, I needed to use a load balancing to process over 100k web request per second (QPS). Kubernetes provides the infrastructure to scale up the number of pods serving web requests, but it’s also necessary to configure the cluster in a way that evenly distributes requests across the cluster. Kubernetes has an"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"Deploying a web application also involves setting up monitoring for the system, to determine if any issues are occurring. When building applications with GCP, StackDriver provides a managed system for logging messages, reporting custom metrics, and setting up alerts. I was able to use this system for monitoring uptime, and firing alerts to Slack and SMS when incidents occurred. It’s useful for data scientists to get hands on with logging libraries, to make sure that systems deployed to the cloud are operating as expected"
8 New Tools I Learned as a Data Scientist in 2020 - KDnuggets,"In 2020, I learned several technologies that are typically associated with engineering roles. As a data scientist, I learned these tools out of necessity, in order to build and maintain an end-to-end system. While many of these technologies are not broadly applicable to data science, the growing role of applied scientist is creating demand for data scientists with broader tech stack experience"
Essential Math for Data Science: Information Theory - KDnuggets,"In the context of machine learning, some of these concepts are used to characterize or compare probability distributions. The ability to quantify information is also used in the decision tree algorithm, to select the variables associated with the maximum information gain. The concepts of entropy and cross-entropy are also important in machine learning because they lead to a widely used loss function in classification tasks: the cross-entropy loss or log loss"
Essential Math for Data Science: Information Theory - KDnuggets,"For instance, if a friend from Los Angeles, California tells you: “It is sunny today”, this is less informative than if she tells you: “It is raining today”. For this reason, is can be helpful to think of the Shannon information as the amount of surprise associated with an outcome. You’ll also see in this section why it is also a quantity of information, and why likely events are associated with less information"
Essential Math for Data Science: Information Theory - KDnuggets,"Bits represent variables that can take two different states (0 or 1). For instance, 1 bit is needed to encode the outcome of a coin flip. If you flip two coins, you’ll need at least two bits to encode the result. For instance, 00 for HH, 01 for HT, 10 for TH and 11 for TT. You could use other codes, such as 0 for HH, 100 for HT, 101 for TH and 111 for TT. However, this code uses a larger number of bits in average (considering that the probability distribution of the four events is uniform, as you’ll see)"
Essential Math for Data Science: Information Theory - KDnuggets,"Let’s take an example to see what a bit describes. Erica sends you a message containing the result of three coin flips, encoding ‘heads’ as 0 and ‘tails’ as 1. There are 8 possible sequences, such as 001, 101, etc. When you receive a message of one bit, it divides your uncertainty by a factor of 2. For instance, if the first bit tells you that the first roll was ‘heads’, the remaining possible sequences are 000, 001, 010, and 011. There are only 4 possible sequences instead of 8. Similarly, receiving a message of two bits will divide your uncertainty by a factor of 2222; a message of three bits, by a factor of 2323, and so on"
Essential Math for Data Science: Information Theory - KDnuggets,"Let’s say that we want to transmit the result of a sequence of eight tosses. You’ll allocate one bit per toss. You thus need eight bits to encode the sequence. The sequence might be for instance “00110110”, corresponding to HHTTHTTH(four “heads” and four “tails”)"
Essential Math for Data Science: Information Theory - KDnuggets,"However, let’s say that the coin is biased: the chance to get “tails” is only 1 over 8. You can find a better way to encode the sequence. One option is to encode the index of the outcomes “tails”: it will take more than one bit, but ‘tails’ occurs only for a small proportion of the trials. With this strategy, you allocate more bits to rare outcomes"
Essential Math for Data Science: Information Theory - KDnuggets,"Let’s take an example: as illustrated in Figure 2 in the bottom panel, you have a discrete distribution with four possible outcomes, associated with probabilities 0.4, 0.4, 0.1, and 0.1, respectively. As you saw previously, the information is obtained by log transforming the probabilities (top panel). This is the last part of the entropy formula:"
Essential Math for Data Science: Information Theory - KDnuggets,"Each of these transformed probabilities is weighted by the corresponding raw probability. If an outcome occurs frequently, it will give more weight into the entropy of the distribution. This means that a low probability (like 0.1 in Figure 2) gives a large amount of information (3.32 bits) but has less influence on the final result. A larger probability (like 0.4 in Figure 2) is associated with less information (1.32 bits as shown in Figure 2) but has more weight"
Essential Math for Data Science: Information Theory - KDnuggets,"Let’s take the example of a fair coin, with a probability of 0.5 of landing ‘heads’. The distribution is thus 0.5 and 1 − 0.5 = 0.5. Let’s use the function we just defined to calculate the corresponding entropy is:"
Essential Math for Data Science: Information Theory - KDnuggets,Figure 5 shows that the true class corresponding the sample you consider in this example is “European Green Woodpecker”. The model outputs a probability distribution and you’ll compute the cross entropy loss associated with this estimation. Figure 6 shows both distributions
Essential Math for Data Science: Information Theory - KDnuggets,"He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog ("
5 Tools for Effortless Data Science - KDnuggets,"Ultimately, Cookiecutter promotes logical standardization. That makes it easy for you, your collaborators, and project stakeholders to find data, notebooks, reports, visualizations, etc. Cookiecutter promotes reproducibility and code quality. Setting up your Data Science experiment with Cookiecutter is fast and supremely useful"
JupyterLab 3 is Here: Key reasons to upgrade now - KDnuggets,"More easily navigate a Jupyter document with the table of contents feature which is included in version 3. It is automatically generated from headings in the Jupyter document, and can be toggled on and off on the left hand side of the interface. This makes jumping around longer documents much easier, and keeps related code and documentation neatly tucked away until needed"
My Data Science Learning Journey So Far - KDnuggets,"Yeah, this one takes lots of your time and energy. This obstacle is one you should deal with right away. I struggled with it in the beginning but in a few months, it died down. I attribute this breakthrough to my daily reading habit"
My Data Science Learning Journey So Far - KDnuggets,"I keep on reading LinkedIn posts (especially from Eric Weber himself). Also, I read a lot on Towards Data Science, Medium, KDnuggets, and individual blogs from different data scientists and machine learning engineers for an hour or two or more every single day. This has taught me the importance of data science when it comes to industrial work:"
My Data Science Learning Journey So Far - KDnuggets,"Don’t fall for this. Don’t take a job description to your heart. Mostly “interview trivia” is a combination of this newness of data science along with a poor communication channel between talent acquisition, data science, and software engineering teams in an organization. Rather than feeling overwhelmed at this, you need to focus on how to crack it"
My Data Science Learning Journey So Far - KDnuggets,"One way to crack this is by looking at reality. If you know any real-life data scientists, data analysts, and machine learning engineers (offline, in the physical world), it will be a great idea to talk to them about their work. If you don’t know anyone then you can always check blogs and articles"
My Data Science Learning Journey So Far - KDnuggets,"I don’t know any professional in this field offline. So I learned by reading blogs and articles. What I learned is companies get many people for interviews, all of the kind who “know” stuff but very few who have “built” stuff. So focus on building stuff than mere learning and education (e. It took me 5–6 months to realize this"
My Data Science Learning Journey So Far - KDnuggets,"So I decided I will experiment and carve my own path to become a data scientist. This is not to say that I will stop reading other people’s journeys, I will still read but instead of following them blindly and trying to copy it into my life, I will use them as a compass, as a guiding mechanism. This has cost me 8 months. Better late than never though"
My Data Science Learning Journey So Far - KDnuggets,"Yes, nothing fancy but only the basics. All the fancy stuff you can do after you land a job. Till then you use Python or R Libraries. Instead of trying to learn Mathematical formulas just like in school or college, try to learn how to use it using Library calls in Python e"
My Data Science Learning Journey So Far - KDnuggets,"This problem went away when I got a take-home assignment from a company who approached me for R related work. After using both R and Python for take-home assignment work, I never wanted to touch R again. From my experience Python suits better for software engineering practices and software engineering practices are definitely needed when it comes to writing data science code for real-life industrial work. It is almost the same as when you are doing software development. I went fully Python after that. Personally, If I ever have to use another language, I will use"
My Data Science Learning Journey So Far - KDnuggets,This mistake I did after the “the math mistake”. I spent months contemplating SQL vs NoSQL. We look at something and we think of it from our viewpoint and think this is what it means. E. Most of it is unstructured. I guessed I should learn NoSQL. But then almost all of the job descriptions mention only SQL. Then I will think of doing SQL
My Data Science Learning Journey So Far - KDnuggets,"Instead of interpreting things in my way, I started looking at the people who landed data science jobs and what they learned. All of them had listed SQL as a skill. So I switched to SQL. A good place to start is"
My Data Science Learning Journey So Far - KDnuggets,This is one area where you need a serious change in mindset and I needed such change too. My computer programming background makes me a 100% tech guy who really does not know how to be more than a team-worker. Contributing to the team is where my social and my communication skills ended
My Data Science Learning Journey So Far - KDnuggets,"I never knew this in beginning but thanks to my reading habit, I came across so many characteristics of data science that put it at odds with other tech jobs. One way I overcome this is by talking about Big Data with people I know or I meet. By explaining data science, machine learning concepts to my friends and other people. But because my freelance work and data science learning require me to spend a lot of time in front of my computer, I don’t get the opportunity to exercise this method much"
My Data Science Learning Journey So Far - KDnuggets,"Data science is not just programming, data science is not just web-development, it is not just about analyzing data and building models. This is half of the story. Another half of data science is being able to communicate to not so tech-savvy people. Business stakeholders, decision-makers in management, and clients are three different types of non-tech people you are going to deal with. So collaborating with people is going to be a big pain if we think of it as “another tech job”. There is an excellent book on communicating data insights titled “"
My Data Science Learning Journey So Far - KDnuggets,"The model you build, the comparisons you did, and the accuracy you achieved, how it is benefiting the business? You see, a data scientist’s job has no meaning if he can’t bring some profit or benefit or some value addition to the business. This is a hard thing to get hold of and become good at if you come from a tech-background like mine. What the tech-mentality does, in this case, is to make your mind focus only on building the model and analyzing data because it is what we do"
My Data Science Learning Journey So Far - KDnuggets,"I could only read blogs, posts and articles to understand what to do. I don’t know any product manager either (I have met one or two managers in IT service but I don’t know if that qualifies). The only method I have come across to solve this is two-fold:"
My Data Science Learning Journey So Far - KDnuggets,Another pitfall you need to avoid. I got stuck in this for a while. I want to implement a paper or two myself but now the first focus of mine is always on “building something”
My Data Science Learning Journey So Far - KDnuggets,"Yes, all those papers look really, really impressive, and beautiful. And papers are mostly about academics. You are trying to land a job in the industry. Academics and industry do not match, with two possible exceptions:"
My Data Science Learning Journey So Far - KDnuggets,"Don’t take me wrong, I love to do research. In fact, back in college, I wanted to do a Ph.D"
My Data Science Learning Journey So Far - KDnuggets,"This one is a biggie. I think I am struggling with this for life. Some people have it and some people don’t. I am inclined to say that maybe smart people don’t have this problem (the smart ones I have met or read about, they don’t). People like me spend a lifetime trying to beat it. It is a jail, trust me. It is quite frustrating to live with a mindset of “only one way to do something”. Ideas don’t have any limits if you look at real-life stories"
My Data Science Learning Journey So Far - KDnuggets,"This is more of a personal-development obstacle than a technical one because no matter which field you will work in, this one will show up there, it absolutely has nothing to do with the tech. I am still trying to work on it. A solution I have found so far is when I can’t find my way around a problem then I will get off the machine and go for a walk if it is evening or read a completely unrelated book if it is not evening (some non-fiction e. Then I will come back later and try to learn the same thing from a different article or blog post while not referring to the original point where I was stuck. Just a fresh new perspective on the same problem from someone else"
My Data Science Learning Journey So Far - KDnuggets,"All of these points overlap with each other when it comes to where I wasted time. It is actually 12 months. Dec 2019 to Nov 2020. For a few months, in the beginning, I did not even know what I needed to do. Things started making sense only in March 2020 this year. I think I could have saved 4–6 months if things were clearer to me but this is just a wild guess, some really smart people have told me: it takes whatever time it takes to break down the obstacles. Let me re-iterate:"
My Data Science Learning Journey So Far - KDnuggets,I was trying to learn neural networks before I could comprehend what kind of problems logistic regression fits better than linear regression. I was doing deep learning before machine learning made any sense. In my case it was because of:
My Data Science Learning Journey So Far - KDnuggets,Deep learning and AI are in media everywhere. We tend to think we need to be better than everyone else and others are already writing highly mathematical blog posts with their flashy formulas along with lots of code. Don’t believe me? Check
My Data Science Learning Journey So Far - KDnuggets,I have seen posts on LinkedIn where several data scientists and machine learning engineers have lost jobs. I have seen them even literally begging to “like and share” that they are looking for a job. It is heartbreaking to see that. Everyone deserves a good life
My Data Science Learning Journey So Far - KDnuggets,"Let’s look at the positive side, this pandemic has disrupted the world, it has brought many businesses to a halt while some businesses have their client number shot sky-high (podcast and video conferencing services for one). In such disruptive times, we need to be more resilient to pain and suffering and find ways to strengthen our resolve. I believe it is not by chance that we were born in a certain year and that is how we got in the middle of this pandemic. I think we were supposed to learn from it, we are supposed to make"
MLOps: Model Monitoring 101 - KDnuggets,"ML models are driving some of the most important decisions for businesses. As such it is important that these models remain relevant in the context of the most recent data, once deployed into production. A model may go out of context if there is data skew i. It may also be that a feature becomes unavailable in production data or that the model may no longer be relevant as the real-world environment might have changed (e. Covid19) or further and more simply, the user behavior may have changed. Monitoring the changes in model’s behavior and the characteristics of the most recent data used at inference is thus of utmost importance. This ensures that the model remains relevant and/or true to the desired performance as promised during the model training phase"
MLOps: Model Monitoring 101 - KDnuggets,"An instance of such a model monitoring framework is illustrated in Fig 2 below. The objective is to track models on various metrics, the details of which we will get into the next sections. But first, let us understand the motivation of a model monitoring framework"
MLOps: Model Monitoring 101 - KDnuggets,"Feedback loops play an important role in all aspects of life as well as business. Feedback loops are simple to understand: you produce something, measure information on the production, and use that information to improve production. It’s a constant cycle of monitoring and improvement. Anything that has measurable information and room for improvement can incorporate a feedback loop and ML models can certainly benefit from them"
MLOps: Model Monitoring 101 - KDnuggets,"A typical ML workflow includes steps like data ingestion, pre-processing, model building & evaluation, and finally deployment. However, this lacks one key aspect i. The primary motivation of any “model monitoring” framework thus is to create this all-important feedback loop post-deployment back to the model building phase (as depicted in Fig 1). This helps the ML model to constantly improve itself by"
MLOps: Model Monitoring 101 - KDnuggets,"A proposed model monitoring metrics stack is given in Fig 3 below. It defines three broad types of metrics based on the dependency of the metric on data and/or ML model. A monitoring framework should ideally consist of one or two metrics from all three categories, but if there are tradeoff then one may build up from the base i. Further, operations metrics should be monitored at a more real time level or at-least daily where stability and performance can be at a weekly or even a larger time frame depending on the domain & business scenario"
MLOps: Model Monitoring 101 - KDnuggets,"They do so by examining how good or bad the existing deployed model is performing viz-a-viz when it was trained (scenario I) or during a previous time frame post-deployment (scenario II). Accordingly, a decision can be taken to re-work the deployed model or not. Examples of these metrics include,"
MLOps: Model Monitoring 101 - KDnuggets,"Model monitoring within the realm of MLOps has become a necessity for mature ML systems. It is quintessential to implement such a framework to ensure consistency and robustness of the ML system, as without it ML systems may lose the “trust” of the end-user, which could be fatal. As such including and planning for it in the overall solution architecture of any ML use case implementation is of utmost importance"
MLOps: Model Monitoring 101 - KDnuggets,"He is currently the Advanced Analytics Practice Lead at Abzooba, wherein apart from project execution he also engages in leading & growing the Practice by nurturing talent, building thought leadership, and enabling scalable processes. Pronojit has worked in the retail, healthcare, and Industry 4.0 domains. Time series analytics and natural language processing are his expertise and he has applied these along with other AI methodologies for use cases like price optimization, readmission prediction, predictive maintenance, aspect-based sentiment analytics, entity recognition, topic modeling, among others"
Where is Marketing Data Science Headed? - KDnuggets,Part of this is a data issue. Not a small amount of the “big data” used in digital marketing consists of imputed and aggregate data that have been mashed together. Marketing scientists and others who analyze and model data know that missing data and even small errors in a single data file can wreak havoc on our analytics
Where is Marketing Data Science Headed? - KDnuggets,"Understanding why consumers behave as they do can help marketers predict their future behavior, communicate effectively with them, and design products they'll like. Inferring why they behave as they do from what they've done in the past is not always straightforward because different people may do the same things for different reasons. They might also do different things for the same reasons"
Where is Marketing Data Science Headed? - KDnuggets,"A simple example would be household cleaners. One person might buy Lysol Clean most often because of its disinfectant properties, and another because it’s easy to find where they usually shop. One person might use Goo Gone because they like its fragrance, and someone else Citrasolv because they like"
Where is Marketing Data Science Headed? - KDnuggets,"Another concept sometimes called the multiple me also comes into play. This notion reflects the simple fact that consumer behavior often varies according to occasion or motivation - buying wine for cooking versus buying wine to celebrate a promotion being one illustration. Our behavior can also vary with time due to changes in personal circumstances, such as marriage, birth of a child, a promotion or relocation. Our tastes may change too"
Where is Marketing Data Science Headed? - KDnuggets,"Other critics say personalized marketing frequently subsidizes purchases that would have been made anyway and, in the process, increases price consciousness among consumers and, over time, can erode brand equity. Smart marketers, indeed, would be wise to avoid encouraging deal shopping. Les Binet and Peter Field have much to say worth listening to regarding these topics"
Where is Marketing Data Science Headed? - KDnuggets,"Moreover, it may appear cost-effective if the investment in data infrastructure and analytics teams are ignored. Cost-effectiveness of targeting can also be deceptive when we have a higher “hit” rate among a much smaller number of consumers than without targeting. Overall sales may actually decline with targeting even when marketing ROI is higher"
Where is Marketing Data Science Headed? - KDnuggets,"I have not mentioned UX (User Experience) or CX (Customer Experience), which are related to marketing and have benefited from having more data, richer data and more sophisticated analytics tools. I should also point out that predictive analytics and other aspects of personalized marketing can be enhanced by traditional marketing research methods such as focus groups and consumer surveys. It’s not either/or"
Where is Marketing Data Science Headed? - KDnuggets,"These “old” methods, which continue to evolve, can be immensely helpful in shedding light on the Why. Awareness of this synergy is growing among marketing researchers, if not among data scientists, who are often focused on data management and programming. I’ve long felt this was an overlooked opportunity for marketing research"
Where is Marketing Data Science Headed? - KDnuggets,"Although I have mostly emphasized threats to marketing data science in this article, as a marketing data science person myself my own biases mostly run in the opposite direction. However, as a businessperson I cannot afford to believe in panaceas or just believe what I wish to believe. Even before the pandemic, I sensed growing skepticism in the business community and among investors regarding AI and the value of big data - see"
"Model Experiments, Tracking and Registration using MLflow on Databricks - KDnuggets","As part of building these data pipelines, data engineers can also perform some of the key transformations needed by data scientists. Some of the common transformations required during data preparation include: data type conversion for fields/columns/features, renaming fields/columns/features, joining datasets, merging datasets, repartitioning, dataset data format conversion (for example, JSON to Parquet for efficient downstream analysis in Apache Spark), etc. All of these transformations and many more are readily supported by StreamSets DataOps Platform"
"Model Experiments, Tracking and Registration using MLflow on Databricks - KDnuggets","Now, a very common requirement is to automate the process of retraining the model as and when more data becomes available–especially if the model hasn’t yet met the evaluation criteria. For example, accuracy can be one of the metrics on how a particular model is evaluated. This type of automation can be implemented by setting up an"
"Model Experiments, Tracking and Registration using MLflow on Databricks - KDnuggets","With recent experience in Big Data, Data Science, and Machine Learning, Dash applies his technical skills to help build solutions that solve business problems and surface trends that shape markets in new ways. Dash has worked for global enterprises and tech startups in agile environments as an engineer and a solutions architect. As a Platform and Technical Evangelist, he is passionate about evaluating new ideas to help articulate how technology can address a given business problem. He also enjoys writing technical blog posts, hands-on tutorials, and conducting technical workshops"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","At KDnuggets, we have brought a number of free eBooks to our readers this past year. Among other articles highlighting such materials, I have written a series of posts since the pandemic erupted, in the case that more people spending more time at home may result in more time for reading. Of course, a packed reading schedule is obviously not what the past nine months has had in store for everyone, but for those who could spare a moment here or there, we hope that some of the eBooks we have shared during this tough time may have been useful"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","P. Kroese, Z.I. Botev, T. Taimre & R. Vaisman"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","Text Mining for R: A Tidy Approach is code-heavy and seems to explain concepts well. The focus is on practical implementation, which should be of no surprise given the book's title, and to an R novice it seems to do a very good job. I have not followed along to the entire book, but I did read the first 2 chapters and feel that I got out of it what was intended"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","In many contemporary books, data science has been reduced to a series of programming tools which, if mastered, promise to do the data science for you. There seems to be less emphasis on the underlying concepts and theory divorced from code. This book is a good example of the opposite to this trend, a book which will undoubtedly arm you with the theoretical knowledge necessary to approach a career in data science with a strong set of foundations"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","The book starts off slow — describing NLP, how Python can be used to perform some NLP programming tasks, how to access natural language content to process — and moves on to bigger concepts, both conceptually (NLP) programmatically (Python). Soon it gets to categorization, text classification, information extraction, and other topics more often thought of as classic NLP. After getting the basics of NLP with this book, you can move on to more modern and cutting edge techniques, perhaps through the use of materials such as some of Stanford's free courses"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","The book is unusual in that it's taught ""top down"". We teach almost everything through real examples. As we build out those examples, we go deeper and deeper, and we'll show you how to make your projects better and better. This means that you'll be gradually learning all the theoretical foundations you need, in context, in such a way that you'll see why it matters and how it works. We've spent years building tools and teaching methods that make previously complex topics very simple"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","448 ratings of the book with an average of 4.6 out of 5 should tell you that many others have also found Python for Everybody useful. The consensus seems to be that the book quickly covers concepts, does so in an easily understandable manner, and jumps right into the corresponding code"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","If you have little to no understanding of what automated machine learning is in practice, don't worry. The book starts off with a solid introduction to the topic, and lays out explicitly what you can expect chapter by chapter, which is important in a book comprised of independent separate chapters. After this, in first section of the book, you get right in to reading about the important topics of contemporary AutoML, and be confident of this since the book was put together in 2019. The next section is a walkthrough of a half dozen tools for implementing these AutoML concepts. The last section is an analysis of the AutoML Challenge Series that existed for a few years during 2015 to 2018, the time that interest in automated approaches to machine learning seemed to explode"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","This is a bottom-up, theory-heavy treatise on deep learning. This is not a book full of code and corresponding comments, or a surface-level hand wavy overview of neural networks. This is an in-depth mathematics-based explanation of the field"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","What makes Dive into Deep Learning (D2K) unique is that we went so far with the idea of *learning by doing* that the entire book itself consists of runnable code. We tried to combine the best aspects of a textbook (clarity and math) with the best aspects of hands-on tutorials (practical skills, reference code, implementation tricks, and intuition). Each chapter section teaches a single key idea through multiple modalities, interweaving prose, math, and a self-contained implementation that can easily be grabbed and modified to give your projects a running start. We think this approach is essential for teaching deep learning because so much of the core knowledge in deep learning is derived from experimentation (vs"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","The first part of the book covers pure mathematical concepts, without getting into machine learning at all. The second part turns its attention to applying these newfound maths skills to machine learning problems. Depending on your desires, you could take either a top-down or bottom-up approach to leanring both machine learning and its underlying maths, or pick one part the other on which to focus"
"15 Free Data Science, Machine Learning & Statistics eBooks for 2021 - KDnuggets","All this is to say that the authors, who are also researchers and instructors, have an approach to how they are conveying their expertise. Their method seems to follow a logical ordered approach to what, and when, readers should be learning. However, individual chapters stand on their own as well, and so picking up the book and heading straight to the chapter on model inferences, for example, will work perfectly well, so long as you already have an understanding of what comes in the book before it"
Six Tips on Building a Data Science Team at a Small Company - KDnuggets,"When a company decides that they want to start leveraging their data for the first time, it can be a daunting task. Many businesses aren’t fully aware of all that goes into building a data science department. If you're the data scientist hired to make this happen, we have some tips to help you face the task head-on"
Six Tips on Building a Data Science Team at a Small Company - KDnuggets,"When a company decides that they want to start leveraging their data for the first time, it can be a daunting task. Many businesses aren’t fully aware of all that goes into building a data science department. If you're the data scientist hired to make this happen, we have some tips to help you face the task head-on"
Six Tips on Building a Data Science Team at a Small Company - KDnuggets,Being the only data scientist at a company is tricky. You may be expected to be the expert at everything data or code related. A good starting point is to break down the most important deliverables in the company. Understanding these deliverables and deconstructing them until you can lay out the most important data sources and processing steps is important in understanding exactly what needs to get done for this company
Six Tips on Building a Data Science Team at a Small Company - KDnuggets,"Staying organized is one of the most important aspects of building a successful team, but you don’t have to reinvent the wheel. There are many project planning practices that can help provide structure to your data processes. For example, The Data Science Hierarchy of Needs is a great resource for staying on track and organized during this planning process"
Six Tips on Building a Data Science Team at a Small Company - KDnuggets,"As the first data scientist, you can realistically expect that your non-technical colleagues will not understand your work and all the effort that goes into it. Therefore, it will be on you to report wins along the way towards deploying your first data model. This will ensure that your company stays up to date with your progress, and build trust in your ability to build and deliver"
Six Tips on Building a Data Science Team at a Small Company - KDnuggets,"For example, a reliable data flow will be the cornerstone of the productivity of any data team. It’s a foundational part of the pyramid and it will empower you to swiftly tackle a variety of problems. While the non-technical decision makers at your company will mostly be concerned with the analytical results that you eventually derive from working with a reliable data flow, setting up that flow is no small feat, and a huge step in the path to getting those results. You should take the time to report that step to your team, and make them understand its importance in the larger process"
Six Tips on Building a Data Science Team at a Small Company - KDnuggets,Data visualization is often overlooked. It will prove to be one of the most important tools in your data science toolkit. Good data visualization is all about practice
Six Tips on Building a Data Science Team at a Small Company - KDnuggets,"Think of data visualization as a tool to communicate the value of your work. It makes a huge difference for non-technical people to understand all you’re trying to convey. Ultimately, it is key for communicating and selling your work outside of your team"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"However, building these systems is hard. And getting them into production and used by the business is even harder. Let’s go through what it takes at Picnic to productize our data science projects, a term we’re affectionately coining ‘"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"Additionally, there is a non-linearity in data science projects that (usually) doesn’t occur in ‘traditional’ software development. We don’t know how well a model will perform before we start building it. It can take a week, three months, or it may not be possible to get an acceptable level of performance. This makes it very difficult to put together a nice project plan with timelines and deliverables that the business wants to see"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"Finally, the importance of model trust is hard to overstate when releasing a model to production. When working with the business to productionalize a model, we are entering a domain in which they are the experts. In many cases, we are looking to automate a manual process or replace a set of carefully crafted business rules. While these rules aren’t perfect, they were built by those with a deep understanding of the business. Handing off a black-box machine learning algorithm and telling the business that it is going to replace their current way of working is a challenging task. In the end, it’s the business that owns the profit/loss from whatever process the model is looking to automate, and we as data scientists need to convince them to put their livelihood in the hands of our models"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"The universe of problems that could be solved by machine learning is massive. You have countless use cases in customer success, supply chain, distribution, finance, and more. Given the ease of accessing high-quality data in Picnic’s beautifully maintained data warehouse, it’s difficult to know where to start. Choosing the correct use case is crucial to the success of a data science project"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"We want to make sure that our data scientists’ time is used most effectively. Let’s say there is a compelling problem that can generate huge value, but a few carefully crafted business rules can get us 80% of the value. Is it the best use of resources to have the data science team spend months trying to get an additional 10%? Probably not"
Data Science as a Product – Why Is It So Hard? - KDnuggets,Making sure there is alignment on the goal of the project seems both simple and obvious. The business wants more accurate forecasts. You are confident you can beat the existing system. What’s the issue?
Data Science as a Product – Why Is It So Hard? - KDnuggets,"Let’s say you build a great model and set up a daily job to have it executed. Well, it might turn out that the business needs to be able to update their forecasts throughout the day. All of a sudden, you need a real-time service. Your model performs well on a majority of the articles/segments/regions, but a new product is launching this quarter. Now your model is making predictions with no historical data (the"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"You can present all the ROC curves, F1 scores, and test set performance you want, but if the first few predictions your model makes happen to be incorrect, will it be given a chance to recover? The basic business rules that were previously in place weren’t great, but the business knew which cases it worked well and when it didn’t, then could intervene accordingly. Your models (hopefully) have an operational impact, and if the business doesn’t trust them, then they won’t be used. Simple as that"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"Discussions on model trust are uncomfortable but essential. You need to understand upfront what it will take for the business to be able to use your model in production. At the very least, an evaluation period with performance metrics needs to be decided and signed off on by both parties"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"Agile development has become the de facto standard for software development but hasn’t made its way to the data science world (yet). Data science projects today tend to happen with the ‘build it and they will come’ mentality. A data scientist meets with the business on the problem, decides what metric to optimize, and asks how they can get access to the data. Then they go off and spend a few months building a beautiful, robust model and present the finished product. And then…"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"For an MVP, the goal is getting an end-to-end solution built as quickly as possible. You build the data pipeline, start with a basic baseline model (also known as linear or logistic regression), and expose the predictions to the end consumer. A real-life example can be seen in"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"The hard part of data science projects isn’t the modeling. It’s everything else. By focusing on an MVP, you can get a working system into production, outputting predictions much faster. You will discover issues sooner and give your customers a new, shiny model within weeks instead of months"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"In the end, our goal isn’t to build a model for the sake of building a model. We are building a product where one component is a model. And we can take the learnings from decades of product development to get us there"
Data Science as a Product – Why Is It So Hard? - KDnuggets,"Building machine learning-based products is not easy. You have all the components of software development with the added complexity of machine learning at its core. It’s a new field without blueprints on how to do it well. By ensuring that you’ve selected the right use case, aligned with the business, and followed tried-and-true agile software development practices, you can set yourself up for success"
Essential Math for Data Science: The Poisson Distribution - KDnuggets,"Priya is recording birds in a national park, using a microphone placed in a tree. She is counting the number of times a bird is recorded singing and wants to model the number of birds singing in a minute. For this task, she’ll assume independence of the detected birds"
Essential Math for Data Science: The Poisson Distribution - KDnuggets,The probability that 5 birds will sing in the next minute is around 0.036 (3.6%)
Essential Math for Data Science: The Poisson Distribution - KDnuggets,"To address the first point, you can consider time as small discrete chunks. Let’s call these chunck ϵϵ (pronounced “epsilon”), as shown in Figure 4. If you consider each chunk as a trial, you have"
Essential Math for Data Science: The Poisson Distribution - KDnuggets,"He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog ("
MLOps – “Why is it required?” and “What it is”? - KDnuggets,"] below. This represents organizational challenges in terms of communication, collaboration, and coordination. The goal of MLOps is to streamline such challenges with well-established practices. Additionally, MLOps brings about agility and speed that is a cornerstone in today's digital world"
MLOps – “Why is it required?” and “What it is”? - KDnuggets,"The evaluation of an ML solution usually comes towards the end of the project lifecycle. With the development teams usually working in silos, the solution becomes a black-box to other stakeholders. This is worsened by the lack of intermediate feedback. These pose significant challenges in terms of time, effort, and resources"
MLOps – “Why is it required?” and “What it is”? - KDnuggets,"]. However, in machine learning (i.0), data scientists write code that defines how to use parameters to solve a business problem. The parameter values are found using data (with techniques such as gradient descent). These values may change with different versions of the data, thereby changing the code behavior. In other words, data plays an equally important role as the written code in defining the output. And both can change independently of each other. This adds a layer of data complexity in addition to the model code as an intrinsic part of the software that needs to be defined and tracked"
MLOps – “Why is it required?” and “What it is”? - KDnuggets,"In Machine Learning, output model can change if algorithm code or hyper-parameters or data change. While code and hyper-parameters are controlled by developers, change in data may not be. This warrants the concept of data and hyper-parameters versioning in addition to algorithm code. Note that data versioning is a challenge for unstructured data such as images and audio and that MLOps platforms adopt unique approaches to this challenge"
8 Places for Data Professionals to Find Datasets - KDnuggets,"Kaggle is a reliable resource for practice data. It incorporates writing and sharing code into some of its datasets, which gives you an additional bonus for mastering the field. You’ll work with data topics like natural language processing (NLP) and image classification"
8 Places for Data Professionals to Find Datasets - KDnuggets,"If you’d like a community-based approach to finding practice data, you can turn to Reddit. Reddit has become a search engine in its own way. The subreddit"
8 Places for Data Professionals to Find Datasets - KDnuggets,You’ll find plenty of contributions from like-minded people about where to find practice data. People will share sites and projects they’ve found helpful and guide you on the right path. You can then share your own projects that you’ve worked with to keep the momentum going
8 Places for Data Professionals to Find Datasets - KDnuggets,"With the countless datasets the United States government works with, it becomes an opportune resource for practice. Specifically, health data is some of the most abundant information you can find. With"
8 Places for Data Professionals to Find Datasets - KDnuggets,"While not a repository, election data is everywhere. This past presidential election has garnered attention like never before in history. Due to the vast number of mail-in ballots and tech-based engagement, this election has generated datasets in new ways"
8 Places for Data Professionals to Find Datasets - KDnuggets,"Similar to election data, census datasets are always changing. The U.S"
8 Places for Data Professionals to Find Datasets - KDnuggets,"GitHub collects its data from public sources like blogs, users and any form of public data. You’ll find datasets on topics from agriculture to museums to software. When you use GitHub, you can match your interests with the best practices"
8 Places for Data Professionals to Find Datasets - KDnuggets,"Now that machine learning is an imperative part of the tech world, it’s critical to understand how it all works. Datasets that focus on machine learning are the best way to master the field. UCI Machine Learning Repository is"
8 Places for Data Professionals to Find Datasets - KDnuggets,"This site collects various databases, data generators and theories which are all critical for machine learning. You’ll analyze algorithms and understand the intricacies of what makes machine learning so valuable today. UCI should be a primary resource for this study"
8 Places for Data Professionals to Find Datasets - KDnuggets,"These resources and repositories are some of the best places on the internet to get thorough practice with datasets. Ultimately, you’ll want to work with the sites that pique your interests. If you want to investigate machine learning, UCI will be an ideal resource. If you want to work with population information, the U.S"
Applications of Data Science and Business Analytics - KDnuggets,"In recent times, a large number of businesses have begun realising the potential of Data Science. Business analytics and data science applications are far and wide. So let us have a look at them in detail"
Applications of Data Science and Business Analytics - KDnuggets,"In recent times, a large number of businesses have begun realising the potential of Data Science. They have been using the latest technologies related to data to solve a variety of problems and gain a strategic advantage. It can be expected that data scientists will shape the way businesses are conducted in the future. IoT, Big Data, algorithm economics, Cloud have already gained a lot of traction as businesses worldwide use them to stay ahead of the competition. In this transition, automation and analytics have played a key role and continue to shape business practices. Business analytics and data science applications are far and wide. So let us have a look at them in detail"
Applications of Data Science and Business Analytics - KDnuggets,"Today, most of us use Google to get answers to all sorts of questions running in our minds. But let’s pause and think for a moment what powers Google and how it can perform the operations so intelligently. It’s not only Google but all other search engines that make use of machine learning. It immensely contributes to helping Google analyse and gather data, and present it to the users in the shortest possible time. In just a matter of a few seconds, users have thousands of results. All this would not have been possible if there was no data science"
Applications of Data Science and Business Analytics - KDnuggets,"The applications of data science offer an excellent level of personalisation when it comes to treatments. It can help doctors find a link between the medicines, genetics, and ailments the patients are suffering from. The genomic data can be combined with different data sets during the research and analysis stage. It helps to develop a better understanding of the ailments concerning the medicines. The acquisition of genome data on a more personal level will help scientists better understand human DNA. It can revolutionise the medical industry and pave the way for a better cure to various ailments"
Applications of Data Science and Business Analytics - KDnuggets,"With the advent of technology, soon, it will become possible that patients can consult doctors virtually. By optimising clinical processes and making everything available in a mobile app, patients can get more effective solutions. With the help of artificial intelligence, mobile apps can assist patients through chatbots. Popular applications like Ada and Your. MD is already doing this. Users just need to raise queries for their symptoms through which they receive answers regarding causes and remedies. This also helps patients by reminding them to take their medicines on time. Overall, it offers excellent assistance to patients who want to live a healthy lifestyle"
Applications of Data Science and Business Analytics - KDnuggets,"While Google can be considered a significant player, advertisers are not far behind. The advertisements that most of us see in our feeds today are based on our browsing patterns and are a reflection of what we like in a real sense. Data science makes it possible for advertisers to understand the trends based on the data they collect in terms of browsing history or any other personal data. This has helped advertisers reap more benefits out of digital advertisements as compared to traditional advertisements"
Applications of Data Science and Business Analytics - KDnuggets,"The competition in the business world is ever-increasing, and businesses that do not think on their feet will not last long. Analytics is one area that companies can target to stay ahead of their competition at all times. Business analytics has the potential to reveal critical personal details of consumers. Right from necessary information like demographic data to more personalised information like preferences, business analytics can help companies understand the minds of their target audience. They can make use of this data to provide more customised recommendations and solutions to their customers. In this age of targeted marketing, personalisation will be the crucial factor for any successful business"
Applications of Data Science and Business Analytics - KDnuggets,"For any business, their core strength is their employees, who have the power to make or break a business. It becomes essential for companies and human resources departments to keep their employees motivated and engaged. A workforce that is motivated can achieve positive results and bring profits to the company in the shortest time. All this can be achieved with the help of business analytics. Human resources departments already have a large amount of employee data. This can be used to find the likes, preference and motivating factors of employees. It can also help them control the attrition rate, which can be very crucial for the success of a business"
Applications of Data Science and Business Analytics - KDnuggets,"Data has the value of gold in today’s times and can help businesses succeed in their efforts. Even when it comes to the finance sector, business analytics has a significant role to play. Financial institutions can make use of business analytics tools to process the essential data available with them. Based on the insights gathered from this data, financial institutions can make predictions on the performance of the stock markets and guide their customers. It helps investors to invest their hard-earned money wisely"
Applications of Data Science and Business Analytics - KDnuggets,"Manufacturing processes can be very lengthy, time-consuming and involve a significant exchange of data. However, they can be handled intelligently by making use of business analytics. Since there is a considerable amount of data at the disposal of professionals from the manufacturing sector, they can leverage that to gain deep insights. Right from risk mitigation plans to supply chain management and inventory management, business analytics can help speed-up the processes and plan for the future. This leads to overall improvement and efficiency of operations"
Applications of Data Science and Business Analytics - KDnuggets,"With the advent of technology, businesses are becoming more tech-savvy. Big data, artificial intelligence, machine learning, etc. The world of IT has seen quite an upsurge in recent years. It will continue to see an upward growth as more data science companies leverage different technologies for better profits"
Applications of Data Science and Business Analytics - KDnuggets,"According to payscale. An entry-level professional can expect INR 5.5 lacs, while experienced professionals can expect around INR 18 lacs per annum. For business analysts, the situation is a bit different. The results from payscale.5 lacs. Mid-career professionals can expect around INR 8.5 lacs per annum, while highly experienced ones can expect around INR 12 lacs. These figures are indicative only as a salary depends on many factors, including location, type of company, etc"
Applications of Data Science and Business Analytics - KDnuggets,"The demand for skilled professionals from data science and business analytics will only increase with the time as businesses begin to up their game. However, there is a lack of a talented workforce that can meet the growing and constantly-evolving demands of the industry. To bridge this gap,"
Applications of Data Science and Business Analytics - KDnuggets,"You can opt for a Data Science, Business Analytics and data analytics course. These courses are delivered by veterans who have played a pivotal role in shaping the industry. Business analytics or data science career can be very rewarding. These"
Data Science and Machine Learning: The Free eBook - KDnuggets,"P. Kroese, Z.I. Botev, T. Taimre & R. Vaisman. The book was published last year, and aside from being freely-available as a PDF can also be purchased in print form (and Kindle)"
Data Science and Machine Learning: The Free eBook - KDnuggets,"Lots of relevant topics are covered here, and in logical succession. I particularly like the transition from Monte Carlo methods to unsupervised learning, and how that happens prior to the introduction of supervised concepts. Classification, though likely more useful in the long run (at least seemingly so at present) seemed far less impactful than did clustering when I first encountered machine learning, and so in my view its introduction prior may prove equally captivating for other new learners"
State of Data Science and Machine Learning 2020: 3 Key Findings - KDnuggets,"Pay for data scientists is covered in the report starting on page 11. Apples to apples comparisons are made of salary distributions for both US and India based data scientists, as are more difficult global distributions and median salaries by a number of select countries. While these apples to oranges comparisons are much less straight forward — considerations such as national cost of living differences and differences between data scientist pay distributions and distributions for"
State of Data Science and Machine Learning 2020: 3 Key Findings - KDnuggets,"Jupyter-based IDEs continue to be the go-to tool for data scientists, with around three-quarters of Kaggle data scientists using it. However, this has decreased from last year’s 83%. Visual Studio Code is in the second spot with just over 33%"
State of Data Science and Machine Learning 2020: 3 Key Findings - KDnuggets,"The rest of the list contains a mix of mostly Python based IDEs and multipurpose text editors, alongside RStudio and MATLAB. This list could provide some evidence of Python's relative dominance in the field, but also remind us of R's continued strong position, as well as MATLAB's persistence. We can also confirm that more traditional IDEs are not leveraged heavily in data science and machine learning, perhaps not as heavily as they may be in other programming disciplines — though we do not have such data readily available for comparison"
State of Data Science and Machine Learning 2020: 3 Key Findings - KDnuggets,"Unsurprisingly, linear regression, logistic regression, various decision tree methods, and gradient boosting machines take the top spots. The next tier of methods consists of a number of neural network architectures alongside Bayesian method. Finally, specialized neural network architectures such as transformers and GANs, as well as evolutionary approaches, round out the list"
Data Science Volunteering: Ways to Help - KDnuggets,"No matter what career level you’re at, you too can participate in “data for good” events and activities. Whether you’re established in or aspiring to a data career, there are plenty of opportunities for you to contribute. You’ll get experience in new domains, new portfolio projects, and new connections with other data enthusiasts, plus you’ll feel great about contributing to a good cause!"
Data Science Volunteering: Ways to Help - KDnuggets,"Sure, you could enter a data competition to seek fame and (maybe) some fortune… but why not choose one with a purpose that benefits others, too? Data-focused hackathons (aka datathons or data challenges) for good causes can offer both. These events are planned and coordinated by different organizations and on varying scales in terms of the number of participants, prizes, and resources. Here are a few places you’ll find these events:"
Data Science Volunteering: Ways to Help - KDnuggets,"There are plenty of challenging problems out there that you can address, even if you prefer to operate independently or can’t commit to a competition or organization. Come up with your own project and find publicly available data that could help you address it in some way, whether through modeling, data visualization, app building, data storytelling, or any other approach. For example, we recently shared lists of data sources related to"
Data Science Volunteering: Ways to Help - KDnuggets,"D. After 15 years as a journalism professor and researcher in academia, Susan shifted her focus to data science and analytics, but still loves to share knowledge in creative ways. She appreciates good food, science fiction, and dogs"
R or Python? Why Not Both? - KDnuggets,"Panels have connections, both input and output, and these interconnections can be configured and reconfigured as needed. Panels can run independently, or in tandem with the panels which come before or after it. Why bother with panels at all?"
R or Python? Why Not Both? - KDnuggets,"Data professionals need to experiment with their data, build tons of plots, and separate the code into different areas. They rarely want to have a single linear script that runs from start to end. This almost inevitably leads to very messy scripts, unclear outputs, multiplicity of confusing plots, and users needing to remember what needs to be commented out to test something. No other IDE is well suited for this"
R or Python? Why Not Both? - KDnuggets,"The days of arguing about which of these are the one language to rule them all are done, but that doesn't mean that either one is the ""winner. So use either — or both — and have a look at prython to see if its unique approach to a data science-centric IDE can make you more productive. After all, getting out of the way and allowing you to be productive is what your tools are"
6 Things About Data Science that Employers Don’t Want You to Know - KDnuggets,"Whether you like it or not, a data scientist is very much a business analyst. Why? Because you need to have a full understanding of the domain that you’re working in and the business problem at hand. Without this, you’ll miss out on key relationships, assumptions, and variables that could be the difference between a 65% accurate model and a 95% accurate model"
Graph Representation Learning: The Free eBook - KDnuggets,"Written by William L. Hamilton of McGill University, the book is currently in pre-publication draft form. It can be downloaded from the website in a"
Graph Representation Learning: The Free eBook - KDnuggets,"Graphs do more than just provide an elegant theoretical framework, however. They offer a mathematical foundation that we can build upon to analyze, understand, and learn from real-world complex systems. In the last twenty-five years, there has been a dramatic increase in the quantity and quality of graph structured data that is available to researchers. With the advent of large-scale social networking platforms, massive scientific initiatives to model the interactome, food webs, databases of molecule graph structures, and billions of interconnected web-enabled devices, there is no shortage of meaningful graph data for researchers to analyze. The challenge is unlocking the potential of this data"
Graph Representation Learning: The Free eBook - KDnuggets,"This book is about how we can use machine learning to tackle this challenge. Of course, machine learning is not the only possible way to analyze graph data. However, given the ever-increasing scale and complexity of the graph datasets that we seek to analyze, it is clear that machine learning will play an important role in advancing our ability to model, analyze, and understand graph data"
Graph Representation Learning: The Free eBook - KDnuggets,"If you are interested in learning about graph representation, or how graphs are utilized in machine learning, data science, or neural networks, this book is likely for you. DO yourself a favor and check out the freely available prepublication now. If you are then interested in purchasing a copy of the book, either physical or digital, you can do so"
Accelerate Your Career in Data Science - KDnuggets,"Nick Caldicott was working as an analytics manager at food giant ConAgra when he enrolled in the part-time Master of Science in Analytics program at the University of Chicago. Since earning his BS in Management Information Systems and Marketing, Caldicott had worked as an analyst—first in health marketing, next as a consultant. But he wanted something more"
5 Free Books to Learn Statistics for Data Science - KDnuggets,"Statistics is a fundamental skill that data scientists use every day. It is the branch of mathematics that allows us to collect, describe, interpret, visualise, and make inferences about data. Data scientists will use it for data analysis, experiment design, and statistical modelling"
5 Free Books to Learn Statistics for Data Science - KDnuggets,Statistics is also essential for machine learning. We will use statistics to understand the data prior to training a model. When we take samples of data for training and testing our models we need to employ statistical techniques to ensure fairness. When evaluating the performance of a model we need statistics to assess the variability of the predictions and assess accuracy
5 Free Books to Learn Statistics for Data Science - KDnuggets,"Statistics is a very broad field, and only part of it is relevant to data science. This book is extremely good at only covering the areas related to data science. So if you are looking for a book that will quickly give you just enough understanding to be able to practice data science then this book is definitely the one to choose"
5 Free Books to Learn Statistics for Data Science - KDnuggets,"It is another book that covers only the concepts directly related to data science and also contains lots of code examples, this time written in Python. It is aimed heavily at programmers and relies on using that skill to understand the key statistical concepts introduced. This book is therefore ideally suited to those who already have at least a basic grasp of Python"
5 Free Books to Learn Statistics for Data Science - KDnuggets,"Bayesian inference is a branch of statistics that deals with understanding uncertainty. As a data scientist uncertainty is something you will need to model on a very regular basis. If you are building a machine learning model, for example, you will need to be able to understand the uncertainty around the predictions that your model is delivering"
5 Free Books to Learn Statistics for Data Science - KDnuggets,"Bayesian methods can be quite abstract and difficult to understand. This book aimed firmly at programmers (so some Python is a prerequisite), is the only material I have found that explains these concepts in a simple enough way for a non-statistician to understand. There are coded examples throughout and the Github repository, where the chapters are hosted, contains a large selection of notebooks. It is, therefore, an excellent hands-on introduction to this subject"
5 Free Books to Learn Statistics for Data Science - KDnuggets,"The book was originally written for students studying a non-mathematics based course where an understanding of statistics is required, such as the social sciences. It, therefore, covers enough theory to understand the techniques but doesn’t assume an existing mathematical background. It is, therefore, an ideal book to read if you are coming into data science without a math-based degree"
5 Free Books to Learn Statistics for Data Science - KDnuggets,"The second half of the book, which covers machine learning algorithms, is some of the best material I have seen on this subject. Each explanation is in-depth and uses practical examples such as the classification of spam data which makes quite complex ideas easier to digest. The book is most suited to those who have already covered the basics of statistics for data analysis and are familiar with some statistical notation"
5 Free Books to Learn Statistics for Data Science - KDnuggets,The books I included in this article cover enough topics for a complete beginner to learn all the statistics needed for data science. They can all be read for free online but most also have a print version that can be purchased if you prefer to read physical books. Statistics is an essential component of the data science toolset and something which often requires in-depth reading to truly understand the concepts. Something which these books can provide
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"This means that random variables can take various values. How can you describe and compare these values? One good way is to use the probability that each outcome will occur. The probability distribution of a random variable is a function that takes the sample space as input and returns probabilities: in other words, it maps possible outcomes to their probabilities"
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,Let’s visualize the quantity of each outcome you got in the random experiment. You can divide by the number of trials to get the probability. Let’s use
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"With a uniform distribution, the plot would have the same height for each outcome (since the height corresponds to the probability, which is the same for each outcome of a die throw). However, the distribution shown in Figure 1 doesn’t look uniform. That’s because you didn’t repeat the experiment enough: the probabilities will stand when you repeat the experiment a large number of times (in theory, an infinite number of times)"
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"With continuous variables, there is an infinite number of possible outcomes (limited by the number of decimals you use). For instance, if you were drawing a number between 0 and 1 you might get an outcome of, for example, 0.413949834. The probability of drawing each number tends towards zero: if you divide something by a very large number (the number of possible outcomes), the result will be very small, close to zero. This is not very helpful in describing random variables"
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"Histograms show how values are distributed. It is a way to model a probability distribution using a finite number of values from the distribution. Since we're dealing with continuous distributions, this histogram corresponds to the number of values for specific intervals (the intervals depends on the parameter"
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"For instance, Figure 3 shows that there are around 347 elements in the interval (0.2, 0.3). Each bin corresponds to a width of 0.1, since we used 13 bins to represent data in the range -0.3 to 1"
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,You can see in Figure 4 that there are more bins in this histogram (24 instead of 13). This means that each bin has now a smaller width. The
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"Let’s take an example with the bar ranging from 0.2 to 0.25, associated with the following density:"
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"In our example, the height of the bar (the one from 0.2 to 0.25) is around 2.8, so the area of this bar is"
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"This means that the probability of getting a value between 0.2 and 0.25 is around 0.14, or 14%"
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,Histograms represent a binned version of the probability density function. Figure 5 shows a representation of the true probability density function. The blue shaded area in the figure corresponds to the probability of getting a number between 0 and 0.2 (the area under the curve between 0 and 0.2)
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"Like probability mass functions, probability density functions must satisfy some requirements. The first is that it must return only non negative values. Mathematically written:"
Essential Math for Data Science: Probability Density and Probability Mass Functions - KDnuggets,"He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris, where he did research on auditory perception using behavioral and electrophysiological data. He previously worked in industry where he built deep learning pipelines for speech processing. At the corner of data science and environment, he works on projects about biodiversity assessement using deep learning applied to audio recordings. He also periodically creates content and teaches at Le Wagon (data science Bootcamp), and writes articles in his blog ("
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","2020 is finally coming to a close. While likely not to register as anyone's favorite year, 2020 did have some noteworthy advancements in our field, and 2021 promises some important key trends to look forward to. As has become a year-end tradition, our collection of experts have once again contributed their thoughts. Read on to find out more"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","To the chagrin of absolutely no one, 2020 is finally drawing to a close. It has been a rollercoaster of a year, one defined almost exclusively by the COVID-19 pandemic. But other things have happened, including in the fields of AI, data science, and machine learning as well. To that end, it's time for KDnuggets annual year end expert analysis and predictions. This year we posed the question:"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","Topics such as ethics and diversity were taking center stage in 2019, and this past year they stayed there. There seems to have been a transition from thinking of diversity and ethics and related subjects as periphery concerns in machine learning to viewing them as core considerations alongside technology. Let's hope this trend continues into 2021 and beyond"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","To my mind, the main developments in 2020 were the emergence of graph neural networks and neuro-symbolic AI as major research directions. I think in 2021 we'll see the latter subsume the former: GNNs are a limited form of relational learning, and before long we'll have neuro-symbolic approaches that accomplish all that GNNs do, and then some. After this, where you turn the dial of representational power for specific applications is mainly the usual matter of overfitting control and scalability. At the high end, how far neuro-symbolic AI gets us toward human-level AI is the trillion dollar question"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","2020 was the year of COVID but also of tech. AI matured through MLOps deployments. The Cloud platforms (ex: AWS, Azure, GCP) continue to drive innovation in all areas of AI including AI on Edge devices. I expect to see much more innovation in this space after the acquisition of ARM by Nvidia"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","In the AI world, the big trend was NLP (GPT-3 and other models). For 2021, the real question is – would few shot learner models (like GPT-3) change the way models are built? Instead of the traditional sequence of building a model from data reflecting a problem, we could flip it. We can think of just a forward pass with very large models i. Model – Problem – Inference. Of course, we need a massive pre-trained model like GPT-3. If this trend does take off – it will be transforming for AI over the next two years"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","In 2021, traditional machine learning models could become a commodity in the sense that everyone would be using some form of basic ML or DL. So, we could shift from data science to decision science. The output of data science is a model with a performance metrics (for example accuracy). With decision science, we could take this further by suggesting actions and execute these actions. That means algorithms like reinforcement learning could be a part of 2021 and beyond"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","2020 has been an extraordinary year and while we've seen many exciting advancements in the field, the most important developments in my opinion have been about consolidation rather than revolution. In previous years, the technology was evolving so quickly that for many companies, it was wise to wait. That calculation has changed now, and there's a much better understanding of what projects are likely to succeed. Building prototypes and applying machine learning to business problems has never been easier but what remains challenging is closing the gap between prototyping and shipping successful projects into production. In 2021, we'll likely keep seeing more focus on the whole lifecycle of a machine learning project: from prototype to production, and from iterative development to ongoing maintenance and monitoring"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets",Convolutional and recurrent neural networks are beginning to show they can’t solve every problem as well as we would like. Two papers this year sum up this trend. The Hardware Lottery (
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","Also, because we’ve been left no choice, we’ve sprung to develop tools and practices for remote instruction, distributed teams, and asynchronous work. The machine learning research environment of 2020 would be unrecognizable to our 2019 selves. In 2021 I predict the quality and quality of online instruction and collaboration will double"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","For 2021, I am sure we are going to see the advent of powerful, yet efficient models especially for both vision and natural language processing. We have already seen progress with efficient transformer models like DistilBERT, Reformer and Performer. Deep Learning frameworks like TensorFlow are focusing on ML for mobile and IoT devices with TFLite and TF"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","I know a lot of folks would probably consider GPT-3 to be a major new development in NLP this year, but I'd consider it a pretty straightforward extension of existing NLP methods on a scale that's utterly impractical for the vast majority of NLP applications. What I personally find far more exciting is the growing trend of focusing on small, efficient models that still perform well. The first SustainNLP workshop ("
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","In 2020, AI has continued to improve incrementally. We've seen iterations on transformer-based models for natural language understanding and generation, the most notable being OpenAI's GPT-3. Autonomous vehicles continue to be almost ready for mainstream use. More broadly, AI has moved from being a buzzword to a critical capability for companies in all industries"
"AI, Analytics, Machine Learning, Data Science, Deep Learning Research Main Developments in 2020 and Key Trends for 2021 - KDnuggets","Meanwhile, 2020 has been dominated by the Covid-19 pandemic. While AI has played a part in the fight against the virus, what's been more interesting is how, because of the pandemic, most people working on or studying machine learning are doing so from home. If the mainstream acceptance of remote work and education persists after the pandemic -- which seems likely -- then we can look forward to two competing trends. On one hand, AI expertise will become truly global rather than tied to specific hubs. On the other hand, technology powerhouses will recruit talent globally, at the expense of smaller regional firms"
Introduction to Data Engineering - KDnuggets,"After researching online, I learned about a type of job I had never heard of before, but thought might be the “Mr. Right” for me: data engineer. I was lost at the time because I did not know where to start or whether this would work out for me. However, I was lucky enough to have a mentor who showed me the ropes and helped me land my first data engineering job. Since then, I have been working full time in the field and am still loving it!"
Introduction to Data Engineering - KDnuggets,"A data warehouse is a data storage system filled with data from various sources and is mainly used for data analysis. A company’s data is often stored in different transactional systems (or even worse, as text files), and transactional data is highly normalized and suboptimal for analytics. The main reason for building a data warehouse is to store all types of data in optimized formats in a centralized place so that data scientists can analyze this data altogether. There are many databases that serve well as a data warehouse, such as"
Introduction to Data Engineering - KDnuggets,"Sometimes data engineers also need to do data analysis and help data scientists integrate ML models into data pipelines. In some data teams, you might find data scientists do data engineering work. There are several overlapped skills between data engineers and data scientists: programming, data pipelining, and data analysis"
Introduction to Data Engineering - KDnuggets,"Today, about 1.7 MBs of new data are generated per second per human being on the planet, and this data contains huge values that cannot be harvested without data engineering. Data engineers help people make smarter decisions, which why I love my job so much"
20 Core Data Science Concepts for Beginners - KDnuggets,"Just as the name implies, data science is a branch of science that applies the scientific method to data with the goal of studying the relationships between different features and drawing out meaningful conclusions based on these relationships. Data is, therefore, the key component in data science. A dataset is a particular instance of data that is used for analysis or model building at any given time. A dataset comes in different flavors such as numerical data, categorical data, text data, image data, voice data, and video data. A dataset could be static (not changing) or dynamic (changes with time, for example, stock prices). Moreover, a dataset could depend on space as well. For example, temperature data in the United States would differ significantly from temperature data in Africa. For beginning data science projects, the most popular type of dataset is a dataset containing numerical data that is typically stored in a comma-separated values (CSV) file format"
20 Core Data Science Concepts for Beginners - KDnuggets,"The process of data wrangling is a critical step for any data scientist. Very rarely is data easily accessible in a data science project for analysis. It is more likely for the data to be in a file, a database, or extracted from documents such as web pages, tweets, or PDFs. Knowing how to wrangle and clean data will enable you to derive critical insights from your data that would otherwise be hidden"
20 Core Data Science Concepts for Beginners - KDnuggets,"Data Visualization is one of the most important branches of data science. It is one of the main tools used to analyze and study relationships between different variables. Data visualization (e. Data visualization is also used in machine learning for data preprocessing and analysis, feature selection, model building, model testing, and model evaluation. When preparing a data visualization, keep in mind that data visualization is more of an"
20 Core Data Science Concepts for Beginners - KDnuggets,"An outlier is a data point that is very different from the rest of the dataset. Outliers are often just bad data, e. Sometimes, outliers could indicate something real such as a malfunction in a system. Outliers are very common and are expected in large datasets. One common way to detect outliers in a dataset is by using a box plot"
20 Core Data Science Concepts for Beginners - KDnuggets,"Outliers can significantly degrade the predictive power of a machine learning model. A common way to deal with outliers is to simply omit the data points. However, removing real data outliers can be too optimistic, leading to non-realistic models. Advanced methods for dealing with outliers include the RANSAC method"
20 Core Data Science Concepts for Beginners - KDnuggets,"Most datasets contain missing values. The easiest way to deal with missing data is simply to throw away the data point. However, the removal of samples or dropping of entire feature columns is simply not feasible because we might lose too much valuable data. In this case, we can use different interpolation techniques to estimate the missing values from the other training samples in our dataset. One of the most common interpolation techniques is"
20 Core Data Science Concepts for Beginners - KDnuggets,"Whatever imputation method you employ in your model, you have to keep in mind that imputation is only an approximation, and hence can produce an error in the final model. If the data supplied was already preprocessed, you would have to find out how missing values were considered. What percentage of the original data was discarded? What imputation method was used to estimate missing values?"
20 Core Data Science Concepts for Beginners - KDnuggets,"In order to bring features to the same scale, we could decide to use either normalization or standardization of features. Most often, we assume data is normally distributed and default towards standardization, but that is not always the case. It is important that before deciding whether to use either standardization or normalization, you first take a look at how your features are statistically distributed. If the feature tends to be uniformly distributed, then we may use normalization ("
20 Core Data Science Concepts for Beginners - KDnuggets,"Large datasets with hundreds or thousands of features often lead to redundancy especially when features are correlated with each other. Training a model on a high-dimensional dataset having too many features can sometimes lead to overfitting (the model captures both real and random effects). In addition, an overly complex model having too many features can be hard to interpret. One way to solve the problem of redundancy is via feature selection and dimensionality reduction techniques such as PCA. Principal Component Analysis (PCA) is a statistical method that is used for feature extraction. PCA is used for high-dimensional and correlated data. The basic idea of PCA is to transform the original space of features into the space of the principal component. A PCA transformation achieves the following:"
20 Core Data Science Concepts for Beginners - KDnuggets,"PCA and LDA are two data preprocessing linear transformation techniques that are often used for dimensionality reduction to select relevant features that can be used in the final machine learning algorithm. PCA is an unsupervised algorithm that is used for feature extraction in high-dimensional and correlated data. PCA achieves dimensionality reduction by transforming features into orthogonal component axes of maximum variance in a dataset. The goal of LDA is to find the feature subspace that optimizes class separability and reduce dimensionality (see figure below). Hence, LDA is a supervised algorithm. An in-depth description of PCA and LDA can be found in this book: Python Machine Learning by Sebastian Raschka, Chapter 5"
20 Core Data Science Concepts for Beginners - KDnuggets,"In machine learning, the dataset is often partitioned into training and testing sets. The model is trained on the training dataset and then tested on the testing dataset. The testing dataset thus acts as the unseen dataset, which can be used to estimate a generalization error (the error expected when the model is applied to a real-world dataset after the model has been deployed). In scikit-learn, the train/test split estimator can be used to split the dataset as follows:"
20 Core Data Science Concepts for Beginners - KDnuggets,"In unsupervised learning, we are dealing with unlabeled data or data of unknown structure. Using unsupervised learning techniques, we are able to explore the structure of our data to extract meaningful information without the guidance of a known outcome variable or reward function. K-means clustering is an example of an unsupervised learning algorithm"
20 Core Data Science Concepts for Beginners - KDnuggets,"In reinforcement learning, the goal is to develop a system (agent) that improves its performance based on interactions with the environment. Since the information about the current state of the environment typically also includes a so-called reward signal, we can think of reinforcement learning as a field related to supervised learning. However, in reinforcement learning, this feedback is not the correct ground truth label or value but a measure of how well the action was measured by a reward function. Through the interaction with the environment, an agent can then use reinforcement learning to learn a series of actions that maximize this reward"
20 Core Data Science Concepts for Beginners - KDnuggets,"These are the parameters in the model that must be determined using the training data set. These are the fitted parameters. For example, suppose we have a model such as"
20 Core Data Science Concepts for Beginners - KDnuggets,"Cross-validation is a method of evaluating a machine learning model’s performance across random samples of the dataset. This assures that any biases in the dataset are captured. Cross-validation can help us to obtain reliable estimates of the model’s generalization error, that is, how well the model performs on unseen data"
20 Core Data Science Concepts for Beginners - KDnuggets,"In k-fold cross-validation, the dataset is randomly partitioned into training and testing sets. The model is trained on the training set and evaluated on the testing set. The process is repeated k-times. The average training and testing scores are then calculated by averaging over the k-folds"
20 Core Data Science Concepts for Beginners - KDnuggets,"In machine learning (predictive analytics), there are several metrics that can be used for model evaluation. For example, a supervised learning (continuous target) model can be evaluated using metrics such as the R2 score, mean square error (MSE), or mean absolute error (MAE). Furthermore, a supervised learning (discrete target) model, also referred to as a classification model, can be evaluated using metrics such as accuracy, precision, recall, f1 score, and the area under ROC curve (AUC)"
20 Core Data Science Concepts for Beginners - KDnuggets,"It is important to build machine learning models that will yield unbiased estimates of uncertainties in calculated outcomes. Due to the inherent randomness in the dataset and model, evaluation parameters such as the R2 score are random variables, and thus it is important to estimate the degree of uncertainty in the model. For an example of uncertainty quantification, see this article:"
20 Core Data Science Concepts for Beginners - KDnuggets,"Most machine learning models are built with a dataset having several features or predictors. Hence, familiarity with multivariable calculus is extremely important for building a machine learning model. Here are the topics you need to be familiar with:"
20 Core Data Science Concepts for Beginners - KDnuggets,"Linear algebra is the most important math skill in machine learning. A data set is represented as a matrix. Linear algebra is used in data preprocessing, data transformation, dimensionality reduction, and model evaluation. Here are the topics you need to be familiar with:"
20 Core Data Science Concepts for Beginners - KDnuggets,"A typical data analysis project may involve several parts, each including several data files and different scripts with code. Keeping all these organized can be challenging. Productivity tools help you to keep projects organized and to maintain a record of your completed projects. Some essential productivity tools for practicing data scientists include tools such as Unix/Linux, git and GitHub, RStudio, and Jupyter Notebook. Find out more about productivity tools here:"
NoSQL for Beginners - KDnuggets,"NoSQL is essentially the response to SQL’s rigid structure. First created in the early 1970s, NoSQL didn’t really take off until the late 2000s, when Amazon and Google both put a lot of research and development into it. Since then, it’s taken off to be an integral part of the modern world, with many big websites around the world using some form of NoSQL"
NoSQL for Beginners - KDnuggets,SQL is built from the ground-up to essentially avoid data duplication. That ultimately means that any SQL project requires an expert designer to spend a long period of time on the schema before implementing it. This step is not only important in the long run to maintain
NoSQL for Beginners - KDnuggets,"On the other hand, since NoSQL doesn’t necessitate the need for a schema, you avoid the expense and time of that initial design stage. Furthermore, the lack of schema means that most NoSQL databases are incredibly flexible, allowing you to change or even mix data types and models. This makes administrating and dealing with the database much easier"
NoSQL for Beginners - KDnuggets,"With SQL, querying data tends to require you to do so across multiple tables. With NoSQL, all the data is contained in one table, and therefore querying is much easier. This has the side-effect of making"
NoSQL for Beginners - KDnuggets,"The only really big downside when it comes to NoSQL is that it isn’t as established as SQL. Keep in mind that SQL has several decades worth of a head start, and this maturity shows in the ease with which information can be found. Similarly, finding an expert on SQL is much easier than finding one for NoSQL"
NoSQL for Beginners - KDnuggets,"This type of data model allows you to store information as any type of data. This is in contrast to SQL, which relies heavily on XML and JSON, and essentially ties the two together, and can make any query inefficient (or less efficient). Since NoSQL doesn’t use a scheme, there’s no need for relational data storage, and no need to tie those two together"
NoSQL for Beginners - KDnuggets,"As the name suggests, this data model stores information using keys and pointers to the specific value. Since both keys and values can be any piece of data you desire, Key-value store data models are quite versatile. Its purpose made for retrieving, storing, and managing arrays and is perfect for high volume applications"
NoSQL for Beginners - KDnuggets,"Whereas SQL traditionally stores data in rows, Column-oriented stores it in columns. These are then grouped into families, which can themselves hold a nearly infinite amount of columns. Writing and reading are also done by columns as well, so the whole system is really efficient, and is made for fast search & access, as well as data aggregation"
NoSQL for Beginners - KDnuggets,"After graduating from the University of London, majoring in IT, Alex worked as a developer leading various projects for clients from all over the world for almost 10 years. Recently, Alex switched to being an independent IT consultant and started his own blog. There, he explores web development, data management, digital marketing, and solutions for online business owners just starting out"
Data Science History and Overview - KDnuggets,"For example, deep learning requires running Jupyter in more powerful environments. Fortunately, platforms like Saturn Cloud let users facilitate the management of the Jupyter development environment. In fact, by managing the resources of the environment, the user can enable more power in terms of CPU, GPU, and memory, just when it is necessary. A platform designed for cloud computing, therefore, allows keeping the environmental costs low, allowing the Data Scientist to pay only for the resources he uses"
Data Science History and Overview - KDnuggets,"Since the beginning of the 21st century, data stockpiles have expanded exponentially, largely thanks to advents in processing and storage that are both efficient and cost-effective at scale. The capability to collect, process, analyze, and display data and information in “real-time” give us an unprecedented opportunity to conduct a new form of knowledge discovery. To process this huge amount of data, Data Scientists need high performance also of a large portfolio of technologies to speed up tasks and data processing in a matter of seconds"
Data Science History and Overview - KDnuggets,"These appear as strong challenges to address. However, we need to realize that for every step forward in a new discipline, new challenges need to be addressed. We must embrace transformative changes, and we must be assured that changes help us ensure continuous improvement, acquiring new skills, expanding our knowledge, and exploring new approaches"
Data Science History and Overview - KDnuggets,"Therefore, Data Scientists are playing an essential role in the business development strategy of every company and organization. As said by Thomas H. Devenport and D.J. Patil, the Data Scientist is the sexiest job of the 21st Century"
Data Science History and Overview - KDnuggets,"An extensive collection of software tools is available to support the Data Scientist to dive into the world of Data Science. Nowadays, the platforms available enable Data Scientists to work at scale using the tools they know best: Python, Jupyter, and Dask. Usually, the services are provided through secure and scalable infrastructures for running data science and machine learning workloads within cloud environments. Data teams can develop and deploy data science models in Python at scale with automated DevOps and ML infrastructure engineering"
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,Object-Oriented Programming or OOP can be a tough concept to understand for beginners. And that’s mainly because it is not really explained in the right way in a lot of places. Normally a lot of books start by explaining OOP by talking about the three big terms —
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,"So, I thought of making the concept a little easier for fellow programmers, Data Scientists and Pythonistas. The way I intend to do is by removing all the Jargon and going through some examples. I would start by explaining classes and objects. Then I would explain why classes are important in various situations and how they solve some fundamental problems. In this way, the reader would also be able to understand the three big terms by the end of the post"
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,"So, we use them a lot when we are working with Python. But why really. What is it with classes? I could do the same with functions?"
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,"Yes, you can. But classes really provide you with a lot of power compared to functions. To quote an example, the"
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,"Apart from this, Class usage can also help us to make the code much more modular and easy to maintain. So say we were to create a library like Scikit-Learn. We need to create many models, and each model will have a fit and predict method. If we don’t use classes, we will end up with a lot of functions for each of our different models like:"
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,"Suppose you are working at a bank that has many accounts. We can create a class named account that would be used to work with any account. For example, below I create an elementary toy class"
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,"So, suppose you are working with Apple iPhone Division, and you have to create a different Class for each iPhone model. For this simple example, let us say that our iPhone’s first version currently does a single thing only — Makes a call and has some memory. We can write the class as:"
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,You would need to define a lot of variables/attributes in your class and copying pasting them for the child class (here iphone1) becomes cumbersome. Thus there exists super(). Here
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,"And hence the object is polymorphic. This has a lot of good properties. For example, We can create a function that works with an"
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,"I hope this has been useful for you to understand classes. There is still so much to classes that remain that I would cover in my next post on magic methods. Stay Tuned. Also, to summarize, in this post, we learned about OOP and creating classes along with the various fundamentals of OOP:"
Object-Oriented Programming Explained Simply for Data Scientists - KDnuggets,I am going to be writing more of such posts in the future too. Let me know what you think about the series. Follow me up at
14 Data Science projects to improve your skills - KDnuggets,"Let’s not take this for granted. Take this time in isolation to learn new skills, read books, and improve yourself. For those interested in data, data analytics, or data science, I’m providing a list of fourteen data science projects that you can do during your spare time!"
14 Data Science projects to improve your skills - KDnuggets,"The 2019–2020 bushfire season, also known as the black summer, consisted of several extreme wildfires starting in June 2019. The fires burnt an estimated 18.6 million hectares and over 5,900 buildings,"
14 Data Science projects to improve your skills - KDnuggets,"WHO created a dataset of the health status of all countries over time and includes statistics on life expectancy, adult mortality, and more. Using this dataset, explore the relationships between various variables. What has the biggest impact on life expectancy?"
14 Data Science projects to improve your skills - KDnuggets,"This dataset is composed of power consumption data from PJM’s website. PJM is a regional transmission organization in the United States. Using this dataset, see if you can build a time series model to predict energy consumption. In addition to that, see if you can find trends around hours of the day, holiday energy usage, and long term trends!"
14 Data Science projects to improve your skills - KDnuggets,"Craigslist is the world’s largest collection of used vehicles for sale. This dataset is composed of scraped data from Craigslist and is updated every few months. Using this data set, see if you can create a dataset that predicts whether a car listing is over or underpriced"
14 Data Science projects to improve your skills - KDnuggets,"This dataset presents transactions that occurred in two days, with 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, with the positive class (frauds) account for 0.172% of all transactions. Learn how to work with unbalanced datasets and build a credit card fraud detection model"
Better data apps with Streamlit’s new layout options - KDnuggets,"Streamlit is all about simplicity. It’s pure Python. Your script runs from top to bottom. Your app renders from top to bottom too. Perfect, right? Well. Users noted that our thinking was a bit"
Better data apps with Streamlit’s new layout options - KDnuggets,The group griped about grids. The community clamored for columns. Fervent friends favored flexibility. You get the idea
Better data apps with Streamlit’s new layout options - KDnuggets,"Coming up are updates with padding, alignment, responsive design, and UI customization. Stay tuned for that, but most importantly, let us know what YOU want from layout. Questions? Suggestions? Or just have a neat app you want to show off? Join us on the"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"Variety on a resume demonstrates flexibility, which is an essential skill for data professionals today. It can be an excellent resume-booster, but jumping from position to position to improve a resume isn’t practical. Thankfully, applicants can add variety without needing to take on many new jobs"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"Job titles aren’t the only things on a resume that showcase variety. If applicants have any publications like books, journal articles or whitepapers under their name, they should list them. These stand out to potential employers as an example that a worker is serious about their field"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"Remember that everything should be relevant to the specific job at hand. Only data-related publications should appear on a resume for a data-related position. If workers don’t have any relevant published works to their name, they can look for opportunities to be part of one"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"Throughout their career, data professionals will work on various projects, either for work or as hobbies. Whether they’re an impressive personal pursuit or an event that saved their company money, these projects are examples of relevant skills. Mentioning specific projects instead of general work descriptions also adds variation to an otherwise repetitive resume"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"Anything that showcases different approaches or skills, or is particularly impressive, is worth mentioning. Descriptions of these projects don’t need to be long and should focus on what makes them unique. It’s best to"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"An applicant’s current company may have projects available that would add variation to a resume. As data professionals work, they should look out for any opportunities to learn or employ new skills. Volunteering to be part of these tasks will help build a more impressive resume"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"Data professionals don’t have to confine themselves to the work available in their current position. Data scientists and analysts are in-demand workers, so they can perform freelance work to boost their resumes. Freelance projects enable professionals to take on tasks they wouldn’t otherwise perform, adding variation without another full-time job"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"Since this type of work enables professionals to choose their own hours, it can fit around their current schedule. Workers don’t need to take on too much extra work. Just a few projects can add some needed variety"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"If professionals want to find another full-time position that will add variety, they may consider working internationally. International companies are more likely to have different projects than those available in the U.S. Even if they don’t, working with different cultures showcases flexibility"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"Even through various positions, data professionals may find that their day-to-day work looks similar. These workers can still find chances to add variation to their resume by mentioning the soft skills they’ve accumulated. Even if the data-centric work looks the same from position to position, different work environments may develop various soft skills"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"Crunching numbers isn’t the only skill that’s important to the work of a data professional. They also need to communicate results to various audiences, work in teams and be adaptable. Highlighting these soft skills instead of tasks that would look more similar adds variety to a resume"
How Data Professionals Can Add More Variation to Their Resumes - KDnuggets,"There’s a lot of variety in data-related work, even if it may not be immediately apparent. Data professionals looking to improve their resumes can follow these steps to showcase this variation. They can then become an even more appealing candidate in an already in-demand field"
Kubernetes vs. Amazon ECS for Data Scientists - KDnuggets,"ECS runs in 69 availability zones and 22 regions. Also, ECS falls under the Amazon Web Services (AWS) umbrella. That means it"
Kubernetes vs. Amazon ECS for Data Scientists - KDnuggets,"Amazon ECS is part of the AWS ecosystem. That brings advantages in some cases and downsides in others. For example, it’s easier and faster to perform the initial setup compared to Kubernetes because you can do it through the AWS Management Console and don’t need to set up the control panel that Kubernetes requires"
Kubernetes vs. Amazon ECS for Data Scientists - KDnuggets,"There are also free tiers associated with AWS products. That sounds like a good thing at first. However, users report receiving large bills after their"
Kubernetes vs. Amazon ECS for Data Scientists - KDnuggets,"That means the prices you pay for using Kubernetes through those providers will vary. The ideal approach is to create a list of products or companies that interest you and offer Kubernetes support. Then, take the time to research their pricing structures and see which ones seem most appropriate for your budget and the extent of data science work you want to do with Kubernetes"
Kubernetes vs. Amazon ECS for Data Scientists - KDnuggets,"However, allowing enough time to learn about each solution is a smart move. Consider any projects in your pipeline that may apply to your use of a containerization solution. Then, review each product's features and determine which ones are most relevant to your situation and expectations. Scrutinizing each option this way will lead to well-informed decisions"
Hypothesis Vetting: The Most Important Skill Every Successful Data Scientist Needs - KDnuggets,"The most successful Data Science starts with good hypothesis building. A well-thought hypothesis sets the direction and plan for a Data Science project. Accordingly, a hypothesis is the most important item for evaluating whether a Data Science project will be successful"
Hypothesis Vetting: The Most Important Skill Every Successful Data Scientist Needs - KDnuggets,"Through experience, I’ve derived a systematic way of approaching Data Science problems which guarantees both a relevant hypothesis and a strong signal as to whether a Data Science approach will be successful. In this article, I outline the steps used to do this: defining a data context, examining available data, and forming the hypothesis. I also take a look at a few completed competitions from Kaggle and run them through the process in order to provide real examples of this method in action"
Hypothesis Vetting: The Most Important Skill Every Successful Data Scientist Needs - KDnuggets,"Much like forming a Free Body Diagram in a physics problem or utilizing Object-Oriented Design in Computer Science, describing all valid entities in a given data context helps to map out the expected interplay between them. The goal of this step is to completely designate all the data that could possibly be collected about anything in the context i. If all of this data were available, then the interactions between the components would be completely defined and heuristic formulas could be used to define every type of cause and effect"
Hypothesis Vetting: The Most Important Skill Every Successful Data Scientist Needs - KDnuggets,"Next, an observation is conducted to determine how much of the available data fits into the perfect dataset defined in the previous step. The more overlap found, the better a solution space for defining interactions between entities. While not a numeric metric, this observation gives a strong signal for intuition of whether the available data is appropriate and relevant enough. If there is a complete overlap, then a heuristic is probably a better solution than fitting a data model. If there is very little overlap, then even the best modeling techniques will be unable to consistently provide accurate predictions. Note that the strongest predictive signals will always be those that are directly related, thus indirect feature relation is given less emphasis in favor of the real thing. The intuition behind focusing mainly on strong signals is that collecting better data with strong signals will guarantee better performance and reliability. Conversely, predictions using weak signals are prime candidates to become obsolete once better data is available"
Hypothesis Vetting: The Most Important Skill Every Successful Data Scientist Needs - KDnuggets,"This project was selected because the data context is simplified, due to the nature that predictions are performed for a virtual world. Predictions of simulations are expected to behave much more consistently and operate along very clearly defined paths, as opposed to real-life systems which may have factors not easily observable. In other words, the data context can very confidently be exhaustively defined. In this video game, players control a single unit which can perform a limited set of actions. The data context has been mapped out below:"
Hypothesis Vetting: The Most Important Skill Every Successful Data Scientist Needs - KDnuggets,"The four major entities which data can be mapped to are: The Player, the Player-Controlled Character, the Map, and the Match itself. I’ve selected these high-level entities based on the significant interactions between them as well as the uniqueness of each entity. Thus, collectively they define most if not all of the Data Context at play in this scenario"
Hypothesis Vetting: The Most Important Skill Every Successful Data Scientist Needs - KDnuggets,"Using these insights, strong candidates for hypotheses can be defined. Again, this technique for generating hypotheses ignores feature correlation or indirect relation in favor of focusing on the absolute strongest signals i. In this competition, predictions involving the Player-Controlled Character and the Match are captured with a good amount of detail"
Hypothesis Vetting: The Most Important Skill Every Successful Data Scientist Needs - KDnuggets,"For the second example, I selected a project with real-world elements which still maintained a fairly simple context. Specifically, this competition’s hypothesis is that poverty level can be predicted using the given data. The data context surrounding this hypothesis can be defined as household costs versus incomes, as has been mapped out in the diagram below:"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"You are intrigued by this exciting new field of Data Science, and you think you want in on the action. The demand remains very high and the salaries are strong. Before taking the leap onto this path, these questions will help you evaluate if you are ready for the challenges and opportunities"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"Data is now considered to be one of the fastest-growing, multibillion-dollar industries. As a result, corporations and organizations are trying to make the most out of the data they already have and determine what data they still need to capture and store. In addition, there continues to be an incredible need for data scientists to make sense of the numbers and uncover hidden solutions to messy business problems. A"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"A data scientist works with data to draw out meaning and insightful conclusions that can drive decision making in an institution or organization. Their job role includes data collection, data transformation, data visualization and analysis, building predictive models, providing recommendations on actions to implement based on data findings. Data scientists work in different sectors such as healthcare, government, industries, energy, academia, technology, entertainment, etc. Some top companies that hire data scientists are Amazon, Google, Microsoft, Facebook, LinkedIn, Twitter, Netflix, IBM, etc"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"How much you make as a data scientist depends on the organization or company you are working for, your educational background, your number of years of experience, and your specific job role. Data scientists make anywhere from $50,000 to $250,000, with the median salary being about $120,000. This"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"While it is important to learn as many data science tools as possible, it is recommended to start with just one or two programming languages for a start. Then once you have built a solid background in data science, you can then challenge yourself to learn about different programming languages or different platforms and productivity tools that can enhance your skillset. According to this"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"Data science projects could be very long and demanding. From problem framing to model building and application, the process could take weeks and even months, depending on the scale of the problem. As a practicing data scientist, hitting a roadblock with a project is something inevitable. Patience, tenacity, and perseverance are key qualities essential for a successful data science career"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"Data science is a very practical field. Remember that you may be very good at handling data as well as building good machine learning algorithms, but as a data scientist, the real-world application is all that matters. Every predictive model must produce meaningful and interpretable results in real-life situations. A predictive model must be validated against reality in order to be considered meaningful and useful. Your role as a data scientist is to draw out meaningful insights from data that can be used for data-driven decisions that can improve the efficiency of your company or improve the way business is conducted, or help increase profits"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"Data scientists need to be able to communicate their ideas with other members of the team or with business administrators in their organizations. Good communication skills would play a key role here to be able to convey and present very technical information to people with little or no understanding of technical concepts in data science. Good communication skills will help foster an atmosphere of unity and togetherness with other team members such as data analysts, data engineers, field engineers, etc"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"Data science is a field that is ever-evolving, so be prepared to embrace and learn new technologies. One way to keep in touch with developments in the field is to network with other data scientists. Some platforms that promote networking are LinkedIn, GitHub, and medium ("
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"As a data scientist, you will be working in a team of data analysts, engineers, administrators, so you need good communication skills. You need to be a good listener, too, especially during early project development phases where you need to rely on engineers or other personnel to be able to design and frame a good data science project. Being a good team player world helps you to thrive in a business environment and maintain good relationships with other members of your team as well as administrators or directors of your organization"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"Ethics and privacy considerations are a must in data science. You need to understand the implication of your project. Be truthful to yourself. Avoid manipulating data or using a method that will intentionally produce bias in results. Be ethical in all phases, from data collection and analysis to model building, analysis, testing, and application. Avoid fabricating results for the purpose of misleading or manipulating your audience. Be ethical in the way you interpret the findings from your data science project"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"You may pursue a master’s degree in data science or in business analytics if your circumstances allow you to do that. If you cannot afford a master’s degree program, you may pursue the self-study route for learning about data science. Generally, if you have a solid background in an analytic discipline such as physics, mathematics, economics, engineering, or computer science, and you are interested in exploring the field of data science, the best way is to begin with massive open online courses (MOOCs). Then after establishing a solid foundation, you may then seek other ways to increase your knowledge and expertise, such as studying from textbooks, engaging in projects, and networking with other data science aspirants"
Is Data Science for Me? 14 Self-examination Questions to Consider - KDnuggets,"The author explains fundamental concepts in machine learning in a way that is very easy to follow. Also, the code is included, so you can actually use the code provided to practice and build your own models. I have personally found this book to be very useful in my journey as a data scientist. I would recommend this book to any data science aspirant. All that you need is basic linear algebra and programming skills to be able to understand the book"
"Top Python Libraries for Deep Learning, Natural Language Processing & Computer Vision - KDnuggets","Our list is made up of libraries that our team decided together by consensus was representative of common and well-used Python libraries. Also, to be included a library must have a Github repository. The categories are in no particular order, and neither are the libraries included within each. We contemplated constructing an ordering arbitrarily by stars or some other metric, but decided against it in order not explicitly stray from placing any perceived value or importance of the libraries within. Their listing here, then, is purely random. Library descriptions are directly from the Github repositories, in some form or another"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"Reinforcement learning topped our list of wanted skills, with 51.9% of respondents stating that they hoped to add it to their skills portfolio. Two comprehensive reinforcement learning resources with which I am familiar are:"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"51.2% of respondents said they would like to either improve upon their TensorFlow skills or add it to their repertoire. And this is for good reason, as TensorFlow remains"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"Deep learning algorithms were found to be a desirable skill to acquire by 50.8% of respondents. The passage of time is solidifying the notion that deep learning isn't a fad that's on its way out; just recently,"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"In fourth place was PyTorch, with 50.1% of respondents noting that they would be interested in adding or increasing their knowledge of the deep learning library. The best place to learn the basics of PyTorch continues to be the PyTorch official tutorials, available here:"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"48.8% of respondents reported their interest in learning AWS, which is a very wide array of related services from Amazon. Given this, the place to go to learn the basics of any of these many related services remains the official Amazon AWS training site, available here:"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"Natural language processing remains a popular skill (or set of skills), as evidenced by their being in demand by 48.7% of our respondents. Learning NLP is a lot more time intensive that taking a few courses, and approaching it casually from solely a data science, computer science, or AI background will only get you so far. However, gaining an understanding of the very basics can be accomplished by a course such as the one from Amazon's Machine Learning University:"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"45.3% of respondents are interested in knowing more about Apache Spark. Big data is no longer worthy of note, since so much of the data we work with is big, and its just assumed that we know how to be able to process it. This is where Apache Spark comes in. The following article provides some insight into where to start with learning Spark given differing objectives, and notes 5 free sources for doing so"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"Docker is becoming increasingly more of a must-know for those in the data development world, especially those in data DevOps and data engineering. It should be of no surprise that 44.9% of respondents are interested in knowing more about the technology in order to add it to their list of skills. Here is an article which addresses approaching Docker from the point of view of a data scientist:"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"NoSQL is a broad term encompassing a wide variety of database engines and technologies which do not fit into the traditional SQL/relational database mold. As such, the term functionally includes database types such as graph databases, key-value stores, columnar databases, and document stores. These are all different, but have in common that they are employed when relation models are not feasible. 43.0% of respondents want to know more, and this article can help scratch the surface and point them in the right direction to learn more"
How to Acquire the Most Wanted Data Science Skills - KDnuggets,"Finally, in the number 10 position, computer vision is a skill (or, again, related set of skills) desired by 42.7% of respondents. Similar to NLP, mastery of the computer vision field of play is much more complicated than taking a course, but — also similar to NLP — a rudimentary understanding of the basics can be achieved via an Amazon Machine Learning University course:"
5 Tricky SQL Queries Solved - KDnuggets,Each query can be written in different ways. Try to think about the approach before moving on to my solutions. You can also suggest different approaches in the response section
5 Tricky SQL Queries Solved - KDnuggets,"Upon initial analysis, we can conclude that if a given node N has its corresponding P-value as NULL it is the root. And for a given Node N if it exists in the P column it is not an inner node. Based on this idea let us write a query"
5 Tricky SQL Queries Solved - KDnuggets,"Similarly, if a given node N is in column P it is an inner node. To get all nodes from column P we wrote a subquery which returns all the distinct nodes in column P. Since we were asked to order the output by node values in ascending order we used the"
5 Tricky SQL Queries Solved - KDnuggets,"To solve this query, we cannot directly count the occurrence of user_id’s and if it is more than one return that user_id because a given user can have more than one transaction on a single day. Hence if a given user_id has more than one distinct date associated with it means he purchased products on multiple days. Following the same approach, I wrote a query"
5 Tricky SQL Queries Solved - KDnuggets,"We are given a subscription table which consists of subscription start and end date for each user. We need to write a query that returns true/false for each user based on the overlapping of dates with other users. For instance, If user1's subscription period overlaps with any other user the query must return"
5 Tricky SQL Queries Solved - KDnuggets,We must also ensure that a user is not compared to his own subscription. We also want to run a left join on itself to match a user with each other user that satisfies our condition. We will create two replicas s1 and s2 of the same table now
5 Tricky SQL Queries Solved - KDnuggets,We can see there exists another user for each user in case the dates overlap. For user1 there are 2 rows indicating that he matches with 2 users. For user 4 the corresponding id is null indicating that he does not match with any other user
5 Tricky SQL Queries Solved - KDnuggets,"Mastering SQL requires lots of practice. In this article, I took 5 tricky questions and explained the approaches to solve them. The specialty of SQL is that each query can be written in many different ways. Do feel free to share your approaches in the responses. I hope you learned something new today!"
Free From MIT: Intro to Computational Thinking with Julia - KDnuggets,"So you have sampled other data science courses online, and perhaps you liked some of them. Maybe they helped you think about problems from a computational standpoint. You were then able to make a transition to doing your own data science projects to try out implementing some other related concepts and skills"
Free From MIT: Intro to Computational Thinking with Julia - KDnuggets,"But let me ask you this: what were the programming languages used in these courses? I'll bet most of them were taught using Python, with some significant minority instead using R. I will further bet that zero were taught using Julia. As you can likely glean from the title of this article and the course that it is spotlighting, Julia is the language utilized in this free offering from MIT"
Free From MIT: Intro to Computational Thinking with Julia - KDnuggets,"Alan Edelman and Prof. David P. Sanders. Given the inclusion of COVID-19 in its title, it shouldn't surprise you that this is a very recent offering of the course as well, having been taught in spring 2020"
Free From MIT: Intro to Computational Thinking with Julia - KDnuggets,"As you can see, the topics shift gears between programming fundamentals, data science concepts, and COVID-specific topics from week to week, the focus of which no doubt helps keep the material fresh and interest high. The courseware includes lecture slides, lecture videos, assignments, external resources, and Jupyter notebooks as applicable. The course's"
Free From MIT: Intro to Computational Thinking with Julia - KDnuggets,"Remember that this course is titled ""computational thinking,"" and so there is nothing related to machine learning directly within. There is, however, a focus on how to solve problems using computational methods, which is a very useful skill to possess regardless of your exact programmatic role; it is definitely necessary for solving data science problems computationally. With this in mind, pairing this course with the high quality freely-available ebook,"
Learn to build an end to end data science project - KDnuggets,"This stage is significant because it helps clarify the customer’s target. The success of any project depends on the quality of the questions asked. If you understand the business requirement correctly, then it helps you collect the right data. Asking the right questions will help you narrow down the data acquisition part"
Learn to build an end to end data science project - KDnuggets,"This is the stage where, once the business problem has been clearly stated, the data scientist can define the analytic approach to solve the problem. This step includes explaining the problem in the sense of statistical and machine-learning techniques, and it is important as it helps to determine what kind of trends are required to solve the issue in the most efficient way possible. If the issue is to determine the probabilities of something, then a predictive model might be used; if the question is to show relationships, a descriptive approach may be required, and if our problem requires counts, then statistical analysis is the best way to solve it. For each type of approach, we can use different algorithms"
Learn to build an end to end data science project - KDnuggets,"We identify the available data resources relevant to the problem domain. To retrieve the data, we can apply web scraping on a related website, or we can use a repository with premade datasets that are ready to use. If you want to collect data from any website or repository, use the Pandas library, which is a very useful tool to download, convert, and modify datasets"
Learn to build an end to end data science project - KDnuggets,"So for this purpose, I have tweaked the web scraper to scrape 1000 job postings from glassdoor. With each job, we get the following: Job title, Salary Estimate, Job Description, Rating, Company, Location, Company Headquarters, Company Size, Company Founded Date, Type of Ownership, Industry, Sector, Revenue, Competitors. So these are the various attributes for determining the salary of a person working in the Data Science field"
Learn to build an end to end data science project - KDnuggets,"So now we can see that the number of rows has come down to 742. We observe that most of our variables are categorical and not numerical. This dataset comprises 2 numerical and 12 categorical variables. But in reality, our dependent variable, Salary Estimate, has to be numerical. So we need to convert that into a numerical variable"
Learn to build an end to end data science project - KDnuggets,"Data can be in any format. To analyze it, you need to have data in a certain format. Data scientists have to prepare data for modeling, which is one of the most crucial steps because the model has to be clean and should not contain any errors or null values"
Learn to build an end to end data science project - KDnuggets,"When we split on the left parenthesis, what happens is, the left and right sides of ‘(‘ of all the rows go into 2 different lists. That’s why we need to include [0] to get the salaries. After obtaining the salaries, replace ‘K’,’$’ with an empty string. In a few entries, the salary is given as ‘employer provided’ and ‘per hour’, so these are inconsistent and should be looked after"
Learn to build an end to end data science project - KDnuggets,Now we shouldn’t have employer provided or per hour in salaries. So we return 2 lists which contain the minimum and maximum salaries of each entry. This becomes our final dependent variable(to predict the average salary of a person)
Learn to build an end to end data science project - KDnuggets,"EDA plays a very important role at this stage as the summarization of clean data helps in identifying the structure, outliers, anomalies, and patterns in data. These insights could help us in building the model. However, I am going to discuss EDA in detail in a separate article, and you can find it in my medium profile"
Learn to build an end to end data science project - KDnuggets,"The data scientist has the chance to understand if his work is ready to go or if it needs review. Modeling focuses on developing models that are either descriptive or predictive. So here, we perform Predictive modeling, which is a process that uses data mining and probability to forecast outcomes. For predictive modeling, data scientists use a training set that is a set of historical data in which the outcomes are already known. This step can be repeated more times until the model understands the question and answer to it"
Learn to build an end to end data science project - KDnuggets,"If we have categorical data, then we need to create dummy variables, so that's why I transformed the categorical variables into dummy variables. I also split the data into train and test sets with a test size of 20%. I tried three different models and evaluated them using Mean Absolute Error. I chose MAE because it is relatively easy to interpret, and outliers aren’t particularly bad for this type of model"
Learn to build an end to end data science project - KDnuggets,"After plotting the graph and checking the value of alpha, we see that an alpha value of 0.13 gives the best error term. Now our error has reduced from 21.09 to 19.25 (which means 19.25K dollars). We can also improve the model tuning the GridSearch"
Learn to build an end to end data science project - KDnuggets,"So here we are getting a smaller value of error than the previous ones, so the Random Forest model is better than the previous models. I have combined the Random Forest model with the Linear Regression model to make a prediction. So I have taken the average of both, which means that I have given 50% weightage to each of the models"
Learn to build an end to end data science project - KDnuggets,"Most of the time, it’s better to combine different models and then make predictions because there are very good chances of increasing our accuracy. These types of models are called ensemble models, and they are widely used. The error may or may not increase because one model might be overtraining"
Learn to build an end to end data science project - KDnuggets,"The tuned Random Forest model is the best here because it has the least error when compared to Lasso and Linear regression. So instead of taking the average of both, we can even merge 90% of the random forest model with 10% of any other models and test the accuracy/performance. Generally, these types of ensemble models are better for classification problems"
Learn to build an end to end data science project - KDnuggets,"The project should not be about trying all the models, but it should be to choose the most effective models and should be able to tell a story as to why we have chosen those specific ones. Usually, Lasso regression should have more effect than linear regression as it has the normalization effect, and we have a sparse matrix, but here the Lasso performed worse than the linear regression. Hence it depends model to model, and we cannot generalize anything"
Learn to build an end to end data science project - KDnuggets,"Data scientists can evaluate the model in two ways: Hold-Out and Cross-Validation. In the Hold-Out method, the dataset is divided into three subsets: a training set, a validation set that is a subset that is used to assess the performance of the model built in the training phase, and a test set is a subset to test the likely future performance of a model. In most of the cases, the training:validation:test set ratios will be 3:1:1, which means 60% of the data to the training set, 20% of the data to the validation set, and 20% of the data to the test set"
Learn to build an end to end data science project - KDnuggets,"After deploying the model, I have made an attempt to predict the salary of a machine learning engineer where the company’s rating is 4, and the company was founded 39 years ago. So, according to my model, the Employee’s Expected salary is 117.31K dollars"
The Four Jobs of the Data Scientist - KDnuggets,"In case it’s not clear, these are not the same question. For example, in Statistics, based on the curriculum from most PhD programs, the core of the field involves statistical methods, statistical theory, probability, and maybe some computing. Data analysis is generally not formally taught (i. Many classes labeled “Data Science” or “Data Analysis” simply teach more methods like machine learning, clustering, or dimension reduction. Formal software engineering techniques are also not generally taught, but in practice, are often used"
The Four Jobs of the Data Scientist - KDnuggets,Whether that is correct or incorrect is not my point. I’m only saying that a distinction has to be made somewhere. Statisticians will always
The Four Jobs of the Data Scientist - KDnuggets,"With data science, I think we are collectively taking inventory of what data scientists tend to do. The problem is that at the moment, it seems to be all over the map. Traditional statistics does tend to be central to the activity, but so does computer science, software engineering, cognitive science, ethics, communication, etc. This is hardly a definition of the core of a field but rather an enumeration of activities"
The Four Jobs of the Data Scientist - KDnuggets,"While this iteration might be familiar or obvious to many, its familiarity masks the complexity involved. In particular, each step of the iteration requires that the data scientist play a different role involving very different skills. It’s like a one-person play where the data scientist has to change costumes when going from one step to the next. This is what I refer to as the"
The Four Jobs of the Data Scientist - KDnuggets,The scientist develops and asks the question and is responsible for knowing the state of the science and what the key gaps are. The scientist also designs any experiments for collecting new data and executes the data collection. The scientist must work with the statistician to design a system for analyzing the data and ultimately construct a
The Four Jobs of the Data Scientist - KDnuggets,"The scientist plays a key role in developing a system that results in our set of expected outcomes. Components of this system might include a literature review, meta-analysis, preliminary data, or anecdotal data from colleagues. I use the term “Scientist” broadly here. In other settings, this could be a policy-maker or product manager"
The Four Jobs of the Data Scientist - KDnuggets,"The statistician, in concert with the scientist, designs the analytic system that will analyze the data generated by any data collection efforts. They specify how the system will operate, what outputs it will generate, and obtain any resources needed to implement the system. The statistician draws on statistical theory and personal experience to choose the different components of the analytic system that will be applied"
The Four Jobs of the Data Scientist - KDnuggets,"The statistician’s role here is to apply the data analytic system to the data and to produce the data analytic output. This output could be a regression coefficient, a mean, a plot, or a prediction. But there must be something produced that we can compare to our set of expected outcomes. If the output deviates from our set of expected outcomes, then the next task is to identify the reasons for that deviation"
The Four Jobs of the Data Scientist - KDnuggets,"For software and data analysis alike, the challenge is that bugs or unexpected behavior can originate from anywhere. Any complex system is composed of multiple components, some of which may be your responsibility, and many of which are someone else’s. But bugs and anomalies do not respect those boundaries! There may be an issue that occurs in one component that only becomes known when you see the data analytic output"
The Four Jobs of the Data Scientist - KDnuggets,"The politician’s job is to make decisions while balancing the needs of the various constituents to achieve a reasonable outcome. Most statisticians and scientists that I know would recoil at the idea of being considered a politician or that politics in any form would play a role in doing any sort of science. However, my thinking here is a bit more basic: in any data analysis iteration, we are constantly making decisions about what to do, keeping in mind a variety of conflicting factors. In order to resolve these conflicts and come to a reasonable agreement, one has to engage a key skill, which is negotiation"
The Four Jobs of the Data Scientist - KDnuggets,"For example, an investigator might say, “We need a p-value less than 0.05. But what they"
The Four Jobs of the Data Scientist - KDnuggets,"Identifying positions versus underlying needs is a key task in negotiating a reasonable outcome for everyone involved. Rarely, in my experience, does this process have to do with the data (although data may be used to make certain arguments). The dominating elements of this process tend to be the nature of relationships between each constituent and the constraints on resources (such as time)"
The Four Jobs of the Data Scientist - KDnuggets,"In any given analysis, the iteration may be applied anywhere from once to dozens if not hundreds of times. If you’ve ever made two plots of the same dataset, then you’ve likely done two iterations. While the exact details and frequency of the iterations may vary widely across applications, the core nature and the skills involved do not change much"
Data Catalogs Are Dead; Long Live Data Discovery - KDnuggets,"If this hits home, you’re not alone. Many companies that need to solve this dependency jigsaw puzzle embark on a multi-year process to manually map out all their data assets. Some are able to dedicate resources to build short-term hacks or even in-house tools that allow them to search and explore their data. Even if it gets you to the end goal, this poses a heavy burden on the data organization, costing your data engineering team time and money that could have been spent on other things, like product development or actually using the data"
Data Catalogs Are Dead; Long Live Data Discovery - KDnuggets,"For instance, a data set pulled from Salesforce has a completely different meaning to a data engineer than it would to someone on the sales team. While the engineer would understand what “DW_7_V3” means, the sales team would be scratching their heads, trying to determine if said data set correlated to their “Revenue Forecasts 2021” dashboard in Salesforce. And the list goes on"
Data Catalogs Are Dead; Long Live Data Discovery - KDnuggets,"A modern data catalog needs to federate the meaning of data across these domains. Data teams need to be able to understand how these data domains relate to each other and what aspects of the aggregate view are important. They need a centralized way to answer these distributed questions as a whole — in other words, a distributed, federated data catalog"
Data Catalogs Are Dead; Long Live Data Discovery - KDnuggets,"Data teams should be able to easily leverage their data catalog without a dedicated support team. Self-service, automation, and workflow orchestration for your data tooling removes silos between stages of the data pipeline, and in the process, making it easier to understand and access data. Greater accessibility naturally leads to increased data adoption, reducing the load for your data engineering team"
Data Catalogs Are Dead; Long Live Data Discovery - KDnuggets,"As companies ingest more and more data and unstructured data becomes the norm, the ability to scale to meet these demands will be critical for the success of your data initiatives. Data discovery leverages machine learning to gain a bird’s eye view of your data assets as they scale, ensuring that your understanding adapts as your data evolves. This way, data consumers are set up to make more intelligent and informed decisions instead of relying on outdated documentation (aka data about data that becomes stale, how meta!) or worse — gut-based decision making"
Data Catalogs Are Dead; Long Live Data Discovery - KDnuggets,"The truth is — in one way or another — your team is probably already investing in data discovery. Whether it’s through manual work your team is doing to verify data, custom validation rules your engineers are writing, or simply the cost of decisions made based on broken data or silent errors that went unnoticed. Modern data teams have started leveraging automated approaches to ensuring highly trustworthy data at every stage of the pipeline, from data quality monitoring to more robust, end-to-end"
Pandas on Steroids: End to End Data Science in Python with Dask - KDnuggets,One common bottleneck theme is the enormity of data size where either the data doesn't fit into memory or the processing time is so large(In order of multi-mins) that the inherent pattern analysis goes for a toss. Data scientists by nature are curious human beings who want to identify and interpret patterns normally hidden from cursory Drag-N-Drop glance. They need to wear multiple hats and make the data confess via repeated tortures(read iterations 😂)
Pandas on Steroids: End to End Data Science in Python with Dask - KDnuggets,"To quench these curiosities, time is of the essence and its criminal to keep the data scientists waiting for 5+ minutes to read a csv file(55 Mn rows) or do a column add followed by aggregation. Also, since the majority of Data Scientists are self-taught and they have been so much used to pandas data frame API that they wouldn't want to start the learning process all over again with a different API like numba, Spark or datatable. I have tried juggling between DPLYR(R), Pandas(Python) and pyspark(Spark) and it is a bit unfulfilling/unproductive considering the need for a uniform pipeline and code syntax. However, for the curious folks, I have written a pyspark starter guide here:"
Pandas on Steroids: End to End Data Science in Python with Dask - KDnuggets,"What makes Dask so popular is the fact that it makes analytics scalable in Python and not necessarily need switching back and forth between SQL, Scala and Python.The magical feature is that this tool requires minimum code changes. It breaks down computation into pandas data frames and thus operates in parallel to enable fast calculations"
Pandas on Steroids: End to End Data Science in Python with Dask - KDnuggets,"Since it is a living blog, I will be writing subsequent parts in Dask series where we will be targeting Kaggle leaderboard using parallel processing. Let me know in comments if you are facing any issues in setting up Dask or unable to perform any Dask Operations or even for a general chit-chat. Happy Learning!!!"
Essential data science skills that no one talks about - KDnuggets,"Let’s pretend that you are a super-human possessing all the above abilities. You code from the age of five, you are a Kaggle grandmaster and your conference papers are guaranteed to get a best-paper award. And you know what? There is still a very high chance that your projects struggle to reach maturity and become full-fledged commercial products"
Essential data science skills that no one talks about - KDnuggets,Recent studies estimate that more than 85% of data science projects fail to reach production. The studies provide numerous reasons for the failures. And I have not seen the so-called
Essential data science skills that no one talks about - KDnuggets,"Am I saying that the above skills are not important? Of course, I’m not. Both hard and soft skills are vital. The point is that they are necessary, but not sufficient. Moreover, they are popular and appear on every google search. So the chance is that you already know if you need to improve your math proficiency or teamwork"
Essential data science skills that no one talks about - KDnuggets,"They are especially useful for building real products with real customers. Regretfully, engineering skills are seldom taught to data scientists. They come with experience. Most junior data scientists lack them"
Essential data science skills that no one talks about - KDnuggets,"Engineering skills have nothing to do with the area of data engineering. I use the term engineering skills to distinguish them from purely scientific or research skills. According to the Cambridge dictionary,"
Essential data science skills that no one talks about - KDnuggets,"In this paper, engineering is the enabler component that transforms science into products. Without proper engineering, the models will keep performing on predefined datasets. But they will never get to real customers"
Essential data science skills that no one talks about - KDnuggets,"I could have filled the whole page with citations devoted to simplicity. Researchers, designers, engineers, philosophers, and authors praised the simplicity and stated that simplicity has a value all of its own. Their reasons changed, but the conclusion was the same. You reach perfection not when there is nothing to add, but when there is nothing to remove"
Essential data science skills that no one talks about - KDnuggets,"Software engineers are absolutely aware of the value of simplicity. There are numerous books and articles on how to make software simpler. I remember that KISS principle — Keep It Simple, Stupid — was even taught at one of my undergraduate courses. Simple software is cheaper to maintain, easier to change, and less prone to bugs. There is a wide consensus on it"
Essential data science skills that no one talks about - KDnuggets,"The reason to seek simplicity in data science is the same reason as in all engineering disciplines. Simpler solutions are much, much cheaper. Real-life products are not Kaggle competitions. Requirements are constantly modified. A complex solution quickly becomes a maintenance nightmare when it needs to be adapted to new conditions"
Essential data science skills that no one talks about - KDnuggets,"It is easy to understand why data scientists, especially fresh graduates, prefer complex solutions. They have just arrived from the academy. They have finished the thesis and probably even published a paper. An academic publication is judged by accuracy, mathematical elegance, novelty, methodology, but seldom by practicality and simplicity"
Essential data science skills that no one talks about - KDnuggets,"A complicated idea that increases the accuracy by 0.5% is a great success for any student. The same idea is a failure for a data scientist. Even if its theory is sound, it may hide underlying assumptions that will prove as false. In any case, incremental improvement is hardly worth the cost of complexity"
Essential data science skills that no one talks about - KDnuggets,Wikipedia describes it as “blind trust in divine providence and counting on pure luck”. Avos’ was behind the decision of the truck’s driver to overload the truck. And it hides behind any non-robust solution
Essential data science skills that no one talks about - KDnuggets,Careful readers may say that robustness is also the ability of an algorithm to deal with errors during execution. They would be right. But it is less relevant to our discussion. It is a technical topic with well-defined solutions
Essential data science skills that no one talks about - KDnuggets,"The necessity to build robust systems was obvious in the pre-big-data and pre-deep world. Feature and algorithm design were manual. Testing was commonly performed on hundreds, maybe thousands of examples. Even the smartest algorithm creators never assumed that they could think of all possible use cases"
Essential data science skills that no one talks about - KDnuggets,"And if people are involved, the reality will always be unexpected and unimaginable. Most of us have difficulty telling what we will have for lunch, not to talk about tomorrow. Data can hardly help with predicting human behavior"
Essential data science skills that no one talks about - KDnuggets,"So what to do in order to make your models more robust? The first option is to read the appropriate papers and implement their ideas. This is fine. But the papers are not always generalizable. Often, you can’t copy an idea from one area to another"
Essential data science skills that no one talks about - KDnuggets,"Safety margins are the basis of any engineering. It is a common practice to take requirements and add 20–30% just to be on the safe side. An elevator that can hold 1000kg will easily hold 1300kg. Moreover, it is tested to hold 1300kg and not 1000kg. Engineers prepare for unexpected conditions"
Essential data science skills that no one talks about - KDnuggets,"The important consequence of this practice is that you will stop chasing incremental improvements. You cannot be robust if your model increases a KPI by 1%. With all the statistical significance tests, any small change in the environment will kill your effort"
Essential data science skills that no one talks about - KDnuggets,Forget the single test / train / validation division. You have to cross-validate your model over all possible combinations. Do you have different users? Divide according to the user ID and do it dozens of times. Does your data change over time? Divide according to timestamp and make sure that each day appears once in the validation group. And then test on dirty data
Essential data science skills that no one talks about - KDnuggets,Decrease the dependence on other untested components. And never build your model on top of another high-risk and not validated component. Even if the developers of that component swear that nothing can happen
Essential data science skills that no one talks about - KDnuggets,Modular design is an underlying principle of all modern science. It is the direct consequence of the analytical approach. The analytical approach is a process where you break down a big problem into smaller pieces. The analytical approach was a cornerstone of the scientific revolution
Essential data science skills that no one talks about - KDnuggets,"The smaller your problem is, the better. And “the better” here is not nice to have. It is a must. It will save a lot of time, effort, and money. When a problem is small, well defined, and not accompanied by tons of assumptions, the solution is accurate and easy to test"
Essential data science skills that no one talks about - KDnuggets,The failure is easy to justify. Modular design requires a method to combine several smaller models into a big one. There exists no such method for machine learning
Essential data science skills that no one talks about - KDnuggets,"There is a constant tension between product managers and data scientists. Product managers want data scientists to focus on low hanging fruits. Their logic is clear. They say that the business cares only about the number of fruits and about where they grow. The more fruits we have, the better we do. They throw in all sorts of buzzwords — Pareto, MVP, the best is the enemy of the good, etc"
Essential data science skills that no one talks about - KDnuggets,"On the other hand, data scientists state that the low hanging fruits spoil fast and taste badly. In other words, solving the easy problems has a limited impact and deals with symptoms and not the cause. Often, it’s an excuse to learn new technologies, but often they are right"
Essential data science skills that no one talks about - KDnuggets,"Recently, I developed my own approach that unifies the two extremes. The typical environment of a data scientist is a dynamic and weird world where trees grow in all directions. And the trees switch the directions all the time. They can grow upside down or sideways"
Essential data science skills that no one talks about - KDnuggets,"The best fruits are indeed at the top. But if we spend too much time building the ladder, the tree will move. Therefore the best is to aim at the top but to constantly monitor where the top is"
The Best Data Science Certification You’ve Never Heard Of - KDnuggets,"It’s possible to sit for the exam online while monitored via webcam ($11 proctoring fee). The format of the exam is multiple choice — choose the single correct option out of five. You can mark questions and come back to them. At the conclusion of test taking, you get immediate feedback on your score"
The Best Data Science Certification You’ve Never Heard Of - KDnuggets,"This is just fine if you’re interested in getting your CDMP Associate certification and moving along. If you’re interested in the advanced tiers of CDMP certification, you’ll have to pass with a 70% (CDMP Practitioner) or 80% (CDMP Master). To get certified at the highest level, CDMP Fellow, you’ll need to attain the Master Certification and also demonstrate industry experience and contribution to the field. Each of these"
"Top Python Libraries for Data Science, Data Visualization & Machine Learning - KDnuggets","This time, however, we have split the collected on open source Python data science libraries in two. This first post (this) covers ""data science, data visualization & machine learning,"" and can be thought of as ""traditional"" data science tools covering common tasks. The second post, to be published next week, will cover libraries for use in building neural networks, and those for performing natural language processing and computer vision tasks"
"Top Python Libraries for Data Science, Data Visualization & Machine Learning - KDnuggets","Our list is made up of libraries that our team decided together by consensus was representative of common and well-used Python libraries. Also, to be included a library must have a Github repository. The categories are in no particular order, and neither are the libraries included within each. We contemplated constructing an ordering arbitrarily by stars or some other metric, but decided against it in order not explicitly stray from placing any perceived value or importance of the libraries within. Their listing here, then, is purely random. Library descriptions are directly from the Github repositories, in some form or another"
"Top Python Libraries for Data Science, Data Visualization & Machine Learning - KDnuggets",VisPy is a high-performance interactive 2D/3D data visualization library. VisPy leverages the computational power of modern Graphics Processing Units (GPUs) through the OpenGL library to display very large datasets. Applications of VisPy include:
10 Principles of Practical Statistical Reasoning - KDnuggets,"After 3 years working on my own data science projects and 3.5 years manipulating data on the trading floor, there is an additional category of learnings. It is fundamentally just as useful as the above and I take them into"
10 Principles of Practical Statistical Reasoning - KDnuggets,"If you have experience of the application of statistical methods, I encourage you to use your experience to illuminate and criticise the following principles. If you have never tried implementing a statistical model, have a go and then return. Don’t see the following as a list to memorise. You’ll get peak synthesis of information if you can relate to your own experience"
10 Principles of Practical Statistical Reasoning - KDnuggets,"Try to consider depth independently to the amount of data available or the technologies available. Just because it is easy/cheap to collect data, doesn’t mean the data are relevant. Same applies to methodologies and technologies. Well-chosen analysis depth supports clear conclusions, and clear conclusions support better decision-making"
10 Principles of Practical Statistical Reasoning - KDnuggets,Practical statistical reasoning addresses the ‘System’. Some parts of the system cannot be determined out of context. Some parts can. Practical statistical reasoning is really just the ability to define your ‘System’ easily and competently. That ability is definitely not limited to these principles
The top courses for aspiring data scientists - KDnuggets,"Here are four courses that can give you the necessary skills to lead businesses in the 21st century. All of them include Python programming as a course component. Most of them require an undergraduate knowledge of statistics, calculus, linear algebra, and probability, so we recommend checking your course of interest for the specifics"
The top courses for aspiring data scientists - KDnuggets,"Here are four courses that can give you the necessary skills to lead businesses in the 21st century. All of them include Python programming as a course component. Most of them require an undergraduate knowledge of statistics, calculus, linear algebra, and probability, so we recommend checking your course of interest for the specifics"
Stop Running Jupyter Notebooks From Your Command Line - KDnuggets,"Jupyter Notebook provides a great platform to produce human-readable documents containing code, equations, analysis, and their descriptions. Some even consider it a powerful development when combining it with NBDev. For such an integral tool, the out of the box start up is not the best. Each use requires starting the Jupyter web application from the command line and entering your token or password. The entire web application relies on that terminal window being open. Some might “daemonize” the process and then use"
Stop Running Jupyter Notebooks From Your Command Line - KDnuggets,"JupyterHub brings the power of notebooks to groups of users. The idea behind JupyterHub was to scale out the use of Jupyter Notebooks to enterprises, classrooms, and large groups of users. Jupyter Notebook, however, is supposed to run as a local instance, on a single node, by a single developer. Unfortunately, there was no middle ground to have the usability and scalability of JupyterHub and the simplicity of running a local Jupyter Notebook. That is, until now"
Stop Running Jupyter Notebooks From Your Command Line - KDnuggets,The architecture of our JupyterHub server will consist of 2 services: JupyterHub and JupyterLab. JupyterHub will be the entry point and will spawn JupyterLab instances for any user. Each of these services will exist as a Docker container on the host
Stop Running Jupyter Notebooks From Your Command Line - KDnuggets,"That leaves us with the last task of authenticating to the server. Since we did not set up a LDAP server or OAuth, JupyterHub will use PAM (Pluggable Authentication Module) authentication to authenticate users. This means JupyterHub uses the user name and passwords of the host machine to authenticate"
Stop Running Jupyter Notebooks From Your Command Line - KDnuggets,"To make use of this, we will have to create a user on the JupyterHub Docker container. There are other ways of doing this such as having a script placed on the container and executed at container start up but we will do it manually as an exercise. If you tear down or rebuild the container you will have to recreate users"
Getting A Data Science Job is Harder Than Ever – How to turn that to your advantage - KDnuggets,"Data Science is among the fastest growing and emerging technologies on the planet, and tons of people are flocking to update their skills to have a shot a making a career as a Data Scientist. And, just in-case you don’t believe me, over 3.5 million people have enrolled in Andrew Ng Machine Learning course (which is an important part of Data Science) on Coursera since its inception"
Getting A Data Science Job is Harder Than Ever – How to turn that to your advantage - KDnuggets,"The fact that it’s difficult to get a job in Data Science should never be the reason you don’t have one. There are many challenges you’ll face at any job in itself, and getting the job is just the qualification phase to see if the employers believe you are capable of facing the challenges and whether you believe the employers are whom you would like to be on your team. Always seek to improve yourself, don’t wait to be ready to apply because you may never feel ready, and don’t be afraid to be rejected or to reject companies that don’t align with where you are going"
The unspoken difference between junior and senior data scientists - KDnuggets,"Being a senior data scientists is treated as a holy grail, though many do not know what it really means to hold a senior position. The most common impression is that being a senior data scientist means that you know everything there is to know about data science and you are truly an expert in it. Which is true, but only to a certain extent because learning in data science never ends. Moreover, there is so much more to being a senior data scientist than just technical knowledge"
The unspoken difference between junior and senior data scientists - KDnuggets,"As a junior data scientist what is expected of you is to have fundamental data science knowledge. Your capability should be enough to conduct your tasks alone or with help from your more senior colleagues. At this point in time, you will not have much professional hands-on experience"
The unspoken difference between junior and senior data scientists - KDnuggets,You should be open to learning and not be afraid of asking a lot of questions. More senior colleagues will be happy to help you along with your learning. It would not be surprising if you learned something new every day as a junior data scientist
The unspoken difference between junior and senior data scientists - KDnuggets,"Your main responsibility will be the tasks that are assigned to you. You will get assistance from more senior data scientists when you encounter problems. Apart from your technical capabilities, you will be expected to have a good understanding of the parts of the domain that is related to your specific tasks"
The unspoken difference between junior and senior data scientists - KDnuggets,"At this point, your knowledge of the main concepts and techniques of data science must be solid. Though it doesn’t mean that you already know everything. Rather, it means that you know many things and you also know what you don’t know. You probably have already gained some good practical experience at this level"
The unspoken difference between junior and senior data scientists - KDnuggets,"Learning never ends so you’re still open to new ideas and approaches. You still ask a lot of questions but you also get asked questions from others. Junior colleagues come to you with their questions. You still learn new things, maybe not every day but every other month. You try to go deeper in our understanding of certain techniques and tools"
The unspoken difference between junior and senior data scientists - KDnuggets,And then comes the senior data scientist position. You are basically everything a data scientist is at this point with some extra capabilities and responsibilities. Let’s see what they are
The unspoken difference between junior and senior data scientists - KDnuggets,You have a solid understanding of main concepts and techniques and also a deeper knowledge of their pitfalls. You gained this knowledge while working on projects. Now you have a solid level of practical experience under your belt
The unspoken difference between junior and senior data scientists - KDnuggets,It is easier for you to learn more advanced topics because you’ve already mastered fundamental concepts. You are still open to learning. Teaching and supporting more junior colleagues is part of your job
The unspoken difference between junior and senior data scientists - KDnuggets,"You are the leader in projects. You are not only part of the decision-making process but you lead it. The success of the project is your responsibility, as well as, in many cases, the happiness of your team members. While leading the project you also need to communicate with the outside world. It is your responsibility to report to the business side. You need to keep non-technical constraints in mind while working on the project and make sure to nudge the technical team in the right direction. You have to have an overall and complete understanding of the context and the domain. It is your responsibility to stay on target and deliver"
The unspoken difference between junior and senior data scientists - KDnuggets,"Of course, this is not how every single data scientist’s career goes in every company in the world. Also, you might be a freelance data scientist or you might start your company and become a CTO, then your path would look very different. But in general, from what I’ve learned talking to people in the data science community, this is a good representation of a general data scientist career path"
The unspoken difference between junior and senior data scientists - KDnuggets,"The reason we looked into this today is that every company has its own structure, own rules and own pathways and you want to know which one to choose when you get the option. Some will tend towards more technical work as you get more senior and some towards more managerial and administrative work. You can use the explanation in this article as a baseline to figure out where you want to be at the senior part of your journey and calibrate your job search accordingly. Of course, plans and preferences change in time. But having an idea of where you want to end up is better than going into it blindly"
The unspoken difference between junior and senior data scientists - KDnuggets,"Apart from my data science work, I like teaching people what I know. I always received lots of questions on LinkedIn about my career and data science in general from people who wanted to end up where I am. That's why I started"
How Automation Is Improving the Role of Data Scientists - KDnuggets,Many people wonder if automation will eventually replace people who work as data scientists. The much more likely outcome — and one that's already happening — is that data automation will enhance how scientists spend their time and improve the results they get. Here are five ways it can help
How Automation Is Improving the Role of Data Scientists - KDnuggets,"Getting insights from data is an aspect decision-makers focus on most often. However, the less-exciting but all-important tasks, like compiling, cleaning and formatting the information, can occupy more space in a project timeline than people realize at first. Investing in automation can make a data science team more productive and agile"
How Automation Is Improving the Role of Data Scientists - KDnuggets,"In one example, the leadership at a bank that utilized data science discovered it took longer than desired to get crucial insights. They supplemented the work with automation. Before making that change, the company completed one to two projects every three months. Adding automation"
How Automation Is Improving the Role of Data Scientists - KDnuggets,The smartest technological tools cannot substitute for people's intelligence and experience. They also may not detect errors that could cause unreliable results. Data science automation excels with repetitive tasks that do not require human knowledge. That approach frees people to use their skills in personally rewarding ways that also benefit their employers
How Automation Is Improving the Role of Data Scientists - KDnuggets,"1%. The analysts cited cloud computing as a significant driver of the increase. For example, if a data scientist uses an automation-as-a-service tool to cut down on manual tasks, they could likely do it via the cloud and get work done anywhere"
How Automation Is Improving the Role of Data Scientists - KDnuggets,"Some people are tempted to let automated tools do as much as possible, but that approach often causes errors. Thus, some experts advocate for so-called augmented intelligence. It combines artificial intelligence (AI) with human knowledge"
How Automation Is Improving the Role of Data Scientists - KDnuggets,"The algorithms achieved an average accuracy rate of 90%, but it fell to 60% in some categories. The business compensated by implementing human expertise for the groupings with low confidence scores. This approach increased accuracy and led to trusted results"
Data Science in the Cloud with Dask - KDnuggets,Scaling large data analyses for data science and machine learning is growing in importance. Dask and Coiled are making it easy and fast for folks to do just that. Read on to find out how
Data Science in the Cloud with Dask - KDnuggets,"Note: Before you get started, it’s important to think about if scaling your computation is actually necessary. Consider making your pandas code more efficient before you jump in. With machine learning, you can measure if more data will result in model improvement by plotting learning curves before you begin"
Data Science in the Cloud with Dask - KDnuggets,"And now what we’ve been waiting for - it’s time to burst to the cloud. If you had access to cloud resources (like AWS) and knew how to configure Docker and Kubernetes containers, you could get a Dask cluster launched in the cloud. This would be time consuming, however"
Data Science in the Cloud with Dask - KDnuggets,"It’s easy to see the power of being able to do this set of analyses in a single workflow. We didn’t need to switch contexts or environments. Plus, it is straightforward to go back to using Dask from Coiled on my local workstation or pandas when we’re done. Cloud computing is great when needed, but can be a burden when it’s not. We just had an experience that was a lot less burdensome"
Data Science in the Cloud with Dask - KDnuggets,"You can get started on a Coiled cluster for free right now. Coiled also handles security, conda/docker environments, and team management, so you can get back to doing data science and focus on your job. Get started today on"
Data Science in the Cloud with Dask - KDnuggets,"He has extensive experience as a data scientist, educator, evangelist, content marketer, and data strategy consultant, in industry and basic research. He also has experience teaching data science at institutions such as Yale University and Cold Spring Harbor Laboratory, conferences such as SciPy, PyCon, and ODSC and with organizations such as Data Carpentry. He is committed to spreading data skills, access to data science tooling, and open source software, both for individuals and the enterprise"
How to ace the data science coding challenge - KDnuggets,"Data science coding projects vary in scope and complexity. Sometimes, the project could be as simple as producing summary statistics, charts, and visualizations. It could also involve building a regression model, classification model, or forecasting using a time-dependent dataset. The project could also be very complex and difficult. In this case, no clear guidance is provided as to the specific type of model to use. In this case, you’ll have to come up with your own model that is best suitable for addressing project goals and objectives"
How to ace the data science coding challenge - KDnuggets,"Generally, the interview team will provide you with project directions and a dataset. If you are fortunate, they may provide a small dataset that is clean and stored in a comma-separated value (CSV) file format. That way, you don’t have to worry about mining the data and transforming it into a form suitable for analysis. For the couple of interviews I had, I worked with 2 types of datasets: one had 160 observations (rows), while the other had 50,000 observations with lots of missing values. The take-home coding exercise clearly differs from companies to companies, as further described below"
How to ace the data science coding challenge - KDnuggets,"This is an example of a very straightforward problem. The dataset is clean and small (160 rows and 9 columns), and the instructions are very clear. So, all that is needed is to follow the instructions and generate your code. Notice also that the instruction clearly specifies that python must be used as the programming language for model building. The time allowed for completing this coding assignment was three days. Only the final Jupyter notebook has to be submitted, and no formal project report is required"
How to ace the data science coding challenge - KDnuggets,"For example, the passenger’s column doesn’t tell if this column is in hundreds or thousands. The units for cabin length, passenger density, and crew are not provided as well. The"
How to ace the data science coding challenge - KDnuggets,These kinds of issues can be addressed by contacting the interview team to ask more about the dataset. It is important to understand the intricacies of your data before using it for building real-world models. Keep in mind that a bad dataset leads to bad predictive models
How to ace the data science coding challenge - KDnuggets,"Since the covariance matrix shows multi-collinearity, it is important to transform features into PCA space before training your model. This is important because multi-collinearity between features can lead to a model that is complex and difficult to interpret. PCA can also be used for variable selection and dimensionality reduction. In this case, only components that contribute significantly to the total explained variance can be retained and used for modeled building"
How to ace the data science coding challenge - KDnuggets,"The dataset has to be divided into training, validation, and test sets. Hyperparameter tuning has to be used to fine-tune the model in order to prevent overfitting. Cross-validation is essential to ensure the model performs well on the validation set. After fine-tuning model parameters, the model is applied has to be applied to the test dataset. The model’s performance on the test dataset is approximately equal to what would be expected when the model is used for making predictions using unseen data"
How to ace the data science coding challenge - KDnuggets,"In this stage, the final machine learning model is selected and put into production. The model is evaluated in a production setting in order to assess its performance. Any mistakes encountered when transforming from an experimental model to its actual performance on the production line has to be analyzed. This can then be used in fine-tuning the original model"
How to ace the data science coding challenge - KDnuggets,"Sometimes the coding exercise would ask you to submit a Jupyter notebook only, or it may ask for a full project report. Make sure your Jupyter notebook is well organized to reflect every stage of the machine learning process. A sample Jupyter notebook can be found here"
How to ace the data science coding challenge - KDnuggets,"The dataset here is complex (has 50,000 rows and 2 columns, and lots of missing values), and the problem is not very straightforward. You have to examine the dataset critically and then decide what model to use. This problem was to be solved in a week. It also specifies that a formal project report and an R script or Jupyter notebook file be submitted"
How to ace the data science coding challenge - KDnuggets,"As in Sample 1 coding exercise, you need to follow the machine learning steps when tackling this problem. This particle problem does not have a unique solution. I attempted a solution using probabilistic modeling based on Monte-Carlo simulation"
How to ace the data science coding challenge - KDnuggets,The solutions presented above are recommended solutions only. Keep in mind that the solution to a data science or machine learning project is not unique. I challenge you to solve these problems before reviewing the sample solutions
How to ace the data science coding challenge - KDnuggets,"In summary, we’ve discussed some useful tips that could be beneficial for any data science aspirant currently applying for data science openings. The coding exercise varies in scope and complexity, depending on the company you are applying to. The take-home coding exercise provides an excellent opportunity for you to showcase your ability to work on a data science project. You need to use this opportunity to demonstrate exceptional abilities in your understanding of data science and machine learning concepts. Don’t let this wonderful opportunity slip away. If there are certain aspects of the project that you don’t understand, feel free to reach out to the data science interview team if you have questions. They may provide some hints or clues"
Free From MIT: Intro to Computational Thinking and Data Science - KDnuggets,"Programming is an important part of data science, as are the underlying concepts of computers science. If we plan to implement computational solutions to data science problems, it is clear that programming is an absolute necessity. To facilitate those looking to establish or solidify these skills, we recently shared"
Free From MIT: Intro to Computational Thinking and Data Science - KDnuggets,"It aims to provide students with an understanding of the role computation can play in solving problems and to help students, regardless of their major, feel justifiably confident of their ability to write small programs that allow them to accomplish useful goals. The class uses the Python 3.5 programming language"
Free From MIT: Intro to Computational Thinking and Data Science - KDnuggets,"This structure gives students the opportunity to learn these distinct concepts without confusing them. Thinking computationally has nothing to do with machine learning; it facilitates the separation of a problem into smaller problems and allows one to think about the most efficient ways to solve these smaller problems. It's a great skill to develop in any aspect of your life or work. However — though not intrinsically linked to machine learning — it does provide practitioners with the requisite insights to understand the inner workings of machine learning algorithms, the solutions to problems using these algorithms, and how to iterate and improve on these solutions to make them more efficient, accurate, and useful"
Goodhart’s Law for Data Science and what happens when a measure becomes a target? - KDnuggets,"One of the stipulations of the law required that schools make adequate yearly progress (AYP) on standardized assessments year over year (i. If schools were continuously unable to meet AYP requirements, there were drastic consequences, including school restructuring and school closure. As such, many district administrators developed internal policies requiring that teachers increase their students’ test scores, using these scores as a metric for teacher quality. Eventually, with their jobs on the line, teachers began to “"
Goodhart’s Law for Data Science and what happens when a measure becomes a target? - KDnuggets,"Another commonly cited example is a call center manager setting a target to increase the number of calls taken at the center each day. Eventually, call center employees increase their numbers at the cost of actual customer satisfaction. In observing employees’ conversations, the manager notices that some employees are rushing to end the call without ensuring that the customer is fully satisfied. This example, as well as the accountability measures of No Child Left Behind, stresses one of the most important elements of Goodhart’s Law —"
Goodhart’s Law for Data Science and what happens when a measure becomes a target? - KDnuggets,"The findings from the analysis were suggestive that Russia, in some way, gamed YouTube’s algorithm to propagate false information on the internet. The problem is further exacerbated by the platform’s reliance on viewership as a metric for user satisfaction. This created the unintended consequence of"
Goodhart’s Law for Data Science and what happens when a measure becomes a target? - KDnuggets,"In this vein, it’s important to think critically about how to effectively measure and achieve desired outcomes in a way that minimizes unintended consequences. A large part of this is not relying too heavily on a single metric. Rather, understanding how a"
How I Levelled Up My Data Science Skills In 8 Months - KDnuggets,"Even though I hadn’t been doing much work with data at work, the thought of not being able to do any meaningful work with data bothered me. Nonetheless, I felt like my options regarding what I could possibly do next were limited since I did not get much practical experience at work. Don’t misunderstand me, I had been doing work as an intern, but I hadn’t done anything to significantly (or even marginally) improve the business (at least in my eyes) in my time. I was in a very low place, lacking self-belief, doubting my skills… For me, the furlough couldn’t come sooner"
How I Levelled Up My Data Science Skills In 8 Months - KDnuggets,"When you make a commitment to do something, a force from within drives you. I wake up every day thinking I must be better today than I was yesterday and that is what drives me. However, for this post, I am going to share the 3 things I did during my furlough period to ensure that I move closer to my goal"
How I Levelled Up My Data Science Skills In 8 Months - KDnuggets,"Whenever I looked on Kaggle to see what solutions people were using, I’d always see some form of Boosting, Bagging or Deep Learning. Boosting and Bagging, I had a good understanding of, but Deep Learning was a no go zone for me. It was when I realized this, I decided to enroll in the"
How I Levelled Up My Data Science Skills In 8 Months - KDnuggets,"Constantly learning is imperative as a Data Scientist. We all know how fast technology is moving, so to remain sharp we must sharpen our axe. However, when you learn a new topic with the intention to regurgitate that information to someone else, although I have done no research about this, I find I absorb the information differently — I think deeper about what I am learning and try to picture it in my mind which all contributes to making learning a seamless process"
How I Levelled Up My Data Science Skills In 8 Months - KDnuggets,"A strong network is a great testing ground for ideas. I have conducted many polls on my LinkedIn which provides me with instant feedback. Additionally, you can get personal referrals. As things stand, I’ve never had to apply for a job because I’ve always known the power of word of mouth and I have used it to my advantage on numerous occassions — regardless of the field"
How I Levelled Up My Data Science Skills In 8 Months - KDnuggets,"If you ask the people I grew up with what Artificial Intelligence is they may respond with something from Black Mirror. Having no friends in the field can be quite lonely because there are definitely times when you will feel tired, unmotivated, and your non-Data Science friends probably would not be able to understand you. Networking with other Data Scientist will allow you to realize that you aren’t the only one in the world facing a certain challenge(s) and it is certainly what lifted me back up when I felt down"
How I Levelled Up My Data Science Skills In 8 Months - KDnuggets,"A key thing to note is that I already had lots of exposure to the field which is what has allowed me to progress as I have; I’d say the most important thing I have done to change the trajectory of my career was to commit. Commitment is a long-term decision and bettering yourself daily is in only your hands. Taking responsibility for where your career is, is the beginning of developing yourself. Although I am nowhere near where I would like my Data Science career to be, I am closer than I was yesterday and much closer than I was 8 months ago"
Software Engineering Tips and Best Practices for Data Science - KDnuggets,"If you’re into data science, then you’re probably familiar with this workflow: you start a project by firing up a jupyter notebook, then begin writing your python code, running complex analyses, or even training a model. As the notebook file grows in size with all the functions, the classes, the plots, and the logs, you find yourself with an enormous blob of monolithic code sitting up in one place in front of you. If you’re lucky, things can go well. Good for you then!"
Software Engineering Tips and Best Practices for Data Science - KDnuggets,"Ok, folks, enough bashing for now. I honestly love jupyter, and I think it’s great for what’s designed to do. You can definitely use it to bootstrap small projects or quickly prototype ideas"
Software Engineering Tips and Best Practices for Data Science - KDnuggets,"For example, you’re working on an NLP project, and you may have different processing functions to handle text data (tokenizing, stripping URLs, lemmatizing, etc. You can put all these units in a python module called text_processing. Your main program will be way lighter!"
Software Engineering Tips and Best Practices for Data Science - KDnuggets,"Refactoring aims at reorganizing the internal structure of the code without altering its functionalities. It’s usually done on a working (but still not fully organized) version of the code. It helps de-duplicate functions, reorganize the file structure, and add more abstraction"
How to Future-Proof Your Data Science Project - KDnuggets,"Of course, sometimes going with a Neural Network may be your best option. Perhaps you’re doing image recognition or natural language processing (NLP). Perhaps you’re working with a very complicated dataset. If you’re using a Neural Net, you should consider how to pare back the model before"
How to Future-Proof Your Data Science Project - KDnuggets,"Data drift is typically the result of changes in the data collection process. For example, a sensor at a manufacturing plant could break, recording several hours of zero temperatures before the problem can be corrected. Then the new may sensor may record temperatures in celsius, rather than the previous measurement in Fahrenheit. Without context on these changes, the zero values and switch to a new standard of measurement will have an adverse effect on the downstream model"
How to Future-Proof Your Data Science Project - KDnuggets,The same can be said for changes to qualitative information. Survey data collection methodology — e. Even slight changes to the way a question is worded will adversely impact a model’s capability to draw longitudinal inferences from the dataset
How to Future-Proof Your Data Science Project - KDnuggets,"Data drift could also result from changes to the definitions of the fields in the dataset. For example, the data owner at the manufacturing plant could decide that the term “scrap” should refer not just to unusable material, but also material that will eventually reprocessed into recycled products. This change in terminology will also impact model performance"
How to Future-Proof Your Data Science Project - KDnuggets,"That will only be possible for a consumer company that has a lot of observations. Generally, business-to-business companies have a small data challenge. So there’s so many competing concerns with refitting your model"
Moving from Data Science to Machine Learning Engineering - KDnuggets,"Wildlife Protection Solutions is a small nonprofit that uses technology to protect endangered species. Recently, they upgraded their video monitoring system to incorporate an object detection model trained to recognize poachers. The model has already"
Moving from Data Science to Machine Learning Engineering - KDnuggets,"OpenAI’s GPT-2 was, at the time of its release, the most powerful text generating model in history. At an insane 1.5 billion parameters, it represented a big step forward in transformer models"
Moving from Data Science to Machine Learning Engineering - KDnuggets,"All of these platforms stand on the shoulders of data science. They wouldn’t work if they couldn’t train a model for their tasks. But, in order to apply these models to real world problems, they need to be engineered into applications"
Moving from Data Science to Machine Learning Engineering - KDnuggets,"Let’s go back to AI Dungeon, the ML-powered dungeon crawler. The game’s architecture is simple. Players input some text, the game makes a call to the model, the model generates a response, and the game displays it. The obvious way to build this is to deploy the model as a microservice"
Moving from Data Science to Machine Learning Engineering - KDnuggets,"For example, look at AI Dungeon. If they were building a normal API—one that didn’t involve GPT-2—they would use something like Lambda to spin up their API in 15 minutes. Because of the ML-specific challenges of serving GPT-2, however, orchestration tools from software engineering won’t work"
Moving from Data Science to Machine Learning Engineering - KDnuggets,"Reproducibility, for example, isn’t only a challenge in machine learning. It’s a problem in software engineering too—but we use version control to solve it. And while traditional version control software like Git doesn’t work for machine learning, you can still apply the principles. DVC (Data Version Control), which applies Git-like version control to training data, code, and their resulting models, does just this:"
Moving from Data Science to Machine Learning Engineering - KDnuggets,"In the future, machine learning is going to become a part of every engineer’s stack. There will hardly be a problem ML doesn’t touch. The pace at which this occurs is entirely dependent on how quickly we can develop platforms like Cortex, and accelerate the proliferation of machine learning engineering"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Most machine learning models are built with a data set having several features or predictors. Hence, familiarity with multivariable calculus is extremely important for building a machine learning model. Here are the topics you need to be familiar with:"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Linear algebra is the most important math skill in machine learning. A data set is represented as a matrix. Linear algebra is used in data preprocessing, data transformation, and model evaluation. Here are the topics you need to be familiar with:"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Programming skills are essential in data science. Since Python and R are considered the two most popular programming languages in data science, essential knowledge in both languages are crucial. Some organizations may only require skills in either R or Python, not both"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Data is key for any analysis in data science, be it inferential analysis, predictive analysis, or prescriptive analysis. The predictive power of a model depends on the quality of the data that was used in building the model. Data comes in different forms, such as text, table, image, voice, or video. Most often, data that is used for analysis has to be mined, processed, and transformed to render it to a form suitable for further analysis"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Very rarely is data easily accessible in a data science project for analysis. It’s more likely for the data to be in a file, a database, or extracted from documents such as web pages, tweets, or PDFs. Knowing how to wrangle and clean data will enable you to derive critical insights from your data that would otherwise be hidden"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Machine Learning is a very important branch of data science. It is important to understand the machine learning framework: Problem Framing, Data Analysis, Model Building, Testing & Evaluation, and Model Application. Find out more about the machine learning framework from here:"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Skills acquired from course work alone will not make you a data scientist. A qualified data scientist must be able to demonstrate evidence of successful completion of a real-world data science project that includes every stage in data science and machine learning process such as problem framing, data acquisition and analysis, model building, model testing, model evaluation, and deploying models. Real-world data science projects could be found in the following:"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Data scientists need to be able to communicate their ideas with other members of the team or with business administrators in their organizations. Good communication skills would play a key role here to be able to convey and present very technical information to people with little or no understanding of technical concepts in data science. Good communication skills will help foster an atmosphere of unity and togetherness with other team members such as data analysts, data engineers, field engineers, etc"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Data science is a field that is ever-evolving, so be prepared to embrace and learn new technologies. One way to keep in touch with developments in the field is to network with other data scientists. Some platforms that promote networking are LinkedIn, GitHub, and Medium ("
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"As a data scientist, you will be working in a team of data analysts, engineers, administrators, so you need good communication skills. You need to be a good listener, too, especially during early project development phases where you need to rely on engineers or other personnel to be able to design and frame a good data science project. Being a good team player will help you to thrive in a business environment and maintain good relationships with other members of your team as well as administrators or directors of your organization"
Data Science Minimum: 10 Essential Skills You Need to Know to Start Doing Data Science - KDnuggets,"Understand the implication of your project. Be truthful to yourself. Avoid manipulating data or using a method that will intentionally produce bias in results. Be ethical in all phases, from data collection and analysis to model building, analysis, testing, and application. Avoid fabricating results for the purpose of misleading or manipulating your audience. Be ethical in the way you interpret the findings from your data science project"
The Best Free Data Science eBooks: 2020 Update - KDnuggets,"Description: A complete foundation for Statistics, also serving as a foundation for Data Science. OpenIntro Statistics offers a traditional introduction to statistics at the college level. This textbook is widely used at the college level and offers an exceptional and accessible introduction for students from community colleges to the Ivy League"
The Best Free Data Science eBooks: 2020 Update - KDnuggets,"Description: Bayesian Methods for Hackers is designed as an introduction to Bayesian inference from a computational/understanding-first, and mathematics-second, point of view. Of course, as an introductory book, we can only leave it at that: an introductory book. For the mathematically trained, they may cure the curiosity this text generates with other texts designed with mathematical analysis in mind. For the enthusiast with a less mathematical background or one who is not interested in mathematics but simply the practice of Bayesian methods, this text should be sufficient and entertaining"
The Best Free Data Science eBooks: 2020 Update - KDnuggets,"Description: This book is aimed at the data scientist with some familiarity with the R programming language and with some prior (perhaps spotty or ephemeral)exposure to statistics. Both of us came to the world of data science from the world of statistics, so we have some appreciation of the contribution that statistics can make to the art of data science. At the same time, we are well aware of the limitations of traditional statistics instruction: statistics as a discipline is a century and a half old, and most statistics textbooks and courses are laden with the momentum and inertia of an ocean liner"
The Best Free Data Science eBooks: 2020 Update - KDnuggets,Description: This book teaches you to use R to effectively visualize and explore complex datasets. Exploratory data analysis is a key part of the data science process because it allows you to sharpen your question and refine your modeling strategies. This book is based on the industry-leading Johns Hopkins Data Science Specialization
The Best Free Data Science eBooks: 2020 Update - KDnuggets,Description: Learn how to program with Python 3 from beginning to end. Python 101 starts off with the fundamentals of Python and then builds onto what you’ve learned from there. The audience of this book is primarily people who have programmed in the past but want to learn Python. This book covers a fair amount of intermediate level material in addition to the beginner material
The Best Free Data Science eBooks: 2020 Update - KDnuggets,"Description: This book is a practical introduction to NLP. You will learn by example, write real programs, and grasp the value of being able to test an idea through implementation. If you haven’t learned already, this book will teach you programming. Unlike other programming books, we provide extensive illustrations and exercises from NLP. The approach we have taken is also principled, in that we cover the theoretical underpinnings and don’t shy away from careful linguistic and computational analysis. We have tried to be pragmatic in striking a balance between theory and application, identifying the connections and the tensions. Finally, we recognize that you won’t get through this unless it is also pleasurable, so we have tried to include many applications and examples that are interesting and entertaining, sometimes whimsical"
The Best Free Data Science eBooks: 2020 Update - KDnuggets,"Description: This book focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of their examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to “train” a machine-learning engine of some sort"
5 Reasons Why Containers Will Rule Data Science - KDnuggets,"Enter containers. Historically, containers were a way to abstract a software stack away from the operating system. For data scientists, containers have historically offered few benefits"
5 Reasons Why Containers Will Rule Data Science - KDnuggets,"Imagine being able to distribute an “Amazon Machine Image”-like environment to all of your data science team’s machines easily. That is, no more inconsistency of versions, pip installs, firewall issues. Containers make this possible"
5 Reasons Why Containers Will Rule Data Science - KDnuggets,"Nearly all companies provide Virtual Machines to their teams of data scientists to accomplish sandbox or production data science jobs. Over time, there is a proliferation of machines in an organization with projects that need to be migrated. Without a strategy for migrating projects, data science jobs break or there is an explosion of nearly worthless VM’s"
5 Reasons Why Containers Will Rule Data Science - KDnuggets,"Kubernetes is all the rage. At the core of this orchestration system are containerized applications. Kubernetes deploys and manages the underlying containers, however, the project must be containerized first"
"Understanding Transformers, the Data Science Way - KDnuggets","Transformers have become the defacto standard for any NLP tasks nowadays. Not only that, but they are now also being used in Computer Vision and to generate music. I am sure you would all have heard about the GPT3 Transformer and its applications thereof"
"Understanding Transformers, the Data Science Way - KDnuggets","So, I thought of putting the whole idea down in as simple words as possible along with some very basic Math and some puns as I am a proponent of having some fun while learning. I will try to keep both the jargon and the technicality to a minimum, yet it is such a topic that I could only do so much. And my goal is to make the reader understand even the goriest details of Transformer by the end of this post"
"Understanding Transformers, the Data Science Way - KDnuggets","This is the picture of the full transformer as taken from the paper. And, it surely is intimidating. So, I will aim to demystify it in this post by going through each individual piece. So read ahead"
"Understanding Transformers, the Data Science Way - KDnuggets","Essentially, a transformer can perform almost any NLP task. It can be used for language modeling, Translation, or Classification as required, and it does it fast by removing the sequential nature of the problem. So, the transformer in a machine translation application would convert one language to another, or for a classification problem will provide the class probability using an appropriate output layer"
"Understanding Transformers, the Data Science Way - KDnuggets","Patience, I am getting to it. So, as I said the encoder stack contains six encoder layers on top of each other(As given in the paper, but the future versions of transformers use even more layers). And each encoder in the stack has essentially two main layers:"
"Understanding Transformers, the Data Science Way - KDnuggets","They are a mouthful. Right? Don’t lose me yet as I will explain both of them in the coming sections. Right now, just remember that the encoder layer incorporates attention and a position-wise feed-forward network"
"Understanding Transformers, the Data Science Way - KDnuggets","And what about the outputs of this layer? Remember that the encoder layers are stacked on top of each other. So, we want to be able to have an output of the same dimension as the input so that the output can flow easily into the next encoder. So the output is also of the shape,"
"Understanding Transformers, the Data Science Way - KDnuggets","Deep Learning is essentially nothing but a lot of matrix calculations and what we are essentially doing in this layer is a lot of matrix calculations intelligently. The self-attention layer initializes with 3 weight matrices — Query(W_q), Key(W_k), and Value(W_v). Each of these matrices has a size of ("
"Understanding Transformers, the Data Science Way - KDnuggets","Till now it is trivial and shouldn’t make any sense, but it is at the second calculation where it gets interesting. Let’s try to understand the output of the softmax function. We start by multiplying the Q and Kᵀ matrix to get a matrix of size ("
"Understanding Transformers, the Data Science Way - KDnuggets",As you can see the diagonal entries are big. This is because the word contribution to itself is high. That is reasonable. But we can see here that the word “quick” devolves into “quick” and “fox” and the word “brown” also devolves into “brown” and “fox”. That intuitively helps us to say that both the words — “quick” and “brown” each refers to the “fox”
"Understanding Transformers, the Data Science Way - KDnuggets","Once we have this SxS matrix with contributions we multiply this matrix by the Value matrix(Sxd) of the sentence and it gives us back a matrix of shape Sxd(4x64). So, what the operation actually does is that it replaces the embedding vector of a word like “quick” with say .75 x (quick embedding) and .2x(fox embedding) and thus now the resultant output for the word “quick” has attention embedded in itself"
"Understanding Transformers, the Data Science Way - KDnuggets","It’s called a multi-head because we use many such self-attention layers in parallel. That is, we have many self-attention layers stacked on top of each other. The number of attention layers,h, is kept as 8 in the paper. So the input X goes through many self-attention layers parallelly, each of which gives a z matrix of shape (Sxd) = 4x64. We concatenate these 8(h) matrices and again apply a final output linear layer, Wo, of size DxD"
"Understanding Transformers, the Data Science Way - KDnuggets","Once we understand the multi-headed attention layer, the Feed-forward network is actually pretty easy to understand. It is just a combination of various linear and dropout layers on the output Z. Consequentially, it is again just a lot of Matrix multiplication here"
"Understanding Transformers, the Data Science Way - KDnuggets","Okay, These two concepts are pretty essential to this particular architecture. And I am glad you asked this one. So, we will discuss these steps before moving further to the decoder stack"
"Understanding Transformers, the Data Science Way - KDnuggets","Since, our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of both the encoder and decoder stacks(as we will see later). The positional encodings need to have the same dimension, D as the embeddings have so that the two can be summed"
"Understanding Transformers, the Data Science Way - KDnuggets","Above is the heatmap of the position encoding matrix that we will add to the input that is to be given to the first encoder. I am showing the heatmap for the first 300 positions and the first 3000 positions. We can see that there is a distinct pattern that we provide to our Transformer to understand the position of each word. And since we are using a function comprised of sin and cos, we are able to embed positional embeddings for very high positions also pretty well as we can see in the second picture"
"Understanding Transformers, the Data Science Way - KDnuggets","Another thing, that I didn’t mention for the sake of simplicity while explaining the encoder is that the encoder(the decoder architecture too) architecture has skip level residual connections(something akin to resnet50) also. So, the exact encoder architecture in the paper looks like below. Simply put, it helps traverse information for a much greater length in a Deep Neural Network. This can be thought of as akin(intuitively) to information passing in an organization where you have access to your manager as well as to your manager’s manager"
"Understanding Transformers, the Data Science Way - KDnuggets","That’s understandable actually. Let me break it in steps. So, our resultant matrix(QxK/sqrt(d)) of shape (TxT) might look something like below:(The numbers can be big as softmax not applied yet)"
"Understanding Transformers, the Data Science Way - KDnuggets","So glad that you are still with me and you appreciate it. Now, coming back to the decoder. The next layer in the decoder is:"
"Understanding Transformers, the Data Science Way - KDnuggets","As you can see in the decoder architecture, a Z vector(Output of encoder) flows from the encoder to the multi-head attention layer in the Decoder. This Z output from the last encoder has a special name and is often called as memory. The attention layer takes as input both the encoder output and data flowing from below(shifted outputs) and uses attention. The Query vector Q is created from the data flowing in the decoder, while the Key(K) and value(V) vectors come from the encoder output"
"Understanding Transformers, the Data Science Way - KDnuggets","No, there is no mask here. The output coming from below is already masked and this allows every position in the decoder to attend over all the positions in the Value vector. So for every word position to be generated the decoder has access to the whole English sentence"
"Understanding Transformers, the Data Science Way - KDnuggets","You can look at the figure where I have done all the weights calculation. I would also ask you to see the shapes of the resultant Z vector and how our weight matrices until now never used the target or source sentence length in any of their dimensions. Normally, the shape cancels away in all our matrix calculations. For example, see how the S dimension cancels away in calculation 2 above. That is why while selecting the batches during training the authors talk about tight batches. That is in a batch all source sentences have similar lengths. And different batches could have different source lengths"
"Understanding Transformers, the Data Science Way - KDnuggets","We are actually very much there now friend. Once, we are done with the transformer, the next thing is to add a task-specific output head on the top of the decoder output. This can be done by adding some linear layers and softmax on top to get the probability"
"Understanding Transformers, the Data Science Way - KDnuggets",As you can see we are able to generate probabilities. So far we know how to do a forward pass through this Transformer architecture. Let us see how we do the training of such a Neural Net Architecture
"Understanding Transformers, the Data Science Way - KDnuggets","We can give an English sentence and shifted output sentence and do a forward pass and get the probabilities over the German vocabulary. And thus we should be able to use a loss function like cross-entropy where the target could be the German word we want, and train the neural network using the Adam Optimizer. Just like any classification example. So, there is your German"
"Understanding Transformers, the Data Science Way - KDnuggets",Okay. Okay. I get you. Let’s do it then
"Understanding Transformers, the Data Science Way - KDnuggets","KL Divergence is the information loss that happens when the distribution P is approximated by the distribution Q. When we use the KL Divergence loss, we try to estimate the target distribution(P) using the probabilities(Q) we generate from the model. And we try to minimize this information loss in the training"
"Understanding Transformers, the Data Science Way - KDnuggets","In the paper, though the authors used label smoothing with α = 0.1 and so the KL Divergence loss is not cross-entropy. What that means is that in the target distribution the output value is substituted by (1-α) and the remaining 0.1 is distributed across all the words. The authors say that this is so that the model is not too confident"
"Understanding Transformers, the Data Science Way - KDnuggets","Yes, it does but intuitively, you can think of it as when we give the target as 1 to our loss function, we have no doubts that the true label is True and others are not. But vocabulary is inherently a non-standardized target. For example, who is to say that you cannot use good in place of great? So we add some confusion in our labels so our model is not too rigid"
"Understanding Transformers, the Data Science Way - KDnuggets","The authors use a learning rate scheduler to increase the learning rate until warmup steps and then decrease it using the below function. And they used the Adam optimizer with β¹ = 0.9, β² = 0.98. Nothing too interesting here just some learning choices"
"Understanding Transformers, the Data Science Way - KDnuggets","This model does piece-wise predictions. In the original paper, they use the Beam Search to do prediction. But a greedy search would work fine as well for the purpose of explaining it. In the above example, I have shown how a greedy search would work exactly. The greedy search would start with:"
"Understanding Transformers, the Data Science Way - KDnuggets",6+0.05). We then repeat this process to get the sentence with the highest probability
"Understanding Transformers, the Data Science Way - KDnuggets",Yes. Since you asked. Here it is:
"Understanding Transformers, the Data Science Way - KDnuggets","I am going to be writing more of such posts in the future too. Let me know what you think about them. Should I write on heavily technical topics or more beginner level articles? The comment section is your friend. Use it. Also, follow me up at"
International alternatives to Kaggle for Data Science / Machine Learning competitions - KDnuggets,"We’ve all heard of Kaggle, but that also means there’s more competition — recently, Kaggle reached 5 million users. Further, not all competitions are open to everyone in the world. Here’s the policy of one competition, for instance:"
International alternatives to Kaggle for Data Science / Machine Learning competitions - KDnuggets,"Besides ranking in competitions, you’ll want to work on practical projects that you can share with the world. Ideally, your projects can resonate with non-technical audiences as well, such as hiring managers who often don’t understand the intricacies of the field. To do so, you can use no-code analytics tools like"
My Data Science Online Learning Journey on Coursera - KDnuggets,"After some research, I found out that if you want to learn about CNN, you need to know about Deep Neural Networks (DNN) in general first. If you want to know about DNN, you need to know about classical Neural Networks first. If you want to know about Neural Networks, you need to know about machine learning first. If you want to know about machine learning in general, you need to know about the fundamental of data science first"
My Data Science Online Learning Journey on Coursera - KDnuggets,"First of all, I don’t mean to endorse Coursera in this article. I just find that they are the best online learning platform for me as there are plenty of courses in data science and machine learning from reputable institutions. Plus, you have the option to audit the course for free and you’ll still get access to the learning materials"
My Data Science Online Learning Journey on Coursera - KDnuggets,I think we all agree that the hardest part of everything is always in the beginning. Same as me when I wanted to get my hands dirty in data science. I kept asking a question: where do I start?
My Data Science Online Learning Journey on Coursera - KDnuggets,"There are 9 courses in this specialization. It starts with the concept and methodology of data science before delving into programming stuff with Python and SQL. Next, it introduces you to the meat of data science — Statistics, Data Analysis, Data Visualization, and Machine Learning"
My Data Science Online Learning Journey on Coursera - KDnuggets,"What I really like about this specialization is how hands-on it was. With the Virtual Machine from Cloudera, we have a chance to apply SQL query to retrieve or to store data with either Apache Hive, Apache Impala, MySQL, or PostgreSQL. You can always revisit the Virtual Machine even after you finished the specialization, so you will always able to revise your SQL skills and play around with the data"
My Data Science Online Learning Journey on Coursera - KDnuggets,"We can agree that statistics is the heart of data science. As I already know statistics before, I took this specialization with the expectation to refresh the fundamental theory of statistics. But in the end, I got more than I was expected"
My Data Science Online Learning Journey on Coursera - KDnuggets,"I would normally use Python when it comes to visualizing data, either with the help of Matplotlib, Seaborn, or Plotly. However, I wanted to learn something new — I wanted to learn how to visualize the data using Business Intelligence tools, either with PowerBI or Tableau. And then I found this specialization"
My Data Science Online Learning Journey on Coursera - KDnuggets,"There are 5 courses including a Capstone project in this specialization. The first three courses will give you a theoretical understanding of data visualization best practice and how to tell a story with your data. The fourth course is basically where you get your hands dirty with Tableau, as you will learn how to create an interactive data visualization dashboard and story with Tableau"
My Data Science Online Learning Journey on Coursera - KDnuggets,I liked how passionate Andrew Ng in teaching us about different types of machine learning algorithms. I liked how easy it was for him to explain and simplify difficult machine learning concepts to us. I also liked the programming assignment and how we had the opportunity to implement Neural Networks algorithms from scratch
My Data Science Online Learning Journey on Coursera - KDnuggets,"The specialization is very well structured. The first course will teach you about the concept of Deep Neural Networks after you learned about the classic Neural Networks in the previous Machine Learning course. Next, it gives the important concepts of Convolutional Neural Networks and Sequence Models"
My Data Science Online Learning Journey on Coursera - KDnuggets,"This specialization is a pure hands-on exercise. You won’t find any theory regarding deep learning in it as its focus is to implement deep learning algorithm with the help of TensorFlow. Thus, it is suggested that you already know about deep learning concepts before taking this specialization"
My Data Science Online Learning Journey on Coursera - KDnuggets,"As a bonus, if you want to take the TensorFlow Developer Certificate in the future, this specialization would also be the best source for you to prepare for it. I recently took the certification and I can say that this specialization is the best source for the preparation. If you’re interested in my experience of taking the certification, you can read it in the link below"
The Most Complete Guide to PyTorch for Data Scientists - KDnuggets,"Tensors are the basic building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU. In this part, I will list down some of the most used operations we can use while working with Tensors. This is by no means an exhaustive list of operations you can do with Tensors, but it is helpful to understand what tensors are before going towards the more exciting parts"
The Most Complete Guide to PyTorch for Data Scientists - KDnuggets,"We can create a PyTorch tensor in multiple ways. This includes converting to tensor from a NumPy array. Below is just a small gist with some examples to start with, but you can do a whole lot of"
The Most Complete Guide to PyTorch for Data Scientists - KDnuggets,Here comes the fun part as we are now going to talk about some of the most used constructs in Pytorch while creating deep learning projects.Module lets you create your Deep Learning models as a class. You can inherit from
The Most Complete Guide to PyTorch for Data Scientists - KDnuggets,"Here we have defined a very simple Network that takes an input of size 784 and passes it through two linear layers in a sequential manner. But the thing to note is that we can define any sort of calculation while defining the forward pass, and that makes PyTorch highly customizable for research purposes. For example, in our crazy experimentation mode, we might have used the below network where we arbitrarily attach our layers. Here we send back the output from the second linear layer back again to the first one after adding the input to it(skip connection) back again(I honestly don’t know what that will do)"
The Most Complete Guide to PyTorch for Data Scientists - KDnuggets,"This is great, and Pytorch does provide a lot of functionality out of the box. But the main power of Pytorch comes with its immense customization. We can also create our own custom datasets if the datasets provided by PyTorch don’t fit our use case"
The Most Complete Guide to PyTorch for Data Scientists - KDnuggets,"So let’s say you are looking to provide batches to a network that processes text input, and the network could take sequences with any sequence size as long as the size remains constant in the batch. For example, we can have a BiLSTM network that can process sequences of any length. It’s alright if you don’t understand the layers used in it right now; just know that it can process sequences with variable sizes"
The Most Complete Guide to PyTorch for Data Scientists - KDnuggets,"Now, we want to provide tight batches to this model, such that each batch has the same sequence length based on the max sequence length in the batch to minimize padding. This has an added benefit of making the neural net run faster. It was, in fact, one of the methods used in the winning submission of the Quora Insincere challenge in Kaggle, where running time was of utmost importance"
The Most Complete Guide to PyTorch for Data Scientists - KDnuggets,"Pytorch provides a lot of customizability with minimal code. While at first, it might be hard to understand how the whole ecosystem is structured with classes, in the end, it is simple Python. In this post, I have tried to break down most of the parts you might need while using Pytorch, and I hope it makes a little more sense for you after reading this"
The Most Complete Guide to PyTorch for Data Scientists - KDnuggets,Thanks for the read. I am going to be writing more beginner-friendly posts in the future too. Follow me up at
Lessons From My First Kaggle Competition - KDnuggets,"I find starting out in a new area of programming a somewhat daunting experience. I have been programming for the past 8 years, but only recently have developed a keen interest in Data Science. I want to share my experience to encourage you to take the plunge too!"
Lessons From My First Kaggle Competition - KDnuggets,"I started out dipping my toe in the ocean of this vast topic with a couple of the Kaggle mini-courses. I didn’t need to learn how to write in Python, but I needed to equip myself with the tools to do the programming that I wanted. First up was"
Lessons From My First Kaggle Competition - KDnuggets,"I spent some time choosing the right competition. I didn’t want to just have a go at doing something with a dataset because I wanted to see my progress and evaluate my success, but I also didn’t want to feel bad about not being able to achieve anything. Having guidance on what to aim for felt like a nice security blanket too"
Statistics with Julia: The Free eBook - KDnuggets,"Julia is first and foremost a scientific programming language. It is perfectly suited for statistics, machine learning, data science, as well as for light and heavy numerical computational tasks. It can also be integrated in user-level applications, however one would not typically use it for front-end interfaces, or game creation. It is an open-source language and platform, and the Julia community brings together contributors from the scientific computing, statistics, and data-science worlds. This puts the Julia language and package system in a good place for combining mainstream statistical methods with methods and trends of the scientific computing world"
Statistics with Julia: The Free eBook - KDnuggets,"Hence, this book is a self-contained guide for the core principles of probability, statistics, machine learning, data science, and artificial intelligence. It is ideally suited for engineers, data-scientists, or science professionals, wishing to strengthen their core probability, statistics, and data science knowledge while exploring the Julia language. However, general mathematical notation and results including basics from linear algebra, calculus, and discrete mathematics are used"
"SIAM launches activity group, publications for data scientists - KDnuggets","Data science is experiencing a transformation unlike many we’ve seen in applied mathematics before. LinkedIn reports 650% job growth in data science, data analysis, machine learning, and related jobs since 2012 and the U.S. Bureau of Labor Statistics expects an estimated 11.5 million new jobs by 2026. The number of students studying these subjects is on the rise, and we’re seeing an increased number of mathematicians making career changes to meet the growing demand. Society for Industrial and Applied Mathematics (SIAM) has listened to the requests of the community and has launched a series of new services and resources to meet these unique and growing needs"
"SIAM launches activity group, publications for data scientists - KDnuggets","Launched this month, the activity group is a community dedicated to advancing the mathematics of data science, focusing on the mathematical, statistical, and computational foundations of data science. The SIAM Conference on Mathematics of Data Science (MDS) will serve as the community’s flagship conference. The inaugural MDS20 took place virtually in May, with the next conference planned for the spring of 2022. The activity group will also sponsor the annual"
"SIAM launches activity group, publications for data scientists - KDnuggets","The books in this new series will address state-of-the-art research covering the mathematical, computational, and scientific aspects of data science. The series will publish high-impact research monographs, in-depth essays on emerging trends, tutorials with a broad reach, advanced surveys, scholarly research retrospectives, and textbooks. We recently published the first book in the series and are accepting proposals for new additions to the collection!"
"SIAM launches activity group, publications for data scientists - KDnuggets","Society for Industrial and Applied Mathematics (SIAM), headquartered in Philadelphia, Pennsylvania, is an international society of more than 14,000 individual, academic and corporate members from 90+ countries. SIAM helps build cooperation between mathematics and the worlds of science and technology to solve real-world problems through publications, conferences, and communities like student chapters, regional sections, and activity groups. Learn more at"
Advice for Aspiring Data Scientists - KDnuggets,"I'll segment this into basic advice, which can be found quite easily if you just google 'how to get into data science' and advice that is less common, but advice that I've found very useful over the years. I'll start with the latter, and move on to basic advice. Obviously take this with a grain of salt as all advice comes with a bit of survivorship bias"
Advice for Aspiring Data Scientists - KDnuggets,"Both groups often use proxies for competence like GPA, school quality, or experience in data from a tech firm (I call these: proof of status). As a result, you should very closely think about the time needed to signal to the reader that you can do whatever job they’re looking to hire for. A rough metric to consider for this is Clicks to Proof of Competence"
Advice for Aspiring Data Scientists - KDnuggets,"If the recruiter has to click on the right repository in your Github and then click through files until they find the Jupyter notebook with unreadable code (without comments nonetheless), you’ve already lost. If the recruiter sees Machine Learning on your resume, but it takes 5 clicks to see any ML product or code that you've made, you've already lost. Anyone can lie on a resume; make a point to direct the reader’s attention quickly, and you’ll be in a significantly better spot"
Advice for Aspiring Data Scientists - KDnuggets,"After you do those three things, see if you can convince someone to pay you to learn data science. There is a great election data science group at UF that I loved (Dr McDonald and Dr Smith run it currently), but if you go to any research group and interview with them they might pay you for your work. Eventually, with experience like that, then you can apply for internships and get paid super well. The key here is to not start out looking for the incredibly fancy DS internships, but locally at companies or research groups that have Data Science tasks but not enough money to hire a full time Data Scientist. Data Science learning compounds quickly, so start now! Given all of that, let’s move on to the more basic advice"
Advice for Aspiring Data Scientists - KDnuggets,"Learn either Python or R and get really good at it. Do something new every day, spend at least 5-10 hours per week on it as soon as possible. Learn SQL after this. You cannot skip around this"
Advice for Aspiring Data Scientists - KDnuggets,"At P&G, my data science work was applied to retail. At Facebook, to integrity problems. At Protect Democracy, to, uh, Democracy. Learning about applications of data science into some business context is hard and takes practice, and often involves a solid understanding of metrics, product analytics and incentive structures. This fits in very well with #2 from the less basic advice"
Are Data Analytics and Data Science Two Separate Fields? - KDnuggets,"Data analytics has been with us since the beginning of time. People have always been interested in analysing and understanding data of all types. For example, in support of my new book"
Are Data Analytics and Data Science Two Separate Fields? - KDnuggets,"If they are professionals, I would suggest that they take classes on Coursera, Udemy or any other on-line educational platform to see if they have a real interest in, and affinity for, analytics. If they do have an interest, then they should start working on analytics for themselves to test out analytical techniques, apply critical thinking and try to understand what they can see or cannot see in the data. If that works out and their interest remains, they should volunteer for"
6 Common Mistakes in Data Science and How To Avoid Them - KDnuggets,"In data science or machine learning, we use data for descriptive analytics to draw out meaningful conclusions from the data, or we can use data for predictive purposes to build models that can make predictions on unseen data. The reliability of any model depends on the level of expertise of the data scientist. It is one thing to build a machine learning model. It is another thing to ensure the model is optimal and of the highest quality. This article will discuss six common mistakes that can adversely influence the quality or predictive power of a machine learning model with several case studies included"
6 Common Mistakes in Data Science and How To Avoid Them - KDnuggets,"Sometimes as a data science aspirant, when you have to work on a data science project, you may be tempted to use the entire dataset provided. However, as already mentioned above, a dataset could have several imperfections, such as the presence of outliers, missing values, and redundant features. If the fraction of your dataset containing imperfections is really small, then you may simply eliminate the subset of imperfect data from your dataset. However, if the proportion of improper data is significant, then methods such as data imputation techniques could be used to approximate missing data"
6 Common Mistakes in Data Science and How To Avoid Them - KDnuggets,"Before implementing a machine learning algorithm, it is necessary to select only relevant features in the training dataset. The process of transforming a dataset in order to select only relevant features necessary for training is called dimensionality reduction. Feature selection and dimensionality reduction are important because of three main reasons:"
6 Common Mistakes in Data Science and How To Avoid Them - KDnuggets,"In order to bring features to the same scale, we could decide to use either normalization or standardization of features. Most often, we assume data is normally distributed and default towards standardization, but that is not always the case. It is important that before deciding whether to use either standardization or normalization, you first take a look at how your features are statistically distributed. If the feature tends to be uniformly distributed, then we may use normalization ("
6 Common Mistakes in Data Science and How To Avoid Them - KDnuggets,Using the wrong hyperparameter values in your model could lead to a non-optimal and low-quality model. It is important that you train your model against all hyperparameters in order to determine the model with optimal performance. A good example of how the predictive power of a model depends on hyperparameters can be found in the figure below (source:
6 Common Mistakes in Data Science and How To Avoid Them - KDnuggets,"Every machine learning model has an inherent random error. This error arises from the inherent random nature of the dataset; from the random nature in which the dataset is partitioned into training and testing sets during model building; or from randomization of the target column (a method used for detecting overfitting). It is important to always quantify how random error affects the predictive power of your model. This would help improve the reliability and quality of your model. For more information about random error quantification, please see the following article:"
6 Common Mistakes in Data Science and How To Avoid Them - KDnuggets,"In summary, we have discussed six common mistakes that can influence the quality or predictive power of a machine learning model. It is useful to always ensure that your model is optimal and of the highest quality. Avoiding the mistakes discussed above can enable a data science aspirant to build reliable and trustworthy models"
A step-by-step guide for creating an authentic data science portfolio project - KDnuggets,"Last but not least, I wanted to check what the most liked post was about. Unfortunately, it is in Germany, but it was indeed a very interesting post, where a German guy was allowed to spend some time on a US aircraft carrier and experienced a catapult take off in a C2 airplane. The post has some very nice pictures and interesting details. Feel free to check it out"
A step-by-step guide for creating an authentic data science portfolio project - KDnuggets,"Come up with a blog post idea that answers a burning question you had or solves your own problem. Ideally, the timing of the topic is relevant and has not been analysed by anyone else before. Based on your experience, website structure, and complexity, choose a framework that matches the scraping job best. During data cleaning, leverage existing libraries to solve painful data cleaning tasks like parsing timestamps or cleaning text. Finally, choose how you can best share your work. Both an interactive deployed model/dashboard or a well written medium blog post can differentiate you from other applicants on the journey to become a data scientist"
4 Tools to Speed Up Your Data Science Writing - KDnuggets,"I’ve been writing about data science on Medium for just over two years. Writing, in particular, technical writing can be time-consuming. Not only do you need to come up with an idea, write well, edit your articles for accuracy and flow, and proofread them. With technical articles you often also need to produce code to illustrate your explanations, ensure that it is accurate and transfer that code from the tool you used to write it, to your Medium post"
4 Tools to Speed Up Your Data Science Writing - KDnuggets,Over time I have found some tools that have hugely sped up the time it takes for me to create and publish an article. Particularly those containing coded examples. The following tools help me to achieve my goal of publishing one to two articles per week around my other life commitments
4 Tools to Speed Up Your Data Science Writing - KDnuggets,"When you select this option you will be presented with a form. At the top, it asks for an integration token. If you are a regular writer on Medium you may have access to this in the settings in your Medium account. If you don’t have a token in your settings you will need to contact Medium to request one via this email address yourfriends@medium"
4 Tools to Speed Up Your Data Science Writing - KDnuggets,"Creating a Gist is very simple, just give it a name, paste in your code and hit publish. Once published you will see this page. To embed the Gist in your Medium post simply code the URL under ‘Embed’ and paste into your article"
4 Tools to Speed Up Your Data Science Writing - KDnuggets,To embed specific snippets of code in a cell. Navigate to the notebook you have uploaded to your online account and the cell containing the code you want to share. Click on
What Does It Take to be a Successful Data Scientist? - KDnuggets,"Data scientists are rare, that’s not new. Lots of educational programs are popping up to train more to meet the demand. Universities are creating data science departments, centers, or even entire divisions and schools. Online universities offer courses left and right. Even commercial providers present data science certifications in just a few weeks or months (or sometimes over a weekend)"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"At some point in the past years, there was hope that a single, simple solution could enable everybody to become a data scientist—if we just gave them the right tools. But similar to a doctor needing to know how the human body functions, a data scientist needs to understand the state-of-the-art models and algorithms to be able to make educated choices and recommendations. We are, after all, talking about data scientists here, not just users of black boxes that were designed by successful data scientists. A doctor isn’t turning us into a doctor by telling us what medicine to take either"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"But is a theoretical education sufficient? My answer here is no. Data science is as much about knowing the tool as it is about having experience applying it to real-world problems, about having that ‘gut feeling’ that raises your eyebrows when the results are suspiciously positive (or just weird). I have seen this countless times with students in our data science classes. Early on, when aspiring data scientists start working on practical exercises, no matter how smart they are, they present results that are totally off. Once asked ‘Are you sure this makes sense?’ they realize and begin to question their results, but this is learned behavior. These are often things as simple as questioning a 98% accuracy on a credit churn benchmark. Rather than wondering if this could point to a data pollution issue (the testing data containing some information about the outcome), the student proudly presents their 25% margin over their fellow students"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"Many wannabe data scientists claim they gained that real-world experience from working on online data analysis challenges—Kaggle or others. But that’s only partly true because these challenges focus on a small, important, but fairly static part of the job. Some data scientist trainers have started building practical exercises, modeling some of those other real-world traps. KNIME, for instance, can be used to create data in addition to analyzing it. We use this for our own teaching courses to create real-world, look-alike databases about artificial customers with given distributions and dependencies to marital status, income, shopping behaviors, preferences, and other features. The data generation modules also allow us to inject outliers, anomalies, and other patterns that break standard analysis methods if not detected earlier. But this is still very similar to learning how to drive on a playground; it doesn’t prepare you for driving in downtown Manhattan. Somehow, we can’t prepare for real life in the privacy of our home or classroom"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"Let’s dive a bit deeper into what a data scientist actually does. Many articles have already covered the horizontal spread of activities: everything from data sourcing, blending, and transforming all the way to creating interactive, analytical applications or otherwise deploying models into production (and I am not even touching upon monitoring and continuously updating those production models). Lots of those online challenges ignore these surrounding activities and focus solely on the modeling part. But that’s not the only problem. Let’s also consider the vertical spread of tasks:"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"This is the easiest setup that we can, at least partially, practice for in isolation. The problem and goal are well-defined, the data is mostly in good shape (and exists!), and the goal is to optimize a model to provide better outcomes. Examples are tasks such as predicting churn of customers and placing online advertisements. These are projects that essentially just support and confirm what the business stakeholder knows and put this knowledge into practice"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"In reality, this job is usually much less well-defined. The business owner knows what they want to optimize, but they don’t have a clear problem formulation, and way too often, they don’t have the right data. Stereotypical statements for this setup are project descriptions of the type ‘We have this data, please answer that question!’ Examples can range from predicting machine failures (‘We measure all those things, just tell us a day before the machine breaks"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"Here our data scientist needs experience communicating with stakeholders and domain experts to identify the data to be collected and to find and train the right models to provide the answers to the right question. This also involves a lot of nontheoretical but practical work around data blending and transformation and ensuring proper model deployment and monitoring. In training, we can help the data scientist by providing blueprints for similar applications, but automation often fails because the data types aren’t quite covered or the model optimization routines miss the mark just a bit. This is also an issue with the maturity of the field: We haven’t yet encountered problems of all types, and many of these types of projects require a touch of creativity in their solution. An automated solution or a solution created by an inexperienced data scientist may seem to provide the right type of answer, but it will often be a long shot from providing the best possible answer"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"The last type of data science activity is actually the truly interesting one. The goal is to create new insights that will then trigger new analytical activities and may completely change how things are done in the future. Setups of this kind are often initially poorly described (‘I don’t know what the solution looks like, but I’ll know it when I see it!’), and the data scientist’s job is to support this type of explorative hypothesis generation. In the past, we were restricted to simple, interactive data visualization environments, but today, an experienced data scientist can help to quickly try out different types of pattern discovery algorithms or predictive models and refine that setup given user feedback. Typically a lot of this feedback will be of the type ‘We know this’ or ‘We don’t care about that,’ which will lead to continued refinement. The true breakthrough, however, is often initiated by comments of the type ‘This is weird, I wonder …,’ triggering a new hypothesis about underlying dependencies"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"For this type of activity, our data scientist needs experience dealing with open-ended—often research type—questions and the ability to quickly iterate over different types of analysis methods and models. It requires out-of-the-box thinking and the ability to move beyond an existing blueprint, and, of course, it requires learning from past experiences. In this type of scenario, often the type of insights generated yesterday aren’t interesting today because the past insights did advance and change the knowledge of both the data scientist and the domain expert!"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"Presumably, this segmentation is a bit blurry; some apprentices will never aspire to become an expert, having job requirements that are well-defined and can be solved using standard techniques. And obviously, this will change over time with the data science field maturing. From what we see at KNIME (our built-in recommendation engine relies on anonymous tool usage information), the famous 90-9-1 doesn’t quite apply here, but it is still only a fairly small percentage of our users (<10%) that regularly use nodes that we’d refer to as expert modules. The vast majority of our users start with one of the example workflows (which, in turn, rely on expert nodes) or use relatively standard modules themselves. This is also a view validated by conversations with our larger customers: Many of the users there rely on workflows as templates to start from instead of creating complex workflows from scratch"
What Does It Take to be a Successful Data Scientist? - KDnuggets,"Data science, like computer science, requires a mix of theory and practice. Similar to how we now run software projects as part of most computer science curriculum, we should add practical projects to data science curricula. But like successful programmers, successful data scientists will require years of practical, real-world experience before being able to tackle real problems independently"
The Online Courses You Must Take to be a Better Data Scientist - KDnuggets,I started in Data Science back in 2015. It was not an intended move but the answer to the needs of my employer. I was working for a company providing automation services to Spanish corporations and we had the need to leverage data to
The Online Courses You Must Take to be a Better Data Scientist - KDnuggets,This last bullet point is the focus of this post. The online courses I am going to present you here are those focused in getting the theoretical foundations of Data Science. Those courses have some common features:
The Online Courses You Must Take to be a Better Data Scientist - KDnuggets,"Nevertheless, they are not the only courses I have done since I started in this field. I am always taking some kind of course, sometimes even two at the same time; most of them are about Data Science, although I sometimes broaden my knowledge about other topics as well such as Urban Design, Energy, among others. Maybe I will cover this topic in another post if there is interest"
9 Developing Data Science & Analytics Job Trends - KDnuggets,"At the moment, we are not seeing any evidence of job offers being lower than expected as a result of the current crisis. From what we’ve seen, larger companies at least are currently staying within their prescribed salary bands. However, as this crisis continues, it is possible that will change, which could have an effect on 2021 salaries. With the prevalence of layoffs and furloughs disrupting the career plans of many candidates, this may also have an impact on their ability to negotiate higher salaries, which has the potential to flatten salary increases, at least temporarily"
9 Developing Data Science & Analytics Job Trends - KDnuggets,"With the increase in startups going under and some companies announcing permanent WFH strategies, it is likely that the movement of candidates out of the West Coast will accelerate. This is particularly affecting areas like the Bay Area and Seattle, which was already underway in recent years as a result of the high cost of living and lack of affordable housing. This could impact salaries if professionals move to lower cost of living areas and, as a result, receive lower salaries. It is also yet to be seen whether lockdown restrictions and avoidance of public transit coupled with a more favorable remote work environment could favor suburban offices"
9 Developing Data Science & Analytics Job Trends - KDnuggets,"For some teams, the crisis has had a positive impact on their hiring. The talent that potential employers would not otherwise have access to may be available if they find themselves laid off, furloughed, or otherwise open to making a change because of industry uncertainty. We’re already seeing an increase in private equity groups, in the interest of accurately timing business decisions, who have been picking up additional analytics talent throughout the crisis"
Data Scientists think data is their #1 problem. Here’s why they’re wrong. - KDnuggets,"I often see articles or posts that identify data integration or preparation as the key issues facing data science projects. This always puzzles me as this is not our lived experience - not what we see when we work with Fortune 500 companies adopting predictive analytics, machine learning, or AI. But I think I have figured it out. The problem is as follows:"
The Most Important Data Science Project - KDnuggets,"Additionally, We can never know the optimal solution from the beginning of a Data Science project, nor should we expect to know the best way to communicate our skills unless we try. Yet, with each solution, we discover more about ourselves and we can build solutions that focus on correcting the errors we are making. Besides, whether it is arriving where we picture ourselves, or being of mass value to the marketplace, it is most likely going to require the outputs from former solutions to ensemble all of the lessons you learned along the way, just like most winning solutions for Data Science competitions"
The Most Important Data Science Project - KDnuggets,"Note: Remember that the healthiest relationships suit both parties. Both sides win! You want a large salary, the world needs competent problem solvers. If you want a major breakthrough, be of major use"
The Most Important Data Science Project - KDnuggets,"As stated above, there are many ways to showcase your Data Science skills, possibly more than I can think of. Your job is to select 2–3 and begin working at each. Without further ado…"
The Most Important Data Science Project - KDnuggets,"Journal writing is an excellent method to keep tabs on your experience and thoughts. In the long run journal writing can be used to identify future goals and aims. In a Data Science environment, we can use journal writing to track our:"
The Most Important Data Science Project - KDnuggets,"Yes, all of this is valuable information and if it allows you to track your progress and project yourself into the future, it also does wonders as a tool to showcase your ability. People in front of you can see where you are going and provide assistance to help you reach the next point in your career faster, as well as lay off some of their responsibilities to you while they focus on task that will take them forward. Additionally, those behind you can learn and be inspired from what you’ve left behind. Regardless of how you look at it, in both scenarios you are useful!"
The Most Important Data Science Project - KDnuggets,"I have 2 things to say about doing this method. Firstly, leave the Titanic challenge alone… Please. Secondly, maintain the project. Don’t just do it until “it works” and stop. Continue to build and develop your project, add functionality, make it come to life. If you’ve created a Dog Classifier, build something (i"
The Most Important Data Science Project - KDnuggets,"This can go hand in hand with your projects in a sense that you may share your trail of thought behind your solution. Another good example would be to summarize other people’s work, It’s good for engagement and you may (more than likely) learn a thing or two. For instance, taking the summary of other people’s work idea; there are many research papers released each week and you can decide to summarize 2 or 3 as they are released"
The Most Important Data Science Project - KDnuggets,"Well if you’re reading this story, you can probably guess one of the options I’ve chosen. Blogging has been absolutely amazing for me (especially during lockdown). Thinking of what to write about drives you to want to learn more and when you have to flesh out a technical subject, you’re a little bit more attentive to the fine details which allows you to learn quicker — I can testify to that. Also, you can build up your reputation as a subject matter expert — Oh and did I add that you can earn extra income?"
The Most Important Data Science Project - KDnuggets,"There is absolutely no reason why you cannot start today. Platforms like Youtube and Medium has made this market extremely accessible and if you’ve learnt something new, it’s likely that someone else may want to know. Don’t ever feel that because someone else has done something already that you cannot — it’s also a good idea to build on work of others"
The Most Important Data Science Project - KDnuggets,I’ve heard this one so many times with Kaggle being the first name that comes to mind whenever Data Science competitions are mentioned. There have been so many debates about whether is useful for real-world Data Science of which I really do not want to get into. The important factor is that to do well you’d have to apply techniques (some of which are applied in the real-world) and this serves as a form of practical learning and if you do well it speaks volumes of your character
Data is everywhere and it powers everything we do! - KDnuggets,"Data monetisation is the new global economy and organisations old/big/small or start-ups will have to capture, store, process and consume intelligently. This has to be a continuous feedback loop data cycle. There is a plethora of tools and technology which can address this data cycle but the success depends upon organisation's direction and governance"
Data is everywhere and it powers everything we do! - KDnuggets,"It will be prosperous if companies have a data-centric architecture approach with overall governing body in place else, just by having data lake/hub is not taking companies anywhere. Data lake will have to be maintained, curated, secured etc. It should not just treat as an enterprise data storage solution and make the data stale"
Data is everywhere and it powers everything we do! - KDnuggets,"If companies want to perform any business intelligence, analytics or data science, leaders of the business units should ensure that data engineers, analysts and data scientists should not take any unscientific shortcuts. They should avoid blind spots in performing their tasks and in building AI models. Overall success should not be just measured by PoC or AI labs prototype results, instead it should be measured from at-scale deployment and impact made to the business and end users"
Data is everywhere and it powers everything we do! - KDnuggets,"Once the corpus of data is ready, organisations can get suitable tool(s) to build and gain insights. Going down the data analytics journey lane, organisations may face few hurdles while building and managing their set data strategy into action (e. These challenges have to be rectified and overcome at the earliest"
"Top Online Masters in Analytics, Business Analytics, Data Science – Updated - KDnuggets","Fig. 1 below shows the ranking vs tuition for schools with rank < 500.
Where ranking is a range, we took the midpoint of that range"
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,"The COVID-19 pandemic has been raging for several months now and will likely continue for months to come. Amid this ongoing crisis, people need resources to stay as safe as they can for as long as they can. Data science and its related technologies are enabling such actions, playing an essential role in global safety"
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,"Times of hardship often serve as proving grounds for systems that people have boasted about in calmer days. The COVID-19 pandemic is no exception, and data science has more than proved its value. Data, and more importantly, the way people use it, is shaping and refining approaches to COVID-19 safety"
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,"Data analysis is central to establishing effective policies for combating the spread of the virus. Politicians can't expect to enact any adequate prevention measures if they don't know anything about how COVID-19 is spreading through their area. If they turn to data scientists, on the other hand, they can gain the knowledge they need to make informed decisions"
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,"There's still a lot that medical professionals don't know about the virus, so COVID-19 safety is a developing subject. As doctors learn more about the disease, they find that some older practices were ineffective. For example, most doctors have"
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,"Data science can go beyond suggesting medical approaches and show if current ones work or not. As authorities continue to implement new practices, they need to keep an eye on data to see if these methods are useful. Better data analytics can provide more conclusive answers about what is and isn't working"
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,"Celsius, or they could compromise the items inside. Data scientists can help companies create smart monitoring systems to make sure these containers maintain the proper environment. Without the right data tools, shipped groceries, vaccines and plants could spoil en route"
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,"Amid all the confusion of the pandemic, misinformation has seen an outbreak of its own. Incorrect information can be a threat to COVID-19 safety, but data science can help. Social media users generate too much data for a human moderator to monitor, but machine-learning-based analytic tools can flag potentially false information"
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,One of the most critical steps in COVID-19 safety is contact tracing. Some of the most effective instances of contact tracing have taken advantage of data science techniques. Countries like South Korea have established systems that
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,"These apps need to employ data science techniques like data acquisition, cleansing and masking to work correctly. Data scientists can lend their expertise to app developers and researchers to help create a functioning contact tracing network. With these tools, numerous areas can take more control over slowing the spread of the virus"
How Data Science Is Keeping People Safe During COVID-19 - KDnuggets,"Through all of its different applications, data science is a source of one essential offering — information. If authorities and citizens are to respond to the pandemic appropriately, they need to understand the situation as a whole. Data science enables people to draw a more cohesive picture from raw data, bolstering the fight against COVID-19"
Data Versioning: Does it mean what you think it means? - KDnuggets,"When we first thought about a tagline for lakeFS, our recently released OSS project, we instinctively used terms such as “Data versioning”, “Manage data the way you manage code”, “It’s git for data”, and any random variation of the three that is a grammatically correct sentence in english. We were very pleased with ourselves for 5 minutes, maybe 7, before realizing these phrases don’t really mean anything, or mean too many things to properly describe what value we bring. It is also commonly used by other players in the domain that address completely different use cases"
Data Versioning: Does it mean what you think it means? - KDnuggets,These interfaces grant easy access and management of different versions of the same data set. Most players in this category also provide collaboration of other aspects of the workflow. Most popular is the ability to collaborate over ML models. In this category you can find the likes of
Data Versioning: Does it mean what you think it means? - KDnuggets,"At this point you might be asking yourself, why would Ops tools be mentioned in the context of the “Data Versioning”? Well, it’s because managing  data pipelines is a major challenge in ML application life cycle. Since ML is a scientific work, it requires reproducibility, and reproducibility means data + code. There are a few MLOps tools that enable data versioning, and they include:"
Data Versioning: Does it mean what you think it means? - KDnuggets,"Structured Data Formats that allow Insert, Delete, and Upsert. The formats are columnar, and provide the ability to change an existing object by saving the delta of the changes into another object. The meta data of those objects include the instructions on how to generate the latest version of an object from its saved delta objects. We add data versioning mainly to provide concurrency control. In  this category you can find open source projects"
Data Versioning: Does it mean what you think it means? - KDnuggets,"Managing multiple data producers and consumers of an object storage based data lake. The consumers access the data using different tools, such as Hadoop/Spark, Presto, and analytics data bases. Coordination between the data contributors and data consumers is challenging. It relies on internal processes and manual updates of catalogs or files. In addition, there’s no easy way to provide isolation without copying data, and there is no way to ensure consistency between multiple data collections"
How to Effectively Obtain Consumer Insights in a Data Overload Era - KDnuggets,The COVID-19 pandemic has brought numerous impacts to our routine. One of the most important factors organizations are discussing is the acceleration of Digital Transformation and its impact when it comes to data and technology. As noted by
How to Effectively Obtain Consumer Insights in a Data Overload Era - KDnuggets,"Among these executives, only 1 out of 5 believe they have the right tools for the job. The exponential increase in the number of customer reviews in e-commerce platforms is a good example. Consumers are not only writing more reviews but also asking and answering more questions online, generating challenging amounts of data to analyze"
How to Effectively Obtain Consumer Insights in a Data Overload Era - KDnuggets,"Returning to today’s context, it is precisely a solution like this that seems to be missing. We all have access to data, but in most cases, we don’t know for sure what to look for: we analyze the wrong data, looking only at a part of the collected content without seeing the correlation of this with the rest, and we even chose the wrong metrics to measure it. More than that, we spend more time trying to understand the past than planning the future"
How to Effectively Obtain Consumer Insights in a Data Overload Era - KDnuggets,"After graduating in Law from USP and Business Administration from FGV-EAESP, she joined Arizona in 2007 to lead the marketing department and was the responsible for product innovation and new markets development projects, such as internationalizing the company (selling to clients in Argentina, Chile, Colombia, and the UK and opening an office in Argentina) and developing digital products. She also co-founded HomeRefill, an online subscription e-commerce and GVAngels, an angel investment group that has already invested over 1 million dollars in Brazilian startups. Pat is a growth hacker graduated by Growth Tribe (Europe) and has experience with B2B Growth and Acquisition in Brazil and the USA. Together with Alex, she saw the opportunity to use product data for insights and drafted the first version of"
"If I had to start learning Data Science again, how would I do it? - KDnuggets","I’m aware that we all learn in different ways. Some prefer videos, others are OK with just books, and a lot of people need to pay for a course to feel more pressure. And that’s OK because the important thing is to learn and enjoy it"
"If I had to start learning Data Science again, how would I do it? - KDnuggets","If you are familiar with Python, you can skip this part. Here you’ll learn basic Python concepts that will help you start learning data science. There will be a lot of things about Python that are still going to be a mystery. But as we advance, you will learn it with practice"
"If I had to start learning Data Science again, how would I do it? - KDnuggets",This is where the exciting part starts. You are going to learn basic but very important concepts to start training machine learning models. Concepts that later will be essential to have them very clear
"If I had to start learning Data Science again, how would I do it? - KDnuggets","Let’s stop here for a moment. It should be clear that these 5 micro-courses are not going to be a linear process, as you are probably going to have to come and go between them to refresh concepts. When you are working in the Pandas one, you may have to go back to the Python course to remember some of the things you learned or go to the pandas documentation to understand new functions that you saw in the Introduction to Machine Learning course. And all of this is fine, right here is where the real learning is going to happen"
"If I had to start learning Data Science again, how would I do it? - KDnuggets","Here you’ll put into practice what you learned in the introductory courses. Maybe it will be a little intimidating at first, but it doesn’t matter because it’s not about being first in the leaderboard, it’s about learning. In this competition, you will learn about classification and relevant metrics for these types of problems such as precision, recall and accuracy"
The List of Top 10 Lists in Data Science - KDnuggets,"Data Science is no doubt the ""sexiest"" career path of the 21st century, made up of people with strong intellectual curiosity and technical expertise to dig out valuable insights from humongous volumes of data. This helps firms add value by improving their productivity, unlocking insights for better decision making and profit gains, just to mention a few. The knowledge of Data Science is desirable and useful across various industries"
The List of Top 10 Lists in Data Science - KDnuggets,"The journey of a Data Scientist is full of twists and turns that will mold you. However, it is not these twists and turns that molds, rather how you handle the ones thrown at you. Many of these challenges can be prevented or minimized by having a foreknowledge of the right tool kits before kickstarting the journey or maneuvering your way in the journey of being a successful data scientist"
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets",The new model was able to predict Kubeflow specific labels with average precision of 72% and average recall of 50%. This significantly reduced the toil associated with issue management for Kubeflow maintainers. The table below contains evaluation metrics for Kubeflow specific labels on a holdout set. The
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets","We don’t explicitly create a directed acyclic graph (DAG) to connect the steps in an ML workflow (e. Rather, we use a set of independent controllers. Each controller declaratively describes the desired state of the world and takes actions necessary to make the actual state of the world match. This independence makes it easy for us to use whatever tools make the most sense for each step. More specifically we use"
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets",Figure 3 illustrates how a reconciler works. A reconciler works by first observing the state of the world; e. The reconciler then compares this against the desired state of the world and computes the diff; e. The reconciler then takes the action necessary to drive the world to the desired state; e
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets","GitOps, Figure 5, is a pattern for managing infrastructure. The core idea of GitOps is that source control (doesn’t have to be git) should be the source of truth for configuration files describing your infrastructure. Controllers can then monitor source control and automatically update your infrastructure as your config changes. This means to make a change (or undo a change) you just open a pull request"
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets",The controller ensures there is only one Tekton pipeline running at a time. We configure our pipelines to always push to the same branch. This ensures we only ever open one PR to update the model because GitHub doesn’t allow multiple PRs to be created from the same branch
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets","We picked Tekton because the primary challenge we faced was sequentially running a series of CLIs in various containers. Tekton is perfect for this. Importantly, all the steps in a Tekton task run on the same pod which allows data to be shared between steps using a pod volume"
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets",Our controller’s reconcile loop will call our lambda to determines whether a sync is needed and if so with what parameters. If a sync is needed the controller fires off a Tekton pipeline to perform the actual update. An example of our
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets","Since we do not have an explicit DAG representing the sequence of steps in our CI/CD pipeline understanding the lineage of our models can be challenging. Fortunately, Kubeflow Metadata solves this by making it easy for each step to record information about what outputs it produced using what code and inputs. Kubeflow metadata can easily recover and plot the lineage graph. The figure below shows an example of the lineage graph from our"
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets","Building ML products is a team effort. In order to move a model from a proof of concept to a shipped product, data scientists and devops engineers need to collaborate. To foster this collaboration, we believe it is important to allow data scientists and devops engineers to use their preferred tools. Concretely, we wanted to support the following tools for Data Scientists, Devops Engineers, and"
"Data Science Meets Devops: MLOps with Jupyter, Git, and Kubernetes - KDnuggets","To maximize each team’s autonomy and reduce dependencies on tools, our CI/CD process follows a decentralized approach. Rather than explicitly define a DAG that connects the steps, our approach relies on a series of controllers that can be defined and administered independently. We think this maps naturally to enterprises where responsibilities might be split across teams; a data engineering team might be responsible for turning weblogs into features, a modeling team might be responsible for producing models from the features, and a deployments team might be responsible for rolling those models into production"
Bring your Pandas Dataframes to life with D-Tale - KDnuggets,"Bring your Pandas dataframes to life with D-Tale.  D-Tale is an open-source solution for which you can visualize, analyze and learn how to code Pandas data structures.  In this tutorial you'll learn how to open the grid, build columns, create charts and view code exports"
Bring your Pandas Dataframes to life with D-Tale - KDnuggets,"D-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures. It integrates seamlessly with ipython notebooks & python/ipython terminals. Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex"
Bring your Pandas Dataframes to life with D-Tale - KDnuggets,"If you open the main menu by clicking on the triangle in the upper lefthand corner you'll be presented with many options, one of which is ""Build Columns"". Click that and you see many options for different ways to build new columns based on your existing data. Here are some examples of a few of them:"
Bring your Pandas Dataframes to life with D-Tale - KDnuggets,"Up until about 3 months ago he had spent his entire career located in Boston where he grew up working in finance. Working with a team of data scientists and being completely immersed in Python he was able to start building a suite of tools using Flask, Pandas & React on the front-end.  Eventually, the problem of finding a way to visualize Pandas dataframes came along and the result was D-Tale. Along with the support of his company at the time he was able to open-source this software and it eventually gained a lot of traction with the data science community. He has spent the last 15 months working on D-Tale (mostly in my spare time since switching jobs) giving presentations at Boston & San Diego Python user groups as well as FlaskCon this past July"
Going Beyond Superficial: Data Science MOOCs with Substance - KDnuggets,"Data science MOOCs are superficial. At least, a lot of them are. What are your options when looking for something more substantive?"
Going Beyond Superficial: Data Science MOOCs with Substance - KDnuggets,"And he's right. There are a lot of introductory level MOOCs out there, competing with one another for introductory level learners. Which is all well and good, if that's what you are looking for. But what if you are a more advanced learner? What are your options then? Are there even valid advanced level MOOC offerings?"
Going Beyond Superficial: Data Science MOOCs with Substance - KDnuggets,"As Tayo points out, there are lots of alternatives to MOOCs for learning data science. There is certainly no reason to treat MOOCs as the go-to option for any reason. However, beyond the relative predominance of introductory level offerings, there also seems to be a movement dismissing the validity of MOOCs for more advanced purposes outright, and there are 2 main reasons for this"
Going Beyond Superficial: Data Science MOOCs with Substance - KDnuggets,"Point 1 omits, notably, Udacity, and other related pay-only platforms. Point 2 omits a platform like Udemy, which is open to anyone for course hosting. Point 3 allows, theoretically, for the logical chaining of courses to build up in-depth related learning concepts over time"
Going Beyond Superficial: Data Science MOOCs with Substance - KDnuggets,"This specialization gives an introduction to deep learning, reinforcement learning, natural language understanding, computer vision and Bayesian methods. Top Kaggle machine learning practitioners and CERN scientists will share their experience of solving real-world problems and help you to fill the gaps between theory and practice. Upon completion of 7 courses you will be able to apply modern machine learning methods in enterprise and understand the caveats of real-world data and settings"
Going Beyond Superficial: Data Science MOOCs with Substance - KDnuggets,"Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems"
Going Beyond Superficial: Data Science MOOCs with Substance - KDnuggets,"Gain expertise in one of the most fascinating and fastest growing areas of computer science through an innovative online program that covers fascinating and compelling topics in the field of Artificial Intelligence and its applications. This MicroMasters program from Columbia University will give you a rigorous, advanced, professional, graduate-level foundation in Artificial Intelligence. The program represents 25% of the coursework toward a Master's degree in Computer Science at Columbia"
Introduction to Statistics for Data Science - KDnuggets,"A confidence interval is a much more accurate representation of reality. However, there are still some uncertainties left. We can never be 100% confident unless we go through the whole population"
Introduction to Statistics for Data Science - KDnuggets,"For example, we can say that we’re 95% positive that the average men height in our city falls somewhere between 175 cm and 180 cm. Keep in mind that you can never say you are 100% confident since for that you would have to go through the entire population (i. Therefore, there is still a 5% chance that the population parameter is outside the expected range"
Introduction to Statistics for Data Science - KDnuggets,"Let’s go back to the previous example. If you wish to be 95% confident that our population parameter is inside that interval, α must be 5%. Hence, if we wish a higher level of confidence, for example, 99% then α will be 1%"
Introduction to Statistics for Data Science - KDnuggets,"Imagine you wish to become a Data Scientist, and you want to learn how much on average a Data Scientist earns. You got to Glassdoor and start to retrieve salary information from several testimonies. You become aware that the standard deviation (σ) for a Data Scientist salary is around $15,000. Furthermore, make use of the CLT you can assume your sample of 30 salaries (n = 30) are normally distributed"
Introduction to Statistics for Data Science - KDnuggets,"In the table, the value will match the value of 1–0.025=0.975. The corresponding Z comes with the sum of the table row and column associated with that cell"
Introduction to Statistics for Data Science - KDnuggets,1.9+0.06 = 1.96
Introduction to Statistics for Data Science - KDnuggets,"Again, we look it up in Glassdoor, but this time we only find 9 compensations. We know the sample standard deviation is of $13 932, the sample mean of $92 533 and therefore we can calculate a standard error of $4 644. Nevertheless, we are unaware of the population’s variance. Therefore, we’ll use the Student’s T Distribution!"
Introduction to Statistics for Data Science - KDnuggets,"Therefore, our confidence interval will be of [$81 806, $103 261]. Notice that when comparing our two results, we observe that when we know the population variance, we get a narrower confidence interval. In contrast, when we don’t know the population’s variance, there is a higher uncertainty reflected by wider boundaries for our interval"
Data Science Internship Interview Questions - KDnuggets,"Data science is an attractive field. It’s lucrative, you get opportunities to work on interesting projects, and you’re always learning new things. Hence, breaking into the world of data science is extremely competitive. One of the best ways to start your data science career is through a data science internship"
Data Science Internship Interview Questions - KDnuggets,"You should know how to write basic functions and have a fundamental understanding of various data structures and their uses. You should also know about Scikit-learn’s basic (yet essential) capacities, like test_train_split, and StandardScaler. For Pandas, you should be comfortable manipulating DataFrames similar to how you would write a query using SQL"
Let’s Be Honest: We’re Drowning in Data - KDnuggets,"The rise of smart devices, new and ever more complex streams of information, and the proliferation of more and more tools to catalog, process, and interpret all of it has brought us to a place where “keeping up” is in some cases the best we can hope for. Technology suffers from a problem of inheritance, and thus as we innovate, we tend to introduce more complexity than potentially necessary. As systems evolve, those of us who help to design them are constantly weighing options against the “necessity” of technical debt. It may help to understand what options we have going forward by first briefly going back to know the limitations and forces that shaped where we stand now"
Let’s Be Honest: We’re Drowning in Data - KDnuggets,"For the first roughly 30 years in the evolution of Data the world was structured, relational databases were the norm, and there was a limited set of languages an engineer had to know in order to derive a result (mostly dominated by SQL). The tools needed to interact with databases were efficient and could count on mostly clean data. Reports generated in the form of dashboards were relatively easy to interpret, and for their part provided meaningful measures upon which to base decisions. These results became even more insightful with the rise of Statistical Analysis and Data Mining in the 1990s"
Let’s Be Honest: We’re Drowning in Data - KDnuggets,"As the 1990s came to a close we saw the rise of the Internet, and with it a new kind of data: Unstructured. The previous systems of structured content, with their mature analysis tools, and by comparison, easy to understand and clearly defined data types were still present. But unstructured data required a new way of doing things, and also opened new opportunities for insights to those who knew how to use it. Web use created a constant stream of media uploaded to social media sites. We saw the beginnings of a network of sensors and smart devices (that would come to be known as the Internet of Things), all generating more and more data by the day. A new way of processing and understanding all of it was required. Old systems and tools evolved into new ones to keep up, and those who could wield them became what we now know as Data Scientists. We also saw the beginnings of long-term storage moving from physical servers and data warehouses to cloud storage, leading to our industry’s evolution from data center focused to being increasingly diversified among various Cloud solutions and services"
Let’s Be Honest: We’re Drowning in Data - KDnuggets,"Through the 2010s we continued to see exponential growth in unstructured data as mobile devices became more and more commonplace. With their adoption we saw the addition of storage for geo-spatial data, and behavioral data from devices to the mix of unstructured data. Further a growing adoption of “smart devices” only accelerated the creation of, and necessity for storage of more and more data"
Let’s Be Honest: We’re Drowning in Data - KDnuggets,"For many senior leaders who deal with technology infrastructure and/or services, a detailed view of how their systems are structured would run dozens of pages long and include dependencies among multiple vendors and service providers. It would also (in most cases) include separate procedures and processes for dealing with each separate class of data they deal with and have to take into account any regulations and laws that have bearing on that data. Further, these procedures require specialists in each step of the operational chain. Whether it’s analysts and data scientists scrubbing data for hygiene to derive valuable insights from the data on hand, or engineers working on integrating disparate systems so they can speak to each other, each role both contributes to and is constrained by the overall system they work within"
Let’s Be Honest: We’re Drowning in Data - KDnuggets,"This system as it has evolved leads inherently to 3 major problems. First, data has a tendency to sit idle unless it is explicitly needed by someone or something. Second, in order to manage these idle data sets, a company must hire more people to meet the demands their system is generating. Be it analysts and data scientists, or engineers to design"
Let’s Be Honest: We’re Drowning in Data - KDnuggets,"If we envision a system designed around these 6 guiding principles, what would it look like for a modern enterprise and what advantages would they now have? First, and foremost, they would have the flexibility to re-align their work groups into areas of the business where human creativity and input can bring greater value. This may lead to exploring new lines of business or services which would have been cost prohibitive previously, or simply impossible to execute given the constraints of the previous system. Further, a re-imagining of our approach to unstructured data may uncover brand new use cases for the data we already possess with which we are currently doing nothing"
AI Papers to Read in 2020 - KDnuggets,"Artificial Intelligence is one of the most rapidly growing fields in science and is one of the most sought skills of the past few years, commonly labeled as Data Science. The area has far-reaching applications, being usually divided by input type: text, audio, image, video, or graph; or by problem formulation: supervised, unsupervised, and reinforcement learning. Keeping up with everything is a massive endeavor and usually ends up being a frustrating attempt. In this spirit, I present some reading suggestions to keep you updated on the latest and classic breakthroughs in AI and Data Science"
AI Papers to Read in 2020 - KDnuggets,"In 2012, the authors proposed the use of GPUs to train a large Convolutional Neural Network (CNN) for the ImageNet challenge. This was a bold move, as CNNs were considered too heavy to be trained on such a large scale problem. To everyone surprise, they won first place, with a ~15% Top-5 error rate, against ~26% of the second place, which used state-of-the-art image processing techniques"
AI Papers to Read in 2020 - KDnuggets,"The proposed network had 60 million parameters, complete insanity for 2012 standards. Nowadays, we get to see models with over a billion parameters. Reading the AlexNet paper gives us a great deal of insight on how things developed since then"
AI Papers to Read in 2020 - KDnuggets,"This last one achieved super-human performance, solving the challenge. After it, other competitions took over the researchers’ attention. Nowadays, ImageNet is mainly used for Transfer Learning and to validate low-parameter models, such as:"
AI Papers to Read in 2020 - KDnuggets,"MobileNet is one of the most famous “low-parameter” networks. Such models are ideal for low-resources devices and to speed-up real-time applications, such as object recognition on mobile phones. The core idea behind MobileNet and other low-parameter models is to decompose expensive operations into a set of smaller (and faster) operations. Such compound operations are often orders-of-magnitude faster and use substantially fewer parameters"
AI Papers to Read in 2020 - KDnuggets,"Most of us have nowhere near the resources the big tech companies have. Understanding the low-parameter networks is crucial to make your own models less expensive to train and use. In my experience, using depth-wise convolutions can save you hundreds of dollars in cloud inference with almost no loss to accuracy"
AI Papers to Read in 2020 - KDnuggets,Common knowledge is that bigger models are stronger models. Papers such as MobileNet show that there is a lot more to it than adding more filters. Elegance matters
AI Papers to Read in 2020 - KDnuggets,"The paper that introduced the Transformer Model. Prior to this paper, language models relied extensively on Recurrent Neural Networks (RNN) to perform sequence-to-sequence tasks. However, RNNs are awfully slow, as they are terrible to parallelize to multi-GPUs. In contrast, the Transformer model is based solely on Attention layers, which are CNNs that capture the relevance of any sequence element to each other. The proposed formulation achieved significantly better state-of-the-art results and trains markedly faster than previous RNN models"
AI Papers to Read in 2020 - KDnuggets,"Most transformer models are in the order of billions of parameters. While the literature on MobileNets addresses more efficient models, the research on NLP addresses more efficient training. In combination, both views provide the ultimate set of techniques for efficient training and inference"
AI Papers to Read in 2020 - KDnuggets,"Transformer / Attention models have attracted a lot of attention. However, these tend to be resource-heavy models, not meant for ordinary consumer hardware. Both mentioned papers criticize the architecture, providing computationally efficient alternatives to the Attention module. As for the MobileNet discussion, elegance matters"
AI Papers to Read in 2020 - KDnuggets,"Big companies can quickly scale their research to a hundred GPUs. We, normal folks, can’t. Scaling the size of models is not the only avenue for improvement. I can’t overstate that. Reading about efficiency is the best way to ensure you are efficiently using your current resources"
AI Papers to Read in 2020 - KDnuggets,"So far, most papers have proposed new techniques to improve the state-of-the-art. This paper, on the opposite, argues that a simple model, using current best practices, can be surprisingly effective. In sum, they proposed a human pose estimation network based solely on a backbone network followed by three de-convolution operations. At the time, their approach was the most effective at handling the COCO benchmark, despite its simplicity"
AI Papers to Read in 2020 - KDnuggets,"Being simple is sometimes the most effective approach. While we all want to try the shiny and complicated novel architectures, a baseline model might be way faster to code and, yet, achieve similar results. This paper reminds us that not all good models need to be complicated"
AI Papers to Read in 2020 - KDnuggets,"Science moves in baby steps. Each new paper pushes the state-of-the-art a bit further. Yet, it does not need to be a one-way road. Sometimes it is worthwhile to backtrack a bit and take a different turn"
AI Papers to Read in 2020 - KDnuggets,"Many times, what you need is not a fancy new model, just a couple of new tricks. In most papers, one or two new tricks are introduced to achieve a one or two percentage points improvement. However, these are often forgotten amid the major contributions. This paper collects a set of tips used throughout the literature and summarizes them for our reading pleasure"
AI Papers to Read in 2020 - KDnuggets,"Many other tricks exist, some are problem-specific, some are not. A topic I believe deserves more attention is class and sample weights. Consider reading"
AI Papers to Read in 2020 - KDnuggets,"Most of us use Batch Normalization layers and the ReLU or ELU activation functions. In the SELU paper, the authors propose a unifying approach: an activation that self-normalizes its outputs. In practice, this renders batch normalization layers obsolete. Therefore, models using SELU activations are simpler and need fewer operations"
AI Papers to Read in 2020 - KDnuggets,"In the paper, the authors mostly deal with standard machine learning problems (tabular data). Most data scientists deal primarily with images. Reading a paper on purely dense networks is a bit of a refreshment"
AI Papers to Read in 2020 - KDnuggets,"If you break an image into jigsaw-like pieces, scramble them, and show them to a kid, it won’t be able to recognize the original object; a CNN might. In this paper, the authors found that classifying all 33x33 patches of an image and then averaging their class predictions achieves near state-of-the-art results on ImageNet. Moreover, they further explore this idea with VGG and ResNet-50 models, showing evidence that CNNs rely extensively on local information, with minimal global reasoning"
AI Papers to Read in 2020 - KDnuggets,"The lottery analogy is seeing each weight as a “lottery ticket. However, most of the tickets won’t win, only a couple will. If you could go back in time and buy only the winning tickets, you would maximize your profits. In the end, you will get a better performing network"
AI Papers to Read in 2020 - KDnuggets,"As for the Bag-of-Features paper, this sheds some light on how limited our current understanding of CNNs is. After reading this paper, I realized how underutilized our millions of parameters are. An open question is how much. The authors managed to reduce networks to a tenth of their original sizes, how much more might be possible in the future?"
AI Papers to Read in 2020 - KDnuggets,"These ideas also give us more perspective on how inefficient behemoth networks are. Consider the Reformer paper, mentioned before. It drastically reduced the size of the Transformer by improving the algorithm. How much more could be reduced by using the lottery technique?"
AI Papers to Read in 2020 - KDnuggets,"Pix2Pix and CycleGAN are the two seminal works on conditional generative models. Both perform the task of converting images from a domain A to a domain B and differ by leveraging paired and unpaired datasets. The former perform tasks such as converting line drawings to fully rendered images, and the latter excels at replacing entities, such as turning horses into zebras or apples into oranges. By being “conditional,” these models allow users to have some degree of control over what is being generated by tweaking the inputs"
AI Papers to Read in 2020 - KDnuggets,"With these twelve papers and their further readings, I believe you already have plenty of reading material to look at. This surely isn’t an exhaustive list of great papers. However, I tried my best to select the most insightful and seminal works I have seen and read. Please let me know if there are any other papers you believe should be on this list"
Feature Engineering for Numerical Data - KDnuggets,"Numeric data is almost a blessing. Why almost? Well, because it is already in a format that is ingestible by Machine Learning models. However, if we translate it into human-relatable terms, just because a PhD level textbook is written in English — I speak, read and write in English — does not mean that I am capable of understanding the textbook well enough to derive useful insights. What would make the textbook useful to me is if it epitomizes the most important information in a manner that considers the assumptions of my mental model, such as “Maths is a myth” (which, by the way, is no longer my view since I am really starting to enjoying it). In the same way, a good feature should represent salient aspects of the data, as well as taking the shape of the assumptions that are made by the Machine Learning model"
Feature Engineering for Numerical Data - KDnuggets,"Feature engineering is the process of extracting features from raw data and transforming them into formats that can be ingested by a machine learning model. Transformations are often required to ease the difficulty of modelling and boost the results of our models. Therefore, techniques to engineer numeric data types are fundamental tools for Data Scientist (Machine Learning Engineers alike) to add to their artillery"
Feature Engineering for Numerical Data - KDnuggets,"As we strive for mastery, it is important to note that it is never enough to know why a mechanism works and what it can do. Mastery knows how something is done, has an intuition for the underlying principles, and has the neural connections that make drawing the correct tool a seamless procedure when faced with a challenge. That will not come from reading this article, but from the deliberate practice of which this article will open the door for you to do by providing the intuition behind the techniques so that you may understand how and when to apply them"
Feature Engineering for Numerical Data - KDnuggets,"There may be occasions where data is collected on a feature that accumulates, thereby having an infinite upper boundary. Examples of this type of continuous data may be a tracking system that monitors the number of visits that all of my blog posts receive on a daily basis. This type of data easily attracts outliers since there could be some unpredictable event that affects the total traffic that my articles are accumulating. For instance, one day, people may decide they want to be able to do data analysis, so my article on"
Feature Engineering for Numerical Data - KDnuggets,"This method contains the scale of the data by grouping the values into bins. Therefore, quantization maps a continuous value into a discrete value, and, conceptually, this can be thought of as an ordered sequence of bins. To implement this, we must consider the width of bins that we create, of which the solutions fall into two categories, fixed-width bins or adaptive bins"
Feature Engineering for Numerical Data - KDnuggets,"Note that if the values span across a large magnitude of numbers, then a better method may be to group the values into powers of a constant, such as to the power of 10: 0–9, 10–99, 100–999, 1000–9999. Notice that the bin widths grow exponentially, hence in the case of 1000–9999, the bin width is O(10000), whereas 0–9 is O(10). Take the log of the count to map from the count to the bin of the data"
Feature Engineering for Numerical Data - KDnuggets,"Why would we want to transform our data to fit the Normal Distribution? Great question! You may want to use a parametric model — a model that makes assumptions of the data — rather than a non-parametric model. When the data is normally distributed, parametric models are powerful. However, in some cases, the data we have may need a helping hand to bring out the beautiful bell-shaped curve of the normal distribution. For instance, the data may be skewed, so we apply a power transformation to assist in helping our feature look more Gaussian"
Feature Engineering for Numerical Data - KDnuggets,"As the name implies, feature scaling (also referred to as feature normalization) is concerned with changing the scale of features. When the features of a dataset differ greatly in scale, then a model that is sensitive to the scale of the input features (i. Ensuring features are within a similar scale is imperative. Whereas, models such as tree-based models (i"
Feature Engineering for Numerical Data - KDnuggets,"There we have it. In this article, we discussed techniques to deal with numerical features, such as quantization, power transformations, feature scaling, and interaction features (which can be applied to various data types). This is by no means the be-all and end-all of feature engineering, and there is always much more to learn on a daily basis. Feature engineering is an art and will take practice, so now that you have the intuition, you are ready to begin practicing"
Setting Up Your Data Science & Machine Learning Capability in Python - KDnuggets,"Your business impact can be measured in many ways. The most high-level objectives are cost optimization, risk optimization, and revenue growth. You may focus on a variety of specific metrics within each objective, such as customer acquisition cost optimization, churn prediction, fraud detection, patient health outcomes, or personalized product recommendations"
Setting Up Your Data Science & Machine Learning Capability in Python - KDnuggets,"This a re-framing of the classic Buy vs. Build discussion in the context of many DSML platforms offering “pay as you go” pricing now, much like Amazon Web Services. I feel it’s necessary to rephrase the discussion because, unlike “Buying” where you pay a fixed cost, whether or not you use it, “Renting” implies that you only pay for when you use it. This is much more convenient for the end-user"
Setting Up Your Data Science & Machine Learning Capability in Python - KDnuggets,"Your team is tasked with developing a customer churn model. If you could predict churn, sales could take proactive measures to retain more accounts. Your company generates $100M in annual sales, and there’s an opportunity to reduce churn from 10% to 5%, or by $5M annually. To keep it simple, we’ll assume you’re a SaaS company with 100% gross margins"
Setting Up Your Data Science & Machine Learning Capability in Python - KDnuggets,"Now let’s assume in both scenarios your team is 9 FTEs, but in the renting scenario, all 9 are dedicated to Data Science & ML. A team of 9 FTEs can produce 50% more output than a team of 6 FTEs, so with the spare capacity, you take on a second project around customer personalization. Let’s assume this project could result in 5% higher software sales in year 1"
Setting Up Your Data Science & Machine Learning Capability in Python - KDnuggets,"Notice that in the renting scenario, you’re actually spending more money, but with the same team size, you can generate higher ROI. By shifting labor spend to Data Science & ML from DevOps, your team is more efficient and can tackle more positive ROI projects in the same period. The owning scenario carries an inherent opportunity cost, which is not inherent in the renting scenario"
Setting Up Your Data Science & Machine Learning Capability in Python - KDnuggets,"I have spoken with hundreds of DSML leaders in the past couple of years. A good portion of them lead their teams into owning DSML architecture without renting, and without assessing the obvious and hidden costs of owning. All too often, they turn back halfway, realizing renting is cheaper, easier, more flexible, and allows them to stay focused. Furthermore, many developers on the teams expected they would be only part of building the architecture upfront, but later had to serve in full-time support roles, spending much less time on interesting scientific projects they joined the company for!"
5 Apache Spark Best Practices For Data Science - KDnuggets,Let’s start with defining skewness. As we mentioned our data is divided to partitions and along the transformations the size of each partition would likely change. This can create a wide variation in size between partitions which means we have a skewness in our data
5 Apache Spark Best Practices For Data Science - KDnuggets,"As we mentioned, by having more tasks than cores we hope that while the longer task is running other cores will remain busy with the other tasks. Although this is true, the ratio mentioned earlier (2-4:1) can’t really address such a big variance between tasks duration. We can try to increase the ratio to 10:1 and see if it helps, but there could be other downsides to this approach"
5 Apache Spark Best Practices For Data Science - KDnuggets,"This one was a real tough one. As we mentioned Spark uses lazy evaluation, so when running the code — it only builds a computational graph, a DAG. But this method can be very problematic when you have an iterative process, because the DAG reopens the previous iteration and becomes very big, I mean very very big . This might be too big for the driver to keep in memory. This problem is hard to locate because the application is stuck, but it appears in the Spark UI as if no job is running (which is true) for a long time — until the driver eventually crashes"
Know What Employers are Expecting for a Data Scientist Role in 2020 - KDnuggets,"Recently, I actively started looking for a job change to Data science, I don’t have any formal education like Masters or Ph.D. I started learning it completely out of my own interest (not just because of hype). It was one of the challenging tracks to opt-in especially if you are working simultaneously on some other technology. I started my journey by enrolling myself in many MOOCs (Massive Open Online Courses) and started reading multiple blogs. Initially, it didn’t make sense, eventually after reading other people’s code and getting my hands dirty with real-time datasets. It slowly started making sense"
Know What Employers are Expecting for a Data Scientist Role in 2020 - KDnuggets,"When I started searching for jobs, there began a new interesting story. I opened a top job portal in India and started searching for jobs, I found few jobs that were relevant to what I was looking for but when I opened one of them, to my surprise the requirements they mentioned were something new to me. Leaving traditional Data analysis, Machine learning, Deep learning apart, some ETL tools and multiple Big Data technologies were mentioned as required skills. I thought It’s okay since every company has their own definition of a data scientist these days and opened another job. This time it showed up with a requirement of some other technologies like AWS, Azure and Power BI"
Know What Employers are Expecting for a Data Scientist Role in 2020 - KDnuggets,"Remember all these openings were tagged under Data scientist only. All these openings have common requirements like Machine learning algorithms, Statistics, Data Analysis, Data cleaning and Deep learning techniques. Along with these skills, a few companies were expecting the candidates to have knowledge in the cloud (AWS, Azure, or GCP) and data visualization tools like Tableau, Power BI, and ETL tools like SSIS. Usually, these technologies are more to do with Data Analyst/Data Engineer roles but the Data scientist role is still evolving and doesn’t really stick to a particular skillset yet"
First Steps of a Data Science Project - KDnuggets,"Many data science projects are launched with good intentions, but fail to deliver because the correct process is not understood. To achieve good performance and results in this work, the first steps must include clearly defining goals and outcomes, collecting data, and preparing and exploring the data. This is all about solving problems, which requires a systematic process"
First Steps of a Data Science Project - KDnuggets,"To be able to have that, we need a plan and a methodology to do a data science project. Sadly, most data science projects fail because the people involved don’t understand clearly what they have to do, or what are the most important things for a company. Your solution needs to be tied to the goals and objectives of the company or its departments"
First Steps of a Data Science Project - KDnuggets,There are several ways of working in the data space and several roles as well. But the common thing they have is that they all use data. And you want to be able to have the best possible data when solving business problems
First Steps of a Data Science Project - KDnuggets,"An amazing amount of data is out there just waiting for you, ready to go, and it's what's called open data. Now, the idea with open data is this. It's data that is free, it's easily accessible, and it's downloadable in convenient forms like, for instance, a CSV or comma-separated values, which is a common form of spreadsheet data"
First Steps of a Data Science Project - KDnuggets,"You need to remember that Data science isn’t about software, knowing how to code, or being able to read data from different databases. It is about solving problems. An analogy would be saying that physics isn’t about calculus, moving objects, algebra; it’s about studying nature, understanding it, and modeling it"
First Steps of a Data Science Project - KDnuggets,"Data science goes from data to value, but we need to have a good starting point. If you have a poor understanding of the business context, even if you are an expert in machine learning, you will not be able to solve the problem. If you don’t collect the data following a systematic process that enables you to get the best sources of information and use the data you already have, it’s impossible to achieve your goals, and finally, if you don’t take the time to analyze and prepare the data, you will not be able to validate the last steps and come up with hypotheses for the drivers of the identified problems"
Labelling Data Using Snorkel - KDnuggets,"In this tutorial, we will walk through the process of using Snorkel to generate labels for an unlabelled dataset. We will provide you examples of basic Snorkel components by guiding you through a real clinical application of Snorkel. Specifically, we will use Snorkel to try to boost our results in predicting"
Labelling Data Using Snorkel - KDnuggets,"Snorkel is a system that facilitates the process of building and managing training datasets without manual labelling. The first component of a Snorkel pipeline includes labelling functions, which are designed to be weak heuristic functions that predict a label given unlabelled data. The labelling functions that we developed for MS severity score labelling were the following:"
Labelling Data Using Snorkel - KDnuggets,"To reiterate, in this article we demonstrate label generation for MS severity scores. A common measurement of MS severity is EDSS or the Expanded Disability Status Scale. This is a scale that increases from 0 to 10 depending on the severity of MS symptoms. We will refer to EDSS in general as the MS severity score but for our keen readers we thought we would provide this information. This score is further described"
Labelling Data Using Snorkel - KDnuggets,"Labelling functions allow you to define weak heuristics and rules that predict a label given unlabelled data. These heuristics can be derived from expert knowledge or other labelling models. In the case of MS severity score prediction, our labelling functions included: key-word search functions derived from clinicians, baseline models trained to predict MS severity scores (tf-idf, word2vec cnn, etc"
Labelling Data Using Snorkel - KDnuggets,"As you will see below, you mark labelling functions by adding “@labeling_function()” above the function. For each labelling function, a single row of a dataframe containing unlabelled data (i. Each labelling function applies heuristics or models to obtain a prediction for each row. If the prediction is not found, the function abstains (i"
Labelling Data Using Snorkel - KDnuggets,"Below shows an example of a key-word search (using regular expressions) used to extract MS severity scores recorded in decimal form. The regular expression functions are applied to attempt to search for the MS severity score recorded in decimal form. If found, the function returns the score in the appropriate output format. Else, the function abstains (i"
Labelling Data Using Snorkel - KDnuggets,"Above we see an example using a key-word search. To integrate a trained classifier, you must perform one extra step. That is, you must train and export your model before creating your labelling function. Here is an example of how we trained a logistic regression that was built on top of tf-idf features"
Labelling Data Using Snorkel - KDnuggets,"To take advantage of Snorkel’s full functionality, we used the ‘Label Model’ to generate a single confidence-weighted label given a matrix of predictions obtained from all the labelling functions (i. L_unlabelled). The Label Model predicts by learning to estimate the accuracy and correlations of the labelling functions based on their agreements and disagreements"
Labelling Data Using Snorkel - KDnuggets,"Snorkel provides some more evaluation tools to help you understand the quality of your labelling functions. In particular, ‘get_label_buckets’ is a handy way to combine labels and make comparisons. For more information, read the"
Labelling Data Using Snorkel - KDnuggets,"Following the procedure outlined above, we developed various labelling functions based on key-word searches, baseline models, and our MS-BERT classifier. We experimented with various ensembles of labelling functions and used Snorkel’s Label Model to obtain predictions for a held-out labelled dataset. This allowed us to determine which ensemble of labelling functions would be best to label our unlabelled dataset"
Labelling Data Using Snorkel - KDnuggets,"As shown in the table below, we observed that the MS-BERT classifier (MSBC) alone outperformed all ensembles that contain itself by at least 0.02 on Macro-F1. The addition of weaker heuristics and classifiers consistently decreased the ensemble’s performance. Furthermore, we observed that the amount of conflict for the MS-BERT classifier increased as weaker classifiers and heuristics were added to the ensemble"
Labelling Data Using Snorkel - KDnuggets,"To understand our findings, we have to remind ourselves that Snorkel’s label model learns to predict the accuracy and correlations of the labelling functions based on agreements and disagreements amongst each other. Therefore in the presence of a strong labelling function, such as our MS-BERT classifier, the addition of weaker labelling functions introduces more disagreements with the strong labelling functions and therefore decreases performance. From these findings, we learned that Snorkel may be more suited for situations where you"
Labelling Data Using Snorkel - KDnuggets,"The MS-BERT classifier was used to obtain ‘silver’ labels for our unlabelled dataset. These ‘silver’ labels were combined with our ‘gold’ labels to obtain a silver+gold dataset. To infer the quality of the silver labels, new MS-BERT classifiers were developed: 1) MS-BERT+ (trained on silver+gold labelled data); and 2) MS-BERT-silver (trained on silver labelled data). These classifiers were evaluated on a held-out test dataset that was previously used to evaluate our original MS-BERT classifier (trained on gold labelled data). MS-BERT+ achieved a Macro-F1 of 0.86238 and a Micro-F1 of 0.92569, and MS-BERT-silver achieved a Macro-F1 of 0.82922 and a Micro-F1 of 0.91442. Although their performance was slightly lower that our original MS-BERT classifier (Macro-F1 of 0.88296, Micro-F1 of 0.94177), they still outperformed the previous best baseline models for MS severity prediction. The strong results of MS-BERT-silver helps show the effectiveness of using our MS-BERT classifier as a labelling function. It demonstrates potential to reduce tedious hours required by a professional to read through a patient’s consult note and manually generate MS severity scores"
Labelling Data Using Snorkel - KDnuggets,"We would like to thank the researchers and staff at the Data Science and Advanced Analytics (DSAA) department, St. Michael’s Hospital, for providing consistent support and guidance throughout this project. We would also like to thank Dr. Marzyeh Ghassemi, and Taylor Killan for providing us the opportunity to work on this exciting project. Lastly, we would like to thank Dr. Tony Antoniou and Dr. Jiwon Oh from the MS clinic at St. Michael’s Hospital for their support on the neurological examination notes"
Data Science MOOCs are too Superficial - KDnuggets,"Data Science, Machine Learning, and Analytics are considered to be among the hottest career paths. The demand for skilled data science practitioners in industry, academia, and the government is rapidly growing. The ongoing “"
Data Science MOOCs are too Superficial - KDnuggets,"MOOCs, for the most part, are free online courses available for anyone to enroll. MOOCs provide an affordable and flexible way to learn new skills. MOOCs cover a broad spectrum of online courses in leadership, analytics, data science, machine learning, professional skills, engineering, business & management, humanities, computer science, and much more. These courses are usually offered by top universities across the world like MIT, Harvard, UC Berkeley, University of Michigan, EPFL, Hong Kong Polytechnic University, The University of Queensland, and much more. Some courses are also offered by big corporations such as IBM, Google, and Microsoft. The greatest advantage of MOOC is the opportunity to learn from leaders and experts, and the privilege of taking courses from the world’s top universities"
Data Science MOOCs are too Superficial - KDnuggets,"Most data science MOOC are introductory-level courses. These courses are good for individuals that already have a solid background in a complementary discipline (physics, computer science, mathematics, engineering, accounting) are trying to get into the field of data science. In my journey to data science, I found the following 3 data science specializations to be among the best in terms of quality and rigor"
Data Science MOOCs are too Superficial - KDnuggets,"The author explains fundamental concepts in machine learning in a way that is very easy to follow. Also, code is included, so you can actually use the code provided to practice and build your own models. I have personally found this book to be very useful in my journey as a data scientist. I would recommend this book to any data science aspirant. All that you need is basic linear algebra and programming skills to be able to understand the book. Other excellent data science textbooks are “"
Data Science MOOCs are too Superficial - KDnuggets,"From my personal experience, I have learned a lot from weekly group conversations on various topics in data science and machine learning by teaming up with other data science aspirants. Network with other data science aspirants, share your code on GitHub, showcase your skills on LinkedIn or Medium, this would help you to learn a lot of new concepts and tools within a short period. You also get exposed to new ways of doing things, as well as to new algorithms and technologies"
Data Science MOOCs are too Superficial - KDnuggets,"In summary, we’ve discussed the advantages and disadvantages of data science MOOC. If you have a solid background in an analytic discipline such as physics, mathematics, economics, engineering, or computer science, and you are interested in exploring the field of data science, the best way is to begin with MOOC. Then after establishing a solid foundation, you may then seek other ways to increase your knowledge and expertise, such as studying from textbooks, engaging in projects, and networking with other data science aspirants"
Foundations of Data Science: The Free eBook - KDnuggets,"First off, it should be noted that this book is not structured like a typical data science book. Neither its chapters nor their progression fit the mold of a standard contemporary data science text in my view. You can see, from the table of contents listed below, that the text really surveys a wide array of disparate topics, as opposed to simply creating an equivalency between data science and machine learning, for example, and progressing as such:"
Foundations of Data Science: The Free eBook - KDnuggets,"Matrix factorization, graph theory, kernel methods, clustering theory, streaming, gradients descent, data sampling; these are all concepts that will serve you well later, when it comes to solving data science problems, and they are all essential building blocks to implementing more complex approaches as well. You won't be able to understand neural networks without gradient descent. You can't analyze social media networks without graph theory. The models you build won't be of value if you can't understand when and why you would sample from data"
Foundations of Data Science: The Free eBook - KDnuggets,"There is no code. There are no Python libraries being leaned on. There is no hand-waviness. There are only thorough explanations leading to understanding of these varied topics, should you spend the necessary time reading"
Foundations of Data Science: The Free eBook - KDnuggets,"While traditional areas of computer science remain highly important, increasingly researchers of the future will be involved with using computers to understand and extract usable information from massive data arising in applications, not just how to make computers useful on specific well-defined problems. With this in mind we have written this book to cover the theory we expect to be useful in the next 40 years, just as an understanding of automata theory, algorithms, and related topics gave students an advantage in the last 40 years. One of the major changes is an increase in emphasis on probability, statistics, and numerical methods"
Foundations of Data Science: The Free eBook - KDnuggets,"In many contemporary books, data science has been reduced to a series of programming tools which, if mastered, promise to do the data science for you. There seems to be less emphasis on the underlying concepts and theory divorced from code. This book is a good example of the opposite to this trend, a book which will undoubtedly arm you with the theoretical knowledge necessary to approach a career in data science with a strong set of foundations"
How Natural Language Processing Is Changing Data Analytics - KDnuggets,"Natural language processing (NLP) is the process by which computers understand and process natural human language. If you use Google Search, Alex, Siri, or Google Assistant, you’ve already seen it at work. The advantage of NLP is that it allows users to make queries without first having to translate them into “computer-speak"
How Natural Language Processing Is Changing Data Analytics - KDnuggets,"For example, a retail marketing manager might want to determine the demographics of customers who spend the most per purchase and target those customers with special offers or loyalty rewards. A manufacturing shift leader might want to test different methods within its operations to determine which one yields the greatest efficiency. With NLP, the commands needed to get this information can be executed by anyone in the business"
How Natural Language Processing Is Changing Data Analytics - KDnuggets,"NLP is not yet widespread. According to the InformationWeek article, “A few BI and analytics vendors are offering NLP capabilities but they're in the minority for now. More will likely enter the market soon to stay competitive"
A Layman’s Guide to Data Science. Part 3: Data Science Workflow - KDnuggets,"There is no specific template for solving any data science problem (otherwise, you’d see it in the first textbook you come across). Each new dataset and each new problem will lead to a different roadmap. However, there are similar high-level steps in many different projects"
A Layman’s Guide to Data Science. Part 3: Data Science Workflow - KDnuggets,"As you already know, at the starting point, you’re asking questions and trying to get a handle on what data you need. Therefore, think of the problem you are trying to solve. What do you want to learn more about? For now, forget about modeling, evaluation metrics, and data science-related things. Clearly stating your problem and defining goals are the first step to providing a good solution. Without it, you could lose the track in the data-science forest"
A Layman’s Guide to Data Science. Part 3: Data Science Workflow - KDnuggets,"It is possible to reformat and clean the data either manually or by writing scripts. Getting all of the values in the correct format can involve stripping characters from strings, converting integers to floats, or many other things. Afterward, it is necessary to deal with missing values and null values that are common in sparse matrices. The process of handling them is called"
A Layman’s Guide to Data Science. Part 3: Data Science Workflow - KDnuggets,"There are many tools that help you understand your data quickly. You can start by checking out the first few rows of the data frame to get the initial impression of the data organization. Automatic tools incorporated in multiple libraries, such as Pandas’ . With this information, you’ll be able to determine which variable is our target and which features we think are important"
A Layman’s Guide to Data Science. Part 3: Data Science Workflow - KDnuggets,"Therefore, the key task of the secondary modeling step is parameter tuning. Each algorithm has a set of parameters you can optimize. Parameters are the variables that a machine learning technique uses to adjust to the data. Hyperparameters that are"
A Layman’s Guide to Data Science. Part 3: Data Science Workflow - KDnuggets,"After inspecting a set of output files, a data scientist, or a group of data scientists can make comparisons between output variants and explore alternative paths by adjusting script code and/or execution parameters. Much of the data analysis process is trial-and-error: a scientist runs tests, graphs the output, reruns them, graphs the output, and so on. Therefore, graphs are the central comparison tool that can be displayed side-by-side on monitors to visually compare and contrast their characteristics. A supplementary tool is taking notes, both physical and digital, to keep track of the line of thought and experiments"
A Layman’s Guide to Data Science. Part 3: Data Science Workflow - KDnuggets,"You can showcase your results with a presentation and offer a technical overview of the process. Remember to keep your audience in mind: go into more detail if presenting to fellow data scientists or focus on the findings if you address the sales team or executives. If your company allows publishing the results, it is also a good opportunity to have feedback from other specialists. Additionally, you can write a blog post and push your code to GitHub so the data science community can learn from your success. Communicating your results is an important part of the scientific process, so this phase should not be overlooked"
"Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions - KDnuggets","The problem is hundreds of thousands of plastics exist as possible candidates for the membranes. That sounds like a good thing since it offers an abundance of choices. However, since each one varies in its chemical makeup, testing and manufacturing a particular material is an exceptionally time-consuming and expensive endeavor. These realities mean researchers have"
"Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions - KDnuggets","The researchers then made the suggested polymers and turned them into thin films. The algorithm showed that despite the lack of a trial, those materials would exceed the performance of membranes currently used to separate carbon dioxide from methane. Real-life tests showed that the polymers separated gases with a capability close to what the algorithm predicted"
"Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions - KDnuggets","While outlining the benefits of the new process, he said, ""It removes the guesswork and the old trial-and-error work, which is very ineffective. You don't have to make hundreds of different materials and test them. Now you're letting the machine learn. It can narrow your search"
"Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions - KDnuggets","Another member of the research team compared this machine learning-driven method to Netflix helping people find the movies they like most. Algorithms assess what a person watched before, then assign percentage scores to available titles to tell a viewer how appropriate of a match they are. Here, machine learning examined characteristics of polymers proven effective for gas separation to increase the chances of good results when using a new polymer"
"Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions - KDnuggets","Sanat K. Kumar, a chemical engineering professor from Columbia University who worked on this project, believes this new application of big data could lead to better polymer designs. Then, researchers may feel more compelled to look at materials they never considered before, possibly ending up with superior results"
"Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions - KDnuggets","Kumar weighed in by saying, ""This work thus points to a new way of materials design. Rather than test all the materials that exist for a particular application, you look for the part of a material that best serves the need that you have. When you combine the very best materials, then you have a shot at designing a better material"
"Data Scientists Have Developed a Faster Way to Reduce Pollution, Cut Greenhouse Gas Emissions - KDnuggets","People who participated in this work said that this process is commercially viable. One of the near-term applications for separating carbon dioxide from methane with a membrane material discovered through this process is in the natural gas industry. Carbon dioxide causes pipeline corrosion, providing a reason to efficiently and thoroughly remove it"
Data Cleaning: The secret ingredient to the success of any Data Science Project - KDnuggets,"In the last few blogs, we saw how to summarise and analyse the data using statistical methods and visualisations. But the raw data need to be processed to convert it into a usable shape. Data Preparation is the most important and foremost part of Data Science. It involves data pre-processing and data wrangling"
Data Cleaning: The secret ingredient to the success of any Data Science Project - KDnuggets,"Raw data is collected from various sources and usually unsuitable for analysis. For example, there might be many entries for a shopper leading to duplicity, or there might a typo in noting down the email id of a customer, or some of the questions might have been left blank by a surveyor. With an uncleaned dataset, no matter what type of algorithm you try, you will never get accurate results. That is why data scientists spend a considerable amount of time on data cleaning"
Data Cleaning: The secret ingredient to the success of any Data Science Project - KDnuggets,"Data is usually collected from many sources and joined together to form one dataset. This might lead to the duplicity of observations. This might not be problematic if observations (small in number) are repeated a few times but could lead to erroneous behaviour if observation(s) are repeated far too many times. Hence, it is better to drop duplicate observations to have a cleaner dataset"
Data Cleaning: The secret ingredient to the success of any Data Science Project - KDnuggets,"If there are outliers in the target variable, tree-based algorithms are good but care must be taken to choose the loss function. Reason being that if we use the mean squared error function, then the difference is squared and would highly influence the next tree since boosting attempts to fit the (gradient of the) loss. However, there are more robust error functions that can be used for boosted tree methods like Huber Loss and Mean Absolute Deviation loss"
Software engineering fundamentals for Data Scientists - KDnuggets,"As a data scientist writing code for your models, it's quite possible that your work will make its way into a production environment to be used by the masses. But, writing code that is deployed as software is much different than writing code for exploratory data analysis. Learn about the key approaches for making your code production-ready that will save you time and future headaches"
Software engineering fundamentals for Data Scientists - KDnuggets,"As a field, Data Science has caused polemic with other disciplines ever since it started to grow in popularity. Statisticians complain about the lack of fundamental statistics knowledge that’s often observed by practitioners, mathematicians argue against the application of tools without a solid understanding of the principles applied, and software engineers point at data scientists’ ignorance of basic principles while programming. And to be honest, they all have a point. In terms of statistics and maths, it is true that you need to have a solid understanding of concepts such as probability, algebra, and calculus. How deep that knowledge needs to be? Well, that depends a lot on your role, but the basics are not negotiable. A similar thing happens when it comes to programming; if your role implies writing production code, then you need to know at least the fundamentals in software engineering. Why? The reasons are many, but I reckon they could be summarised in a nutshell according to five principles:"
Software engineering fundamentals for Data Scientists - KDnuggets,"In theory, almost everything we’ll cover in this story could be considered tools or tips for writing cleaner code. However, in this specific section, we’ll focus on the strict definition of the word clean. As Robert Martin says in his book"
Software engineering fundamentals for Data Scientists - KDnuggets,"This one I reckon is one of the most important points for data scientists and data analysts and a very common source of discussion with software engineerings given that we’re very used to code in tools such as Jupyter Notebooks. These tools that are amazing for Exploratory Data Analysis, but not meant for writing production code. In fact, Python is by nature an object-oriented programming language, it is not within this scope to cover in-depth what does that means. But, in short, unlike procedural programming, where you code a list of instructions for a script to execute, object-oriented programming is about building modules with their own characteristics and actions. Take the following example:"
Software engineering fundamentals for Data Scientists - KDnuggets,"In practice, these characteristics are known as attributes, and the actions will be methods. In the example above, the objects Computer and Printer would be independent classes. A class is a blueprint containing the attributes and methods of all the objects of that specific type. I. The concept behind this idea is called encapsulation. Encapsulation means that you can combine functions and data all into a single entity or module. And when you break a program into modules, different modules don’t need to know how something is accomplished if they are not responsible for doing it. And why is this useful? Well, not only for the code to be reusable, avoiding repetition, and gaining efficiency across classes of your code as mentioned before but it also makes it easier to debug if needed"
Software engineering fundamentals for Data Scientists - KDnuggets,"I reckon that the definition speaks by itself, but apart from it, we could add that refactoring gives us a chance to clean and modularize our code after we've got it working. It gives us as well a chance to improve the efficiency of our code. And what I’ve learned so far, is that when a software engineer talks about efficient code, they usually refer to either of these:"
Software engineering fundamentals for Data Scientists - KDnuggets,"When you’re doing some analysis in your Jupyter Notebook, it doesn’t matter if calculating those pairwise distances takes you two, five, or ten minutes. You can leave it running, answer some Slack messages, go to the bathroom, fill up a cup of coffee, and come back to see your code done. However, what happens when there’s a user waiting on the other side? You can’t just leave them hanging while your code compiles, right?"
Software engineering fundamentals for Data Scientists - KDnuggets,"I know that Pandas Dataframes are super easy to use, and we all Pythonistas love them. However, when writing production code, it is better to simply avoid them. Much of the operations we perform with Pandas can be done as well with Numpy. Let’s see some other examples:"
Software engineering fundamentals for Data Scientists - KDnuggets,"Parallelization implies writing a script to process data in parallel, using several or all the available processors in the machine. Why can this lead us to a massive improvement in speed? Because most of the time, our scripts compute data serially: they solve one problem, and then the following, and then the following, and so on. When we write code in Python, that’s usually what happens, and if we want to take advantage of parallelization, we have to be explicit about it. I’ll be writing an independent story about this soon, however, if you’re eager for learning more about it, among all the libraries available for parallelization, my favourites so far are:"
Software engineering fundamentals for Data Scientists - KDnuggets,"If you delete objects, then the memory is available to new Python objects, but it won’t be free() back to the system. Moreover, as mentioned before, Pandas is a great tool for Exploratory Data Analysis, but apart from being slower for production code, it is also quite expensive in terms of memory. However, there a few things we can do to keep that memory use under control:"
Software engineering fundamentals for Data Scientists - KDnuggets,"Tests in Data Science are needed. Other software related areas usually complain about the lack of testing in data scientist’s code. While in other kinds of algorithms or scripts, the program might just stop working if there’s a mistake, in Data Science this is even more dangerous cause the program might actually run, but end up with wrong insights and recommendations due to values encoded incorrectly, features being used inappropriately or data breaking assumptions that the models are actually based on"
Software engineering fundamentals for Data Scientists - KDnuggets,"Pytest requires you to create a python script containing the function or functions to be tested, along with another set of functions asserting the output. The file needs to be saved with the prefix ‘test’, and then it just needs to be run as any other Python script. Pytest was originally developed to use from the command line, but there’s a hacky way of using it from a Jupyter Notebook if you’re still at early stages of a project; you can create and save a"
Software engineering fundamentals for Data Scientists - KDnuggets,"In the future, I’ll write another story to talk about more complex examples of unit tests, and how to test an entire class if that’s what you need. But meanwhile, this should be more than enough to get you started and test some of your functions. Mind that in the examples above, I’m testing the exact number being returned, but you could test as well the shape of a dataframe, the length of a NumPy array, the type of the object returned, etc"
Software engineering fundamentals for Data Scientists - KDnuggets,"This approach or methodology for testing consists of writing the unit tests to be performed for a piece of code, even before starting to develop. Next step, you’ll want to write the simplest and/or quickest piece of code you can as to pass the tests you wrote down initially, this will help you to ensure quality, by focusing on the requirements before writing your code. Also, it will force you to keep the code simple, clean, and testable by breaking it down into small chunks of code, in accordance with the test or tests are written initially. Once you have a piece of code that actually passes those tests, just then, you’ll focus on refactoring for improving the quality or further functionalities of your code"
Software engineering fundamentals for Data Scientists - KDnuggets,"Code reviews benefit everyone in a team to promote best programming practices and prepare code for production. The main goal of code reviews is to catch errors. However, they are also helpful for improving readability and check that the standards are met among a team, so no dirty or slow code is fed into production. Beyond these points, code reviews are also great for sharing knowledge, as members of the team get to read pieces of code from people with different backgrounds and styles"
Software engineering fundamentals for Data Scientists - KDnuggets,"Nowadays, the tool that is excellent for code reviewing is the platform GitHub with pull requests. A pull request is a solicitude for integrating a change in a piece of code or an entirely new script, into a certain code environment. It is called pull request because their submission implies precisely requesting someone to pull the code you wrote into the repository"
How Much Math do you need in Data Science? - KDnuggets,"Thanks to these packages, anyone can build a model or produce a data visualization. However, very solid background knowledge in mathematics is essential for fine-tuning your models to produce reliable models with optimal performance. It is one thing to build a model, and it is another thing to interpret the model and draw out meaningful conclusions that can be used for data-driven decision making. It’s important that before using these packages, you have an understanding of the mathematical basis of each, that way you are not using these packages simply as black-box tools"
How Much Math do you need in Data Science? - KDnuggets,"Without a sound math background, you wouldn’t be able to address the questions raised above. The bottom line is that in data science and machine learning, mathematical skills are as important as programming skills. As a data science aspirant, it is therefore essential that you invest time to study the theoretical and mathematical foundations of data science and machine learning. Your ability to build reliable and efficient models that can be applied to real-world problems depends on how good your mathematical skills are. To see how math skills are applied in building a machine learning regression model, please see this article:"
How Much Math do you need in Data Science? - KDnuggets,"Linear algebra is the most important math skill in machine learning. A data set is represented as a matrix. Linear algebra is used in data preprocessing, data transformation, dimensionality reduction, and model evaluation"
How Much Math do you need in Data Science? - KDnuggets,"In summary, we’ve discussed the essential math and theoretical skills that are needed in data science and machine learning. There are several free online courses that will teach you the necessary math skills that you need in data science and machine learning. As a data science aspirant, it’s important to keep in mind that the theoretical foundations of data science are very crucial for building efficient and reliable models. You should, therefore, invest enough time to study the mathematical theory behind each machine learning algorithm"
Exploring the Real World of Data Science - KDnuggets,"Data science, machine learning and artificial intelligence have been hot domains for a few years now. Many people want to work as data scientists and are putting in an immense effort to upgrade their skills through university, online course or self-study. However, there are a lot of challenges in the real world in terms of working and solving a business problem. Non-technical skills are equally important in order to work as a data scientist. In this blog, I am sharing my personal experience that I have come across in my work as a data scientist"
Exploring the Real World of Data Science - KDnuggets,"There are a lot of challenges in the real world problem that students don’t necessarily face at the University. In school, they used to get a structured problem and a popular dataset and eventually get the exact solution. However, the problem in the industry will often be unstructured and complex. Any assumptions on the problem will backfire in the real world. It is better to understand the business problem completely before diving into the analysis. Understanding business problems involves doing more research on the problem and its domain, planning, asking the clients the right questions and discuss with team members"
Exploring the Real World of Data Science - KDnuggets,"Data science is about logical thinking, generating more ideas and creativity in solving the problems. Hence, teamwork plays an important role in data science. It is also necessary to think multidimensionally rather than one dimensional. Team members could be coming from diverse backgrounds with different kinds of skill sets. Take the strength of each team member and distribute the work accordingly. This helped me to solve the problem in different ways and learn new things"
Exploring the Real World of Data Science - KDnuggets,"The other key skill is to be a good listener. Data science is about sharing and collaboration. Basically the person needs to understand the views of others in the team. Many times, other team members come up with good ideas and the ideas might be a unique one and it is necessary to listen and understand them in order to successfully implement it in the project. As I said above, data science is not a one-man show and it is always a team effort"
Exploring the Real World of Data Science - KDnuggets,"Data science or AI is a fast-evolving field, and as a result, there will always be something new and crucial to learn. It is very hard to remember everything and documentation facilitated me to overcome this challenge. Also, It helped me to crystallize my own thought process. I used to document my learnings, analysis, model process, experiments and the code. Also, I write my failed experiments and it’s reasons in a detailed manner, and it helped me to sharpen my ideas in the long run. In addition to that, it helped me to improve my communication and understanding the concepts in detail. You can document even small things that you learned or come across that make a big difference in the long run. Use your own convenient tools to document"
Exploring the Real World of Data Science - KDnuggets,"Working in an agile environment gives me clear planning, prioritisation, and direction at the start of each sprint. Having an agile mindset helps in responding to change and handling uncertainty. If you come across uncertainty, try out options, collect feedback and improve iteratively. It also gave me an opportunity to collaborate with different teams. Presenting a minimum viable product (MVP) in the form of a machine learning model at the end of each sprint to the stakeholders helped me to shape my projects in a better form. Also, feedback from the end of each sprint helped me to correct my mistakes and deliver the project efficiently"
Exploring the Real World of Data Science - KDnuggets,"Storytelling is an important part of data science. We are crunching the data and creating a model, and finding the insights. But, what does this model says in business terms? In other words how this model generates money for the company or solve the problem? Stakeholders and management are not interested in p-value or any other statistics. The main challenge here is explaining the model in simpler terms to a non-technical audience in an engaging manner. One way to explain the model via a short story. This is one of my biggest learning in the last year. Always, include good visualization and it helps to convey the message as a story. Storytelling is an art and it takes time and a lot of practice"
Exploring the Real World of Data Science - KDnuggets,I significantly improved my coding skills during the last 8 months. One thing I have learned in my work as well as in competitions is to write functional or object-oriented code to have maximum code reusability. This will help to use the code in future projects as well as reduces time in the current one. I used to document the code function whenever I referred to stackoverflow or google and this helped me to learn new things on coding. Always follow best practices and keep your code reader-friendly
Exploring the Real World of Data Science - KDnuggets,"Data science is a blend of computer science, statistics, machine learning, and domain expertise. Hence it is required to have skills from handling different steps from cleaning data to interpreting the final model and deploying it. Don’t be intimidated"
Exploring the Real World of Data Science - KDnuggets,"AI is the new buzz in the IT industry and let’s come face to face with the fact that all of which can’t be assimilated by anyone in a short period of time. Decide to take it strategically by investing in one or two hours every day to learn new concepts and solve new problems which will include learning a new algorithm, coding, reading a blog, doing personal projects etc. Apart from all this, I would highly recommend reading non-technical books that help a lot on the flow and storytelling technique which will be a useful trait as we move on"
Exploring the Real World of Data Science - KDnuggets,"He has 9 years of experience with specialization in various domains related to data including IT, marketing, banking, power, and manufacturing. He is passionate about NLP and machine learning. He is a contributor to the"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"I realized how deeply seated some cognitive biases are. In fact, we often don’t even consciously realize when our thinking is being affected by one. For data scientists, these biases can really change the way we work with data and make our day-to-day decisions, and generally not for the better"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"As data scientists, our job is to make sense of the facts. In carrying out this analysis, we have to make subjective decisions, though. So even though we work with hard facts and data, there’s a strong interpretive component to data science"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"As a result, we data scientists need to be extremely careful, because all humans are very much susceptible to cognitive biases. We’re no exception. In fact, I have seen many instances where data scientists ended up making decisions based on pre-existing beliefs, limited data or just irrational preferences"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"During World War II, researchers from the non-profit research group the Center for Naval Analyses were tasked with a problem. They needed to reinforce the military’s fighter planes at their weakest spots. To accomplish this, they turned to data. They examined every plane that came back from a combat mission and made a note of where bullets had hit the aircraft. Based on that information, they recommended that the planes be reinforced at those precise spots"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,Let’s think about how this might apply to our work in data science. Say you begin working on a data set. You have created your features and reached a decent accuracy on your modelling task. But maybe you should ask yourself if that is the best result you can achieve. Have you tried looking for more data? Maybe adding weather forecast data to the regular sales variables that you use in your ARIMA models would help you to forecast your sales better. Or perhaps some features around holidays can tell your model why your buyers are behaving in a particular fashion around Thanksgiving or Christmas
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"One way to mitigate this bias is by thinking in a rigorous, scientific way about the problem at hand and then brainstorming about any type of data that could help solve it (rather than just starting with the data). These approaches may seem similar, but the second method restricts your vision because you don’t know what’s missing from your work. By using the first approach, you will know what data you were not able to get, and you will end up factoring that into your conclusions"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"This often happens with data science projects. A project might run for more than two years without results, but an investigator continues running it because so much time, money and effort have already been invested. Or a data scientist might defend her project wholeheartedly because she has invested so much in it, failing to realize that putting in more work won’t help her or the company in the long run and that it is best if the project is scrapped"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"A way to save yourself from this cognitive bias is by focusing on future benefits and costs rather than the already lost past costs. You have to develop the habit, hard as it is, of ignoring the previous cost information. Of course, it is never easy for us data scientists to just disregard data. For myself, I have found that a methodical way works best in this case. I take a pen and paper to get away from all the distractions and try to come up with all the additional costs required to do a project along with the benefits I might get in the future. If the cost part of the task seems overly significant, then it is time to move on"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"As data scientists, we are always in search of patterns. The tendency means that sometimes we even find patterns where none really even exist. Our brains are so trained in this way that we will even make sense of chaos to the extent that we can"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"Because our training wires us to seek out patterns, it’s crucial to remember the simple maxim that correlation does not imply causation. Those five words are like the hammer of the data science toolbox without which you can’t accomplish anything. Just because two variables move in tandem doesn’t necessarily mean that one causes the other"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"As data scientists, we need to be mindful of this bias when we present findings. Often, variables that might seem causal might not be on closer inspection. We should also take special care to avoid this type of mistake when creating variables of our models. At each step of the process, it’s important to ask ourselves if our independent variable is possibly just correlated to the dependent variable"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,This often happens in the data science world. Data scientists tend to get and work on data that’s easier to obtain rather than looking for data that is harder to gather but might be more useful. We make do with models that we understand and that are available to us in a neat package rather than something more suitable for the problem at hand but much more difficult to come by
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"A way to overcome availability bias in data science is to broaden our horizons. Commit to lifelong learning. Read. A lot. About everything. Then read some more. Meet new people. Discuss your work with other data scientists at work or in online forums. Be more open to suggestions about changes that you may have to take in your approach. By opening yourself up to new information and ideas, you can make sure that you’re less likely to work with incomplete information"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"We all hold some beliefs, and that’s fine. It’s all part of being human. What’s not OK, though, is when we let those beliefs inadvertently come into the way we form our hypotheses"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"We read the news on the site that conforms most closely to our beliefs. We talk to people who are like us and hold similar views. We don’t want to get disconcerting evidence because that might lead us to change our worldview, which we might be afraid to do"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"For example, I have seen confirmation bias in action in data science during the cost-benefit analysis stage of a project. I’ve seen people clinging to the data that confirms their hypothesis while ignoring all the contradictory evidence. Obviously, doing this could have a negative impact on the benefits section of the project"
Five Cognitive Biases In Data Science (And how to avoid them) - KDnuggets,"Sometimes it is useful to be able to make some sense out of the world based on limited information. In fact, we make most of our decisions without thinking much, going with our gut feelings. The potential harm of most of our day-to-day actions is pretty small. Allowing our biases to influence our work, though, can leave us in an unfortunate situation. We may end up losing money or credibility if we make a vital decision that turns out to be wrong"
"Unit Test Your Data Pipeline, You Will Thank Yourself Later - KDnuggets","One common mistake that data scientists, especially beginners, make is not writing unit tests. Data scientists sometimes argue that unit testing is not applicable because there is no correct answer to a model that can be known ahead of time or to test with. However, most data science projects start with data transformation. While you cannot test model output, at least you should test that inputs are correct. Compared to the time you invest in writing unit tests, good pieces of simple tests will save you much more time later, especially when working on large projects or big data"
Top 6 Reasons Data Scientists Should Know Java - KDnuggets,"It’s a platform-independent, useful, and robust language. Developers across the world use Java to build applications, web tools, and software development platforms. Java also has significant uses in machine learning and data science"
Top 6 Reasons Data Scientists Should Know Java - KDnuggets,"But as a data scientist, you should know how to use Java as it offers a host of other services to create a business application. As mentioned above, Java has many uses in the machine learning and artificial intelligence domain. Many big companies like Uber, Spotify, and Airbnb are based on Java"
Top 6 Reasons Data Scientists Should Know Java - KDnuggets,"Most developers feel confident in coding with Java. Besides the fact that it has an extensive user base, Java is also one of the most sought-after skills in the market, as companies typically use it for all quickly executable projects. Java is also a legacy language - i"
Top 6 Reasons Data Scientists Should Know Java - KDnuggets,"Java’s unique syntax is accepted worldwide for its ease of understanding. This syntax allows developers to understand conventions, requirements for a variable, and coding methodology. Java is strongly typed - i"
Top 6 Reasons Data Scientists Should Know Java - KDnuggets,"Most major companies maintain a standard syntax for their code repository. Doing so ensures that all developer code according to conventions for production codebase. Java helps them by automatically maintaining its own standard conventions, which can be adhered to"
Top 6 Reasons Data Scientists Should Know Java - KDnuggets,"Online transaction processing systems (OLTP), along with data warehousing, typically use mainframe systems for batch processing. Java, more than other languages, ties more naturally into that architecture. You can integrate Java with COBOL and middleware software"
Top 6 Reasons Data Scientists Should Know Java - KDnuggets,"Java is an object-oriented, versatile, and unique language that offers tons of functionality. Its excellent performance and speed makes it one of the most sought after skills in the market. It also provides security capabilities, network-centric programming, and platform-independence"
Top 6 Reasons Data Scientists Should Know Java - KDnuggets,"For data scientists, Java provides a host of data science functionalities such as data analysis, data processing, statistical analysis, data visualization, and NLP. Java can help apply machine learning algorithms to real-world applications. It allows you to build adaptive and predictive models based on batch and stream processing techniques. And along with that REPL and lambda expression, it simplifies the creation of large scale applications"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"Most of the analysis is in support of the field epidemiology, which focuses very much on the operational response, in terms of collecting data, analyzing data, and then using the data to inform action, which includes advocacy. It takes time for data to be centrally collected and especially for more complex analyses -- for example, forecasting or modeling. Data often has to be sent to specialized centers in the northern part of the world, and after analysis the data has to be sent back. This adds many delays, moving from data gathering to using meaningful results from analyses. The field of modern outbreak analytics mostly looks at how you can perform real-time analytics, and how you can perform it locally"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"Absolutely. Data from the field is often very messy, not because of ill intent but because of the tremendous pressure many people work under. Data collection is incredibly important because data you can't make sense of, due to typos or whatever, is basically lost data"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"Imagine this: During an Ebola outbreak, you’d get maybe five or maybe 100 different Excel sheets daily from different health facilities, which you have to merge into a coherent database. Merging them only works when there is consistency among datasets. The use of spaces or special characters -- for example, in variable names -- makes it less straightforward to merge such data and will cost a lot of time"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"You need to start analyzing. But then you discover that in the variable ‘age,’ someone is 167 years old. Sometimes people write down male as a 1 and a zero for female, and there's no data dictionary attached to the datasets you received. Statistical packages just don't know how to deal with that. These are just a few of the practical examples of first shaping the data in a way that it can be analyzed. This can sometimes take up 90% of your time. Now imagine having to do this on a daily basis; how much time do you then actually have to do your real job, to use the data for action"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"Now, if you can automate this type of process, imagine the amount of time that you would win back with it, time that you can actually spend on doing your job. Maybe you’re working as an epidemiologist in the field, and there's a big outbreak of Ebola ongoing. On a day-to-day basis, you need to produce reports. The format of the reports doesn't change much. But the figures and the tables might change depending on changing trends, right? A few cases more there, less cases there. Normally, this would be a manual procedure. But you can fully automate this process, which means you win a lot of time you can now spend on data interpretation. How should we interpret the observed trends, and what type of action should be connected to that?"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"The reliance on modeling might sometimes be a little bit too much, but there is a stronger collaboration between mathematicians, classical epidemiologists and field epidemiologists these days. At one point, you say, ""Well, it’s amazing,"" because you don't understand it, and you produce nice graphs, and it tells you something about the future. And who doesn't want to know what's going to happen in the future?"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"We're not very good at communicating uncertainty. Good modelers always mention the caveats, but you find them downstairs in the report. And I always think they should be on top, in red, in capitals. That's where you outline all the assumptions you have made and all the uncertainties around the model and data you used"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"When operational people and modelers work closer together, you are more likely to gain better estimates of many of these parameters. Operational people will be able to say to modelers, ""Well, I see what your model says, but I don't think that that is happening in the field, because your model estimates, in this region, a really steep increase in cases. And that was inflated because one event happened here and there, and that boosted the numbers. It's not the true trend for the whole country"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"That's why we had different people in the R Epidemics Consortium: the operational people, the field people, the people who are advanced in methodology, the programmers, and the more policy-oriented people. That's incredibly important. That’s a healthy mixture of the different disciplines that you draw together"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"Unfortunately, I don't think we have done very well in this pandemic yet with regard to interdisciplinary work. Most of us epidemiology folks are very much into health indicators, but we also always have social scientists and anthropologists around the table for their feedback into whether what we're seeing in the trend is explainable, not just medically plausible -- the things that we see in behavior, things that you hear and feel. That's incredibly important, especially during this outbreak, where due to lack of vaccines and treatments, all the interventions we have are aimed at behavior. The multidisciplinary nature of all of this work has been our best output. I don't think it has been solidly adopted more widely in the current response, but then again, change is slow"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"When it comes to the utility I see in these types of tools, what’s really important is reproducibility and transparency on how the analysis was performed. The open-source nature allows for a lot of people to develop all kinds of analytical packages. So they're much more powerful beyond the traditional analytical packages. For example, with the inclusion of geospatial data, you can use R to make maps. With more software having open APIs, you can more easily connect to other data sources. There is an increase in data that has been made publicly: for example, on"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"There's also a lot of hype, and I think that's important to address. When I hear people talking about AI or blockchain and its utility in the humanitarian sector -- I mean, I am all about innovation, but we need to be realistic in terms of what is usable in the humanitarian setting and how slow transitions are. Technologies haven't been truly developed; the user case hasn't been fully defined, nor how this technology is actually going to help us when there are still major worries about quality of data and how we even collect data. At times we focus a little too much energy on the hype and not so much on the solutions that we should be providing. Many of the needed solutions are relatively simple evolutions of existing practices"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"By fusing these different disciplines, you keep everybody close to the reality check. You let technical people explain what is possible in terms of technology. You let the operational people tell you, ""Well, great. But this will work, or this will not work. That takes a lot of time and effort but is the most collaborative way of working towards usable solutions"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"But then it comes to adding a layer of data -- for example, geographical data. Where are the roads? How are the buildings looking? Where are the households? This type of data is available on different platforms like Humanitarian OpenStreetMap. All this data has been collected by a lot of hard work and then been made publicly available for use, and it can actually strengthen your analysis and estimates. It's the same for climatological factors, which basically is remote-sensing imagery. This data is available -- in some parts of the world, it's available in excess. But for a lot of it, you have to pay for expensive licenses"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"Other incredibly important “background” data is what we call denominator data. So you want to compare how many cases are in this region and that region over time. But the only fair way of comparing that is by knowing what the exact population is in the different areas, right? The characteristics of population and age -- these often come from central bureaus of statistics. Some countries have those, some countries don't, or data might be outdated"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"This is a complicated matter because often, not just in epidemics but in humanitarian health emergencies, in some areas there can be stigma. Maybe the law doesn't take into account data privacy. But there might be implications for names coming out, or even prosecution. You never know how data will be mined or used in the future"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"Data literacy is really important -- not necessarily being 100% versatile in coding yourself, but the understanding of what data can and cannot tell you. Hobby modelers have written all these pieces around what is happening with the trends, and some are creating more noise than actually making factual contributions. And, to be honest, academics have also contributed to this"
Fighting Disease with Data: Q&A with Epidemiologist Amrish Baidjoe - KDnuggets,"If you want to contribute, you should always start doing things. But you should carefully look at what's out there already. If we can get all this expertise and, more importantly, all these motivated people together, we can actually utilize our strengths to make something better"
Best GIS Courses in 2020 - KDnuggets,"2020 saw GIS Analysis appear more frequently in the mainstream data science waves. As a result of the coronavirus, analysts flocked to use mapping api's in a dash to visualize and provide a summary of their findings. GIS was not a default choice for the masses in terms of data science as compared to finance, manufacturing and marketing"
Best GIS Courses in 2020 - KDnuggets,"Esri needs no introduction for those in the world of GIS. Esri is the premier service provider of GIS software - ArcGIS. Along with QGIS and CARTO, it is one of the top 3 best GIS software on the market"
Best GIS Courses in 2020 - KDnuggets,"This course has been developed in partnership with Esri and uses ArcGIS. Throughout the course, students can learn fundamental spatial analysis as well as Satellite Imagery Analysis. This is offered as a beginner course"
Introduction to Pandas for Data Science - KDnuggets,Think of Series as a single column in an Excel sheet. You can also think of it as a 1d Numpy array. The only thing that differentiates it from 1d Numpy array is that we can have Index Names
Introduction to Pandas for Data Science - KDnuggets,"Here, it added 5 to every single element in Series newSeries. This is also known as element-wise operations. Similarly, for other operations such as *, /, -, **, and other operators as well. We will see only ** operator, and you should try it for other operators too"
Introduction to Pandas for Data Science - KDnuggets,"Both of these syntaxes are correct, but we have to be careful about choosing one. If our column name has space in it, then definitely we can not use the 2nd method. We have to use the first method. We can only use the 2nd method when there is no space in the column name"
Introduction to Pandas for Data Science - KDnuggets,"Here, we can see that some values in “Cabin” columns are True. True means that the value is NaN or missing. We can see that this is unclear to see and understand, so we can use the sum() function to get more detailed info"
Introduction to Pandas for Data Science - KDnuggets,"The sum function is used to sum all the values in a data frame. Remember that True meaning 1 and False meaning 0, so to get all the True values returned by isnull() function, we can use sum() function. Let’s check it out"
Introduction to Pandas for Data Science - KDnuggets,"Here, we can see that it tells us how many non-null entities we have, such as in-case of Age, we have 714 Non-Null float64 type entities. It also tells us about memory usage, which is 83.6 KB in this case"
Introduction to Pandas for Data Science - KDnuggets,It helps us a lot when we are trying to extract important features from our dataset. Let’s take an example where we only want to see the entries where “Sex” is “male”. Let’s see how we are going to approach this
Introduction to Pandas for Data Science - KDnuggets,Give this code a second to read and understand what is happening. We are collecting all the rows from the dataframe where df[“Sex”] == “male” and df[“Survived”]==1. The returned value is a dataframe with all the rows of male passengers who survived
Introduction to Pandas for Data Science - KDnuggets,"This brings us to the end of this article. Now obviously there are tons of other important functions in Pandas which are very important such as groupby, apply, iloc, rename, replace etc. I recommend you to check the"
Complex logic at breakneck speed: Try Julia for data science - KDnuggets,"The inception of this programming language can be traced back to 2009. The lead developers Alan Edelman, Jeff Bezanson, Stefan Karpinski, and Viral Shah started working on creating a language that can be used for better and faster numerical computing. The developers were able to launch a commercial release in February 2012"
Complex logic at breakneck speed: Try Julia for data science - KDnuggets,"Numpy is seriously fast. It is a library with super-optimized functions (many of them pre-compiled), with a dedicated focus of giving Python users (particularly useful for data scientists and ML engineers) near-C speed. Simple Numpy functions like sum or standard deviation"
Complex logic at breakneck speed: Try Julia for data science - KDnuggets,Python code is below. We kept the same functional nature of the code (Julia is a functional language) to keep the comparison fair and easy to verify. The for-loop takes
Complex logic at breakneck speed: Try Julia for data science - KDnuggets,"For Julia, the code change will be fairly straightforward. We will just use the for-loop, check if an element of the array is divisible by 2, and if not (odd number), then add it to the running sum. As pedantic as one can get!"
Complex logic at breakneck speed: Try Julia for data science - KDnuggets,"So, here is the Julia code. Again, simple and sweet. Took ~"
Why Learn Python? Here Are 8 Data-Driven Reasons - KDnuggets,"Python is considered as one of the most in-demand and popular programming languages in the world of programming languages. In a recent Stack Overflow survey, Python has taken over C, C++, Java, and has made its way to the top. This is one of the major reasons why many programmers and newbie developers prefer to learn Python and consider Python certification as one of the most sought-after programming certifications. Through this blog, I will list out the major reasons why you should learn Python and the 8 major data-driven reasons for learning it"
Why Learn Python? Here Are 8 Data-Driven Reasons - KDnuggets,"Every piece of knowledge becomes full when you start learning from the basics. Therefore, before you get to know the reasons why you learn Python, let me get you some introduction on Python, only to realise its power as a programming language. Python was introduced in the year 1991 by Guido van Rossum. When developing this language, he had some intentions in mind and he succeeded at developing a programming language which can used for:"
Why Learn Python? Here Are 8 Data-Driven Reasons - KDnuggets,"The year 2020 gives Python the high popularity as a language. As per the surveys conducted in February 2020, Python language ranks in the number 3 slot. It is honored with the title of fast-rising programming language that comes under the top 50"
Why Learn Python? Here Are 8 Data-Driven Reasons - KDnuggets,Python as a programming language will continue to expand into new areas of computing. It will be the primary IoT programming language. The current market trends prove that a Python Developer has huge possibilities in the coming years. Whether you are an experienced programmer or a fresher the
Why Learn Python? Here Are 8 Data-Driven Reasons - KDnuggets,"With more than 10 years of experience in the education industry, he successfully runs Edoxi Training Institute in Dubai and Time Training Center Abu Dhabi. He is a strategist and a leader able to steer the company to the most profitable direction while also implementing its vision, mission and long term goals. He always makes sure that his business continues to grow by identifying, developing and implementing the new strategies. He leads his team in alignment with the company's vision and values while preparing and implementing comprehensive business plans to facilitate achievement"
Top 10 Data Visualization Tools for Every Data Scientist - KDnuggets,"One of the most well-settled fields of study and practice in the IT industry today, Data Science has been in the limelight for nearly a decade now. Yes, that's right! It has proven to be a boon in multiple industry verticals. From top of the line methodologies to analyzation of the market, this technology primarily includes obtaining valuable insights from data"
Top 10 Data Visualization Tools for Every Data Scientist - KDnuggets,Data Visualization is very important when it comes to analyzing big datasets. When data scientists analyze complex datasets they also need to understand the insights collected. Data Visualization will make it easier for them to understand through graphs and charts
Top 10 Data Visualization Tools for Every Data Scientist - KDnuggets,"Nowadays, to hire an Android developer or iOS developer depends upon the kind of tools and techniques that they use to an extent. As for businesses around the world, using these tools can help gain business insights and stay ahead in the race. The majority of top iOS and"
Top 10 Data Visualization Tools for Every Data Scientist - KDnuggets,It is an interactive data visualization software. This tool is used for effective data analysis and data visualization in the industry. It has a drag and drop interface and this feature helps it to perform tasks easily and very fast
Top 10 Data Visualization Tools for Every Data Scientist - KDnuggets,The software doesn’t force its users to write codes. The software is compatible with a lot of data sources. The tool is a bit expensive but it is the most preferred choice of a top company like Amazon. Qlik view is the biggest competitor of tableau and the tool is extensively used because of its unique drag and drop feature
Top 10 Data Visualization Tools for Every Data Scientist - KDnuggets,"It supports HTML, CSS, and SVG. Developers can present data in the form of creative pictures and graphics. It is a very flexible platform as it allows variations for the creation of different graphs"
Top 10 Data Visualization Tools for Every Data Scientist - KDnuggets,This software helps to improve the data visualization process. The tool is preferred by well-established data scientists to analyze large scale data. Qlik view is used across 100 countries and has a very strong community
Top 10 Data Visualization Tools for Every Data Scientist - KDnuggets,The app can be best used by beginners who want to start their career in data visualization. This app is the most user-friendly app for a data scientist. The tool is widely used in media organizations where there is a high need for presenting everything through stats and graphs. The tool is the most popular choice because it has a simple and easy interface
Learn Data Science from Top Universities for Free - KDnuggets,"MIT is not the only university that does this. Many of the high ranking US universities make courses, lectures and other learning material available for free. Amongst this, is a wealth of material that is highly and often directly applicable to learning data science, machine learning and artificial intelligence"
Learn Data Science from Top Universities for Free - KDnuggets,"The material in the repo is actually some of the best I’ve seen covering the area of actually applying machine learning in the real world. In addition to covering all aspects of the machine learning process from data exploration and cleaning to model evaluation and tuning, it also covers Github, unit testing and continuous integration. All extremely important aspects when you are applying machine learning in a real-world situation"
Learn Data Science from Top Universities for Free - KDnuggets,There is such a wealth of free material available online for learning data science. This article covers some of the more traditional lecture-based approaches from high ranking universities. For more alternative resources I previously published my top 5
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"If you want to land a Data Science job at your dream company, the time is now. But you can’t sit back and let it happen on its own. You’ll have to put in some work to get yourself noticed, to show that you’re talented, willing, and able to take the opportunity. Here’s how"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"It’s the one platform for job search. Sure, you can pimp your other social media, too — and you should. But your first priority is to get a killer deck on LinkedIn"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"Try to fill in every profile section. Add a background photo, add some skills, and follow notable people in your industry. Join some relevant groups as well"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"Also, there’s no way around adding a profile photo. Yes, you always look stupid on photos. And seeing your own image in the corner feels awkward. I get it. But do it anyway. A bad photo on which you look ugly is still better than no photo at all. And you can always replace the photo once you have a better one"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,Try to tell a story with your profile. Think about a cool headline: “Data Scientist at company XYZ” is boring. How about this: “Unlocking the secrets of data for company XYZ
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"I make mine stand out with lots of visual elements, color contrasts, and a clear message. I want a recruiter to know that I put some thought into this, that I’m bold, and that I know who I am. This is a recent CV of mine:"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"Note that I’ve used some visual language with stars, other symbols, and timelines. I’ve also added the same profile photo that I use everywhere — that’s personal branding 101. Feel free to copy any of my tricks"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"Granted, you’ll hardly manage to get such a design with a standard word file. I used Adobe Illustrator to make this CV. You could also use"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"But the best way to master an uncomfortable activity is by doing it regularly. There’s no harm in sending out applications even if you have a secure job. You can always withdraw it. And even in that case, you can make valuable connections for years to come"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"And if your job is not so secure, what do you have to lose? For the time being, plan to send out five applications each week. Or however many you might need. Focus on quality, though. And also apply for jobs that you don’t feel one hundred percent qualified for. The worst answer you can get is no"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"If it’s the latter, welcome to the way most people get their jobs. Especially the good jobs. It’s about networking!"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"It may seem awkward at first, and you’ll wonder what they think of you. So don’t spam, and be respectful. If you follow that, more people will respond to you than you imagined"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,Even if they’re poorly paid or bring no money at all. Even if you only landed them by chance on Upwork or Fiverr. Even if your mom or your neighbor was just asking you for a favor
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"Before I landed my current position, I worked for free for two full years. Then I interned for another whole year with a pay that didn’t cover half my expenses. I did other gigs here and there to make ends meet. It wasn’t easy, but it landed me where I am today"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"And even now that I have a pretty safe position, I carry on doing gigs. Paid or unpaid doesn’t matter much. Each time I view it as an opportunity to add people to a network that, at some point, will be bullet-proof"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"The point in gigs is to get to know people. The more people you know, the higher the chance that someone will give you a paycheck. And recommend you to other people who will give you a bigger one"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"I chose Medium as a platform not only because it’s paid, but also because I like the community. Every time I hit publish, something great happens. Someone leaves a really inspiring comment. A stranger with an amazing profile connects with me on LinkedIn. A celebrity joins my small but growing following on Twitter. Every time I hit publish, I grow my network"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,Maybe you’re not impressed with your writing skills. I wasn’t impressed with mine when I started on Medium. That’s okay!
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"You can start with small steps. If you’re uncomfortable with full-length writing, maybe you enjoy tweeting. Aim to make two tweets a day about whatever is interesting to you"
How use the Coronavirus crisis to kickstart your Data Science career - KDnuggets,"Your aim is consistency, not perfection. You won’t write a glittery novel or a viral tweet in one day. You won’t connect with Bill Gates or LeBron James in one day either. You probably won’t in your lifetime"
3 Key Data Science Questions to Ask Your Big Data - KDnuggets,"Data, Data Everywhere, not a drop to act! Every organization is collecting data today, but very few know what to do with it. Part of the challenge is, organizations don’t know what to ask of data. Where to begin? They have made multi-million $ investments in instrumentation and collecting BIG data through"
3 Key Data Science Questions to Ask Your Big Data - KDnuggets,"For the CEO of the apparel business, it means how is the apparel business doing, and for the marketing manager, it means how is his/her department doing? There are many ways to answer this question, including laying out a financial measurement framework, a balanced scorecard, or something in between. But the most important part is to understand and agree company-wide on the KPI or set of KPIs (Key Performance Indicator) for your business. Is it revenue that you are optimizing or margins? Is it penetration or NPS (Net Promoter Score) or some integrated index?"
3 Key Data Science Questions to Ask Your Big Data - KDnuggets,"Once you have a KPI identified, you need to understand what drives that KPI. Some of this can be derived mathematically, and some could be a mere hypothesis that needs to be validated. For example, the number of visitors and conversion are important drivers of revenue for the apparel business. On the other hand, for a gaming company, with a freemium model and less than 1% paying customers, the total number of players is not a driver of revenue, but the number of players on a certain game might be. What’s important here to understand the dynamics of your specific business through portfolio analysis"
3 Key Data Science Questions to Ask Your Big Data - KDnuggets,"Customer is central to all of our businesses. And we know all our customers are not alike. Some are more sophisticated, a heavy user of our product; others have used our product only once. For the apparel business, there might be two macro customer segments, those who would buy product wholesale to retail in their boutiques and those who are buying for personal use. Their needs from the eCommerce site and the company would be different. Companies who understand their customers and customize their offering, messaging, marketing channel accordingly, delight their customers, securing their future revenue (or KPI). And that is, ultimately, what we all want to do, to drive our KPI in the right direction"
3 Key Data Science Questions to Ask Your Big Data - KDnuggets,"The process of unraveling and understanding of your own business or department is an iterative one. The process begins by asking these 3 questions at the highest level and then iteratively asking hundreds of cascading questions to get deeper breakthrough insights needed to maximize the ROI. And to truly incorporate this data-driven process of running the business, all individuals in the organization; be it the marketing professional, product manager, sales professional, financial analyst or business analyst, need to know how to start asking the right questions of the data, to optimize their own KPI’s"
Five Cool Python Libraries for Data Science - KDnuggets,Python is a best friend for the majority of the Data Scientists. Libraries make their life simpler. I have come across five cool Python libraries while working on my NLP project. This helped me a lot and I would like to share the same in this article
Five Cool Python Libraries for Data Science - KDnuggets,"Amazing library to convert text numerics into int and float. Useful library for NLP projects. For more details, please check PyPI and this"
Five Cool Python Libraries for Data Science - KDnuggets,It is widespread to find missing values in a real-world dataset. We need to understand the missing values before imputing. Missingo offers a quick and helpful way to visualize the missing values
Five Cool Python Libraries for Data Science - KDnuggets,"Dummy dataset has 11 rows and four columns. Missing values presented in Min, Temp, and city variables. We can visualize using a bar graph and matrix. It also supports heatmap, dendrogram. For more details, please check this"
Five Cool Python Libraries for Data Science - KDnuggets,We might come across a situation where we need to generate some test data or use some dummy data in our analysis. One way to get dummy data is by using the Faker library. This will generate fake data for you very quickly when you need to
Five Cool Python Libraries for Data Science - KDnuggets,Chartify is a visualization library that aims to make it as easy as possible for data scientists to create charts. It comes with user-friendly syntax and consistent data formatting compared to other tools. It takes less time to create beautiful and quick charts. This was developed by Spotify labs
Five Cool Python Libraries for Data Science - KDnuggets,"He has 9 years of experience with specialization in various domains related to data including IT, marketing, banking, power, and manufacturing. He is passionate about NLP and machine learning. He is a contributor to the"
Don’t Democratize Data Science - KDnuggets,"Every few years, some academic and professional field gets a lot of cachet in the popular imagination. Right now, that field is data science. As a result, a lot of people are looking to get into it. Add to that the news outlets calling data science sexy and various academic institutes promising to make a data scientist out of you in just a few months, and you've got the perfect recipe for disaster"
Don’t Democratize Data Science - KDnuggets,"Of course, as a data scientist myself, I don't think the problem lies in people choosing data science as a profession. If you're interested in working with data, understanding business problems, grappling with math, and you love coding, you're probably going to thrive in data science. You'll get a lot of opportunities to use math and coding to develop innovative solutions to problems and will likely find the work rewarding. The main issue here is that the motivations people have for entering the field are often flawed"
Don’t Democratize Data Science - KDnuggets,"For some, the appeal is money, while others like the way the title sounds. Even worse, some people are probably just responding to the herd mentality that our society has instilled. For instance, not long ago, every graduate aspired to do an MBA. And I myself am guilty of the same. It took me a GMAT test and a couple of rejections to realize that I didn't really want the degree. Ultimately, those rejections were the best thing that happened to me because, after that, I finally looked at data science as an option. Once I got into it, I found that I love the math involved and all the different ways in which I get to use data science to help solve problems for businesses"
Don’t Democratize Data Science - KDnuggets,"A lot of people want to do it, but they don't know what the job really entails. And this has resulted in a lot of people calling themselves data scientists and a lot of bad decision making. In fact, a lot of people considering entering the profession probably don't even know what data science is"
Don’t Democratize Data Science - KDnuggets,"A lot of AutoML packages aim at democratizing data science. They provide a repository of models, automate the hyperparameter tuning process, and sometimes offer a way to put these models into production. The availability of such packages has led a lot of people to think that data science could be fully automated, eliminating the need for data scientists altogether. Or, if the processes can't be automated, these tools will allow anyone to become a data scientist"
Don’t Democratize Data Science - KDnuggets,"The work of data science includes understanding and identifying the problem at hand and setting up the right evaluation metrics. You also have to analyze the profitability of the project: most businesses don't want to spend money on projects that don't positively affect the bottom line. You can work with existing data, but sometimes you might need to come up with ways to set up new data pipelines to gather data to solve the problem. This requires talking to the stakeholders and gaining a holistic understanding of the problem. A data scientist also needs to be able to carry out data munging and feature creation to churn more performance out of existing models. In the end, model testing and setting the feedback loop require endless hours of discussions with the business and are pretty specific to each project. Someone who just runs code might not be able to add value to such discussions as they don't really understand what goes behind the models they have used in AutoML"
Don’t Democratize Data Science - KDnuggets,Then there comes the issue of domain knowledge. Processes that are acceptable in a retail domain are not applicable in the finance domain where a small change could result in your customers losing a lot of money. Some things just can't be automated since they require domain knowledge and an understanding of the business you're working with
Don’t Democratize Data Science - KDnuggets,"I have become skeptical of what I call the New Data Scientist. Almost every day, I seem to meet a person calling themselves a data scientist when they are just glorified code runners, which refers to a person who just runs the code without understanding what goes on behind it. With so many academies and institutes providing boot-camp-based courses,"
Don’t Democratize Data Science - KDnuggets,"I get a lot of requests where someone asks me whether they should take a certified course from XYZ institute or a boot camp from ABC academy. My answer is neither. I find that these institutes that promise to make data scientists in droves are mainly just in the money-making business. Ultimately, going through a few notebooks and running somebody else's code doesn't truly make a person a data scientist"
Don’t Democratize Data Science - KDnuggets,"Now, don't get me wrong. If someone learns best through a top-down approach where they run some code first and then read in-depth about the principles behind it, that's perfectly fine. Data science is about more than just running code, though. Until you really understand the math and theory behind all the code, you haven't mastered data science"
Don’t Democratize Data Science - KDnuggets,"I tend to think of this as a novice effect. It's a problem that plagues people in the early stages of learning a new skill. In my view, there are three stages to a data scientist's journey"
Don’t Democratize Data Science - KDnuggets,"Now, the Dunning-Kruger effect is something that most of the beginners will face. The joy of running your first program and executing it perfectly really takes you to the top of the world. And it's totally fine to be at this stage. The problem comes when newcomers are incapable of leaving this stage and moving on to the next ones in a timely fashion. I have seen a few people who get stuck at this stage because they got into data science with the wrong expectations, thinking that it's sexy and exciting, without understanding the field's depth"
Don’t Democratize Data Science - KDnuggets,"For instance, I recently interviewed a guy who had two years of experience in the field. He seemed confident. He had used data science in his job and had worked on a couple of Kaggle projects. The first few minutes of the interview went really well. He explained the higher-level concepts well enough that I decided to dig a little deeper into his mathematical understanding of the techniques he had applied in his projects. And that was where things changed. I asked him to tell me about the log loss function. When he said,"
"Data Scientists, Corporate Fortune Tellers - KDnuggets","Many years passed, and I continued consulting, teaching, and developing AI solutions in industry and academia. My long-time self appointed title of “fortune teller” for a data scientist was not comprehensive at all, as they are responsible for many things beyond future predictions. However, by that time, I realized that from a corporate perspective, “fortune teller” was not entirely off from the role of a “data scientist”"
"Data Scientists, Corporate Fortune Tellers - KDnuggets","AI is a widely used term with applications ranging from image analysis to robotics. Machine learning is considered a subset of AI, targeting a narrower range of activities which in fact is applied in real-world problems. Meanwhile, Data Science uses machine learning to analyze data and make predictions about the future. It combines machine learning with other disciplines such as big data analytics, research analytics, domain knowledge, among others. Data Science is a combination of domain knowledge, computer programming skills, math, and statistics (machine learning) that is being exercised in companies for today’s world problems"
"Data Scientists, Corporate Fortune Tellers - KDnuggets","Today, Data Science has become one of the hottest jobs in the industry, since it is a relatively new job with higher than average salary ranges, and prestige. And usually, the minimum educational requirement for this job is a master’s degree. Companies, on the other hand, are trying to hire more and more data scientists, as they realized that they can use machine learning and AI to provide more insights from their data. Isn’t it great? Well, it sounds so at first, but to become a data scientist who is well prepared can be a big hassle. Data scientists must be skilled in various fields including mathematics, machine learning, computer programming, statistical modeling, data engineering, visualization, pattern recognition, uncertainty modeling, data warehousing, cloud computing, and often big data. As their job market is volatile and based on modern technologies and methods, data scientists need to update their knowledge regularly. Furthermore, they need to have some domain knowledge of the field of business they are working in. During the recruitment process, it is extremely common to have questions concerning operating the most recent technologies. Requirements vary completely from one domain to another. The questions are not only technical, but also domain knowledge related issues, and conceptual analytics"
"Data Scientists, Corporate Fortune Tellers - KDnuggets","Many make the correlation that Data Science is all about data, and data means revenue. Nowadays, companies are excited about hiring data scientists and AI engineers to make data-driven decisions. However, the majority of them miss an important issue:"
"Data Scientists, Corporate Fortune Tellers - KDnuggets","Usually, data scientists have strong scientific and domain knowledge paired with outstanding technical and research skills. This combination of skill sets can bring value to the company. Not only can they use the company’s data to provide valuable insights, but they can also help move the company towards a data-driven structure for better and more accurate fortune-telling capabilities"
Appropriately Handling Missing Values for Statistical Modelling and Prediction - KDnuggets,"Missing values are an eventual reality in almost every dataset. Sometimes it may even be a cause for concern if there were no missing values just because you expect there to be data missing. Dealing with missing values have been a subject of debate for decades among statisticians due to it compromising the reliability of sample studies if left unchecked or incorrectly dealt with. In many university courses, students are simply taught to impute the missing values with the mean for continuous data, median for categorical data or to simply remove rows with missing values if it is just a small representation of your dataset (heuristically, less than 10% has been the threshold). Many statisticians in industry agree that blindly imputing the missing values in your dataset is a dangerous move and should be avoided without first understanding why the data is missing in the first place"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"Articles on the website are categorized by topic (environment, economy, abortion, etc…) and by political leaning (left, center, and right). I used All Sides because it was the best way to web scrape thousands of articles from numerous media outlets of differing biases. Plus, it allowed to me download the full text of an article, something you cannot do with the New York Times and NPR APIs. After a long and arduous process I ended up scraping a total of"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"When I first started this project, I conceded that this would not be the perfect project. The purpose of this project was to see how far I could get in creating a fake news classification and what insights could be drawn from that, then used towards a better model. My game plan was to treat this project the same way as a routine spam detection project"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"Building a model based on a count vectorizer (using word tallies) or a tfidf matrix (word tallies relative to how often they’re used in other articles in your dataset) can only get you so far. These methods do not consider important qualities like the word ordering and context. It’s very possible for two articles that are similar in their word counts to be totally different in their meaning. I did not expect my model to be adept at handling fake and real articles whose words and phrases overlap. Nonetheless, I expect some valuable insights to come from this project"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"Given my expectations that I outlined earlier in this post, I was surprised and almost baffled at the high scores my model produced. My model’s cross-validated accuracy score is 91.7%, recall (true positive rate) score is 92.6%, and its AUC score is 95%"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"If I were to decide on a threshold for a model based on this graph, I would choose one that produces a FPR at around 0.08 and a TPR at around 0.90, because at that point in the graph the trade off between false positives and true positives is equal"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"Using a technique I borrowed from Kevin Markham of Data School, here’s how I derived the “fakest” and “realest” words in the corpus. First I started off with a table two columns wide and 10558 rows long (that’s how many words there are in the corpus). The first column represented how many times a given word appeared in articles classified as “FAKE” and the second column was how many times a word appeared in a “REAL” article.  Then I divided the fake column by the total number of fake articles my model classified and so on for the real column. Next, I added the number one to every value in the data because I created a new column of “Fake:Real” ratios and didn’t want to get an error by dividing zero. This “Fake:Real” is a pretty good but by no means perfect metric of just how “fake” or “real a certain word. The logic is pretty simple, if a word shows up a bunch in “fake” articles and rarely in “real” articles then its fake to real ratio score will be pretty high"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"These two graphics exhibit some baffling results. The words in the “fake” chart are a mixed bag that includes some typical internet terminology such as PLEASE, Share, Posted, html, and Widget and words that aren’t even words such as tzrwu. However I was not surprised to see infowars mentioned nor terms like “Sheeple” or “UFO” make it in the top 20 “fakest” words. Infowars is a right-wing conspiracy-laden outlet led by Alex Jones that promotes conspiracy theories about chemtrails and 9/11"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"The “real” chart is dominated by names and politicians and words frequently used in political articles, comprising 60% of the bars in the chart. Seven of the twenty terms, including four of the top six, are politician names. This begs the question, are articles about politicians more likely to be true? No of course not, if anything you’d expect there to be numerous fake news articles spreading falsehoods about politicians. I would be committing a huge error if I came to the conclusion that articles mentioning politicians are more likely to be to factual"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,I and another party had a considerable amount of influence in shaping this dataset. I made the decision on which articles to use for the “real” dataset. The articles in the “fake” dataset were determined by a
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"There is a significantly high amount of subjectivity going into determining what is and what isn’t “fake news”. The reason why politician names are rated as “real” so highly is most likely because that half of the corpura disproportionately comes from political news. In addition, I did find a couple of articles from what I find to be reputable sources of news. One such article came from The Intercept, a news organization with high journalism standards. And yes, my model did indeed flag this supposed “fake news” article as real"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"To make matters even more complicated, we have to decide how to set the threshold probability for our model. If I was a data scientist at Facebook tasked with implementing a model that sorts out real and fake news in users’ feeds, I’d be faced with the dilemma between choosing a model that blocks all or most fake news and some real news or a model that allows all or most real news and some fake news. But before I make that decision, I need to figure what is the cost of failing to prevent fake news vs the cost of blocking real news? How does one attempt to answer such an abstract question? Unless we can train a model with a 100% true positive rate and 0% false positive rate, we’ll be stuck with this quandary"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"Classifying “fake news” provides a novel challenge to the data science community.  In many machine learning projects, the distinction between the different classes you want to predict is clear, whereas it’s a lot murkier in this case. This project validates the notion in data science that intuition and intimacy with your data is just as or more important than any model or tool at your disposal"
Machine Learning Finds “Fake News” with 88% Accuracy - KDnuggets,"Looking for opportunities in data science and/or journalism. Impossibly curious and passionate about learning new things. Before completing the Metis Data Science Bootcamp, he worked as a freelance journalist in San Francisco for Vice, Salon, SF Weekly, San Francisco Magazine, and more"
Anonymization and the Future of Data Science - KDnuggets,"To start with, think of anonymization as a technique to remove an individual’s identifying information from a dataset so that the remaining data cannot be linked to that individual. But that’s not the end of the game: the remaining data needs to actually be useful as well. This is what I call the “privacy vs.  In the cybersecurity world, there’s a saying that the safest computer is one that won’t function. And here the same point applies: ensuring anonymity usually requires sacrificing utility"
Anonymization and the Future of Data Science - KDnuggets,"This data included pickup and dropoff locations, pickup and dropoff times, fares, and tip amounts. At first glance, the data didn’t raise too many privacy flags on its own, which is why New York released the data in the first place. But then a Northwestern University graduate student used timestamped photos of celebrities getting into cabs in New York City to find specific cab rides within the released data. This meant he could determine how much a certain celebrity tipped the driver, making what was thought to be anonymous data suddenly newsworthy material"
Anonymization and the Future of Data Science - KDnuggets,"There are other ways to implement a similar link attack. If you had a taxi receipt, for example, you could use the cost of the ride and dropoff date/time based on when the receipt was created to discover the trip. For what it’s worth, New York did try to anonymize the data by hashing (a technique to mask data) the taxi medallion number (though even that attempt was crackable through a"
Anonymization and the Future of Data Science - KDnuggets,"In addition to masking the taxi medallions, the city could have applied further anonymization to this dataset. If they generalized the pickup/dropoff to the nearest hour, then the link attack would fail. The same query I used above would return zero results because no entry would exist between those two minutes, since every entry would be rounded to the hour -"
Anonymization and the Future of Data Science - KDnuggets,"This is a technique called k-anonymization. In k-anonymization, high cardinality values are generalized, or “blurred”, to try to prevent link attacks. In fact, you can think of k-anonymization as giving you"
Anonymization and the Future of Data Science - KDnuggets,"Unfortunately, k-anonymization is by no means foolproof. Even if we round the pickup/dropoff date/time in the data to prevent from this attack, the adversary could still break NYC’s efforts to preserve the anonymity of the dataset. If the adversary knew the location of the pickup"
Exploring the Impact of Geographic Information Systems - KDnuggets,"Geographic Information Systems are seeing a new popularity in recent times partly thanks to the COVID-19 Outbreak that has prompted may people to use spatial analysis to showcase the spread of the virus. GIS has mostly been behind more popular buzzwords like machine learning and deep learning. GIS has always been around us in the background being used in government, business, medicine, real estate, transport, manufacturing etc"
Exploring the Impact of Geographic Information Systems - KDnuggets,"In the world of franchising, franchisees are assigned a location and/or territory by their franchisor to operate their own business using the franchisor's brand. Using GIS analysis, franchisors can give their franchisees insights about their location such as a demographic profile, competitor analysis, traffic generators etc to market the attractiveness of franchise locations. These services can help businesses to have a better assessment of their site to expand while minimizing risk"
Exploring the Impact of Geographic Information Systems - KDnuggets,"Educational institutions operate on the base of demographics. By using spatial data, education institutions can isolate areas of interest that have an abundance of their core demographic (e. Countries are divided by spatial boundaries and these spatial files can carry valuable demographic information that can help these institutions make decisions"
Exploring the Impact of Geographic Information Systems - KDnuggets,"Smart City GIS is an integrated cross-sectoral platform to collect, manage, compile, analyze and visualize spatio-temporal information for sustainable urban planning, development and management. GIS is deployed at every stage of planning and development of a Smart City. The underlying framework is served by ICT (Information and Communications Technologies), while the focus is on the ‘spatial’ or GIS. The common platform operates through all stages of the life cycle – from modeling, planning, building to managing – across the full spectrum of functionalities"
Exploring the Impact of Geographic Information Systems - KDnuggets,"In 2020, Public Health applications of GIS is arguably the most mainstream application at the moment. GIS has been used to study and visualize the spread of COVID-19 across the globe and inform the public of what is happening. GIS was the catalyst for the spread of useful information for governments, businesses and the public. Epidemiologists are tasked with studying the spread of diseases in society and GIS has enabled parts of their analysis to reach the billions of people around the world"
Analytic Professionals – Share your views: Participate in the 2020 Data Science Survey - KDnuggets,"Rexer Analytics has been conducting the Data Science Survey since 2007.  Each survey explores the analytic behaviors, views and preferences of data scientists and analytic professionals.  Over 1100 people from around the globe participated in the 2015 and 2017 surveys.  A report summarizing the 2020 Survey findings will be available free to everyone in 4Q-2020"
"ODSC East 2017: Accelerate your Deep Learning & Machine Learning Knowledge and Career Opportunities, Boston, May 3-5 - KDnuggets",With change comes opportunity! The pace of data science innovation continues to quicken. New tools and techniques are constantly emerging especially around deep learning and machine learning. In 2017 many more companies are embarking on data science projects
"ODSC East 2017: Accelerate your Deep Learning & Machine Learning Knowledge and Career Opportunities, Boston, May 3-5 - KDnuggets","Connecting with over 4,500 of your data science peers is essential for acquiring the contacts necessary to advance your career. Get hired at the ODSC Career Fair. Data science is a huge field and it takes time to find the right role for you. This is your chance to meet face-to-face with recruiters from a number of cool startups and Fortune 500 companies"
"ODSC East 2017: Accelerate your Deep Learning & Machine Learning Knowledge and Career Opportunities, Boston, May 3-5 - KDnuggets","Being an exceptional data scientist is not easy. It takes the right combination of learning, training and networking to maximize your full career potential. Data science is your life’s journey and events like ODSC East should be on that road. We hope you’ll join us, here’s a special offer"
How to stay out of analytic rabbit holes: avoiding investigation loops and their traps - KDnuggets,"The process goes as follows: a data scientist defines a hypothesis, then explores the data, gains insights into the data that help explain the hypothesis better. After this step the loop begins – a new information allows to refine the hypothesis and start “digging deeper” while repeating the data exploration, insight generation and… re-refining the hypothesis again. This is where the loop starts and it’s important to be conscious about it from the very beginning. Falling into an analytic rabbit hole starts here if one thing isn’t defined – a supported decision"
How to stay out of analytic rabbit holes: avoiding investigation loops and their traps - KDnuggets,"If the decision is not defined or it’s not the main goal of the analytic investigation – the project will go down the drains to the rabbit hole. Why? Because the over-analysis begins when the data scientist starts focusing on the hypothesis instead of the decision. While the two might look very similar, in reality this makes a fundamental difference between a successful data science project and an “analytic rabbit hole”. I am going to describe the two approaches and how one leads to success while the other is doomed to fail"
How to stay out of analytic rabbit holes: avoiding investigation loops and their traps - KDnuggets,"As the data exploration goes, the hypothesis is constantly refined and new insights are discovered. The curse of this process is that since the goal is to find the perfect answer or a solution to the hypothesis a data scientist will fall for many traps such as spurious correlations where relationship between un-related though correlated variables are discovered. Eventually the breadth of ways of analyzing and cutting through the data start having their side effect – the hypothesis is broken out into sub-segments each of which have a series of data points, assumptions and conflicting conclusions of their own. A typical end for this project is a happy data scientist presenting these immense findings to a non-technical team who get lost in the details faster than the data scientist starts explaining a second bullet-point. A question that knocks this effort down goes something like this – “can we do something about it?” That’s it. Weeks spent and one question derails the whole effort"
How to stay out of analytic rabbit holes: avoiding investigation loops and their traps - KDnuggets,The focus of this exploration is to find ways to influence and improve a decision. And to test whether it moves the needle as soon as possible. Then and
How to stay out of analytic rabbit holes: avoiding investigation loops and their traps - KDnuggets,"This doesn’t close the analytic loop, but it ensures that the focus of the data scientist is to discover insights that can improve the impact of the underlying decision. In this case the focus is on how the project’s output impacts the environment, and both the data scientist and the business can learn from the response the environment has to the data-refined actions. Hypothesis testing without any actual intervention that uses the generated generated is a perfect example of an analytic rabbit hole"
How to stay out of analytic rabbit holes: avoiding investigation loops and their traps - KDnuggets,"While this may sound very trivial, the amount of time data scientists waste on hypothesis-focused projects is incredibly high. If this hypothesis-focused philosophy is left unchallenged it might even ruin their careers, while others can end the trust put into the data science department. And believe me – it’s very tempting to wake up your inner geek and fall into the analytic rabbit hole trap every time you are handed with a very cool and interesting hypothesis"
How to stay out of analytic rabbit holes: avoiding investigation loops and their traps - KDnuggets,Data scientist’s inner gut feeling tells that the main task of the job is to answer complex questions and gain in-depth insights. While in reality it’s all about solving problems – and the only way to solve a problem is to act on it. Our goal as data scientists is to support tough & complex decisions with actionable data-based recommendations. We are the ultimate internal consultants that drive actions through insights. And action with some insights is always better than
Outbreak Analytics: Data Science Strategies for a Novel Problem - KDnuggets,"You walk down one aisle of the grocery store to get your favorite cereal. On the dairy aisle, someone sick from COVID-19 coughs. Did your decision to grab your cereal before your milk possibly keep you healthy? How can these unpredictable, near-random choices be included in complex models?"
Outbreak Analytics: Data Science Strategies for a Novel Problem - KDnuggets,"It's the kind of question we're all asking as we make simple daily decisions during this global pandemic. But imagine now that it's your job to create a model for the spread of COVID-19 that accounts for humans' unpredictable, near-random choices. Your model also could include government mandates for social distancing, hospital care availability, pre-existing conditions among a population, and more"
Outbreak Analytics: Data Science Strategies for a Novel Problem - KDnuggets,"This strategy helped identify not only that SARS-CoV-2 should be correctly classified with other Coronaviridae and Betacoronavirus pathogens, but also that it has important similarities to other viruses found among bats. The researchers argue that their approach is faster (under 10 minutes, including 10-fold cross-validation) and can compare more, and more diverse, samples than previous analytic processes. While there may be insights here for the current pandemic, this kind of approach could be helpful in future outbreaks"
Outbreak Analytics: Data Science Strategies for a Novel Problem - KDnuggets,"However, generating such a model is never easy. Outbreak data can be impossible to gather accurately for many reasons, including institutional barriers, lack of testing, unknown or asymptomatic cases, and so forth. And, as we've seen, governments have implemented varied social distancing and isolation measures at different times, which can have unpredictable results on the number of people in that ""susceptible"" compartment"
Outbreak Analytics: Data Science Strategies for a Novel Problem - KDnuggets,"The eSIR model's forecasts aim to reveal ""turning points"" in the outbreak. Turning points include when the daily number of new cases stops growing, and when the number of infected cases reaches its highest point. The model can also provide an estimate of the R"
Outbreak Analytics: Data Science Strategies for a Novel Problem - KDnuggets,"The researchers are distributing an R package called eSIR that generates the models, ggplot2 objects, and summary statistics. What's useful about this approach is that it can help determine which quarantine strategies might be most useful and when. As the researchers say, "". This model provides one approach to that important calculation"
Outbreak Analytics: Data Science Strategies for a Novel Problem - KDnuggets,"Finally, the research I reviewed frequently mentioned the challenge of getting quality COVID-19 data upon which to build good models. Even in normal times, it can be hard to get the kind and quality of data we need. Models for aspects of the pandemic -- or any other phenomenon -- are only as good as the data upon which they are built. Data abounds everywhere, but not all are trustworthy, usable, or relevant. Every organization needs solid data gathering, management, and analytics structures in place (as"
"What Is Data Science, and What Does a Data Scientist Do? - KDnuggets","We’ve already discussed the business domain and communication pillars, which represent business acumen and top notch communication skills. This is very important for the discovery and goal phase. It’s also very helpful in that data scientists typically have to present and communicate results to key stakeholders, including executives"
"What Is Data Science, and What Does a Data Scientist Do? - KDnuggets","So strong soft skills, particularly communication (written and verbal) and public speaking ability are key. In the phase where results are communicated and delivered, the magic is in the data scientist’s ability to deliver the results in an understandable, compelling, and insightful way, while using appropriate language and jargon level for her audience. In addition, results should always be related back to the business goals that spawned the project in the first place"
"What Is Data Science, and What Does a Data Scientist Do? - KDnuggets","Education-wise, there is no single path to becoming a data scientist. Many universities have created data science and analytics-specific programs, mostly at the master’s degree level. Some universities and other organizations also offer certification programs as well"
"What Is Data Science, and What Does a Data Scientist Do? - KDnuggets","Company’s collect a ton of data, and much of the time it’s neglected or underutilized. This data, through meaningful information extraction and discovery of actionable insights, can be used to make critical business decisions and drive significant business change. It can also be used to optimize customer success and subsequent acquisition, retention, and growth"
DataScience Launches Interactive Tool For Exploring Data Science Trends - KDnuggets,"Because DataScience Trends was built on top of a three terabyte-dataset, the possibilities for exploration are nearly endless. To get users started, DataScience has included data from 10,000 of GitHub’s most popular repos, which can be viewed in terms of development activity, popularity, and collaboration. DataScience Trends also includes several other useful features for exploring open source software data:"
What Top Firms Ask: 100+ Data Science Interview Questions - KDnuggets,"A fresh scrape from Glassdoor gives us a good idea about what applicants are asked during a data scientist interview at some of the top companies. Unfortunately for us, almost every company has their interviewees sign NDAs. Since Glassdoor allows anonymity, a few brave souls have given us some fantastic examples of what they were asked during the interview process at top companies like Facebook, Google, and Microsoft"
What Top Firms Ask: 100+ Data Science Interview Questions - KDnuggets,"Given a list of followers in the format:123, 345234, 678345, 123…Where column one is the ID of the follower and column two is the ID of the followee. Find all mutual following pairs (the pair 123, 345 in the example above). How would you use Map/Reduce to solve the problem when the list does not fit in memory?"
What Top Firms Ask: 100+ Data Science Interview Questions - KDnuggets,"Suppose you’re given two binary strings, write a function adds them together without using any builtin string-to-int conversion or parsing tools. For example, if you give your function binary strings 100 and 111, it should return 1011. What’s the space and time complexity of your solution?"
What Top Firms Ask: 100+ Data Science Interview Questions - KDnuggets,"Split a large string into valid words and store them in a dictionary. If the string cannot be split, return false. What’s your solution’s complexity?"
What Top Firms Ask: 100+ Data Science Interview Questions - KDnuggets,"Imagine you are working with a hospital. Patients arrive at the hospital in a Poisson Distribution, and the doctors attend to the patients in a Uniform Distribution. Write a function or code block that outputs the patient’s average wait time and total number of patients that are attended to by doctors on a random day"
10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"Spring. Rejuvenation. Rebirth. Everything’s blooming. And, of course, people want free ebooks. With that in mind, here's a list of 10 free machine learning and data science titles to get your spring reading started right"
10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"Here is a quick collection of such books to start your fair weather study off on the right foot. The list begins with a base of statistics, moves on to machine learning foundations, progresses to a few bigger picture titles, has a quick look at an advanced topic or 2, and ends off with something that brings it all together. A mix of classic and contemporary titles, hopefully you find something new (to you) and of interest here"
10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,Think Stats emphasizes simple techniques you can use to explore real data sets and answer interesting questions. The book presents a case study using data from the National Institutes of Health. Readers are encouraged to work on a project with real datasets
10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"The Bayesian method is the natural approach to inference, yet it is hidden from readers behind chapters of slow, mathematical analysis. The typical text on Bayesian inference involves two to three chapters on probability theory, then enters what Bayesian inference is. Unfortunately, due to mathematical intractability of most Bayesian models, the reader is only shown simple, artificial examples. This can leave the user with a so-what feeling about Bayesian inference. In fact, this was the author's own prior opinion"
10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides a theoretical account of the fundamentals underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics, the book covers a wide array of central topics unaddressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds"
10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"This book descibes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book"
10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"This book provides an introduction to statistical learning methods. It is aimed for upper level undergraduate students, masters students and Ph.D. The book also contains a number of R labs with detailed explanations on how to implement the various methods in real life settings, and should be a valuable resource for a practicing data scientist"
10 Free Must-Read Books for Machine Learning and Data Science - KDnuggets,"This guide follows a learn-by-doing approach. Instead of passively reading the book, I encourage you to work through the exercises and experiment with the Python code I provide. I hope you will be actively involved in trying out and programming data mining techniques. The textbook is laid out as a series of small steps that build on each other until, by the time you complete the book, you have laid the foundation for understanding data mining techniques"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"The United States has come a long way from the days of total denial of global-warming. Despite how politicized the subject has become, people on both sides of the aisle have started to acknowledge the presence of climate change. Today, concern has shifted from questioning its"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"Stretching from our prairies and forests to our cities and farmland, humans have a long history of altering the environment to suit their needs, for better or for worse. The larger climate hasn’t been exempt from the human touch either. Through the use of tree rings and ice core samples, scientists have been able to identify, with precision, the concentration of greenhouse gases as far back as prehistoric times"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"The graph on the right presents an expanded view of recent history. This chart focus on an important change, the Industrial Revolution, and the 1750’s mark the point in time when humans began to produce carbon dioxide on a massive scale. We’ve seen continual growth since this point, with a concentration of 400 parts per million (PPM) in 2015 compared to the historical average of 243 PPM. This distinct break from the historical norm becomes even more alarming when you combine this with our knowledge of the behavior of greenhouse gases, known as the greenhouse effect"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,Scientists have been testing and verifying the greenhouse gas effect for nearly 200 years. Physicist Joseph Fourier first documented the heat trapping properties of gases like CO2 in 1824. This long tested principle is the foundation that demonstrates carbon dioxide’s significant role in altering our climate
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"Since first measuring the radiative forcing of each greenhouse gas, carbon dioxide’s influence has continually grown. Today, scientists understand that CO2 has exceeded historical norms and they can measure its impact on the retention of solar energy on our planet. Now, what is its impact?"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"Here’s what we know. The climate is changing and humans are to blame for this break from historical norms. We now stand at a crucial time in the future of our planet’s health and the climate that sustains all known life. The Environmental Protection Agency (EPA), the National Oceanic and Atmospheric Administration (NOAA), and the remainder of references included in this paper, provide excellent information and data on climate change. Information that is readily available to our nation’s leaders, such as Scott Pruitt, Administrator of the EPA. As we see record-breaking sea ice melts and warming global temperature year after year, Pruitt may find it prudent to examine all the data his agency has to offer, before continuing to spread false and misleading information"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"Etheridge, D.M.P. Steele, R.L. Langenfelds, R.J. Francey, J. Barnola, and V.I. Morgan. 1998. Historical CO2 records from the Law Dome DE08, DE08-2, and DSS ice cores. In: Trends: A compendium of data on global change. Oak Ridge, TN: U.S. Department of Energy. Accessed September 14, 2005"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"Neftel, A. Friedli, E. Moor, H. Lötscher, H. Oeschger, U. Siegenthaler, and B. Stauffer. 1994. Historical carbon dioxide record from the Siple Station ice core. In: Trends: A compendium of data on global change. Oak Ridge, TN: U.S. Department of Energy. Accessed September 14, 2005"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"NOAA (National Oceanic and Atmospheric Administration). 2016. Annual mean carbon dioxide concentrations for Mauna Loa, Hawaii. Accessed April 14, 2016"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"NOAA (National Oceanic and Atmospheric Administration). 2016. Monthly mean carbon dioxide concentrations for Barrow, Alaska; Cape Matatula, American Samoa; and the South Pole. Accessed April 14, 2016"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"Steele, L.P.B. Krummel, and R.L. Langenfelds. 2007. Atmospheric CO2 concentrations (ppmv) derived from flask air samples collected at Cape Grim, Australia, and Shetland Islands, Scotland. Commonwealth Scientific and Industrial Research Organisation. Accessed January 20, 2009"
It’s Getting Hot In Here: Data Science vs Fake News - KDnuggets,"Chamard, P. Ciattaglia, A. Monteleone. 2001. Atmospheric carbon dioxide record from flask measurements at Lampedusa Island. In: Trends: A compendium of data on global change. Oak Ridge, TN: U.S. Department of Energy. Accessed September 14, 2005"
"Standardization and Specialization in Analytics, Data Science, and BI - KDnuggets","Our research points to the beginnings of what may be standardization and specialization within the field. We are seeing evidence of a baseline graduate analytics curriculum that covers proficiencies in mathematics, statistics, computer science, IT systems, and organizational communications. We’ve also found programs that are tailored to specializations like data science (DS) and business intelligence (BI), as well as programs that offer specializations in areas like business and marketing analytics, government policy analytics, and healthcare analytics"
"Standardization and Specialization in Analytics, Data Science, and BI - KDnuggets","Aaron Gowins, an NIH data scientist, spoke to this development in a separate interview. There is great demand for data scientists, and there have been relatively few quality advanced degrees being offered. Therefore, the current trend is toward preferring experience and demonstrated ability, which online courses have done a remarkable job providing"
"Standardization and Specialization in Analytics, Data Science, and BI - KDnuggets","In an OnlineEducation. In his view, the line separating data analytics from data science is a bit fuzzier. When challenged to define the point that separates analytics from data science, however, I can’t, and argue feebly there’s a continuum from analytics to data science on a data/computation axis with endpoints [at] ‘not so much’ and ‘lots"
"Standardization and Specialization in Analytics, Data Science, and BI - KDnuggets","Even on the BI side of analytics, there can be diversification and specialization. As Jill Dyché, VP of Best Practices at SAS, explained in an interview, “A well-run BI program will include programmers, software experts, business analysts, data scientists, statisticians, and consultants. It will likely involve executives – Chief Analytics Officer and Chief Data Officer are newly visible roles in the BI space. And, of course, there are business users, who typically represent every major business unit in the company"
"Standardization and Specialization in Analytics, Data Science, and BI - KDnuggets","Our research indicates that online master’s in analytics programs are on a parallel track with the aforementioned developments. General data analytics programs are more prevalent those in data science and BI. These data analytics programs, which we group with business analytics programs because the curricula are so similar, target core proficiencies in applied mathematics, statistics, computer programming, data mining, and descriptive and predictive modeling. Some offer students the option of specialization through courses in subjects like marketing analytics, government analytics, and clinical research analytics. Data Science programs build off of the core analytics curriculum, incorporating advanced computer science coursework in highly technical areas like machine learning and artificial intelligence. BI programs emphasize database design, data warehousing, and dashboarding"
"Standardization and Specialization in Analytics, Data Science, and BI - KDnuggets","In an environment like the one Wetherill describes, standardization and specialization may be more an ideal than an imminent reality. We have also found this to be the case in academia. Schools are attempting to define clear curricula for data analytics, data science, and BI programs, and these curricula are becoming easier to identify. However, these programs reflect the state of the professions, where clear lines are still coming into focus"
Put Your Best Face Forward: The New Frontier of Communication - KDnuggets,"And I put together a great team of advisors, too. Roger Magoulas, VP of Business Strategy and Analytics at O’Reilly Media, keeps our business strategy focused and always one step ahead. Mike Sly, Advisor to Facebook Events, fuels our sales strategy and helps us get quality people in the room. And Eduardo Arino de la Rubia, Chief Data Scientist at Domino, is our industry squawk box and data science guru. Our advisor panel is rounded out by Jack Palmer, Founder and CEO of Plotly, Q McCallum, Senior Advisor at O’Reilly Media. Together, these influencers power our acceleration and steer our success"
"17 More Must-Know Data Science Interview Questions and Answers, Part 3 - KDnuggets","In the traditional data warehouse environment, comprehensive data quality assessment and reporting was at least possible (if not, ideal). However, in the Big Data projects the scale of data makes it impossible. Thus, the data quality measurements can at best be approximations (i. We also need to re-define most of the data quality metrics based on the specific characteristics of the Big Data project so that those metrics can have a clear meaning, be measured (good approximation) and be used for evaluating the alternative strategies for data quality improvement"
"17 More Must-Know Data Science Interview Questions and Answers, Part 3 - KDnuggets","The tremendous pace of data generation and collection makes it incredibly hard to monitor data quality within a reasonable overhead on time and resources (storage, compute, human effort, etc. So, by the time data quality assessment completes, the output might be outdated and of little use, particularly if the Big Data project is to serve any real-time or near real-time business needs. In such scenarios, you would need to re-define data quality metrics so that they are relevant as well as feasible in the real-time context"
"17 More Must-Know Data Science Interview Questions and Answers, Part 3 - KDnuggets","One of the biggest data quality issues in Big Data is that the data includes several data types (structured, semi-structured, and unstructured) coming in from different data sources. Thus, often a single data quality metric will not be applicable for the entire data and you would need to separately define data quality metrics for each data type. Moreover, assessing and improving the data quality of unstructured or semi-structured data is way more tricky and complex than that of structured data. For example, when mining the physician notes from medical records across the world (related to a particular medical condition) even if the language (and the grammar) is same the meaning might be very different due to local dialects and slang. This leads to low data interpretability, another data quality measure"
"17 More Must-Know Data Science Interview Questions and Answers, Part 3 - KDnuggets","Data from different sources often has serious semantic differences. For example, “profit” can have widely varied definitions across the business units of an organization or external agencies. Thus, the fields with identical names may not mean the same thing. This problem is made worse by the lack of adequate and consistent meta-data from each data source. In order to make sense of data, you need reliable metadata (such as to make sense of sales numbers from a store, you need other information such as date-time, items purchased, coupons used, etc. Usually, a lot of these data sources are outside an organization and thus, it is very hard to ensure good metadata for such data"
"17 More Must-Know Data Science Interview Questions and Answers, Part 3 - KDnuggets","Veracity, one of the most overlooked Big Data characteristics, is directly related to data quality, as it refers to the inherent biases, noise and abnormality in data. Because of veracity, the data values might not be exact real values, rather they might be approximations. In other words, the data might have some inherent impreciseness and uncertainty. Besides data inaccuracies, Veracity also includes data consistency (defined by the statistical reliability of data) and data trustworthiness (based on data origin, data collection and processing methods, security infrastructure, etc. These data quality issues in turn impact data integrity and data accountability"
"17 More Must-Know Data Science Interview Questions and Answers, Part 3 - KDnuggets","Data users and data providers are often different organizations with very different goals and operational procedures. Thus, it is no surprise that their notions of data quality are very different. In many cases, the data providers have no clue about the business use cases of data users (data providers might not even care about it, unless they are getting paid for the data). This disconnect between data source and data use is one of the prime reasons behind the data quality issues symbolized by Veracity"
"17 More Must-Know Data Science Interview Questions and Answers, Part 3 - KDnuggets","A common and old definition of data quality is that it is the “fitness of use” for the data consumer. This means that data quality is dependent on what you plan to do with the data. Thus, for a given data two different organizations with different business goals will most likely have widely different measurements of data quality.This nuance is often not well understood – data quality is a “relative” term. A Big Data project might involve incomplete and inconsistent data, however, it is possible that those data quality issues do not impact the utility of data towards the business goal. In such a case, the business would say that the data quality is great (and will not be interested in investing in data quality improvements). For example, for a producer of mashed potato cans a batch of small potatoes would be of same quality as a batch of big potatoes. However, for a fast food restaurant making fries, the quality of the two batches would be radically different"
"17 More Must-Know Data Science Interview Questions and Answers, Part 3 - KDnuggets","Data quality in Big Data projects is a very complex topic, where the theory and practice often differ. I haven’t come across any standard theory yet that is widely-accepted. Rather, I see little interest in the industry towards this goal.In practice, data quality does play an important role in the design of Big Data architecture. All the data quality efforts must start from a solid understanding of high-priority business use cases, and use that insight to navigate various trade-offs (samples given below) to optimize the quality of the final output"
"17 More Must-Know Data Science Interview Questions and Answers, Part 3 - KDnuggets","Given the magnanimous scope of work and very limited resources (relatively!), one common way for data quality efforts on Big Data projects is to adopt the baseline approach, in which, the data users are surveyed to identify and document the bare minimum data quality needed to ensure that the business processes they support are not disrupted. These minimum satisfactory levels of data quality are referred to as the baseline, and the data quality efforts are focused on ensuring that data quality for each data does not fall beyond its baseline level. It looks like a good starting point and you may later move into more advanced endeavors (based on business needs and available budget)"
The Most Underutilized Function in SQL - KDnuggets,"Over the past nine months I’ve worked with over a dozen venture-funded startups to build out their internal analytics. In doing so, there’s a single SQL function that I have come to use surprisingly often. At first it wasn’t at all clear to me why I would want to use this function, but as time goes on I have found ever more uses for it"
The Most Underutilized Function in SQL - KDnuggets,This case is similar to #1 in its execution but it solves a very different puzzle. Imagine the following case. You have the same Facebook Ads dataset as referenced earlier but this time you have a new challenge: join that data to data in your web analytics sessions table so that you can calculate Facebook
The Most Underutilized Function in SQL - KDnuggets,"Unfortunately that doesn’t work, for a really simple reason: it’s extremely common for some subset of those fields to be null, and a null doesn’t join to another null. So, that 6-field join is a dead end. You can hack together something incredibly complicated using a bunch of conditional logic, but that code is hideous and performs terribly (I’ve tried it)"
The Most Underutilized Function in SQL - KDnuggets,"Instead, use md5(). In both datasets, you can take the 6 fields we mentioned and concatenate them together into a single string, and then call md5() on the entire string. Here’s a code snippet from a client project where we did exactly this:"
Data Scientists Might Have It Made For 2017 - KDnuggets,"Companies all over the world have placed a lot of value on getting more insights from big data analytics. That’s not without good reason. After all, big data represents a new frontier in the business world. Now it’s possible to analyze vast amounts of data, allowing organizations to predict trends before they happen, make their operations more efficient, and better reach out to customers, just to name a few examples. Of course, using that big data isn’t always easy, which is why it has become imperative for businesses to hire the best big data talent available. Data scientists, like the insights derived from big data itself, are in high demand right now. That much was true in 2016, and it will definitely be true in 2017. With so much demand following them around, it’s safe to say that data scientists are in a very enviable position to kick off the year"
Data Scientists Might Have It Made For 2017 - KDnuggets,"Number one on the list is none other than Data Scientist. So what makes the data scientist the top rated job in the country? The survey took into account three factors: the number of job openings, overall job satisfaction, and salary. Needless to say, data scientist scored high in all categories. Job openings are plentiful as more companies realize they need to hire data scientists right away. Data scientists also get a lot of satisfaction from their jobs compared to other careers out there. And the salary for a data scientist is high"
Data Scientists Might Have It Made For 2017 - KDnuggets,"The importance of that last factor can’t be stressed enough. Organizations want data scientists on their teams, and they’re willing to pay top dollar for what at the moment is a rare skill set. It’s a simple example of the law of supply and demand. There’s a lot of demand for data scientists, while the current supply is low. As a result, data scientist salaries have skyrocketed in recent years. According to the same survey, the median base salary for data scientists is $110,000"
Data Scientists Might Have It Made For 2017 - KDnuggets,"Even the starting pay for data scientists is impressive, with some beginning at just under $90,000. Data science offers a lucrative career for those willing to put in the time and effort to hone their skills and gain the knowledge needed to excel. And when you take into account things like annual bonuses and even signing bonuses, data scientists have good reason to think they have it made. All signs indicate that 2017 will continue this trend"
Data Scientists Might Have It Made For 2017 - KDnuggets,"Virtualization remains a popular topic as organizations attempt to adopt virtualized and converged systems for their work. Nearly every business wants to become more “data-driven” in basically everything they do. None of this is possible without a qualified data scientists or two on board. That includes data scientists skilled in mathematics, programmings languages like Python and R, database knowledge, and excellent communication skills. So whether they’re combing through customer interactions or mining data across multiple"
Data Scientists Might Have It Made For 2017 - KDnuggets,"As such, companies are attempting to bridge what is commonly referred to as the big data skills gap. Finding the right people for data scientist positions is not an easy job. There are, however, quite a few data science programs being implemented by universities all over the country with the hopes they’ll be able to prepare the new generation for the demands of big data in the future. Many of these programs have actually partnered up with some of the biggest tech companies, like Cisco, to ensure that data scientist positions are always filled. Does that means the supply of data scientists will catch up to demand? For the moment, it’s difficult to say, in part because the demand for big data continues to rise. For now, it looks like demand will always be higher than supply. And that’s a position that data scientists are happy to be in"
Neuroscience for Data Scientists: Understanding Human Behaviour - KDnuggets,"Neuroscience, to me, refers loosely to the study of the brain. This can be accomplished in a variety of ways, ranging from “single neuron recordings” in which electrodes are placed into the neurons of simple organisms all the way up to cognitive neuroimaging of humans via methods such as functional magnetic resonance imaging (fMRI), positron emission tomography (PET), electroencephalography (EEG), and others. With single neuron recording of the brains of simple organisms, we get a very direct measure of activity – you can actually measure the neuronal activity over time (in response to presentation of some stimulus, for instance). Obviously we can’t typically do this type of recording on human beings; fMRI, PET, etc. These are maybe the two extremes of the neuroscience spectrum. There is overlap between “neuroscience” and “psychology” but not all people involved in what I think of as neuroscience are psychologists – there are also engineers, physicists, applied mathematicians, and, of course, statisticians"
Neuroscience for Data Scientists: Understanding Human Behaviour - KDnuggets,"I don’t think there is consensus among neuroscientists about anything relating to how people process information, make decisions, and the like! “Controversial” is perhaps too strong a word, but these things are very complex. Kahneman’s framework is appealing, and it provides a lens for understanding many phenomena that are observable in the world. I don’t think it’s the whole story, though. And, although I’ve not kept up with it, I believe that more recently there have been some studies that also disrupt the System 1/System 2 dichotomy. Clearly we have some way to go before we will reach deeper understanding"
Neuroscience for Data Scientists: Understanding Human Behaviour - KDnuggets,"First, I don’t consider all of these measures to be “neuroscientific” in nature, at least not as I understand the term. Changes in heartbeat and respiration, galvanic skin response – these are physiological responses, for sure, but even more indirect measures of brain activation than are EEG and fMRI. That’s not to say that they are unconnected altogether to how individuals are reacting to specific marketing stimuli. But, I think one should be careful in drawing sweeping conclusions based on these tools, which are imperfect, imprecise, and indirect. Second, as for fMRI, EEG, and other neuroimaging techniques, these are obviously closer to the source. I am skeptical, however, of the ability of some of these to capture “fast thinking. EEG has better time resolution, but its spatial resolution is poor. Reaching specific conclusions about where, when, and how our brains respond to marketing stimuli requires both temporal resolution and spatial resolution to be high"
Neuroscience for Data Scientists: Understanding Human Behaviour - KDnuggets,"I’ve mentioned some of this already: resolution is the key. With very basic statistical analysis, we can localize activation. That’s an advantage and a big part of the popularity of fMRI as an imaging technique. It’s harder from a statistical modeling perspective to understand, say, the order in which brain regions activate, or if activation in one region is leading to activation in another, which are often the real questions of interest to scientists (and, presumably, to those involved in neuromarketing as well). Many statisticians, applied mathematicians, and computer scientists are working on developing methods to answer these more sophisticated questions, but we’re not really there yet"
Neuroscience for Data Scientists: Understanding Human Behaviour - KDnuggets,"Another disadvantage of fMRI for neuromarketing is, I think, the imaging environment itself. What I mean by this is that you need to bring subjects to a location that has an MRI machine, which is this big very powerful magnet. They are expensive to acquire, install, and run, which is a limitation even for many research institutions. You have to put your test subjects into the scanner. Some people have claustrophobia, and can’t endure the environment. If you’ve ever had an MR scan, you know that the machine is very noisy, and that can bother and distract as well. It also means that the research (marketing research in this case, but the same holds true for any fMRI study) is carried out under highly artificial conditions; we don’t usually watch commercials while inside a magnetic resonance imaging scanner"
Neuroscience for Data Scientists: Understanding Human Behaviour - KDnuggets,"The resolution issues for EEG and fMRI are the opposite of each other. EEG has very good temporal resolution, so it is potentially able to record changes in neuronal activity more in real-time. For those who are attempting to pinpoint subtle temporal shifts, that can be an advantage. In terms of the imaging environment, EEG is much friendlier and easier than fMRI in general. Individuals just need to wear a cap with the electrodes, which is not that cumbersome or unnatural. The caps themselves are not expensive, which is a benefit for researchers as well"
Neuroscience for Data Scientists: Understanding Human Behaviour - KDnuggets,"On the other hand, the spatial resolution of EEG is poor for two reasons. One is that the number of electrodes on the cap is not typically large – several hundred spaced over the surface of the scalp. That may seem like a lot at first glance, but when you think about the area that each one covers, especially compared to the millimeter-level precision of fMRI, localization of activation is very amorphous. In addition, the electrodes are on the scalp, which is far removed from the brain in terms of the generated signal. All of this means that with EEG we have a very imprecise notion of where in the brain the activation is occurring"
Neuroscience for Data Scientists: Understanding Human Behaviour - KDnuggets,"The data are notoriously noisy, and furthermore tend to go through many stages of preprocessing before the statisticians even get to see them. This means that an already indirect measure undergoes uncertain amounts of data manipulation prior to analysis. That’s a huge challenge that many of us have been grappling with for years. Regarding noise, there are many sources, some coming from the technology and some from the subjects. To make it even more complex, the subject-driven noise can be related to the experimental stimuli of interest. For example, in fMRI studies of eye motions, the subject might be tempted to slightly shift his or her entire head while looking in the direction of a stimulus, which corrupts the data. Similarly, in EEG there is some evidence that the measured signal can be confounded with facial expressions. Both of these would have implications on the use of imaging for neuromarketing and other trendy applications. Furthermore, the data are large; not “gigantic” in the scale of many modern applications, but certainly big enough to cause challenges of storage and analysis. Finally, of course, the fact that we are not able to get direct measurements of brain activity and activation, and possibly will never be able to do so, is the largest measurement challenge we face. It’s hard to draw solid conclusions when the measured data are somewhat remote from the source signal, noisy, and highly processed"
Neuroscience for Data Scientists: Understanding Human Behaviour - KDnuggets,"I’ll admit to being skeptical that within 10-15 years we will fully understand the human mind. That’s a short time horizon and the workings of our mind are very complex. Also, what is meant by “cracking the code”? At the level of neurons and their interconnections I find it hard to believe that we will get to that point soon (if ever). That is a very fine scale; with billions of neurons in the human brain, there are too many connections to model. Even if we could do that, it’s not evident to me that the exercise would give us true insight into what makes us human, what makes us tick. So, I don’t think we will be building the artificial intelligence or computer that exactly mimics the human brain – and I’m not sure why we would want to, what we would learn from that specifically. Perhaps if we think instead of collections of neurons – what we call “regions of interest” (ROIs) and the connections between those, progress can be made. For instance, how do the various ROIs involved in language processing interact with each other to allow us to understand and generate speech? Those types of questions we might be closer to answering, although I’m still not sure that 10-15 years is the right frame. But then, statisticians are inherently skeptical!"
We Didn’t Start The Big Data Fire - KDnuggets,"Unless you are hiding under a rock, you know that the Big Data fire is raging. Whole lot of people have contributed significantly to the advancement of Big Data over the years and quite a few are continuing to influence. I learn every day from these pioneers and influencers so I wanted to recognize my ‘guru’s and I had couple of options:"
We Didn’t Start The Big Data Fire - KDnuggets,Here is my crude attempt to parody that great song in honor of Big Data pioneers and influencers. I relied on few sites to pick my list (sources are at the end of this article) and I am honored to say that I have interacted with a few of them personally. Challenge in coming up with a list like this is that there are lot more people that I am not aware of but are contributing significantly to big data and data science in general. Apologies in advance for not including many more but it is not intentional
We Didn’t Start The Big Data Fire - KDnuggets,"The following song has links to their linkedin profiles or twitter handles or their web sites. I strongly advise you to follow them if you can. With that out of the way, shall we start? Remember, keep that tune in your head as you hum (or read) along. Let’s take it from the top, shall we?"
How to Get a Data Science Job: A Ridiculously Specific Guide - KDnuggets,"Eventually you’ll reach a point, where you can’t quite bring yourself to follow through on step five. The position isn’t exactly what you were looking for but it feels so right, you can’t let it go. Then you’re done"
How to Get a Data Science Job: A Ridiculously Specific Guide - KDnuggets,"Every time a company doesn’t make you an offer, it will hurt. Even though you were going to turn them down, it will still feel like a punch to the stomach. You’ll question whether you are smart enough or young enough or experienced enough or prepared enough. This is normal. It sucks. Embrace the disappointment. Vent to your boyfriend or call your brother or go dancing or have a tall glass of Scotch. Whatever your process, work through it, all the while repeating in your mind “I am enough. Then loop back to step one and give it another go"
How to Get a Data Science Job: A Ridiculously Specific Guide - KDnuggets,"This method is guaranteed to work. It is a while loop, and the termination criterion is success. Or in the words of John Lennon, “Everything will be okay in the end. If it is not okay, then it is not the end"
How to Get a Data Science Job: A Ridiculously Specific Guide - KDnuggets,Any job search advice you’d like to add? Connect on LinkedIn and message me. I’ll gather it up and share it back around. Be sure to let me know if you’d like to stay anonymous
"Every Intro to Data Science Course on the Internet, Ranked - KDnuggets","I’m almost finished now. I’ve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role"
"Every Intro to Data Science Course on the Internet, Ranked - KDnuggets","It outlines the full process and provides real-life examples. At 21 hours of content, it is a good length. Reviewers love the instructor’s delivery and the organization of the content. The price varies depending on Udemy discounts, which are frequent, so you may be able to purchase access for as little as $10"
"Every Intro to Data Science Course on the Internet, Ranked - KDnuggets","In gretl, we will be able to do the same modeling just like in R and Python but we won’t have to code. That’s the big deal here. Some of you may already know R very well, but some may not know it at all. My goal is to show you how to build a robust model and"
"Every Intro to Data Science Course on the Internet, Ranked - KDnuggets",Kirill is the best teacher I’ve found online. He uses real life examples and explains common problems so that you get a deeper understanding of the coursework. He also provides a lot of insight as to what it means to be a data scientist from working with insufficient data all the way to presenting your work to C-class management. I highly recommend this course for beginner students to intermediate data analysts!
"Every Intro to Data Science Course on the Internet, Ranked - KDnuggets","It covers the data science process clearly and cohesively using Python, though it lacks a bit in the modeling aspect. The estimated timeline is 36 hours (six hours per week over six weeks), though it is shorter in my experience. It has a 5-star rating over one review. It is free"
"Every Intro to Data Science Course on the Internet, Ranked - KDnuggets",The videos are well-produced and the instructor (Caroline Buckey) is clear and personable. Lots of programming quizzes enforce the concepts learned in the videos. Students will leave the course confident in their new and/or improved NumPy and Pandas skills (these are popular Python libraries). The final project — which is graded and reviewed in the Nanodegree but not in the free individual course — can be a nice add to a portfolio
"Every Intro to Data Science Course on the Internet, Ranked - KDnuggets","It covers the full data science process and introduces Python, R, and several other open-source tools. The courses have tremendous production value. 13–18 hours of effort is estimated, depending on if you take the “R 101” course at the end, which isn’t necessary for the purpose of this guide. Unfortunately, it has no review data on the major review sites that we used for this analysis, so we can’t recommend it over the above two options yet. It is free"
"Every Intro to Data Science Course on the Internet, Ranked - KDnuggets","Our #1 pick had a weighted average rating of 4.5 out of 5 stars over 3,068 reviews. Let’s look at the other alternatives, sorted by descending rating. Below you’ll find several R-focused courses, if you are set on an introduction in that language"
Predictions for Data Science in 2017 - KDnuggets,"The only possibility to give AI a chance into the real world is to enhance traditional apps and services with intelligence sprinkled here and there. This will lead to applications and services that slowly solve the same tasks just with a more intelligent approach. Very few might notice the difference however. Another alternative is to train specialized networks that solve very specific tasks. Then find a way to connect these independent units together in order to solve what appears to be a more complex and general task.  We see many specialized machines in the near future, rather than a super intelligence that is good at everything. This strategy comes from many biologic systems that work like that. Even the Romans solved problems the same way and summarized this strategy under the term"
Predictions for Data Science in 2017 - KDnuggets,"We still agree with our previous statement by adding a fundamental condition. We do not think this is going to happen tomorrow due to the fact that human beings are not ready to delegate to any machine some of their critical tasks. If there will be bureaucracy to ask an algorithm to drive a car to the airport, there will be even more bureaucracy to ask the same algorithm to deliver the prognosis for a cancer patient. Humans are not ready for that, not because of lack of technology. Rather the need to decline responsibility to a physical institution/person for their mistakes"
Predictions for Data Science in 2017 - KDnuggets,"As a conclusion, loss of jobs, as stated by many, will not yet be the main socio-political consequence of AI as humans still prefer to keep control of their tasks until a trained algorithm will perform that task with almost-zero errors and definitely cheaper. This can happen soon but not in 365 days. Moreover, it will not necessarily mean that there will be people losing jobs. Many are ignoring the fact that in such a scenario humans can and will focus on other aspects of analytics or just other tasks"
Predictions for Data Science in 2017 - KDnuggets,Data becomes more and more available as it is easier to collect and cheaper to store. Big data solutions that were considered quite premature so far will become more a necessity for many. This will bring new products and new hardware solutions for small enterprise and for home computing too. If
Predictions for Data Science in 2017 - KDnuggets,"We do not expect unsupervised learning to come back in the near future. As data becomes more and more available, all the methods designed to deal with few observations, such as unsupervised learning and neural network pre-tuning will cease to make sense. There are already much better techniques to cope with small datasets, one of which being"
Predictions for Data Science in 2017 - KDnuggets,"There are major examples of data science used to improve the outcomes of epidemics and predicting patient behaviour. Only in 2015, data scientists helped predict further West Nile virus outbreaks in the United States, with 85% accuracy. Earlier this year, a team of scientists developed a model that can"
Predictions for Data Science in 2017 - KDnuggets,"But, in order to bring the technology that has proved to be effective in domains like social media and finance, we really believe that figures living at the border between healthcare and the technology itself will make a difference. These figures will be essential to bridge the gap between two worlds that really seem to speak very different languages. The major reason why AI has not taken over healthcare yet is because there are a lot of critical tasks that are pretty much surrounded by bureaucracy and responsibilities that are difficult to transfer. We have seen a similar scenario to the one encountered in the self-driving car example where, as previously mentioned, there is not really a technological gap, rather a bureaucratic one"
Predictions for Data Science in 2017 - KDnuggets,"Who is going to pay the eventual damage inflicted by an AI that goes mistaken and drives over a pedestrian or another car? In healthcare it would be difficult to deal with wrong prognoses. An artificial intelligence that is known to be wrong 5% of the times and that is scaled to millions, should deliver thousands of wrong prognoses and misleading predictions. Are we ready to abandon the idea that there is always someone to blame when things do not work?"
Predictions for Data Science in 2017 - KDnuggets,"We strongly believe that 2017 will be the year of deep learning. This will happen at one condition: practitioners and people from the business should first become aware of such technology. Hence, researchers must educate their business counterparts about the potential benefits of DL and convince that it is indeed a game changer"
Predictions for Data Science in 2017 - KDnuggets,This is partially happening with the benchmarks usually provided by the literature. But that will not be sufficient. We do not believe that DL will be massively adopted within a well established domain like healthcare if a neural model has been benchmarked on ImageNet or digit recognition tasks or any other standard benchmark used in published literature
7 Types of Data Scientist Job Profiles - KDnuggets,"There is no one profile for the Data Scientist, but I tried to make a few generic job profiles that can somewhat fit job descriptions of different companies. I think there is way too much variety, but I had to narrow down on a set of profiles. Check out the list"
7 Types of Data Scientist Job Profiles - KDnuggets,"I recently got in question on Quora asking something on lines of What exact skills do companies look for when they are recruiting a Data Scientist ? and is there a definition of Data Scientist profile ? As is pretty obvious, there is no one profile, as every company is solving its own set of problems. But I tried to make a few generic job profiles that can somewhat fit JDs of different companies. I think there is way too more variety, but I had to narrow down on a set of profiles, so here is the list:"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","The researchers test too many hypotheses without proper statistical control, until they happen to find something interesting.  Then they report it.  Not surprisingly, next time the effect (which was partly due to chance) will be much smaller or absent"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets",These flaws of research practices were identified and reported by John P. A. Ioannidis in his landmark paper
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","Let’s take an example below. Fig. 1 (a) shows 10 data points in one dimension i. It can be easily represented on a line with only 10 values, x=1, 2, 3. 10"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","But if we add one more feature, same data will be represented in 2 dimensions (Fig.1 (b)) causing increase in dimension space to 10*10 =100. And again if we add 3rd feature, dimension space will increase to 10*10*10 = 1000. As dimensions grows, dimensions space increases exponentially"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","This exponential growth in data causes high sparsity in the data set and unnecessarily increases storage space and processing time for the particular modelling algorithm. Think of image recognition problem of high resolution images 1280 × 720 = 921,600 pixels i. 921600 dimensions. OMG. And that’s why it’s called"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","In applied machine learning, success depends significantly on the quality of data representation (features).  Highly correlated features can make learning/sorting steps in the classification module easy. Conversely if label classes are a very complex function of the features, it can be impossible to build a good model [Dom 2012]. Thus a so-called"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","Redundant and irrelevant features are well known to cause poor accuracy so discarding these features should be the first task. The relevance is often scored using mutual information calculation. Furthermore, input features should thus offer a high level of discrimination between classes. The separability of features can be measured by distance  or variance ratio between classes. One recent work [Pham 2016] proposed a systematic voting based feature selection that is a data-driven approach incorporating above criteria. This can be used as a common framework for a wide class of problems"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","Parallelism is a good idea when the task can be divided into sub-tasks that can be executed independent of each other without communication or shared resources. Even then, efficient implementation is key to achieving the benefits of parallelization. In real-life, most of the programs have some sections that need to be executed in serialized fashion, and the parallelizable sub-tasks need some kind of synchronization or data transfer. Thus, it is hard to predict whether parallelization will actually make the"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","It is important to note that not every program can be effectively parallelized. Rather, very few programs will scale with perfect speedups because of the limitations due to sequential portions, inter-communication costs, etc. Usually, large data sets form a compelling case for parallelization. However, it should not be assumed that parallelization would lead to performance benefits. Rather, the performance of parallelism and sequential should be compared on a sub-set of the problem, before investing effort into parallelization"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","Imagine you are playing the game “Who wants to be millionaire?” and reached up to last question of 1 million dollars. You have no clue about the question, but you have audience poll and phone a friend life lines. Thank God. At this stage you don’t want to take any risk, so what will you do to get sure-shot right answer to become millionaire?"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","So if you have different models built for same data and same response variable, you can use one of the above methods to build ensemble model. As every model used in the ensemble has its own performance measures, some of the models may perform better than ultimate ensemble model and some of them may perform poorer than or equal to ensemble model. But overall the ensemble methods will improve overall accuracy and stability of the model, although at the expense of model understandability"
"17 More Must-Know Data Science Interview Questions and Answers, Part 2 - KDnuggets","The silhouette method measures the similarity of an object to its own cluster -- called cohesion -- when compared to other clusters -- called separation. The silhouette value is the means for this comparison, which is a value of the range [-1, 1]; a value close to 1 indicates a close relationship with objects in its own cluster, while a value close to -1 indicates the opposite. A clustered set of data in a model producing mostly high silhouette values is likely an acceptable and appropriate model"
Creativity is Crucial in Data Science - KDnuggets,"Many of the most successful people come across as innovative thinkers when they have interviews with us. They have no choice, moulding the data in unique and unexpected ways is their job. Just as Einstein found inspiration in his violin playing, many leading data scientists find that when their creative juices are flowing, they often find the most elegant solutions to the challenges that they face. These Data Creatives are some of hardest to find candidates – mainly due to the subjectivity involved"
Creativity is Crucial in Data Science - KDnuggets,"With endless ways of interpreting vast amounts of data, the role of human creativity cannot be underestimated. As increasingly more people see Artificial Intelligence as the answer to all our data needs, I think that we will come up against a road block when humans will always have that crucial upper hand. Artificial intelligence might be able to do a good job of interpreting the data (in a fraction of the time), but it will never do an outstanding job. For that, creativity is the missing link"
Creativity is Crucial in Data Science - KDnuggets,"Looking at the employee cultures of many of the leading global players, it is clear that harnessing their employees’ creativity is the key to getting them to collaborate more closely and have more empathy for each other. Data science does not exist in a vacuum, and it needs to relate to other areas in the business for it to have the maximum impact. Creatively packaging the numbers in a way that the rest of the business can understand is key to allow them to see the whole picture and therefore get behind finding the right solution"
Creativity is Crucial in Data Science - KDnuggets,"However, there is one big “but” in this matter. To be creative with the data requires a decision to dive beneath the surface, and the risk that this entails. Accepting the data at face value will often be good enough, you will keep your job, and you won’t ruffle any feathers with whacky conclusions. When you are creative to suggest something that is not obvious to the untrained eye, you are sticking your neck out, and this requires courage. However, you only have to do it successfully a couple of times for your wider team to give you the latitude to do so in the future. If the employer culture is resistant to such deviant thinking, it will be that little bit harder, but it is still possible"
"Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar Data, On Disk and In-Memory - KDnuggets","Columnar data structures provide a number of performance advantages over traditional row-oriented data structures for analytics. These include benefits for data on disk – fewer disk seeks, more effective compression, faster scan rates – as well as more efficient use of CPU for data in memory.  Today columnar data is very common and is implemented by most analytical databases, including Teradata, Vertica, Oracle, and others"
"Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar Data, On Disk and In-Memory - KDnuggets","In 2012 and 2013 several of us at Twitter and Cloudera created Apache Parquet (originally called Red Elm, an anagram for Google’s Dremel) to bring these ideas to the Hadoop ecosystem. Four years later, Parquet is the standard for columnar data on disk, and a new project called Apache Arrow has emerged to become the standard way of representing columnar data in memory. In this article we’ll take a closer look at why we need two projects, one for storing data on disk and one for processing data in memory, and how they work together"
"Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar Data, On Disk and In-Memory - KDnuggets","Most systems are engineered to minimize the number of disk seeks and the amount of data scanned, as these operations can add tremendous latency. In transactional workloads, as data is written to a table in a row-oriented database, the columns for a given row are written out to disk contiguously, which is very efficient for writes. Analytical workloads differ in that most queries read a small subset of the columns for large numbers of rows at a time. In a traditional row-oriented database, the system might perform a seek for each row, and most of the columns would be read from disk into memory unnecessarily"
"Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar Data, On Disk and In-Memory - KDnuggets","A columnar database organizes the values for a given column contiguously on disk. This has the advantage of significantly reducing the number of seeks for multi-row reads. Furthermore, compression algorithms tend to be much more effective on a single data type rather than the mix of types present in a typical row. The tradeoff is that writes are slower, but this is a good optimization for analytics where reads typically far outnumber writes"
"Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar Data, On Disk and In-Memory - KDnuggets","Parquet was also designed to handle richly structured data like JSON. It was very beneficial to us at Twitter and many other early adopters, and today most Hadoop users store their data in Parquet. There is pervasive support for Parquet across the Hadoop ecosystem, including Spark, Presto, Hive, Impala, Drill, Kite, and others. Even outside of Hadoop it has been adopted in some scientific communities, such as CERN’s ROOT project"
"Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar Data, On Disk and In-Memory - KDnuggets","The trade-offs being for columnar data are different for in-memory. For data on disk, usually IO dominates latency, which can be addressed with aggressive compression, at the cost of CPU. In memory, access is much faster and we want to optimize for CPU throughput by paying attention to cache locality, pipelining, and SIMD instructions"
"Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar Data, On Disk and In-Memory - KDnuggets","One of the funny things about computer science is that while there is a common set of resources – RAM, CPU, storage, network – each language has an entirely different way of interacting with those resources. When different programs need to interact – within and across languages – there are inefficiencies in the handoffs that can dominate the overall cost. This is a little bit like traveling in Europe before the Euro where you needed a different currency for each country, and by the end of the trip you could be sure you had lost a lot of money with all the exchanges!"
"Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar Data, On Disk and In-Memory - KDnuggets","We viewed these handoffs as the next obvious bottleneck for in-memory processing, and set out to work across a wide range of projects to develop a common set of interfaces that would remove unnecessary serialization and deserialization when marshalling data. Apache Arrow standardizes an efficient in-memory columnar representation that is the same as the wire representation. Today it includes first class bindings in over 13 projects, including Spark, Hadoop, R, Python/Pandas, and my company, Dremio"
"Apache Arrow and Apache Parquet: Why We Needed Different Projects for Columnar Data, On Disk and In-Memory - KDnuggets","Pandas is a good example of using both projects. Users can save a Pandas data frame to Parquet and read a Parquet file to in-memory Arrow. Pandas can directly work on top of Arrow columns, paving the way for a faster Spark integration"
"Webinar: The Data Science Sandbox as a Service, March 8 - KDnuggets","The data scientists at Carlson Wagonlit Travel have no shortage of data or ideas. They use R, SQL and advanced analytics on a huge volume and variety of corporate travel data from many sources. Find out how the team uses its Data Science Sandbox on Azure from Cazena. The platform includes everything storage, data movers, processing and embedded analytics tools including RStudio Server Pro, Hue Notebooks and others. See it in action at this expert"
3 minute demo:  Data Science Sandbox as a Service - KDnuggets,"Cazena is available on Microsoft Azure and AWS, and includes everything in one service: storage, compute, data movers, security and management. Use embedded analytics interfaces or connect with other visualization tools. Cazena is faster, easier and more cost-effective than ""DIY,"" with critical enterprise security and management features"
17 More Must-Know Data Science Interview Questions and Answers - KDnuggets,"For 2017, KDnuggets Editors bring you 17 more new and important Data Science Interview Questions and Answers. Because some of the answers are quite lengthy, we will publish them in 3 parts over 3 weeks. This is part 1, which answers the 6 questions below. Here is"
17 More Must-Know Data Science Interview Questions and Answers - KDnuggets,"Just before the Nov 8, 2016 election, most pollsters gave Hillary Clinton an edge of ~3% in popular vote and 70-95% chance of victory in electoral college. Nate Silver's FiveThirtyEight had the highest chances of Trump Victory at ~30%, while New York Times Upshot and Princeton Election Consortium estimated only ~15%, and other pollsters like Huffington Post gave Trump only 2% chance of victory. Still, Trump won. So what are the lessons for Data Scientists?"
17 More Must-Know Data Science Interview Questions and Answers - KDnuggets,"If we toss a fair coin 100 million times, we have the expected number of heads (mean) as 50 million, the standard deviation =10,000 (using formula 0.5 * SQRT(N)), and we can predict that 99.7% of the time the expected number of heads will be within"
17 More Must-Know Data Science Interview Questions and Answers - KDnuggets,"We had another example of statistically very unlikely event happen in Super Bowl LI on Feb 5, 2017.  After the half time, Atlanta Falcons were leading 21:3 after halftime and 28:9 after 3rd quarter. ESPN estimated Falcons win probability at that time at almost 100%"
17 More Must-Know Data Science Interview Questions and Answers - KDnuggets,"Never before has a team lost a Super Bowl after holding such advantage.  However, each Super Bowl is different, and this one was turned out to be very different.  Combination of superior skill (Patriots, after all, were favorites before the game) and luck (e"
17 More Must-Know Data Science Interview Questions and Answers - KDnuggets,"This is similar to case b, but applies to situation when data is not static -  we have a stream of data and we periodically sample it to develop predictive models of future behavior.  This happens in adversarial classification problems, such as spam filtering and network intrusion detection, where spammers and hackers constantly change their behavior in response. Another typical case is customer analytics where customer behavior changes over time.  A telephone company develops a model for predicting customer churn or a credit card company develops a model to predict transaction fraud.  Training data is historical data, while (new) test data is the current data"
6 Business Concepts you need to become a Data Science Unicorn - KDnuggets,"In medieval times, a Unicorn was a rare and mythical creature with great powers. In today’s world, a similar mythical creature is a Data Science Unicorn, who knows equally well the technology, data science, and business. Such professional is a most valuable resource of any data science team. Many data professionals are experts in the first two areas – technology and data science, but lack business/domain skills"
6 Business Concepts you need to become a Data Science Unicorn - KDnuggets,"Data science projects are not just about ETL and building models but rather about understanding business and its strategic problems, asking the right questions, and using technology along with data science to solve those problems. Failure in clearly understanding the business and its problems can doom data science/analytics projects and ultimately whole business strategy. Typical analytics teams consist of business/data analysts (who know business, define business problems, and interpret the data science results from business point of view), data scientists (who are data science experts) and data engineers (who are technology experts, developers, testers and administrators)"
6 Business Concepts you need to become a Data Science Unicorn - KDnuggets,"Though these 3 types of experts are on the same team, there might be a gap in correctly understanding the problem or interpreting the results after integrating their expertise. So, a champion is the one who knows all 3 areas and helps the team to better understand business and its problems, to find innovative solutions and interpret results in correct way to add value to the business. This  blog gives an overview of the business side of data science, especially for technology and data science professionals who want to advance their analytics career and become a “Unicorn”"
6 Business Concepts you need to become a Data Science Unicorn - KDnuggets,6. These decisions will affect the entire direction of the firm. An example may be to become the market leader in their field
6 Business Concepts you need to become a Data Science Unicorn - KDnuggets,"Tactical decisions are medium term, less complex decisions made by middle managers. They follow on from strategic decisions and aim to meet the objectives stated in any strategic decision. For example, in order to become the market leader, a firm may have to launch new products/services or open new branches"
The Data Science of NYC Taxi Trips: An Analysis & Visualization - KDnuggets,It feels like dream come true when you decide to work on a data which is truly “Big Data”. The data which is about to make me go gaga over it is NYC Taxi Trip Data. Thanks to open source technology believers who have helped many budding Data Scientists like me to learn and develop their skills. NYC Taxi & Limousine Commission shared almost 1.1 Billion taxi trips information in New York (from January 2009 to June 2015). The commission released newer version of 1.3 Billion taxi trips data (additional trips till June 2016)
The Data Science of NYC Taxi Trips: An Analysis & Visualization - KDnuggets,"It didn’t take me too long to realise that My Acer Laptop was totally outclassed when I tried exploring this data on it. Being Computer Scientist at Mind & Business Woman at Heart, I was looking for an alternative that can help me in testing my Data Scientist skills on this dataset. As always, Google turned out to be the solution as it is for most problems! I used Google BigQuery which stores massive datasets in the Cloud that can be explored using SQL queries. As Google says “BigQuery is Google's fully managed, petabyte scale, low cost analytics data warehouse”"
The Data Science of NYC Taxi Trips: An Analysis & Visualization - KDnuggets,BigQuery has Public Data Sets that can be explored and integrate into our software applications for Free (Priced/ Charged after a limit- You could look at the Pricing Calculator). BigQuery’s NYC TLC Trips public dataset has information till 2015 trips. This data include trips recorded from Yellow taxis in NYC. Here are the links of the data:
The Data Science of NYC Taxi Trips: An Analysis & Visualization - KDnuggets,PS: The next assignment that I will deal with is to perform Predictive Analysis. I plan to build a multi variate Regression model to find Tip Percentage for new trips. I hope you find this helpful. Share your comments and suggestions!
"KDnuggets Exclusive: Analytics and Data Science leaders in San Francisco, Apr 3-5 - KDnuggets",The Marketing Analytics & Data Science Conference is coming to San Francisco April 3-5. Exclusive offer for KDnuggets readers saves you 20% with VIP code MADS17KDN. Reserve your spot today!
"KDnuggets Exclusive: Analytics and Data Science leaders in San Francisco, Apr 3-5 - KDnuggets","It's dizzying. Now is the time to come together, compare notes, hear from experts and build relationships to share ideas. We are calling all professionals that want to harness analytics and data science"
"KDnuggets Exclusive: Analytics and Data Science leaders in San Francisco, Apr 3-5 - KDnuggets","The 2017 Agenda gives you access to laser focused, expertly curated content and workshops on mission critical issues, challenges and opportunities to ensure data security and customer privacy. Put customer needs and preferences at the center of your strategy. Now that's some pretty powerful stuff!"
The Data Science Project Playbook - KDnuggets,"I found it to be an inspiring and humbling experience in terms of seeing how bigger and more established companies are attacking these challenges. The talks were a mix of recipes, reflections, and advice. Since I have more of an engineering mind, some of the content was over my head. Overall, though, I learned so much from the presenters and other attendees. I walked away with some thoughts on how we as startup product leaders might best tackle data science challenges. I wanted to try to organize these thoughts as a playbook others could use for getting started"
The Data Science Project Playbook - KDnuggets,"Certainly, data science/machine learning/AI has achieved critical mass as a standalone industry. There is no shortage of platforms, tools, and algorithms available from a variety of vendors to tackle just about any application. On the other hand, finding experts with availability to tackle your challenges is a different matter. The big companies are all at war to lure away each other’s data scientists. That doesn’t leave much opportunity for the rest of us who are looking to build the next great chatbot or insight-driven application"
The Data Science Project Playbook - KDnuggets,"If you’re lucky enough to already have a data scientist on your payroll, then make this person your partner in planning and executing your projects. Meanwhile, understand that data scientists many times don’t have the same expertise and experience as other engineers in building and scaling all the other complex parts of applications. Be sure to get both data scientists and engineers involved in the planning of projects to best ensure success"
The Data Science Project Playbook - KDnuggets,"In the absence of a relationship with a subject matter expert, how should product leaders still pursue meaningful data science-driven features for their applications? I advocate for an extremely practical approach — as with most other product planning processes, get ready for a number of trade-offs. Luckily, the highly competitive environment of tools and platforms means that almost any dreamy feature can be built. For a product leader, then, the focus needs to be on finding the right feature and balancing the implications"
The Data Science Project Playbook - KDnuggets,"Our engineering and product teams are excellent at building and delivering features, but they may not yet have the experience and expertise to do this all on their own today. A data scientist provides a higher-level understanding of the possibilities in a given dataset, the right tool/technique to build a feature, and then (and equally important) how to put it into production. Fortunately, the internet is full of courses, learning materials, applications, and APIs that can help companies launch data science features even if they don’t have their own data scientist"
The Data Science Project Playbook - KDnuggets,"Today, almost every algorithm and technique can be pulled off the shelf. The real focus for our engineering teams should be on data preparation and loading, training and selection of models/algorithms/tools, and implementing those tools into production. Teams shouldn’t be building anything from scratch — it’s a waste of precious resources"
The Data Science Project Playbook - KDnuggets,"Just like with any other feature, it needs to be clear from the beginning how customer satisfaction will be measured. Given the additional complexities of data science projects, it’s even more important to create a tight loop between customer feedback and feature iteration. With the enormous dependency on data and potentially complicated models, it can be difficult for teams to isolate exactly why a given feature isn’t as effective as planned. The product leader has an important role in understanding how much work is anticipated for each iteration and will likely have to make judgment calls in terms of the value of additional work. Sometimes, it may be necessary to abandon a feature altogether if it seems that too much work will be required or if the results remain unpredictable"
The Data Science Project Playbook - KDnuggets,"Josh Wills powerfully observed that for many companies, data science efforts were just part of the portfolio of their product investments. He continued that in most cases, one or two of those efforts will pay for all of the rest of them. Getting started at data science is really hard — he called it an act of faith. He said companies like Facebook, Google, and Amazon are already way over that initial hump and now data science drives almost everything they do. Machine learning and data science are the tools those companies are using to create value, and user experience becomes a function of delivering insights and opportunities to make customers’ lives easier through automation"
"50+ Useful Machine Learning & Prediction APIs, updated - KDnuggets","Within each group of applications, we list in alphabetical order. API overview is based on information as it have appeared on  its URL as of  3-Feb-2017. Check out where these APIs are put into use! If we missed some popular active API, please suggest them in the comments below"
Forrester Study: Companies Using Data Science Platforms Are Surpassing The Competition - KDnuggets,"Forrester also found that because platforms unify technology and infrastructure, they help address tool sprawl — the number one business challenge cited by respondents. In fact, 46% of respondents said they lack an integrated approach to their data science technology stack. Platforms create institutionalize knowledge and promote collaboration, which is crucial in a market with widespread talent scarcity and retention concerns"
Forrester Study: Companies Using Data Science Platforms Are Surpassing The Competition - KDnuggets,"Whether a company is an Insights Leader or Laggard, Forrester found that businesses ultimately agree on the importance of data science. In fact, 99% of respondents consider data science an important discipline to develop, and 74% of survey respondents consider data science among their most important initiatives. But working across an abundance of tools that don’t integrate efficiently — as well as focusing too much on data collection and not enough on action — makes data science out of reach for many companies. Data science platforms can help"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","A common theme in these requests, however (and I say this with the utmost respect), is a general lack of understanding of what it is they are actually asking. And that's fine; everyone needs to start somewhere, no matter what it is they are learning. Instead of answering these similar requests one by one, this post will serve to lay out some"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","Before going any further, read the following articles. I mean it. Read. These. Articles"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","The first article provides a general overview of some of the dominant concepts in data science, with the second being an update to these concepts from earlier this year. The third article provides a deeper treatment of the concepts of data science and Big Data. The fourth and final article is a quick discussion touching on some of the complexities and nuances surrounding the use of the term ""data science"" versus a number of other terms"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","This is essentially an IT role, akin to the database administrator. The data management professional is concerned with managing data and the infrastructure which supports it. There is little to no data analysis that takes place in such a role, and the use of languages such as Python and R is likely not necessary. SQL may be of use, as well as Hadoop-related query languages such as Hive or Pig"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","This is the big Big Data non-analytic career path. The data infrastructure mentioned in the previous career path? Well, it needs to be designed and implemented, and the data engineer does that. If the data management professional is the car mechanic, data engineering is the automotive engineer. But don't get it twisted; both of these roles are crucial to both the delivery and continued functioning of your car, and are of equal importance when you are driving from point A to point B"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","While the previous pair of roles were related to designing the infrastructure to manage the data, as well as actually managing the data, business analysts are chiefly concerned with pulling from the data, more or less as it currently exists. This can be contrasted with the following 2 roles (machine learning researcher/practitioner and the data-oriented professional), both of which focus on eliciting insight from data above and beyond what it already tells us at face value. As such, business analysts require a unique set of skills among the roles presented"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","Machine learning researchers and practitioners are those crafting and using the predictive and correlative tools used to leverage data. Machine learning algorithms allow for the application of statistical analysis at high speeds, and those who wield these algorithms are not content with letting the data speak for itself in its current form. Interrogation of the data is the"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","This is the best description I could come up with for what could otherwise be referred to as the ""real"" data scientist. You know, the unicorns. Except, there are no unicorns, and anyone who says differently is lying"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","The data management professional and data engineer were concerned with the infrastructure which houses the data. The business analytics professional is concerned with pulling facts from the data as it exists. The machine learning researcher and practitioner are concerned with advancing and employing the tools available to leverage data for predictive and correlative capabilities, with both roles being algorithm-based (either developing, or utilizing, or both). The data-oriented professional is concerned primarily with the data, and the stories it can tell, regardless of what technologies or tools are needed to carry out that task"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","As an introductory article, I have intentionally left out any mention of the Internet of Things (IoT). This is for 2 reasons: first, I don't want to add any additional confusion for anyone trying to absorb all of this new material, and second, IoT is but a special case of data, and each of these roles can apply to IoT data with, perhaps, some modifications. But the core truths remain"
"5 Career Paths in Big Data and Data Science, Explained - KDnuggets","I hope this overview has been of use to some people looking to start off on a ""Data Science"" or ""Big Data"" career path, but weren't quite sure where or how to begin. Keep in mind that this is in no way an exhaustive curriculum for taking on any of the roles mentioned herein. It is a good place to start for individuals with little understanding of data professions, however"
Fixing Deployment and Iteration Problems in CRISP-DM - KDnuggets,This article discusses the two related problems of how analytics are deployed and managed once the analytic work is complete. A focus on the business decision-making involved helps address both problems. In CRISP-DM this involves changes to the Deployment phase and the iteration loop as shown in Figure 1
Fixing Deployment and Iteration Problems in CRISP-DM - KDnuggets,"A depressingly large number of analytic models that would help the business never actually do so. Many analytic models get developed but are not deployed and used in a reasonable time-frame (or indeed at all). As the survey results on a recent International Institute for Analytics webinar showed, 60% of projects failed to act on their analytics in “months”. Results from other surveys and research suggest that for many of these analytics, months turn into years and that many analytics are never deployed at all. Teams that want to develop analytics that make a difference need to include deploying their analytic into production, and integrating it with their business environment, as a critical step in the project. This is what the Deployment phase of CRISP-DM is for.Several elements go into successful deployment"
Fixing Deployment and Iteration Problems in CRISP-DM - KDnuggets,"The team must select an appropriate analytic delivery option. The style of analytic (is it visual or numeric, fixed or interactive for instance) must match the style of decision making intended. An automated decision, embedded in a website, requires a different approach to a manual decision being made by a call center rep. An analytic that must be used to make a decision by a mobile worker is different from one that supports a desktop worker and so on. A decision model reveals the organizational impact and shows whose behavior will have to change as well as the systems or processes will have to be re-coded around the analytical decision"
Fixing Deployment and Iteration Problems in CRISP-DM - KDnuggets,"Analytic teams must also remember that decision-making requires an analytic model to be wrapped with decision-making logic to make it actionable.  A decision model shows how to do this, modeling the non-analytic elements of the decision-making as well as showing how the analytic is used by the decision. For instance, a decision model for claims processing might include a fraud prediction as well as audit, regulatory and policy-based exclusions"
Fixing Deployment and Iteration Problems in CRISP-DM - KDnuggets,"Finally, there is the organizational change component of the project. Changing the way people make decisions, or changing the degree to which their systems make decisions for them, is almost always complex. A decision model provides a framework for understanding who is making which decision where and when. It also ties this decision-making to new and existing business metrics. This provides a clear picture of the organizational change that will be necessary"
Fixing Deployment and Iteration Problems in CRISP-DM - KDnuggets,Policies and regulations change. Markets and competitors react. What makes a good decision today might not make a good decision tomorrow. Analytics also age as data changes and must be monitored and kept up to date. An analytic model that is deployed and left unmonitored and unchanged risks becoming less effective and potentially even harmful
Fixing Deployment and Iteration Problems in CRISP-DM - KDnuggets,"Analytic teams need to identify the business and environmental factors that might cause an analytic model to become less effective. They must define the data that needs to be captured to show that the analytic works and to identify when it starts to work less well for any of these reasons.They should consider if the machine learning and adaptive analytic techniques are appropriate to continually refine the model and what boundary or alert conditions the business might require for such models. If manual updates are going to be required, then the schedule or triggers for these updates need to be defined. A clear plan should be developed for evolving each analytic deployed"
Fixing Deployment and Iteration Problems in CRISP-DM - KDnuggets,"A decision model shows the structure of a decision. This structure can be used to determine what interim results and business outcomes should be tracked. Combined with data about the model and its behavior, this allows ongoing monitoring and improvement of both the analytic model and the business decision it influences"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","My path has been a circuitous one. I had a childhood fascination with robots that was born from watching Luke Skywalker's prosthetic hand in the Empire Strikes Back. I played with computers and automobiles in high school, got an undergraduate degree in mechanical engineering, and studied robots for my masters and PhD at MIT. That was followed by a decade-long research career at Sandia National Laboratories and then a transition to industry through agriculture and tech"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","The metamorphosis from robotics researcher to data scientist was organic. Robotics requires the integration of different types of information. Signals can be noisy, sensors can be broken and the data can be extremely difficult to interpret. Robotics involves computer vision, signal processing, navigation, object recognition, decision making and all incarnations of machine learning. When you strip away the robots, the underlying problems look a lot like business and product problems that industry data scientists grapple with everywhere"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","A friend helped me discover the field of data science. He works as a professor, and one day when I was talking with him he pointed me to a job posting at an agricultural company. I realized that I was a good match for the skills required, and was surprised to find the job title was data scientist. I got the job and decided that I absolutely loved it. Coaxing insights and decisions out of big collections of numbers and values continues to hold a fascination for me bordering on addiction"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","Of all the classes that I took, the most useful professionally were those for my creative writing minor. While writing short stories I learned some fundamental ideas like writing for your reader and editing ruthlessly. Those more than any other skill have been useful in my career. I've found that I really deeply enjoy, if not the process of writing itself, then creating a written work at reaches someone or in someway sparks their understanding. This has motivated me to devote some of my free time to capturing answers to questions and sharing them around. I've also found that writing is a great way to talk to more people. There are some questions that I get asked often. It's very effective to set aside some quiet time, reflect and write a thoughtful answer. Then I can share that with people who ask me the same question in the future. I also use tutorial writing to help me understand machine learning concepts. Some of my most popular blogs, like"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","I have been somewhat surprised at how little truly introductory information exists on machine learning and data science topics. It seems that there is a natural tendency when we are familiar with the topic to forget what it's like not understand it. When I'm writing a tutorial I imagine that I'm explaining it to one of my children. That helps me to avoid using words and concepts that are unnecessarily complex, and when I do need to use them I take the time to define them in language that a 12-year-old would understand"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","I think of learning algorithms as tools in my toolbox. I don't have strong emotional attachments to anyone of my screwdrivers, but when I'm doing a job and I find just the right one for the job it makes me happy. I don't have favorite algorithms, but I do have favorite problems. We humans do things all the time that machines find extremely challenging. I interact with my world, receiving a massive amount of multimodal sensory information, but no labels or direct symbolic inputs of any sort. It's up to me to create my own set of symbols that meets my needs and to make decisions in a time constrained environment with partial and sometimes flawed information. And to do this well, I have to create complex models of the world, other people and myself, and I have to update them in a reasonable way on an ongoing basis. This set of problems, sometimes labeled the big AI problem or artificial general intelligence, capture my imagination like no others. I love that there are no tools that seem capable of solving this problem at the moment. That is a powerful motivator for me to join forces with the other people who have been enchanted by this challenge, to work on the edges of this problem and try to make a bit of headway"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","There are lots of smaller problems that I love as well. Anything that requires modeling of a complex system, such as how corn grows or how the brain works or how large network of people interact and make decisions. Existing naïve methods don't seem to do very well on complex problems like these. Folk wisdom and domain knowledge is a useful starting point, but incomplete. I deeply enjoy the process of codifying domain knowledge as much as possible and building it into a learning approach that is flexible enough to unlearn the few pieces of domain knowledge that happened to be counterfactual"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","There are two things that are easy to overlook when entering the data science profession. The first is clichéd: communication. A great data scientist is a bridge between technical people (software engineers, statisticians, machine learning experts) and non-technical people (program managers, business development experts, corporate leaders, and the public). The ability to tell a story or convey information in an understandable way is not just a plus, it's essential"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","The second thing that's really easy to overlook is more subtle. It is that it is important to understand the stories behind your data. How was each data point measured? Where they all measured by the same person? Are you certain that they mean exactly what you think they mean? How are they defined? What filtering or preprocessing has been done? Are the details of the methodology the same in every case? Are there systematic differences in how they're sampled? And when they were measured? What assumptions were made? Every single number has a story behind it. Of course you can't learn all their stories. But every minute of time that you invest learning a few of their stories will be paid back many times over in dead ends and misinterpretations avoided, and shortcuts taken to the answers that you need"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","Becca is the current state of my efforts to solve the artificial general intelligence problem. It's researchy and still only handles toy environments, but I'm proud to share it around and let anyone who wants to kick the tires. It has been sitting on the shelf for the last little while, but I hope to return to it again at some point in the not-too-distant future"
"KDnuggets Top Blogger: An Interview with Brandon Rohrer, Top  Data Scientist - KDnuggets","More relevant to this question, Becca has been my vehicle for learning Java and Python. It's been my tool for learning classes and object oriented programming. It is been my path to understanding numerical computation and to implementing my very own variations of deep neural networks and reinforcement learning. The piece of advice I give to anyone looking to become a data scientist or to become a better data scientist is to build things. Becca is what I'm building"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"More and more application workloads are moving to the different cloud platforms. This could be a move to a public, private or hybrid cloud (where the latter is a mixture of public and private). Big data and analytics application workloads are on the move too. It is important that the data science/engineering community has a good understanding of these clouds at a deeper level so as to make the best use of them for doing their analytics work more effectively"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"Although non-virtualized servers may be supported in some clouds, it is very rare in our experience that a cloud deployment would use this native hardware approach – and it can become inefficient. No cloud service provider wants to be duty bound to acquire and provision new hardware servers when you want to expand your analytics processing cluster or other distributed application– that kind of setup can take some time! Virtualization is the answer here, through rapid provisioning of virtual machines for this purpose – given the hardware capacity to do so, of course. Multi-tenancy on the cloud is also achieved through virtualization. Two tenant workloads may live on common servers, but are separated from each other through their encapsulation in virtual machines"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"Data scientists and data engineers have been accustomed to running their data processing and analysis work on a bare metal or physical environment up to now. But with the recent rapid growth in cloud infrastructure, these folks need to understand the new virtualized infrastructure within their clouds, as it is now underlying and controlling their workloads. We will go through the main points of interest for the data scientist in this virtualization area and the benefits from using it here"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"Many data science/engineering workloads are based on Hadoop and Spark platforms today, with Python, Scala, R, Java or others as the programming environment that operate on them. We see a big growth in interest in deploying these platforms to all types of clouds over recent months. As one example, the Databricks company, a leader in the development of Spark, deploys the platform to the public cloud, first. The Spark technology is absolutely suitable for the private cloud too, as we will see in a later section"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"Leading Hadoop distributors such as Cloudera, Hortonworks and MapR have developed tools for deploying their distributions to public and private clouds. The pure play analytics/machine learning vendors, like H2O and Turi also deploy their software as a service on the cloud. We see many smaller software companies deploying their big data products or infrastructure from day one to some form of cloud, no longer just to bare metal"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"There was an early misconception that virtualization in this way would slow the big data analytics workload down. However, extensive testing has shown that quite the opposite is true. Performance is as good as bare metal for virtualized private cloud-based big data workloads that use the underlying virtualization layer in the right way. We will show some testing results here to prove that point. The result of all of this is that the pace of companies’ moving to cloud is now picking up"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"Business managers ask their data science teams to find the answers to key business questions. Data scientists depend on their data engineers to integrate, load, cleanse, index and manage the data so it is suitably organized for their queries. These queries or jobs can range from questions about fraud detection, customer pattern analysis, product feature use, sentiment analysis, product quality or many other business areas"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"Data scientists often iterate several times on the solution to a data analytics question. They refine queries that give different answers to a question over time before they are happy with the results. They expand or contract the quantity of data used for queries and along with that, the processing framework that holds the data – such as a Spark cluster as one example. This is a dynamic environment, where the amount of compute power and storage needed to support the analysis is unpredictable. Demand on the infrastructure can fluctuate widely over the course of a single project’s lifetime. This variability means that the application infrastructure must be open to expansion and contraction at will, according to user needs"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"The types of software services that data scientists need will vary too – requiring a lot of freedom of configuration by the end user community. One group may be using dashboards, another workbooks/notebooks, others SQL engines for querying data while others still will write programs in Python and Scala to process the data. The toolkit for the data scientist/data engineer is growing continually with new features appearing regularly"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"This is the key area in which virtualization and cloud can help these communities.  When managers such as the CDO are concerned with keeping their data analytics teams operating at maximum efficiency, they don’t want the infrastructure getting in the way.  By carving up their total set of computing resources into pools that can be allocated to teams flexibly, they can avoid the single-use purpose to which many physical clusters were initially put and use cycles from elsewhere that happen to be available"
Why the Data Scientist and Data Engineer Need to Understand Virtualization in the Cloud - KDnuggets,"In big data, one infrastructure does not fit all processing needs. At the Hadoop level, for example, older distributed platforms may be suitable for some batch-type workloads whereas Spark should be used for other more interactive requirements or for iterating over a dataset. This means that there will almost certainly be more than one distribution of the platform (e. We have found many versions and two or more distributions of this software in use at once at many of the enterprises that we interact with, for example. Virtualization provides the key to running all of these variants at once, with separation between them"
"Learn Tools, Tricks & Techniques of Data Science in Boston, Apr 3-5 - KDnuggets","One of the biggest challenges that organizations face for implementation of big data and data science programs is finding the right people with the right skills. A lack of understanding of big data technologies, the influx of too many new technologies and too much 'hype' are common barriers to adoption. Our experts can help you sort through the hype and understand the technologies needed for implementation"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"Data science is an exciting, changing field. Curious minds and enthusiastic investigators can often get bogged down by algorithms, models, and new technology. If we’re not careful, we forget what we’re actually here to do: solve real problems. And if what we do is just theory, what’s the point?"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"In short, data science should result in real applications. The reality of this is multi-faceted. One important problem is managing data teams to get to that real world result. By using"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"It’s a fact that data science results are probabilistic and unpredictable. At the start of a project, it can often look like there’s an obvious route from A to B. When you get started, it’s never that simple. Agile teams do away with strict planning and go into projects with a creative mindset; they embrace uncertainty instead of shying away from it"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"At the same time, the agile planning method focuses hard on application to the customer’s problem. Otherwise, it’s easy for us to get lost down the rabbit hole of stringent rules about hypotheses, models, and results. In the latter scenario, we end up producing things that work—that validate our hypotheses—but that have little application to the real world scenario we’re producing them for. Wasting time is not good for us or our customers"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"It’s great to have a method, but it helps to see how it’s used to solve a real problem. At SVDS, we used this method to create a system that tells train riders when the Caltrain is running late to a stop, and its approximate time of arrival. Let’s dive into how that worked"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"Stories are completed during sprints, which is a set chunk of time to work on tasks, typically two weeks, with the goal of producing new results. A sprint starts with sprint planning, where we’ll decide, with customer stakeholders, which epics to start or continue. Data scientists will break these down into stories for that sprint"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"Each day during the sprint, the team gathers in a standup meeting. Here they report their progress, say what they’re going to do next, and coordinate to remove blockers where people are stuck on their work. Standups, as their name suggests, aren’t for long discussions or problem solving. The point is to get information out there quickly, and set up any further discussion. Optionally, a customer stakeholder may attend these standup meetings. Alternatively, we schedule one or two updates with them separately each week. It’s important to keep them closely involved with progress"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"The last step in the sprint process is to hold a review meeting, where the team presents and evaluates results. Customer stakeholders also attend this meeting. We present the work, and the group discusses it. Is it good enough? Should we keep working on it? Will it be useful, or should we abandon it now? We typically combine our review meetings, which are an assessment of the work done, with our sprint retrospective meetings, which are an assessment of how the work was done"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"This keeps the group from spending long periods of time working on things that won’t actually benefit customers. If the work is incomplete, you discuss how to move forward. If it’s finished, you discuss what you learned from the experience. Agile teams are always learning from previous work"
Getting Real World Results From Agile Data Science Teams - KDnuggets,"Agile data science teams work in a way that is adaptable, collaborative, and produces usable results. They subscribe to the idea that data science can be creative and innovative. They embrace the unknown instead of making assumptions, and they don’t waste time beating their head against a wall for things that aren’t working"
Bringing Business Clarity To CRISP-DM - KDnuggets,Two of these problems relate to a lack of clarity about the business problem that undermines the business value of any analytic developed. Analytic teams need to model business decision-making to connect their analytics to the business goals as this will bring clarity to the business problem and enable analytic results to be evaluated in business terms. In CRISP-DM solving these problems involves changes to the Business Understanding and Evaluation phases as shown in Figure 1
Bringing Business Clarity To CRISP-DM - KDnuggets,"The first phase of CRISP-DM, Business Understanding, is all about bringing clarity to the business problem and so providing a business focus for the project. The need for this clarity is persistently under-called by analytic teams and overlooked in a rush to find data, analyze it and try new techniques. At best, analytic teams identify the business metrics they want to improve and then dive straight in"
Bringing Business Clarity To CRISP-DM - KDnuggets,"But analytics cannot improve metrics directly. Take customer churn, for instance. If you need to reduce customer churn you might develop various analytics – churn predictions, propensity to accept retention offer predictions, life time value predictions etc. But none of these analytics will, by themselves, improve your customer churn rate. Unless you change your behavior – unless you do something different – your churn rate is not going to improve. The analytics might give you clues as to what change you need to make and how you might need to change but unless you actually change behavior you will not get a different outcome"
Bringing Business Clarity To CRISP-DM - KDnuggets,"So what is it you need to change? Decision-making is the key – only if people and systems make different choices, different decisions, will results change. If that decision-making can be improved with analytics, then results will improve. For instance, simply having a churn model will not reduce churn but deciding to focus valuable retention offers on those customers most likely to churn will"
Bringing Business Clarity To CRISP-DM - KDnuggets,"To ensure their analytic results will improve business results, analytic teams need to connect the metrics they are focused on to specific decision-making that can be improved with analytics. And they need to do this explicitly in the Business Understanding step of CRISP-DM. They need to be able to capture which decisions must change, how these decisions are made, and by whom. This is where a decision model comes in"
Bringing Business Clarity To CRISP-DM - KDnuggets,"Decision modeling begins by understanding the business measures of success and finding the business decisions that impact those measures – especially the day to day, repeatable, decisions that do so. One or more of these decisions will have to be made differently – more analytically – if the project is going to make any business difference. A decision model clarifies this decision-making in terms of the question that must be answered to make the decision as well as the possible answers. It then breaks down the decision-making and represents it using a simple but powerful notation like Decision Model and Notation (DMN) standard to clarity the decision-making approach"
Bringing Business Clarity To CRISP-DM - KDnuggets,"Figure 2 shows an example decision model. It shows the overall business decision to be improved (the rectangle at the top) so the project’s business focus is clear. It breaks this decision-making down into its component sub-decisions (more rectangles), both to make it clear how the decision is made and to identify the role of the specific decision that should be made analytically. It shows the regulations, policies and relevant experience involved in making the decision (as Knowledge Sources, the document shapes), proposed analytic models and currently used data (as Input Data, the flattened ovals). Such a model meets all the requirements defined in CRISP-DM for Business Understanding"
Bringing Business Clarity To CRISP-DM - KDnuggets,"With a decision model in hand, analytic teams also have something to evaluate their models against. The analytic measures – how accurate is this model, what’s its lift – can be used to see how well it would help make the specific analytic decision. Changes in the analytical decision can be put into the broader business context to see how that will impact overall decision-making. This will show whether the analytic approach is a match for the original business intent. Knowing who is making the various decisions involved and in what context can also help make sure the style of analytic selected will work. For instance, not using a black box analytic for a decision that involves regulators or a visual analytic for a largely automated decision"
Bringing Business Clarity To CRISP-DM - KDnuggets,"If models don’t evaluate successfully against the decision model, then the project is back to square one. It might only need to look at more data or analyze the data in a different way, but more likely the team will need to sit down with the business and discuss how else the decision could be made. Many projects lose their way at this point, continuing to add new data or new refinements to an analytic without checking that they are still working toward a valid business objective. The data may not support the kind of analytic that would improve the decision as currently made but perhaps it will have revealed a way to improve a different decision or that the metric is not the right one. A return to Business Understanding and an updated decision model may be required to re-focus and give the project the purpose it needs to move forward"
Career Advice for Analytics & Data Science Professionals - KDnuggets,"In our experience working with many quantitative professionals over the years, the two main areas that contribute to long-term career growth are networking and continuous learning. Even though both can open up a lot of career opportunities down the road, networking can seem daunting for many, and when it comes to lifelong learning many professionals simply aren’t sure where to start. Here is our best advice about how to prepare for the road ahead!"
"The Data Science Puzzle, Revisited - KDnuggets","Deep learning still isn't as widespread as one might think, given its dramatic uptick in attention and hyped success this past year. However, I definitely see it (slowly) becoming more of a go-to tool for data scientists over the next few years. As it becomes a go-to classification technology, however, it becomes increasingly important to remember that"
"The Data Science Puzzle, Revisited - KDnuggets","As you may have noticed, however, AI has a perception problem. Just as data mining used to be a mainstream term that struck fear into the hearts of many (mostly related to the invasion of privacy), AI frightens the masses from an entirely different viewpoint, one that evokes SkyNet-style fears. I don't know whether we"
"The Data Science Puzzle, Revisited - KDnuggets","There is also this: though machine learning, artificial intelligence, deep learning, computer vision and natural language processing (along with a variety of other applications of these ""intelligent"" technologies) are all separate and distinct fields and application domains, even practitioners and researchers have to admit that there is some continually evolving ""concept creep"" going on any more, beyond the regular ol' confusion and confounding that has always taken place. And that's OK; these fields all started out as niche sub-disciplines of other fields (computer science, statistics, linguistics, etc. While it is important on some level to ensure that everyone who"
"The Data Science Puzzle, Revisited - KDnuggets","I'm actually still quite comfortable with this definition. Of particular note, I have found over the past year that considerable confusion exists between the terms data science and data mining. While it's not settled law, I stand by the following:"
Identifying Variables That Might Be Better Predictors - KDnuggets,I love the simplicity of the data science concepts as taught by the book “Moneyball. But I recommend to my students to start with the book “Moneyball. One of my favorite lessons out of the book is the definition of data science:
Identifying Variables That Might Be Better Predictors - KDnuggets,"This approach takes advantage of what the business stakeholders know best – which is the business. And this approach takes advantage of what the data science team knows best – which is data transformation, data enrichment, data exploration and analytic modeling. The perfect data science team!"
Identifying Variables That Might Be Better Predictors - KDnuggets,"Some of our best ideas come from people whose voices typically don’t get heard. Our Big Data Vision Workshop process considers all ideas to be worthy of consideration. If you do not embrace that concept, then you risk constraining the creative thinking of the business stakeholders, or worse, miss out on surfacing potentially valuable data insight"
Identifying Variables That Might Be Better Predictors - KDnuggets,"From this list of variables and metrics, the data science team sought to create an “Attrition Score” that can be used to identify (or score) at-risk customers. The data science team embraced the iterative, “fail fast / learn faster” process in testing different combinations of variables and metrics.   The data science team tested different data enrichment and transformation techniques and different analytic algorithms with different combinations of the variables and metrics to see which combinations of variables yielded the best results (see Figure 2)"
The Most Popular Language For Machine Learning and Data Science Is … - KDnuggets,"What programming language should one learn to get a machine learning or data science job?  That’s the silver bullet question.  It is debated in many forums.  I could provide here my own answer to it and explain why, but I’d rather look at some data first.  After all, this is what machine learners and data scientists should do: look at data, not opinions"
The Most Popular Language For Machine Learning and Data Science Is … - KDnuggets,"So, let’s look at some data.  I will use the trend search available on indeed.  It looks for occurrences over time of selected terms in job offers.  It gives an indication of what skills employers are seeking.  Note however that it is not a poll on which skills are effectively in use.  It is rather an advanced indicator of how skill popularity evolve (more formally, it is probably close to the first order derivative of popularity as the latter is the difference of hiring skills plus retraining skills minus retiring and leaving skills)"
The Most Popular Language For Machine Learning and Data Science Is … - KDnuggets,"Enough speaking, let’s get data.  I searched for skills used in conjunction with “machine learning” and “data science”, where skills are one of the prominent programming language Java, C, C++, and Javascript.  I also included Python and R which we know are popular for machine learning and data science, as well as Scala given its link to Spark, and Julia that some think is the next big thing here.  Running"
The Most Popular Language For Machine Learning and Data Science Is … - KDnuggets,"Third, Python is the clear leader, followed by Java, then R, then C++.  Python lead over Java is increasing, while the lead of Java over R is decreasing.  I must admit I have been surprised to see Java at the second place, I was expecting R instead"
The Most Popular Language For Machine Learning and Data Science Is … - KDnuggets,"Fourth, Scala growth is impressive.  It was almost non existent 3 years ago, and is now in the same ballpark as more established languages.  This is easier to spot when we switch to the"
The Most Popular Language For Machine Learning and Data Science Is … - KDnuggets,"There, Python is still the leader, but C++ is now second, then Java, and C at fourth place.  R is only at the fifth rank. There is clearly an emphasis on high performance computing languages here.  Java is growing fast though.  It could reach second place soon, as for machine learning in general.  R isn’t going to be near the top anytime soon.  What surprises me is the the absence of Lua, although it is used in one of the major deep learning frameworks (Torch).  Julia isn’t present either"
The Most Popular Language For Machine Learning and Data Science Is … - KDnuggets,"The answer to the original question should now be clear.  Python, Java, and R are most popular skills when it comes to machine learning and data science jobs.  If you want to focus on deep learning rather than machine learning in general, then C++, and to some lesser extent C, are also worth considering.  Remember however, that this is only one way of looking at the problem.  You may get a different answer if you are looking for a job in academia, or if you just want to have fun learning about machine learning and data science during your spare time"
The Most Popular Language For Machine Learning and Data Science Is … - KDnuggets,"Besides having support from many top machine learning frameworks, Python is good fit for me because I have a computer science background.  I would also feel comfortable with C++ for developing new algorithms, given I’ve programmed in that language for most of my professional life.  But this is me, and people with different background may feel better with another language.  A statistician with limited programming skills will certainly prefer R.  A strong Java developer can stay with his favorite language as there are significant open sources with Java api.  And a case can certainly be made for any of the languages on these charts"
"ODSC Masterclass Summit, San Francisco, March 1-2: Premium Data Science Training + Save 25% thru Jan 31 - KDnuggets","Deep Learning is on everyone's list of top skills to learn in 2017.  ODSC Masterclass Summit, San Francisco, March 1-2 offers 2 intense days of hands-on training in deep learning. Use discount code: KD25 for an extra 25% Off before it expires Jan 31 at 11 PM!"
"Marketing Analytics and Data Science conference, Apr 3-5, San Francisco – KDnuggets Offer - KDnuggets",The Marketing Analytics and Data Science conference is happening April 3-5 in San Francisco. Exclusive KDnuggets offer saves you 20% with VIP Code MADS17KDN. Register today!
"Marketing Analytics and Data Science conference, Apr 3-5, San Francisco – KDnuggets Offer - KDnuggets","The Supercharged 2017 Agenda gives you access to laser focused, expertly curated content and workshops on mission critical issues, challenges and opportunities to ensure data security and customer privacy. Put customer needs and preferences at the center of your strategy. Now that's some pretty powerful stuff!"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","The way to increase the number of women in AI, ML and data science is two-fold.  First, we must expand the definitions of the fields to include their interaction with the other sciences, including the biological and social sciences.  A prime example of the contribution of the social science approach to AI is development of new field to make ML more fair, accountable and transparent (FATML), a community in which many of the leaders are women.  Second, we need to establish networks among the women who are already in the fields.  A great example is Women in ML http://wimlworkshop.  The first WiML workshop took place at NIPS 2006 with almost 100 participants.  WiML 2016 had almost 600 participants, which was roughly 10% of NIPS.  The networks established through WiML tend to keep women in the field, and in particular to expose them to new opportunities"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","Natural language. Writing this feels like an anachronism, since I was working with a natural language processing back in the early 1990s. Some smart folks at a company called AICorp had figured out how to pose English questions to a database. A program would parse the question into words that would be referenced against an established lexicon that could be used to automatically translate the request into SQL. Thus the question, “What were last year’s revenues for Widget x compared to the prior year’s?” would reliably (though not necessarily quickly) return an accurate answer"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","But then again, in the tech world women are separated from men, both physically (more women work in the service sector, more men have offices with doors) and metaphorically. So I’ll stand by my knee-jerk answer: Offer women communities. After all, there are fewer of us around, so it’s much harder to find other like-minded colleagues who share our experiences. And we want to hear from men—especially from men in power—about how to widen the circle. Bring us together and acknowledge we’re a sub-group who wants to participate, be better, and help our companies thrive. Invite more of us to the recruiting table, and into the board room"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","In 2017, I think we can expect to continue to see an influx of people and dollars into the field. The deep learning hardware market will become more competitive and more complex, with new specialized hardware taking on GPUs. Companies that were not traditionally associated with machine learning will build machine learning teams to streamline their business"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","This is a complex issue and I will comment on just one aspect of it where I have some experience. This year OpenAI ran the first-ever Self-Organizing Conference on Machine Learning. We made a major effort to boost participation from groups that have traditionally been underrepresented in machine learning, and we learned a lot in the process. For conference organizers, I can definitely suggest having a good code of conduct (we used one created by the Ada Initiative) that ensures everyone will feel welcomed. We also found that it's very important to do a lot of outreach. We sought out several people from underrepresented groups and explicitly invited them to the conference"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","We found that people from underrepresented groups were less likely to be able to attend (due to not being able to get time off from work, obligations to care for family members, etc. Finally, we gave out travel grants to members of these groups who want not be able to attend otherwise. Overall, we feel that these efforts greatly improved the conference for all participants"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","1. At RE•WORK events over the course of 2016, both unsupervised learning and reinforcement learning became a more prominent feature in talks and discussions, startups taking part to the industry leaders in the field. I'm sure this will continue to advance further over the next year. In 2017, I am expecting to see further advancements in applying deep learning to understand and predict videos, working towards summarizing what happens in a video clip"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","2. It's a subject we are all aware of at RE•WORK as we are currently an all-female team! To try to get more women involved in the field, we have created a series of 'Women in Machine Intelligence' dinners to provide a welcoming and supportive environment for women to present their latest work to their peers, but also for women to attend to meet other women working in the field. The attendees are always a mix of large established companies, startups and researchers, to provide an interesting networking environment for women looking for inspiration, new job roles, or new collaborative partners at the event. We'll continue to grow these type of events in 2017 and hope to see yet more women becoming role-models for the next generation interested in AI and data science!"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","So much has happened in 2016, it's hard to say where to begin. On the industry side, I would say that 2016 was the year that data science went beyond analytics. For a long time, machine learning academics have had big ideas about intelligent agents making real-time decisions in real-world settings. And we've had some experimental success and fancy mathematics to support these ideas. But when you looked at industry, most people have clung to more mundane ""analytics"" applications. In this shallow view, machine learning algorithms are useful mainly for data visualization, building dashboards, basically decision support. For many years, a small cadre of major players like Google, IBM and Microsoft have integrated machine learning into live systems. But it's been rare. Over the last year, we've seen story after story of ML powering self-driving cars, neural machine transalation, personal assistants, and more. I expect these trends to continue"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","First, supervised learning models conditional probabilities P(Y|X), assuming data is observed, but not tampered with. Supervised learning tells us about correlation, but not causality. It tells us nothing about what might happen to Y when we"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","From a research persepctive, I saw 2016 as a year in which the primary focus of the applied machine learning community shifted beyond supervised learning. Two developments stand out: deep reinforcement learning (DRL) and generative adversarial networks (GANs). While these ideas have roots in papers from 2013 and 2014, respectively, this year each gave birth to a large community of dedicated researchers. Deep reinforcement learning weds the representational power of deep neural networks with the reinforcement learning paradigm, in which an agent optimizes a policy to increase the reward signal it receives. While the methods were made famous on games like Atari and Go, we're now starting to see more research on deep reinforcement learning for practical applications. Generative adversarial networks are a creative new approach to unsupervised learning, developed by Ian Goodfellow. Initially GANS were used primarily for generative modeling, but new papers extend them to tasks like auto-encoding and semisupervised learning"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","Looking forward to 2017, I see a few big trends. For one, there's a sense in industry and academia that dialogue systems may be the next big area to fall. With recent progress in speech recognition and sequence tagging, the major primitives are already in place. We should note, there are still some big open questions. What should the objectives of an artificial interlocutor be? But for constrained domains, I expect rapid progress. I also expect to see more advanced efforts to apply DRL to real-world problems. A final prediction is that machine learning will start to realize its promise in the medical domain. For some problems, like radiology, the basic technology should already be in place and the remaining hurdles may mostly be HCI and regulatory issues. For other problems, like predicting treatment response, I expect to see researchers start to think seriously about reinforcement learning for medicine"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","The lack of women and traditionally underrepresented racial groups in machine learning, and computer science broadly is a question I think about constantly. I'd like to help in any way I can. And yet I'm also aware that as a Caucasian man, there may be certain limitations on my ability to be a leader in this movement. Perhaps the biggest thing we can do is to give the women who are entering this field and embarking on their careers every possible opportunity to succeed. I think this includes some amount of affirmative action. It also includes being critical of the selection criteria we use. If CS departments prioritze students with many years of programming experience, this could reinforce the discrimination that keeps women out of CS in high school"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","Another thing we can do is support the organizations that are already working well. Women in Machine Learning (WiML) is a fantastic group that has organized workshops co-located with NIPS. Companies and universities should sponsor their events and support the great work they do to promote women in our field and showcase their work. This year, I attended as the co-author of a paper by my collaborator at UCSD, Subarna Tripathi. As stark as the demographic numbers seem, I'm optimistic about gender diversity in machine learning. Already, there are so many prominent ladies at the top of the field that everyone, regardless of gender, can look up to. Anima Anandkumar, Joelle Pineau, Jennifer Chayes, Finale Doshi-Velez, Anca Dragan, Fei-Fei Li, Suchi Saria are a few that pop out at me. Among my peer group I've also been grateful to learn a lot from Been Kim, Hanie Sedghi, Subarna Tripathi, and my collaborator on"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","From an application perspective, we're still at the very beginning of meaningful automation. In 2017 we'll see significant automation in very narrow domains. I expect much of this will be for high-value problems, so think of finance and medicine, not consumer applications, just yet"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","One of the things that I love about working in data science is that people come to the practice from a variety of different backgrounds, both academically and culturally, and that makes working in data science richer than other, more rigidly defined, disciplines. Diversity isn't just a matter of gender, either. Nor is it a pipeline problem. There's a textbook of content that could go here, and I'm hardly an expert, but this is just common sense: If you want to see more diversity in the set of people working in data science and machine learning, offer opportunities to a more diverse set of people. They'll be amazing. And when you invite them into your environment, do your best to make it welcoming"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","For 2017, I see more action in all these areas, especially in “systems of insight” – turning insights from visual and predictive analytics in to actions. This includes real-time streaming analytics for rapid intervention and action, at moments of truth in business processes. I also see less of a boundary between data preparation and visual analytics, as data wrangling and insight discovery become more intertwined"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","My TIBCO Data Science team currently comprises ~40% women. I've made no deliberate attempt to favor women in the hiring process. On analysis, I've been drawn to the work ethic, passion, productivity and professionalism of the women on my team"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","I like the dynamic of men and women on my team. It mirrors our day to day world, and I think it helps bring forth a rich spectrum of ideas. My team is also heterogenous on age, race, industry background and skills across stats, computing, data, viz, software development and presentation. The entire team enjoys the mentoring and collaboration across these dimensions"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","In my view 2016 has been a very good year for AI, Data Science (DS) and Machine Learning (ML). Firstly, the breadth of industries and areas where AI, DS and ML are being applied has seen considerable growth. From art and literature to science and business, I am seeing new applications almost on a daily basis. Secondly, the awareness of this field and its potential has greatly increased, more and more people are looking to study it, or change career. Finally, 2016 has delivered timely lessons of caution about the degree of reliance on models"
"AI, Data Science, Machine Learning: Main Developments in 2016, Key Trends in 2017 - KDnuggets","As in Computer Science or other technical fields, it may take some time to achieve a good gender representation. However, I am convinced that DS and ML will attract more women in the near future. This is because a successful data scientist requires skills such as story telling and an eye for detail which often come naturally to women. Again, the breadth of application will ensure that female data scientists can pick a niche, like data journalism or data science applications in psychology and sociology, if they choose to do so. I have personally been inspired by both female and male role models, and as long as we maintain supportive and collaborative work and study environments, the gender misbalance will go away"
Data Science Basics: Power Laws and Distributions - KDnuggets,"Many power laws exist. Furthermore, there are all sorts of other relationships between phenomena in the world. Investigating and understanding as many relationships as possible can be very insightful for data scientists looking to make sense of the data they are studying, and wanting to put it in terms others can understand. Beware the flip side, however; not all relationships can be approximated as such, and trying to force data into an artificial description can be misleading. Power laws (and descriptive relationships, in general) should be treated as another potential tool at your disposal"
"Big Data and the Internet of Things don’t make business smarter, Analytics and Data Science do - KDnuggets","Big Data does not convert data into actionable information. Big Data does not create value. But Data Science does, and it does not have to be complex or expensive, or even big"
"Big Data and the Internet of Things don’t make business smarter, Analytics and Data Science do - KDnuggets","It’s one thing to know how to play with numbers, but it’s more important to understand what insights these numbers reveal on the business, and what actions to take based on these insights. Experience and business knowledge plays a role, as well as curiosity and passion. Sometimes the best results come from unlikely people just because of their desire and persistence"
"Big Data and the Internet of Things don’t make business smarter, Analytics and Data Science do - KDnuggets",1.    Ask the right question by deciding which specific business problem to tackle. Make sure you are focused and spend every minute on high value business problems
"Big Data and the Internet of Things don’t make business smarter, Analytics and Data Science do - KDnuggets","2.    Sourcing and accessing the appropriate data sources. Organic big data are cheap and non-intrusive, but often not representative to the business problem. Seeking out the best, most representative data is a key initial task"
"Big Data and the Internet of Things don’t make business smarter, Analytics and Data Science do - KDnuggets",3.    Clean and transform these data sources. Massaging available data to address the business problem is time-consuming but crucial. The best analytics methods can’t make up for poor quality data
"Big Data and the Internet of Things don’t make business smarter, Analytics and Data Science do - KDnuggets",4.    Clearly define the response variables and explanatory features that inform the business problem. Crisp definition of variables and features is central to deriving actionable insights
"Big Data and the Internet of Things don’t make business smarter, Analytics and Data Science do - KDnuggets","8.    Refresh with source data as often as practical for the business problem at hand. Some data don’t change very fast and/or can’t be actioned right away. In other cases, we can drive action intraday or even in the moment"
"Big Data and the Internet of Things don’t make business smarter, Analytics and Data Science do - KDnuggets","Each of these steps is rooted in common sense, and keeping intensely focused on the business problem at hand. With this kind of focus, data science drives significant value across industries and functional areas. For business problems where data are in motion, we can get all the way through these 9 steps and drive extreme value to the business. Some examples include:"
"Big Data and the Internet of Things don’t make business smarter, Analytics and Data Science do - KDnuggets","D. He has been working on statistical software applications for the past 20 years, and has published more than 50 papers and several software packages on statistical methods. Michael has Ph.D"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"I’ve never been very big on New Year’s resolutions. I’ve tried them in the past, and while they are nice to think about, they are always overly vague, difficult to accomplish in a year, trite, or just don’t get done (or attempted). This year I decided to try something different instead of just"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"Open source software is only as good as its community and/or developer(s). Developers are human and typically cannot manage all bugs and feature requests themselves. My goal is to routinely contribute back to the community either with new features, or by fixing bugs that I discover. This not only helps the community at large, but also helps me as a software engineer. There is no better way to become an even better engineer than by wading through someone else’s code. While this is something I did all day every day at my $DAYJOB, I do it less while on my sabbatical"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"When I am asked, I typically recommend that people only post on their GitHub completed projects that they are proud of. I am thinking of using a secondary GitHub account for exploration. There are often times I start a project, get distracted and never complete it. But, many times I learn some interesting techniques or hacks that somebody else could use and do not have time to blog about it. Right now, all of that knowledge goes to waste. By sharing this work somebody else can find these gems, even if the project itself is not complete(d). In academia there is the mantra"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"This concept of sharing also applies to my persona at-home habit of writing one-off scripts. In software engineering we focus on reusable code. I want to start taking these one-off scripts and turning them into scripts with at least a command-line interface. This of course assumes that the script has some use to someone other than myself. It is my job to try to make it so, all in the name of sharing and contributing. It is not always about the"
Data Scientist New Year Resolutions for 2017 - KDnuggets,People say that communication is a big part of being a Data Scientist. I believe this depends on the type of Data Scientist role. A Data Science Engineer focuses more of his/her time on
Data Scientist New Year Resolutions for 2017 - KDnuggets,"I’ve built machine learning systems, but at the time I did not appreciate the full lifecycle of the system. The system needs to sell itself. Not only should it implement a model in a scalable way, it also needs to adapt to new data (online learning and tuning) and also . At the time, I thought this was a pain, but I now realize that this is what makes a system speak to the user: a full feedback loop"
Data Scientist New Year Resolutions for 2017 - KDnuggets,Most of my experimentation has been on localhost. I want to create a real app on Amazon EC2 or running on this host… just somewhere other than 127.0.0.1
Data Scientist New Year Resolutions for 2017 - KDnuggets,"Naturally, I do a lot of machine learning. Over the past 2 to 3 years, Deep Learning has been named the solution to every problem under the sun and I am sure it can be used to find that missing sock. I knew about neural networks but it wasn’t the branch of machine learning I focused much on, so I put the whole deep learning thing on the “not right now” list. During 2016, the field went from just a bunch of headlines by well-known practitioners, but I also saw what people I respected thought on the matter as well as academia"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"I never had an interest in computer vision because I was not sure if we could ever solve vision problems on consumer hardware and yet here we are. While deep learning is part of that, I feel that it may be a more natural fit for vision and it would be more accessible to me and others. Of course, I am also very interested in applications to"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"I love Python. I also love R. Both of them do pretty much everything I need to do as a Data Scientist. Ok, I also need SQL and Bash quite a bit as well. As a developer, I do not want to get stuck in my ways and for my brain to start to rust so I would like to learn a new language. Scala is the one that is really calling my name. I suppose its rise was due to the rise of Spark, but it seems to have idioms that make it very useful for Data Science. I am very conservative when it comes to learning new programming languages because I have been caught up in the fad of new languages that end up being popular for a year or two and then falling out of popularity, even if they are still useful. Think Haskell, Erlang…"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"If you look anywhere on Twitter or in the blogosphere, you are sure to read about some gadget somebody has developed using a Raspberry Pi, Arduino, or just plain ole’ circuit boards and components. Developing software can be exciting, but it can only do so much. We need circuit boards, sensors and other components to physically"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"Of course this something can be talking to servers, software, the cloud or other devices. I have very little idea of how electronics work at this basic level and I am looking for a challenge. Right now, my most advanced “gadget” is actually just a tiny computer powered by Raspberry Pi, which serves as a snowcam"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"My parents need a new doorbell, one that has a camera that always runs, has decent motion detection and sends alerts over multiple different channels of communication. We have the Ring which is proprietary and just does not do a great job of this. Wireless performance is terrible, and the bell only rings in one room. With some electronic components and either an Arduino or Raspberry Pi, I am convinced I can do better at least for our purpose. I can also access all of the video and alerts on my own server rather than having to pay and deal with the cloud. Another thing… my mother has an elaborate Christmas display in the front yard connected to several timers. The timers are"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"Who knows where I will end up in 2017, but if it involves me mountain biking somewhere other than Mammoth, Lake Tahoe or Southern California, I will consider that a success. Some places on my wishlist include Moab, UT, Bend, OR, Ashland, OR, Whistler, BC, Downieville (not really a trip though), Crested Butte, CO, Park City, UT and maybe Brevard, NC. Scotland?"
Data Scientist New Year Resolutions for 2017 - KDnuggets,"His interests include text mining, natural language processing and geospatial analysis. He previously worked at Facebook where he developed NLP systems and is finishing a Ph.D"
Data Science & Ancestry - KDnuggets,"History is exciting, especially if we can relate to it. If our ancestors have been involved in any historic event, we are much more likely to attribute importance to this happening, develop a desire to know the details well beyond storytelling. Who have our ancestors been? What traces did they leave in history? How far back can we track their blood lines? Many people share a passion for ancestry, travel to archives in different countries, puzzle together information from different documents to gather new insights about their roots. The internet provides those efforts with new and powerful tools for research. Not only are historic documents being digitized, but online networks of people with a passion for ancestry, , e"
Data Science & Ancestry - KDnuggets,"Anyone who has been involved in genealogy, who has been trying to track down distant relatives from the 16th century is well aware of one of the key challenges: surnames change overtime. The current surname is most often just a derivative of previous versions, phonetically adjusted overtime. Some names change completely, others might have gradually evolved with the centuries along with rules of orthography. Connecting the dots when combing through dusty documents in church and municipal archives is already a challenging endeavor, but automating this process at scale is rather difficult"
Data Science & Ancestry - KDnuggets,"There are fairly well-functioning machine learning algorithms, which account for potential phonetic transformations, spelling mistakes and allow for a clustering of those first and surnames. Nevertheless, the digitization of personal research endeavors and thus the respective family trees, allows for more fine tuning of those algorithmic libraries. For once, more information about the actual historic name transformation is feeding the algorithms – not just assumptions about potential phonetic iterations. As a matter of fact, the adaptations improve performance – which in turn enables genealogy research platforms to allow its users comprehensive searches"
Data Science & Ancestry - KDnuggets,"At the same time, searching for names – when not extremely rare – oftentimes does not narrow down the search to a manually controllable sub-set. With millions of documents available for reference, a lot of context information for the respective individual is available. Certain individuals might thus for example automatically be tagged with geographic locations. Sophisticated search algorithms make historic data accessible for the time-constrained researcher. And make the casual trip to the distant library obsolete. Ancestry. Powerful backend algorithms are of necessity, to make sense of this history overflow and make precisely that information accessible, which the user searches for. At the same time, when users build their family tree online, the platform is able to automatically give hints and recommendations to fill-out blank spots within the different branches. Having a large and active user base, machine learning is used to power the record linking activities, which build the engine for the “recommendation” and family history discoveries"
Data Science & Ancestry - KDnuggets,"Because looking at the potential is just too attractive: online genealogy platforms could contribute to the creation of a fully personalized immersion in history. Registered users could, for example, get location-based information about their family members historic presence at the respective place. Linked with relevant documents (personal and general) it would function almost like a portable, location-based museum. With increasing sophistication of image processing, it could even be sufficient to take a picture of the location to get contextual information"
Data Science & Ancestry - KDnuggets,"The organization of family history is fascinating, as it is composed of so many intersecting family trees. Attaching and providing information at the relevant nodes with data science is a key challenge. It bears not only benefits for users on their specific research quest, but also forms the backbone for a more personalized history experience. To add some craziness, ancestry"
"50+ Data Science, Machine Learning Cheat Sheets, updated - KDnuggets","Thus, there are thousands of packages and hundreds of programming functions out there in the data science world! An aspiring data enthusiast need not know all. A cheat sheet or reference card is a compilation of mostly used commands to help you learn that language’s syntax at a faster rate. Here are the most important ones that have been brainstormed and captured in a few compact pages"
"50+ Data Science, Machine Learning Cheat Sheets, updated - KDnuggets","Python is a popular choice for beginners, yet still powerful enough to back some of the world’s most popular products and applications. It's design makes the programming experience feel almost as natural as writing in English. Python basics or Python Debugger cheat sheets for beginners covers important syntax to get started. Community-provided libraries such as numpy, scipy, sci-kit and pandas are highly relied on and the NumPy/SciPy/Pandas Cheat Sheet provides a quick refresher to these"
"50+ Data Science, Machine Learning Cheat Sheets, updated - KDnuggets",The R's ecosystem has been expanding so much that a lot of referencing is needed. The R Reference Card covers most of the R world in few pages. The Rstudio has also published a series of cheat sheets to make it easier for the R community. The data visualization with ggplot2 seems to be a favorite as it helps when you are working on creating graphs of your results
"50+ Data Science, Machine Learning Cheat Sheets, updated - KDnuggets",For a data scientist basics of SQL are as important as any other language as well. Both PIG and Hive Query Language are closely associated with SQL- the original Structured Query Language. SQL cheatsheets provide a 5 minute quick guide to learning it and then you may explore Hive & MySQL!
"50+ Data Science, Machine Learning Cheat Sheets, updated - KDnuggets","Apache Spark is an engine for large-scale data processing. For certain applications, such as iterative machine learning, Spark can be up to 100x faster than Hadoop (using MapReduce). The essentials of Apache Spark cheatsheet explains its place in the big data ecosystem, walks through setup and creation of a basic Spark application, and explains commonly used actions and operations"
"50+ Data Science, Machine Learning Cheat Sheets, updated - KDnuggets",Hadoop emerged as an untraditional tool to solve what was thought to be unsolvable by providing an open source software framework for the parallel processing of massive amounts of data. Explore the Hadoop cheatsheets to find out Useful commands when using Hadoop on the command line. A combination of SQL & Hive functions is another one to check out
Data Science Basics: What Types of Patterns Can Be Mined From Data? - KDnuggets,"Data mining functionality can be broken down into 4 main ""problems,"" namely: classification and regression (together: predictive analysis); cluster analysis; frequent pattern mining; and outlier analysis. There are all sorts of other ways you could break down data mining functionality as well, I suppose, e. However, this is a reasonable and accepted approach to identifying what data mining is able to accomplish, and as such these problems are each covered below, with a focus on what can be solved with each ""problem"
Data Science Basics: What Types of Patterns Can Be Mined From Data? - KDnuggets,"Classification is one of the main methods of supervised learning, and the manner in which prediction is carried out as relates to data with class labels. Classification involves finding a model which describes data classes, which can then be used to classify instances of unknown data. The concept of"
Data Science Basics: What Types of Patterns Can Be Mined From Data? - KDnuggets,"Regression is similar to classification, in that it is another dominant form of supervised learning and is useful for predictive analysis. They differ in that classification is used for predictions of data with distinct finite classes, while regression is used for predicting continuous numeric data. As a form of supervised learning, training/testing data is an important concept in regression as well"
Data Science Basics: What Types of Patterns Can Be Mined From Data? - KDnuggets,"Data instances are grouped together using the concept of maximizing intraclass similarity and minimizing the similarity between differing classes. This translates to the clustering algorithm identifying and grouping instances which are very similar, as opposed to ungrouped instances which are much less-similar to one another. As clustering does not require the pre-labeling of classes, it is a form of unsupervised learning"
Data Science Basics: What Types of Patterns Can Be Mined From Data? - KDnuggets,"First, and most importantly to this discussion, outlier analysis is not its own method of mining as are the other problems above, but instead can actually use the above methods for its own goals (it's an end, as opposed to a means). Second, outlier analysis can also be approached as an exercise in descriptive statistics, which some would argue is not data mining at all (holding that data mining consists of, by definition, predictive statistical methods). However, in the interests of being exhaustive, it has been included here"
A Tasty approach to data science - KDnuggets,"Third, a new generation of agile companies have entered the markets. Local craft producers, for example, create strong brands with high quality, on-trend products. Their development cycle is shorter than multinational companies, creating a shorter time to market"
A Tasty approach to data science - KDnuggets,"Our data scientists mine public online data, which gives us general trend insights. We then complete an analysis using our molecular approach to food items. Here, we analyze aromas and measure their compatibility with other flavors. We also complete a brand analysis, which uses qualitative methods to describe the brand itself, including its visual identity and tone of voice, as well as the particular product’s attributes, such as texture and taste. We then can score flavor options against our analysis of the brand identity"
A Tasty approach to data science - KDnuggets,"The current toolbox in the fuzzy front end is not sufficient anymore. Marketing managers, development chefs, and R&D managers lack actionable market and consumer insights. Nowadays, research techniques work great in evaluation, but not during the idea generation phase"
A Tasty approach to data science - KDnuggets,"For us, it’s about ensuring that the right flavor comes onto the right market at the right moment. This is key to make a new flavor line successful. The market research we provide is tightly linked to consumer preference at that moment and into the future"
A Tasty approach to data science - KDnuggets,"We feel the time is now to embrace “data science” in the food & drink industry. We’ve set this ball in motion by becoming more active in market research organizations to increase the industry’s visibility. As an outsider in market research circles, we are excited at the prospective of challenging players in this sector to always be thinking of the next application of data"
A Tasty approach to data science - KDnuggets,She has MS in Business Analytics and Big Data from the IE Business School in Madrid and BS in Industrial Engineering. Natalia’s first passion is food. Before completing engineering school she attended culinary school
Sound Data Science: Avoiding the Most Pernicious Prediction Pitfall - KDnuggets,"Prediction is booming. Data scientists have the “sexiest job of the 21st century” (as Professor Thomas Davenport and US Chief Data Scientist D.J. Patil declared in 2012). Fueled by the data tsunami, we’ve entered a golden age of predictive discoveries. A frenzy of analysis churns out a bonanza of colorful, valuable, and sometimes surprising insights:[i]"
Sound Data Science: Avoiding the Most Pernicious Prediction Pitfall - KDnuggets,"But something had gone terribly wrong. The “orange car” insight later proved inconclusive. The statistical test had been applied in a flawed manner; the press had ran with the finding prematurely. As data gets bigger, so does a potential pitfall in the application of common, established statistical methods"
Sound Data Science: Avoiding the Most Pernicious Prediction Pitfall - KDnuggets,"Big data brings big potential—but also big danger. With more data, a unique pitfall often dupes even the brightest of data scientists. This hidden hazard can undermine the process that evaluates for statistical significance, the gold standard of scientific soundness. And what a hazard it is! A bogus discovery can spell disaster. You may buy an orange car—or undergo an ineffective medical procedure—for no good reason. As the aphorisms tell us, bad information is worse than no information at all; misplaced confidence is seldom found again"
Sound Data Science: Avoiding the Most Pernicious Prediction Pitfall - KDnuggets,"This peril seems paradoxical. If data is so valuable, why should we suffer from obtaining more and more of it? Statistics has long advised that having more examples is better. A longer list of cases provides the means to more scrupulously assess a trend. Can you imagine what the downside of more data might be? As you’ll see in a moment, it’s a thought-provoking, dramatic plot twist"
Sound Data Science: Avoiding the Most Pernicious Prediction Pitfall - KDnuggets,"The fate of science—and sleeping well at night—depends on deterring the danger. The very notion of empirical discovery is at stake. To leverage the extraordinary opportunity of today’s data explosion, we need a surefire way to determine whether an observed trend is real, rather than a random artifact of the data. How can we reaffirm science’s trustworthy reputation?"
Sound Data Science: Avoiding the Most Pernicious Prediction Pitfall - KDnuggets,"Statistics approaches this challenge in a very particular way. It tells us the chances the observed trend could randomly appear even if the effect were not real. That is, it answers this question:[iii]"
Sound Data Science: Avoiding the Most Pernicious Prediction Pitfall - KDnuggets,"Statistics is the resource we rely on to gauge probability. It answers the orange car question above by calculating the probability that what’s been observed in data would occur randomly if orange cars actually held no advantage. The calculation takes data size into account—in this case, there were 72,983 used cars varying across 15 colors, of which 415 were orange.[iv]"
Sound Data Science: Avoiding the Most Pernicious Prediction Pitfall - KDnuggets,"Looks like a safe bet. Common practice considers this risk acceptably remote, low enough to at least tentatively believe the data. But don’t buy an orange car just yet—or write about the finding in a newspaper for that matter"
"Supercharge Your Data Science Team with AnacondaCON Team Discount, till Jan 16 - KDnuggets","AnacondaCON '17 will help you conquer your biggest data science challenges. Learn from industry experts sharing what #OpenDataScienceMeans and their best practices. Get 2 for 1 ticket price thru Jan 16, 2017"
"3rd Annual Global Data Science Conference, Santa Clara, March 27-29, 2017 - KDnuggets","Global Data Science Conference emphasizes on sharing real world experiences, how to create a balanced big data science team, Panel Sessions, Keynote Sessions and workshop. It will be the largest vendor agnostic conference in Data Science space. The Conference allows practitioners to discuss Data Science through effective use of various techniques"
"3rd Annual Global Data Science Conference, Santa Clara, March 27-29, 2017 - KDnuggets","You’ll get up to speed on emerging techniques and technologies by analyzing case studies, develop new technical skills through in-depth workshop, share emerging best practices in Data Science and  future trends.  The depth and breadth of what’s covered at the annual Global Data Science conference requires multiple tracks/sessions.  You can either follow one track from beginning to end or pick the individual sessions that most interest you"
The 5 Basic Types of Data Science Interview Questions - KDnuggets,"So this is one of those ""show your work"" moments. Trace out every step of your thinking and write down the equations. As you’re writing out the solution, describe your thought process so the interviewer can see your mathematical logic at work"
The 5 Basic Types of Data Science Interview Questions - KDnuggets,"Proving your mettle requires showing you understand the fundamentals of statistics. But more than that, interviewers also want to see whether you're capable of using the technical language and logic of statistics to grapple with ideas you may not often approach that way—and still communicate them clearly. So be no-nonsense in your response. Use the relevant statistical knowledge to arrive at your answer, but be as direct as possible about whatever you're asked to define"
The 5 Basic Types of Data Science Interview Questions - KDnuggets,"In any event, this type of question tests your understanding of matrix computation and how to deal with vectors and matrices. So start by going through a sample set of inputs and outputs, and manually work out the answer. As you do, keep an eye on"
The 5 Basic Types of Data Science Interview Questions - KDnuggets,"This question is meant to see how you envision your work delivering products or services from end to end. Scenario questions don’t test for knowledge in every field; they're meant to explore a product's life cycle from beginning to delivery and see what limits the candidate might have at each stage of that process. But these questions also evaluate holistic knowledge—for instance, what it takes to manage a team to deliver a final product—to determine how candidates perform in team situations"
The 5 Basic Types of Data Science Interview Questions - KDnuggets,"Here, too, the usual job-interview advice applies: Be honest about where you can add a lot of value, but don’t be shy about where you expect to get a little bit of help from your teammates. Try to relate how your technical knowledge can help with business outcomes, and always explain the thought process behind your choices and the assumptions that guide them. And"
Springboard launches data science bootcamp with a job guarantee - KDnuggets,"The company tracked 50 of its graduates and saw that all 50 got a job within six months -- with a median increase of $18,000 in first-year salary. They've placed graduates at Boeing, Amazon, Pandora, and reddit. Now they’ve put their money where their mouth is by guaranteeing graduates of its new bootcamp a data science job or a 100% tuition refund"
Springboard launches data science bootcamp with a job guarantee - KDnuggets,"Springboard says the reason it’s confident it can live up to the promise is its world-class mentor network of data scientists, who have worked at companies like Facebook, Instacart, Jawbone and more. Springboard’s mentors are working practitioners who know what it takes to break into data science, and many of them are hiring managers themselves. This inside perspective gives their students an unprecedented leg up when it comes to getting data science jobs"
Springboard launches data science bootcamp with a job guarantee - KDnuggets,"Springboard’s rigorous 200-hour curriculum for Data Science Career Track was curated by experts from IBM, Cisco, and Pindrop Security. The course structure is self-paced and flexible enough to accommodate those working full-time. Students can expect to finish in six months if they spend 8-10 hours a week on the bootcamp. The online and self-paced curriculum allows engaged learners the ability to pick up industry-recognized certification without having to quit their full-time jobs"
Springboard launches data science bootcamp with a job guarantee - KDnuggets,"Once students learn the intricacies of machine learning, statistics, Python, SQL, Spark and Hadoop, they are given career resources and two final capstone projects. This is where they concretely use the technical skills and knowledge they’ve gained to build a meaningful data science project. Once it’s approved by their mentor, it can become an integral part of a data science portfolio"
Springboard launches data science bootcamp with a job guarantee - KDnuggets,"Throughout the course, students also benefit from 24-7 teaching assistants to help them accelerate their learning. They’re also paired with a dedicated career coach who will help them with interview prep and resume review. Finally, Springboard is developing partnerships with key employers to help surface placement opportunities for its graduates. Six months after the course finishes, if a graduate doesn’t get a data science job,"
Springboard launches data science bootcamp with a job guarantee - KDnuggets,"The new Career Track program is selective, and requires applicants to have some prior experience with statistics and programming. The job guarantee applies to participants in most major US cities. Springboard hopes to expand to more geographies soon"
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets","I have been working as a Data Science Professional for last 11 years and had the chance to engage with a number of employers, clients for their data science needs both as an employee and entrepreneur across multiple domains starting with Financial Services, Retail/FMCG/CPG, Telecom, Media and Entertainment, Digital Media, Education and Technology etc. During the last 11 years, I had the chance to observe and participate in the management practices and enterprise strategy with regards to data science and have closely observed the success and failures of those initiatives. I look back and reflect on the top 5 reasons that in my view inhibit the growth of Data Science Strategy"
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets","Some companies joined the bandwagon of data science because they wanted to be part of hype rather than be part of real value creation. These superficial goals often reflect that the top leadership is either not fully convinced about the effectiveness of data based strategy or was undertaking it due to one person’s fancy love (not the real knowledge) for the word “big data”. In the absence of clear mandate for data science as an input for business strategy, the organizational goals never got synced with the data science road-map and that led to unrealistic targets thereby resulting in abysmal failure"
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets","The first and foremost thing to implement an effective enterprise strategy with regards to Data Science (irrespective whether your business model is B2B or B2C) is to embrace change. Often the rigid hierarchies, departmental silos, complex political organizational dynamics become a barrier in implementing a central data strategy that does not foster real innovation. Everyone wants to stake a claim of the portion of the pie or the complete pie itself without knowing whether they are the right person in the first place to stake that claim or without understanding the changes it would involve in the ecosystem in ownership of a portion of that pie"
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets","Data Science could add more value to an organization if those hidden walls or barriers between groups are demolished and the data is unified. Politics often becomes a barrier. Or even if it is not possible to have a central data science structure, then a loosely held data science center of competence or excellence (resulting from a close partnership between 2 teams-For example IT and Marketing (or any other business function relevant to the data strategy) can be created that effectively engages to build a robust use case to solve a problem that becomes an example for other departments to follow the lead. If organisational dynamics do not allow even that, an Experimental Data Science Labs could be created which does not fall within the borders of specific Department/Team (but still has free access to the data source systems for experimentation, a parallel unit/team to existing technology units/teams) and is free from the politically motivated agendas and is headed by an able, objective leader (more on that in the next point-the 'go to' person)"
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets","Some initiatives are only undertaken to showcase involvement with data rather than the actual need. Flawed Hypothesis are built, leading to numerous hidden assumptions that result in crap business use cases. Effective Data Science is a healthy combination of Domain, Data, Mathematics and Statistics, Algorithms, Programming, Research/Experimentation and Art or simply science and art. We can automate science, but it is difficult to automate art or quantify what is abstract. Modeling rare events is so tricky and the best example of that is the recent US election where trump defeated all the opinion polls, predictions and all the experts fell flat on their face"
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets","There is a constant battle within the organizations between different stakeholders for political reasons wherein each stakeholder either wants to protect his territory or expand his existing territory. A ‘domain/business expert’ would try to emphasize the domain or the art to showcase his importance and stamp his authority in the area of communication and business strategy and would often come up with fancy questions without even knowing if there is supporting data for the same or whether it results in a useful metric or not, the Data Scientist would try to over-emphasize the mathematical part to showcase why every problem needs to be solved using mathematics and how PCA could solve a supply chain optimization problem even when there might be no need for it, and the data engineer would try to emphasize the technology (data warehousing) or implementation part to showcase why his role is no less than that of an Astronaut, which is such a prejudiced debate that the right question is often lost in the background. Each of them is important but they need to operate in tandem"
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets",Why and which model needs to be deployed in production goes back to initial hypothesis/business question under consideration. And that business question has to be tied back to specific goal or a metric and lot of times that is an empty field and an empty field for metric does not qualify for deployment. Even a periodicity of 12 model runs per year or even greater does not qualify for deployment unless you are producing a business metric that strongly ties back to the initial hypothesis and adds incremental value to the business
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets","In the absence of the right question, what would technology or implementation guy do? Implement a flawed hypothesis-does it make sense? And if we reverse the equation-in the presence of the right question but a wrong technical implementation/adoption of algorithm/model/sample (that fails miserably on out of time samples)-does that help either, which again takes me back to the point 2 above-lack of synergy between different stakeholders. An objective leader who has a right combination of strong Business/domain context (big picture) with equally strong Data Science context (core technical aspects in terms of core Data Science/Analytics topics) and is also open to embrace change could be your ‘go to’ man, but these people come at a premium and even if you find these people you need to allow them to function with freedom and authority to set up that ecosystem but that freedom and flexibility is often missing. Frustrated with the system, they often leave (for lack of acknowledgement for their passion and commitment to the cause by the management)"
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets","Every organization has numerous data sources and hence several hypothesis around the same that need to be prioritized in the order of incremental business value we can extract out of them and also according to the quality of data available. In the absence of that-we often waste time and resource on low value business questions that would not have generated much incremental value in terms of business outcome. To ensure that, we need to streamline activities around data sourcing and storage across all business functions to improve the quality of available data for analysis and then prioritize it according to the business value. Every Model is as good as its data and that needs to be always remembered"
"Top Reasons Why Big Data, Data Science, Analytics Initiatives Fail - KDnuggets","Treating Data Science projects with a Defined Beginning and Outcome could be a blunder as before an Organization reaches stability in its Analytics and Data Science paradigm, it will go through an extensive period (couple of years at-least or even more) of trial and error phase of what works and what does not, what data is relevant, what model performs and what model needs to be deployed eventually that produces a useful business metric that generates an incremental impact. The flexibility and agility to learn quickly from mistakes has to be part of DNA of the system but typically lot of people believe that Analytics/Data Science is a magic wand, and a button click of the algorithm will radically change the business outcomes for good-nothing could be more stupid than this. That is a sad reality"
The Data Science Delusion - KDnuggets,"], so to speak. One wonders, though, how many such unicorns could exist. It is also a laudable goal to move away from the extreme specialization of traditional or academic research ["
The Data Science Delusion - KDnuggets,"Yes. It’s good and bad. I think there’s this interesting question of, Well, what is a data scientist? Isn’t that just a scientist? Don’t scientists just use data? So what does that term even mean?"
The Data Science Delusion - KDnuggets,"You’ve had one of my co-authors, Hilary Mason, on the show, and the thing we joke about and we wrote about together, is that the number one thing about data scientists’ job description is that it’s amorphous. There’s no specific thing that you do; the work kind of embodies all these different things. You do whatever you need to do to solve a problem"
The Data Science Delusion - KDnuggets,"]. Of course, despite the poor coinage, data science may one day evolve into a real science by settling on sound foundational principles (like computer science did over its first couple of decades). For now, it seems to function more as a buzzword designed to attract talented scientists from various disciplines to work for business ["
The Data Science Delusion - KDnuggets,"The last decade has seen many areas of research (parallel processing, machine learning, visualization, statistical programming languages) maturing into technology, which had made it possible for one person (perhaps an expert in one discipline) to take a project through the stages of data ingestion and manipulation, statistical modeling, and visualization entirely on his or her own. This democratization of algorithms and platforms, paradoxically, has a downside: the signaling properties of such skills have more or less been lost. Where earlier you needed to read and understand a technical paper or a book to implement a model, now you can just use an off-the-shelf model as a black-box. While this phenomenon affects many disciplines, the vague and multidisciplinary definition of data science certainly exacerbates the problem"
The Data Science Delusion - KDnuggets,"]. These are sound suggestions, essentially acknowledging the difficulty of finding unicorn data scientists, and urging the creation of teams with the full range of skills. Such teams then face the problem of melding a disparate set of skills"
The Data Science Delusion - KDnuggets,"However, it is not simply an intersection of skills that is lethal, but these skills applied to the wrong task. In fact, the very set of skills that work well in one arena could be dangerous in another. Moreover, the fact that these skills rarely come together in one person, often leads to problems when data scientists work with each other"
The Data Science Delusion - KDnuggets,"Movement along the Modeling-Difficulty axis brings an increasing need for a specialized data scientist (e. For instance, a classification problem, where the domain already provides data in the required form (say lots of historical data was manually tagged, and the task is to automate the classification), probably slots into Quadrant-1 (Q1). Another example of a Q1 task would be sentiment-tagging when you have an off-the-shelf model pre-trained on data from the same domain. The task would move to Q3 if the data from the target domain is quite different and needs to be preprocessed and modeled anew"
Data Science and Big Data: Definitions and Common Myths - KDnuggets,"Data are quickly becoming a new form of capital, a different coin, and an innovative source of value. It is extremely important to learn how to channel the power of big data into an efficient strategy to manage and grow a business. A well-set data strategy is becoming fundamental to every business, regardless the actual size of the datasets used. However, in order to establish a data framework that works, there are a few misconceptions that need to be clarified:"
Data Science and Big Data: Definitions and Common Myths - KDnuggets,"Not all data are good quality data, and tainting a dataset with dirty data could compromise the final products. It is similar to a blood transfusion: if a non-compatible blood type is used, the outcome can be catastrophic for the whole body. Secondly, there is always the risk of overfitting data to the model, yet not derive any further insight — “if you torture the data enough, nature will always confess” (Coase, 2012). In all applications of big data, you want to avoid striving for perfection: too many variables increase the complexity of the model without necessarily increasing accuracy or efficiency. More data always implies higher costs and not necessarily higher accuracy. Costs include: higher maintenance costs — both for the physical storage and for model retention; greater difficulties in calling the shots and interpreting the results; more burdensome data collection and time-opportunity costs. Undoubtedly the data used do not have to be conventional or used in a standard way — and this is where the real gain is locked in — and they may challenge the general wisdom, but they have to be proven and validated. In summary, smart data strategies always start from analyzing internal datasets, before integrating them with public or external sources. Do not store and process data just for the sake of having them, because with the amount of data being generated daily, the noise increases faster than the signal (Silver, 2013). Pareto’s 80/20 rule applies: the 80% of the phenomenon could be probably explained by the 20% of the data owned"
Data Science and Big Data: Definitions and Common Myths - KDnuggets,"Let’s also not forget that people are affected by a wide range of behavioral biases that may invalidate the objectivity of the analysis. The most common ones between both scientists and managers are: apophenia (finding patterns where there are no patterns at all), narrative fallacy (the need to fit pattern to series of disconnected facts), confirmation bias (the tendency to use only information that confirms some priors - and the corollary according to which the search for evidence will eventually end up with evidence discovery), and selection bias (the propensity to use always some type of data, possibly those that are best known). A final interesting big data curse to be pointed out is nowadays becoming known as “Hathaway’s effect”: it appeared that when the famous actress appeared positively in the news, stock prices in Warren Buffett’s Berkshire Hathaway company increased. This suggests that sometimes there exist correlations that are either spurious or completely meaningless and groundless"
Data Science and Big Data: Definitions and Common Myths - KDnuggets,"Data on its own are meaningless, if you do not pose the right questions first. Readapting what DeepThought says in The Hitchhikers’ Guide to the Galaxy, big data can provide the final answer to life, the universe, and everything, as soon as the right question is asked. This is where human judgment comes in: posing the right question and interpreting the results are still competences of the human brain"
10 Tips to Improve your Data Science Interview - KDnuggets,"Interviews are designed to weed out candidates that aren’t qualified to do a job.  However, they’re not perfect and there are aspects of interviews that are not representative of the real-life job.  Some people take a cynical approach and use this fact as an excuse when they get rejected.  They say things like, “Why would they ask me that?  Anyone could just look that up!”  Although that may be true, ranting doesn’t get you a job.  For a company it makes more sense to reject good people than to hire a bad one.  As Data Scientists we should know all about the tradeoffs of false positive vs true positive rates!  In this article I’ll give you some tips and resources for improving your interviewing skills so that you’re less likely to get rejected for a bad reason"
Data Science Trends To Look Out For In 2017 - KDnuggets,"Machine Learning is here to stay, with more firms following Google and Facebook  in the race to attract the best machine learning experts and Data Scientists.  We also see a merger of IoT and Data Science.  Read on for more trends"
Data Science Trends To Look Out For In 2017 - KDnuggets,"The world of data science, big data and IoT (Internet of Things) is continuing to grow and adapt at an astronomical rate. Businesses are slowly able to piece together more information from different sources, meaning that they are able to make more sense of their data. Using data has become more and more important in creating new business opportunities and growth. Companies are still discovering the potential of utilising data and the importance of monetising that data in some form to benefit the business. Here’s what we can expect to see from the data science industry in 2017 – and how it might affect you"
Data Science Trends To Look Out For In 2017 - KDnuggets,"You also need to be comfortable working with relational databases, so SQL is incredibly important. In 2015 SQL was listed as the most important skill to have from a study of 3500 LinkedIn job listings. Hadoop, Python and Java were also prevalent"
Data Science Trends To Look Out For In 2017 - KDnuggets,"So how does that work out in the real world? Think about it like this. In the not too distant future, you won’t need keys to enter your home. As you approach the front door, it will sense your presence and automatically unlock itself for you. Then, as soon as you leave the house, it will ask all of the non-essential energy units in the house to turn off – in turn saving the homeowner money"
Data Science Trends To Look Out For In 2017 - KDnuggets,"Data science has already been invaluable in improving the outcomes of epidemics and predicting patient behaviour. In 2015, data scientists helped predict further West Nile virus outbreaks in the United States, with 85% accuracy. And earlier this year, a team of scientists developed a model that can"
Data Science Trends To Look Out For In 2017 - KDnuggets,"With the rise of electronic healthcare records the amount of data at our disposal is at an all-time high. While massive amounts of data has its benefits and drawbacks, there are many lucrative opportunities for scientists looking to decipher this data in 2017. If you’re looking for an emerging market to work in, this is it"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"In practice, correct evaluation is incredibly difficult – and I am not even talking in vs. Those are the table stakes, but not what matters most in applications. Almost anybody can build hundreds if not thousands of models on a given dataset, but being sure that the one you picked is indeed most likely the best for the job is an art! The question is typically not even which algorithm (logistic regression, SVM, decision tree, deep learning), but rather the entire pipeline from sampling a training set, preprocessing, feature representation, labeling, etc. None of this has anything to do with just ‘out-of-sample’ evaluation. So here is your compass for doing it right:"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,You basically want to come as close as you can to simulating having that model in production and track the impact as far to the bottom line as you can. That means in a perfect world you need to simulate the decision that your model is going to influence. This is often not entirely possible
Interviews with Data Scientists: Claudia Perlich - KDnuggets,Here is an example: You want to evaluate a new model to predict the probability of a person clicking on an ad. The first problem you have is that almost surely you have neither adequate training not evaluation data … Because until you actually show the ads you have nothing to learn from. So welcome to the chicken and egg part of the world with a lot of literature on exploration vs exploitation. So already getting a decent data set to use for evaluation is hard. You can of course consider some ideas from transfer learning and build your model on some other ad campaign and hope for the best – which is fine for learning but really adds just one more question to your evaluation – which alternative dataset is best suited and of course you still have not data for evaluation
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"But let’s for the moment assume that you have a somewhat right dataset. Now you can of course calculate all kinds of things. But again, you only added to the many questions – what should you look at: Likelihood, AUC, Lift (at what percentage), Cost per click? And while there are some statistical arguments for one over the other, there is no right answer"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"Finally – have a baseline! One thing is to know when you are doing better or worse. But there is still the question – is it even worth it or is there a simple solution that gets you close? Having a simple solution to compare to is a fundamental component of good evaluation. At IBM we always used ‘Willie Sutton’. He was a bank robber and when asked why he did it, the answer was because that’s “where the money was”. Any sales model we build was always compared to Willie Sutton – just rank companies by revenue. How much better does your fancy model get than that?"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,All this might suggest that the answer is no. I in fact would say NO in the usual interpretation of domain knowledge. But here is where things change dramatically:
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"I do not need to know much about the domain in general, BUT I need to understand EVERYTHING about how the data got created and what it means. Is this domain knowledge? Not really – if you talk to a garden variety oncologist, he or she will be near useless at explaining the details of the fMRI data set you just got. The person you need to talk to is probably the technician who understand the machine and all the data processing that is happening in there including stuff like calibration"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"I have to admit that I had never heard of the concept of ‘data blending’ before reading the reference. After some digging, I am contemplating whether it is simply a sales pitch for some ‘new’ capability of a big data solution or a somewhat general attempt to cover a broad class of feature construction and ‘annotation’ that are based on some form of a ‘fuzzy join’ where you do not have the luxury of a shared key. Giving this the benefit of doubt, I will go with the second. There are a few ways to look at the need for adding (fuzzy or not) data:"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"1) On the most abstract level it is a form of feature construction. I my experience, features often trump algorithm – so I am a huge fan of feature construction. And if you are doing the predictive modeling right – the model will tell you if your blending worked or not. So you really have little to lose and you can try all kinds of blending even of the same information. This tends to be the most time consuming (and also in my case fun) part of modeling, so having some tools that simplify this and in particular allow for fuzzy matches and automated aggregation would be neat …"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"2) Let me put on my philosopher’s hat: All you do with blending is navigating the bias-variance tradeoff in the context of the limitations of the expressiveness of your current model. Most often the need to blend arises around identifiers of events/entities. Say you have a field that is ZIP code (or Name). You might want to blend some actual features of ZIP codes at a certain time – so you are really just dealing with some identity of a combination of time and space. You can add in some census data based on ZIP and date and hope that this improves your model. But in some information theoretical sense, you in fact did not add any data. ZIP and date implicitly contain all you need to know (think of it as a factor). In a world of infinite data you do not need to bring in that other stuff because a universal function approximator can learn all you can bring in directly from the ZIP date combination"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"This of course only works in theory. In practice it matters how often that ZIP and date appear in your training set and whether your model can deal naturally with interaction effects, which for instance linear models cannot unless you add them. In order to learn anything from it, it has to appear multiple times. If it does not – blending in information de facto replaces the super high-dimensional identifier space (ZIP and date combination) with a much lower common space of say n features (average income, etc). So in terms of bias variance, you just managed a huge variance reduction but you may also have lost all the relevant information (huge increase of bias): say some hidden feature like the occurrence of a natural catastrophe that was not available in the blend but that ZIP and date as a combination was a good proxy for …"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"In terms of related experience, I in fact spend a good 3 years (my dissertation) on something very closely related. It was not so much on the ‘fuzzy’ part but rather on the practical question how to automatically create features in multi-relational databases. I did assume that the link structure (keys) were known to join between tables. We published this work and some conceptual thoughts around the role of identifiers in the Machine Learning Journal:"
Interviews with Data Scientists: Claudia Perlich - KDnuggets,"And then I spent a good 3 years at IBM wrangling the data annotation problem for Company names. We had to build a propensity model for IBM sales accounts. While all kinds of internal info was available for an account, we had no external set of features. Each account was linked somehow to a real company. However, that match was fuzzy at best. What we needed for a model was some information about industry, size, revenue, etc. So in this case, each ‘identifier’ is unique in my dataset and that nice theory gets me nothing. The match between accounts and Dun & Bradstreet entities was something of a n-to-m string matching. For a while we used their matching solution and eventually replaced it with our own (took us a good 2 years)"
Questions To Ask When Moving Machine Learning From Practice to Production - KDnuggets,"After building, training and deploying your models to production, the task is still not complete unless you have monitoring systems in place. A crucial component to ensuring the success of your models is being able to measure and quantify their performance. A number of questions are worth answering in this area"
Data Science Deployments With Docker - KDnuggets,"Deploying machine learning models has always been a struggle. Most of the software industry has adopted the use of container engines like Docker for deploying code to production, but since accessing hardware resources like GPUs from Docker was difficult and required hacky, driver specific workarounds, the machine learning community has shied away from this option. With the recent release of NVIDIA’s"
Data Science Deployments With Docker - KDnuggets,"Docker is designed to be hardware and platform agnostic. GPUs are specialized hardware that is not necessarily available on every host. Because of this, the Docker binary does not include GPU support out of the box, and requires a fair amount of configuration to get things working properly. When we first started using Docker in production and needed to enable access to GPU devices from within the container, we had to roll our own solution. It was educational to have to understand the mechanisms by which hardware like GPUs are exposed to an operating system (primarily the"
Predictive Science vs Data Science - KDnuggets,"We can talk about nutrition science, which focuses on what we eat. We can talk about exercise science, focusing on what we do with those calories. Or we can talk holistically about the outcome -- which I would argue deals with an issue of greater importance about which most people ultimately care -- health science. Given the argument that the outcome, health science, is generally more interesting than the raw material, nutrition, or the process for turning raw materials into something more useful, exercise, why do we tend to speak about data science as opposed to predictive science?"
Predictive Science vs Data Science - KDnuggets,"To be clear, data science is much more than prediction or classification. It includes other machine learning techniques, such as clustering and frequent itemset mining. It also includes data visualization and data storytelling. It can also encompass the various aspects of traditional"
Predictive Science vs Data Science - KDnuggets,"But taking a step back, it is inarguable that data is input, a raw material. In this sense, data science places the emphasis on the ""what"" in predictive processes. While the data is a prime ingredient in the predictive puzzle, and possibly the most difficult to procure or otherwise come across, ""data science"" seems to neglect the other major component as well as the interesting insights"
Predictive Science vs Data Science - KDnuggets,"Whether prediction or hypothesis, one of these 2 will be the most interesting piece of the holistic predictive science puzzle. Predictive science - or prediction science, if that strikes you better - sounds pretty good. But really, isn't that just """
Predictive Science vs Data Science - KDnuggets,"Add in prescriptive statistics, and this seems like a step in the right direction. However, the emphasis in this case is on the application of statistical processes at the expense of. Yet I would argue that this actually does not place the proper emphasis on inferential and prescriptive statistics, and perhaps implies too much reliance on descriptive, and thus seems to also fall short in describing the science of prediction"
Predictive Science vs Data Science - KDnuggets,"Predictive analytics? Maybe the closest fit, but this term seems closer to the business world at this point than to the world of science. I don't see this term brought up in research at all, and it generally seems to be the sole domain of big business. And that's fine for what it is, but what it is does not seem to put science at its forefront (though, clearly, science underlies its usage)"
Predictive Science vs Data Science - KDnuggets,"I don't know that there is a solution. To be fair, I don't even know that this is a problem that exists outside of my own head. But I think everything about boils down to the following, and can be generalized beyond the prediction aspect of data science: Does the term"
Predictive Science vs Data Science - KDnuggets,"I don't purport to have a recommendation, and even if I did I'm sure it would be passed over. Which is fine. But as someone who is not terribly excited by, or comfortable with, the term ""data science,"" I think it's worth being introspective about what it is we do, and how we categorize those tasks. Sure, there is a convenience at being able to put a name to a broad profession of somewhat related tasks, but do we lose the trees through this forest?"
RCloud – DevOps for Data Science - KDnuggets,"The process of accelerating the data analysis insights by reducing the time between coding and deployment, now called DevOps, has become more relevant with the emerging role of data science teams in large organizations.  Effective data science teams must share their findings with each other and the organization at large, be agile enough to embed new features or address additional goals during development, and move results from data wrangling, exploratory data analyses (EDA) and predictive analytics into automated visualizations, diagnostics and reports intended for wider consumption.  In the recent past, data wrangling, EDA and predictive analytics were done with one set of tools and automated visualizations, recommendations and reports were done with another"
RCloud – DevOps for Data Science - KDnuggets,"This separation often extended to the very systems where the tools were located (e.  Separating the tools and the environments hinders mid-process feedback and development modifications and by its very nature creates time lags between results discovery and results sharing. In addition, reproducing or modifying projects could become a project itself if the original development environment was no longer in existence or the data scientist who created it had left the firm"
RCloud – DevOps for Data Science - KDnuggets,"RCloud differs from these systems by providing browser-based access to extensive social coding functionality, including the ability to search notebooks, automated version control and most importantly a user directory that provides access to every registered user’s notebooks.  Having access to all the notebooks in your cloud (e.  Code and widget libraries are an ancillary benefit that comes with the creation of every RCloud notebook.  Rather than recreating the wheel, these “knowledge assets” may be leveraged on similar projects or to train other employees, for example"
RCloud – DevOps for Data Science - KDnuggets,"In this environment, data scientist teams benefit from easier sharing of scripts and data feeds, experiments, annotations and automated recommendations which are well beyond what traditional individual or locally based development environments provide. In addition, since RCloud access is provided in a web-browser, data scientist teams may work from anywhere with an internet connection.  RCloud promotes data science by allowing Data Scientists to easily share ideas and techniques with each other"
RCloud – DevOps for Data Science - KDnuggets,"In RCloud, every notebook is named by a URL, so converting collaborative work into reports and recommendations is accomplished by sharing (e.  RCloud Data Scientists can switch from an executed notebook view (“view.R”) to the underlying code by changing the notebook to edit mode by amending the URL in the browser (“edit"
RCloud – DevOps for Data Science - KDnuggets,"Unlike similar systems, RCloud shared notebooks are not static webpages, but rather code that is being executed “live”. RCloud’s unique “notebook.R” web-service interface means that any notebook asset (e.R, . This gives developers an enormous amount of flexibility to create any type of complex interactive widget, notebook or dashboard (e. In addition, as a fundamental feature of DevOps, RCloud notebooks may be “published” so that both registered and unregistered users (e"
RCloud – DevOps for Data Science - KDnuggets,"RCloud was specifically designed to leverage existing systems and standards so communication between most parts of the system happens through HTTP.  The communication between a web browser and an active R session as a user edits a notebook is performed by a combination of HTTP and Websockets; RCloud can perform parallel connections to multi-server systems.  This means Data Scientists can run big data packages like iotools (fast data import using chunk loading),"
RCloud – DevOps for Data Science - KDnuggets,"RCloud supports efficient, secure, client/server connections via the FastRWeb package and an adopted discipline known as the Object-Capabilities (ocap). RCloud uses the RServe protocol to implement the ocap methodology and the client-server communication. This means that web browsers never directly instruct the RCloud backend to execute arbitrary code which prevents unauthenticated clients from making unauthorized calls to the RCloud runtime environment and RCloud notebooks may be encrypted for added security; read more about RCloud security features on our"
RCloud – DevOps for Data Science - KDnuggets,"In addition, all of the elements of an RCloud notebook are stored on a Github (or Git) server; it is a core design principle of RCloud that no command should be run without being saved, even if it gets deleted later. The Github server supports continual and automatic background versioning of notebooks as they are developed and modified. This feature provides the benefits of basic Git version control for casual users and the option to use more sophisticated Git features for advanced users.  RCloud also maintains an automatic Git based trail of code modifications which document the development history of an RCloud based project"
Combining Different Methods to Create Advanced Time Series Prediction - KDnuggets,"The results from combining methods for time series prediction have been quite promising. However, the degree of error for long-term predictions is still quite high. Sounds like a challenge, so some new experiments are forthcoming!"
Combining Different Methods to Create Advanced Time Series Prediction - KDnuggets,"Today, businesses need to be able to predict demand and trends to stay in line with any sudden market changes and economy swings. This is exactly where forecasting tools, powered by Data Science, come into play, enabling organizations to successfully deal with strategic and capacity planning. Smart forecasting techniques can be used to reduce any possible risks and assist in making well-informed decisions. One of our customers, an enterprise from the Middle East, needed to predict their market demand for the upcoming twelve weeks. They required a market forecast to help them set their short-term objectives, such as production strategy, as well as assist in capacity planning and price control. So, we came up with an idea of creating a custom time series model capable of tackling the challenge. In this article, we will cover the modelling process as well as the pitfalls we had to overcome along the way"
Combining Different Methods to Create Advanced Time Series Prediction - KDnuggets,"With the emergence of the powerful forecasting methods based on Machine Learning, future predictions have become more accurate. In general, forecasting techniques can be grouped into two categories: qualitative and quantitative. Qualitative forecasts are applied when there is no data available and prediction is based only on expert judgement. Quantitative forecasts are based on time series modeling. This kind of models uses historical data and is especially efficient in forecasting some events that occur over periods of time: for example prices, sales figures, volume of production etc"
Combining Different Methods to Create Advanced Time Series Prediction - KDnuggets,"It is clear that one particular forecasting technique cannot work in every situation. Each of the methods has its specific use case and can be applied with regard to many factors (the period over which the historical data is available, the time period that has to be observed, the size of the budget, the preferred level of accuracy) and the output required. So, we faced the question: which method/methods to use to obtain the desired result? As different approaches had their unique strengths and weaknesses, we decided to combine a number of methods and make them work together. In this way, we could build a time series model capable of providing trustworthy predictions to ensure data reliability and time/cost saving. And this is how we did it"
Combining Different Methods to Create Advanced Time Series Prediction - KDnuggets,"An important first step in describing various components of the series is smoothing, although it does not really provide you with a ready-to-use model. In the beginning, we estimated the trend (behavior) component. Such methods as Moving Average, Exponential Smoothing, Chow’s Adaptive Control, Winter’s Linear and Seasonal Exponential Smoothing methods did not provide us with the trend estimation accuracy we expected. The most reliable result was obtained using the"
Combining Different Methods to Create Advanced Time Series Prediction - KDnuggets,"You may notice that there are some negative values present, showing that something unusual was happening during that period of time. We aimed to find out the circumstances causing such behaviour, so we came up with an idea to compile the outliers with a simple calendar and discovered that the negative values tightly correlate with such public holidays as Ramadan, Eid Al Fitr and other. Having collected and summarized all the data, we applied Machine Learning methods based on previous data points as entry features and Machine Learning Strategies for Time Series Prediction"
Combining Different Methods to Create Advanced Time Series Prediction - KDnuggets,"When building our model, we attempted not only to use the available information, but also discover the factors which could affect the results. This approach helped us develop the model generating more accurate forecasting results faster than the existing models. For example, to train the developed model to make a prediction for 300 different cities, we need about 15 minutes, while other methods require about 6 hours"
Combining Different Methods to Create Advanced Time Series Prediction - KDnuggets,"Well, the results are quite promising. And there is a long way we can go further in improvement of this model, so it could provide accurate long-term forecasts as well. As for now, the degree of error for long-term predictions is still quite high. Sounds like a challenge? So stay tuned! Some new experiments are coming!"
Combining Different Methods to Create Advanced Time Series Prediction - KDnuggets,"A mathematician by education and by calling, Taras develops his professional skills at a postgraduate program, and teaches operations research and data analysis. Moreover, he never misses any interesting data science or machine learning news in blogs, and having a lot to contribute, he also decided to blog. When he’s done with math and data science, Taras is aso into football and music"
"Data Science and Big Data, Explained - KDnuggets","These are extremely important fields and concepts that are becoming increasingly critical. The world has never collected or stored as much data, and as fast as it does today. In addition, the variety and volume of data is growing at an alarming rate"
"Data Science and Big Data, Explained - KDnuggets","Are these new fields? There are many debates as to whether data science is a new field. Many argue that similar practices have been used and branded as statistics, analytics, business intelligence, and so forth. In either case, data science is a very popular and prominent term used to describe many different data-related processes and techniques that will be discussed here. Big data on the other hand is relatively new in the sense that the amount of data collected and the associated challenges continues to require new and innovative hardware and techniques for handling it"
"Data Science and Big Data, Explained - KDnuggets","This article is meant to give the non-data scientist a solid overview of the many concepts and terms behind data science and big data. While related terms will be mentioned at a very high level, the reader is encouraged to explore the references and other resources for additional detail. Another post will follow as well that will explore related technologies, algorithms, and methodologies in much greater detail"
"Data Science and Big Data, Explained - KDnuggets","Data is everywhere, and is found in huge and exponentially increasing quantities. Data science as a whole reflects the ways in which data is discovered, conditioned, extracted, compiled, processed, analyzed, interpreted, modeled, visualized, reported on, and presented regardless of the size of the data being processed. Big data (as defined soon) is a special application of data science"
"Data Science and Big Data, Explained - KDnuggets","You may be wondering why the term Big Data has become so buzzworthy. We’ve collected a lot of data of various types on a large variety of data storage mechanisms for a long time, right? Yes we have, but we’ve never before enjoyed such inexpensive data collection, storage capabilities, and computational power as we do today. Further, we’ve previously not had such easy access to as inexpensive and capable raw data sensing technologies, instrumentation, and so forth that lead to the generation of today’s massive data sets"
"Data Science and Big Data, Explained - KDnuggets","Structured data refers to data that is stored as a model (or is defined by a structure or schema) in a relational database or spreadsheet. Often it’s easily queryable using SQL (structured query language) since the “structure” of the data is known. A sales order record is a good example. Each sales order has a purchase date, items purchased, purchaser, total cost, etc"
"Data Science and Big Data, Explained - KDnuggets","Unstructured data is data that’s not defined by any schema, model, or structure, and is not organized in a specific way. In other words, it’s just stored raw data. Think of a seismometer (earthquakes are a big fear of mine by the way!). You’ve probably seen the squiggly lines captured by such a device, which essentially represent energy data as recorded at each seismometer location. The recorded signal (i. There is no structure in this case, it’s just variations of energy represented by the signal"
"Data Science and Big Data, Explained - KDnuggets","It follows naturally that Semi-structured data is a combination of the two. It’s basically unstructured data that also has structured data (a. Every time you use your smartphone to take a picture, the shutter captures light reflection information as a bunch of binary data (i. This data has no structure to it, but the camera also appends additional data that includes the date and time the photo was taken, last time it was modified, image size, etc. That’s the structured part. Data formats such as XML and JSON are also considered to be semi-structured data"
"Data Science and Big Data, Explained - KDnuggets","Often data is also mined in order to be analyzed visually. Many people are able to understand data quicker, deeper, and in a more natural way through the strategic use of appropriate graphs, charts, diagrams, and tables. These methods of displaying information can be used to show both categorical and quantitative data. The application of these display types to represent data is known as data visualization"
"Data Science and Big Data, Explained - KDnuggets",Let’s begin by discussing database technologies. Database management systems (DBMS) and their relational counterparts (RDBMS) were the most widely used database systems for a long time since the 1980s. They are generally very good for transaction-based operations and adhering to the ACID principles in general
"Data Science and Big Data, Explained - KDnuggets","The downside to relational systems is that these databases are relatively static and biased heavily towards structured data, represent data in non-intuitive and non-natural ways, and incur significant processing overhead and are therefore less performant. Another downside is that the table-based stored data does not usually represent the actual data (i. This is known as the object-relational impedance mismatch, and thus requires a mapping between the table-based data and the actual objects of the problem domain. Database Management systems as described include Microsoft SQL Server, Oracle, MySql, and so on"
"Data Science and Big Data, Explained - KDnuggets","NoSql databases are therefore largely used for high-scale transactions. NoSql database systems include MongoDB, Redis, Cassandra, and CouchDb to name a few. Note that there are multiple types of NoSql databases, which include document, graph, key-value, and wide-column"
"Data Science and Big Data, Explained - KDnuggets","Practitioners of Big Data have seen the creation and proliferation of specific technologies needed for high-scale data storage, processing capabilities, and analytics of enormous amounts of data. The most popular systems include Apache Hadoop, Cloudera, Hortonworks, and MapR. There are many others trying to compete in this space as well"
"Data Science and Big Data, Explained - KDnuggets","We have never before collected as much varying data as we do today, nor have we needed to handle it as quickly. The variety and amount of data that we collect through many different mechanisms is growing exponentially. This growth requires new strategies and techniques by which the data is captured, stored, processed, analyzed, and visualized"
"Top 10 Amazon Books in Data Mining, 2016 Edition - KDnuggets","The recent explosion of interest in data science, data mining, and related disciplines has been mirrored by an explosion in book titles on these same topics. One of the best ways to decide which books could be useful for your career is to look at which books others are reading. This post details the 10 most popular titles in Amazon's"
"Top 10 Amazon Books in Data Mining, 2016 Edition - KDnuggets","This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book"
"Top 10 Amazon Books in Data Mining, 2016 Edition - KDnuggets","If you have an aptitude for mathematics and some programming skills, author Joel Grus will help you get comfortable with the math and statistics at the core of data science, and with hacking skills you need to get started as a data scientist. Today’s messy glut of data holds answers to questions no one’s even thought to ask. This book provides you with the know-how to dig those answers out"
"Top 10 Amazon Books in Data Mining, 2016 Edition - KDnuggets","This book fills the need for a concise and conversational book on the growing field of Data Analytics and Big Data. Easy to read and informative, this lucid book covers everything important, with concrete examples, and invites the reader to join this field. The chapters in the book are organized for a typical one-semester course. The book contains case-lets from real-world stories at the beginning of every chapter"
"Top 10 Amazon Books in Data Mining, 2016 Edition - KDnuggets","Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world’s leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you’ll soon be able to answer some of the most important questions facing you and your organization"
"Top 10 Amazon Books in Data Mining, 2016 Edition - KDnuggets","R in Action, Second Edition teaches you how to use the R language by presenting examples relevant to scientific, technical, and business developers. Focusing on practical solutions, the book offers a crash course in statistics, including elegant methods for dealing with messy and incomplete data. You'll also master R's extensive graphical capabilities for exploring and presenting data visually. And this expanded second edition includes new chapters on forecasting, data mining, and dynamic report writing"
"Top 10 Amazon Books in Data Mining, 2016 Edition - KDnuggets","Big Data teaches you to build big data systems using an architecture that takes advantage of clustered hardware along with new tools designed specifically to capture and analyze web-scale data. It describes a scalable, easy-to-understand approach to big data systems that can be built and run by a small team. Following a realistic example, this book guides readers through the theory of big data systems, how to implement them in practice, and how to deploy and operate them once they're built"
The Experience of Being a High-Performing Data Scientist - KDnuggets,"For the working data scientist, pain points may dominate the fabric of their experience. High-performance data scientists are those who automate, accelerate, and streamline the more tedious aspects of their jobs so that they can focus on finding data-driven insights. They will embrace any tool, platform, or approach that can help free up mental bandwidth for tasks that demand their creativity and judgment"
The Experience of Being a High-Performing Data Scientist - KDnuggets,Data scientists do exceptionally complex work. Their productivity depends on having access to tools and practices they can use to streamline and accelerate the details in which they immerse themselves. As discussed in
The Experience of Being a High-Performing Data Scientist - KDnuggets,"One of the most frustrating experiences for any data scientists is when they have to work with disparate, fragmented tools and platforms in support of various lifecycle tasks, such as source discovery, data preparation, statistical modeling, and visualization. Considering that data science is increasingly a team-oriented discipline, it’s essential that diverse data professionals—including statistical modelers, data engineers, business analytics, visualization designers, app developers, and others—be able to pool their efforts within open, collaborative environments. The collaboration tools that data scientists employ can make a huge difference whether they experience productivity or frustration in their daily tasks. Lack of an integrated development environment has unfortunate consequences for data scientist productivity: results that are unshareable, team collaborations that are are awkward, and cross-project visibility that is limited or non-existent"
Reasons Why Data Projects Fail - KDnuggets,Data science continues to generate excitement and yet real-world results can often disappoint business stakeholders. How can we mitigate risk and ensure results match expectations? Working as a technical data scientist at the interface between R&D and commercial operations has given me an insight into the traps that lie in our path. I present a personal view on the most common failure modes of data science projects
Reasons Why Data Projects Fail - KDnuggets,This talk is based on conversations I've had with many senior data scientists over the last few years. Many companies seem to go through a pattern of hiring a data science team only for the entire team to quit or be fired around 12 months later. Why is the failure rate so high?
Reasons Why Data Projects Fail - KDnuggets,"Do a data audit before you begin. Check for missing data, or dirty data. For example, you might find that a database has different transactions stored in dollar and yen amounts, without indicating which was which. This actually happened"
Reasons Why Data Projects Fail - KDnuggets,"No it isn't. Data is not a commodity, it needs to be transformed into a product before it's valuable. Many respondents told me of projects which started without any idea of who their customer is or how they are going to use this ""valuable data"". The answer came too late: ""nobody"" and ""they aren't"""
Reasons Why Data Projects Fail - KDnuggets,Don't torture your data scientists by witholding access to the data and tools they need to do their job. This senior data scientist took six weeks to get permission to install python and R. She was so happy!
Reasons Why Data Projects Fail - KDnuggets,He was a product manager at an online auction site that you may have heard of. His story was about an A/B test of a new prototype algorithm for the main product search engine. The test was successful and the new algorithm was moved to production
"Top KDnuggets tweets, Nov 2-8: 35 #OpenSource tools for Internet of Things; An Introduction to Ensemble Learners - KDnuggets",21 Must-Know #DataScience Interview Questions with Answers; Big Data Science: Expectation vs. Reality; Big #DataScience: Expectation vs. Reality; The 10 Algorithms #MachineLearning Engineers Need to Know
How to Rank 10% in Your First Kaggle Competition - KDnuggets,"This post presents a pathway to achieving success in Kaggle competitions as a beginner. The path generalizes beyond competitions, however. Read on for insight into succeeding while approaching any data science project"
How to Rank 10% in Your First Kaggle Competition - KDnuggets,"I made many wording changes and added several updates to this post. Note that Kaggle has went through some major changes since I published this post, especially with its ranking system. Therefore some descriptions here might not apply anymore"
How to Rank 10% in Your First Kaggle Competition - KDnuggets,"We can perform some statistical tests to confirm our hypotheses. Sometimes we can get enough intuition from visualization, but quantitative results are always good to have. Note that we will always encounter non-i. So we have to be careful about which test to use and how we interpret the findings"
How to Rank 10% in Your First Kaggle Competition - KDnuggets,The advantage is that you get to see the results immediately and are able to modify or rerun code blocks. This also makes it very convenient to share your approach with others. After all
"Agilience Top Data Mining, Data Science Authorities - KDnuggets","Agilience developed a new way to find authorities in social media across many fields of interest. We review the top authorities in Data Mining and Data science, which include KDnuggets, Kirk. D. Borne, Kaggle, Vincent Granville, and more"
"Agilience Top Data Mining, Data Science Authorities - KDnuggets","33.2K Tweets, 67.3K followers"
"Agilience Top Data Mining, Data Science Authorities - KDnuggets","2.6K Tweets, 60.5K followers"
"Agilience Top Data Mining, Data Science Authorities - KDnuggets",The most trusted #MachineLearning brand in the world. Driven by the crazy idea that #DataScience must be simple & profitable. Spinning data into gold
"Agilience Top Data Mining, Data Science Authorities - KDnuggets","14.5K Tweets, 22.4K followers"
"Agilience Top Data Mining, Data Science Authorities - KDnuggets","1.7K Tweets, 6.3K followers"
"Agilience Top Data Mining, Data Science Authorities - KDnuggets","1.5K Tweets, 2.6K followers"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","Social media now not only shares friendship connections or photos of “selfies” but also spreads from political media to science information. Social network members are tending to more eagerly learn about big data, data science and machine learning through groups.  We review the ten largest Facebook groups in this area"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","This post introduces ten largest Facebook groups in big data, data science and machine learning. These groups have more than ten thousand of members and are listed in order of group size from large to small as of Nov 18, 2016.  Group overview is based on information as it has appeared on Facebook"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","This is the largest group in the field up to date maintained by eight admins (created on August 18, 2007). The group, comprised of 29,638 members, shares interest in various aspects of data mining, machine learning, human computer interaction and artificial intelligence. The current event of this group is on 19-20 Nov 2016 at Oakland about"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","This is a public group of 26,427 members, monitored by Guru Talreja since 8 years ago. Roughly more than twenty posts each day by members. These posts include networking events for data science and life sciences and files shared for using R packages"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","There have been 17,980 members  (created on November 18, 2012). The group is created by Amit R S Bansal (Director, SQL MCM, SQL MVP, MCT at SQLMaestros). It is introduced as a group of SSGAS, Asia’s largest SQL conference"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","The group contains 17,080 members and was created on March 1, 2008. It is currently maintained by Siddharth Tiwari (Head of R&D at Dell EMC, joined about 5 years ago) and John F.X. Berns (Sr. VP, Head of Data Science at Lazada Group joined about 3 years ago). Though the group have been interested in a distributed computing platform written in Java, Hadoop, it also includes key tags of Big data · Machine Learning. Recently they have events  in Big data such as Master Big Data and Hadoop Step-By-Step from Scratch (en Français) and Face Detection using MapReduce (April 2016)"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","Being created in 2012 by Karan Gulati, a Software Developer Engineer (BI/Big Data) at Microsoft Redmond Campus, the group has 16,860 members. These days it is  also managed by  Sudhir Rawat (Lead Technical Consultant at Microsoft). The group discuss about Big Data and Microsoft's Hadoop distribution aka HDInsight. The group administrators host fortnightly sessions on Hadoop and Big Data and all of them are live in their YouTube channel - https://www"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","This  is a group based in Bangalore, an IT Hub of India, and is supported by Bangalore .NET UG (BDotNet), Bangalore ITPro UG (BITPro), SQLPass and Microsoft.  Discussion topics of the group are limited to SQL Server, NoSQL, Big Data, and BI. The current group size is 13,078 members. According to their event information, they often meet in events every few months with hundreds of guests"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","This is a closed group of 11,923 members. It is just recently created in 2015, maintained by four administrators.  They have frequent events for conferences and seminars for R and data science. For example the user meeting"
"Top 10 Facebook Groups for Big Data, Data Science, and Machine Learning - KDnuggets","This is a forum for Big Data, Data Science, Data Mining and Statistics, created by Henrik Nordmark (Head of Data Science at Profusion) on December 28, 2012. The forum is a diverse group of 10,814 people with many different backgrounds and skills.  They have two threads for learning resources, one for online courses and one for books"
Data Avengers… Assemble! - KDnuggets,"So here they are, Earth's mightiest analysts. Data Avengers. Assemble!"
Data Avengers… Assemble! - KDnuggets,"Strong. Virtuous. A true leader. Captain America is clearly an executive. I envision Steve Rogers - his real name - to be the capable Chief Data Officer of Avengers, Inc. He may not be hands-on any longer, but he came up in the rough scrabble world of data munging, so he gets the everyday struggle the data scientists working under him go through"
Data Avengers… Assemble! - KDnuggets,"More certain am I of this than I am that Marvel is clearly superior to DC, Hulk is definitely that data scientist who would try to solve every problem using the Map Reduce algorithm. Think about it: Map Reduce smashes problems down into smaller pieces and then uses brute strength to further process. Hulk maps whatever is in his way and reduces the rubble"
Data Avengers… Assemble! - KDnuggets,"Munge data? Run some analysis? Whip up a classifier using libraries? Implement some neural nets from scratch? Build and scale a production-ready system? Python can really do all of this. Tony loves a prototype, too, and name a programming language preferred by data scientists that can do all of this. No, that other one can't"
Data Avengers… Assemble! - KDnuggets,"Hulk may be beastly strong, but Thor is godly strong. Also, he knows not one algorithmic approach is capable of solving all of his problems. But he understands the power of a single framework running atop arguably the strongest data processing engine in the universe as a central piece in his problem-solving approach. Also, he is very focused when he works, and never allows anyone to pick up his keyboard"
Data Avengers… Assemble! - KDnuggets,"The web-slinger has been both an Avenger and close confidant of the crew at different times. Peter Parker, his alter ego, is a studious young man with the gift of high intelligence. His scientific mind wants to solve problems of consequence, and is not interested in the petty, necessary practicalities which facilitate this ambition"
Data Avengers… Assemble! - KDnuggets,"Your friendly neighborhood Spider-Man is an analyst's analyst. He isn't really concerned with building production systems or implementing his own algorithms, and as such software is merely a tool for him to use to gain insight and solve problems. R is his tool of choice, since it is built for exactly what he needs, nothing more. He doesn't mind that learning R is wrought with confusion, since he does not come from a computer science background and is unencumbered by the knowledge of how other programming languages are implemented"
Data Avengers… Assemble! - KDnuggets,"In the comics, Dr. Stephen Strange, MD, becomes Sorcerer Supreme, guardian of the entire universe. He uses magic to mystify his foes, confounding them while solving problems. In this sense, it seems that Dr. Strange would be an advocate of blackbox algorithms. Better yet, as Sorcerer Supreme, and the most powerful entity in the Marvel Universe, not really understood by anyone else, perhaps he would"
Data Avengers… Assemble! - KDnuggets,"Black boxes are his first choice for everything. Iris dataset? Neural nets! Weather dataset? Random Forests! When he uses ensemble methods he prefers stacking. He will entertain the idea of a Support Vector Machine, but only at very high dimensionality"
Data Avengers… Assemble! - KDnuggets,"Vision is an android created by. His role as a Data Avenger is to perform automated machine learning to help the others. Vision takes a hybrid Bayesian and genetic algorithm approach to feature selection and model building, performing training and testing on vast numbers of models in parallel in order to come up with the most accurate results and help point the other team members in the right direction"
Data Avengers… Assemble! - KDnuggets,"The real important point, here, is that Vision has not supplanted the other team members. He is complementary to the more fleshy data scientists, and has not aimed to take over their profession and leave them all unemployed. Wink, wink"
Data Avengers… Assemble! - KDnuggets,"J.A.R.V.I.S. They have decided to test the waters of cognitive computing, and implemented such a system from scratch"
Practical Data Science: Building Minimum Viable Models - KDnuggets,"When we talk about innovative services or products, many startups follow a smoother model of development. This allows them to minimize the risk to be able to have improvements when collecting capital to finance themselves. Once they found the market fit, the issue will be about the growth, to achieve a balance point"
Practical Data Science: Building Minimum Viable Models - KDnuggets,"It is the data scientist or the data science team’s task to find that/ those model/s, but finding it/them (determine the modelling technique, setting parameters and adjustment) may be a very long, and sometimes, non-aligned task with the business times. For example: it does not make sense a model to “predict the results of a football match” that finds the results after the match was played. So, how startups can minimize this risk when launching a new app? Do they need so much deployment to enter the market, do they have the necessary resources? In 7Puentes we understand they do not and that is why we coined a new concept: MVM (Minimum Valuable Model)"
Practical Data Science: Building Minimum Viable Models - KDnuggets,"A data science model with 75% of accuracy, which is acceptable to guarantee the well-functioning of the app, takes 25% of the time. To escalate to a 100%, i. If we think a model of recommendation for an app like “Tinder”, a MVM does not need the 10 offers to be ideal, but, to have out of 10 offers an average of good offers and, maybe, a low percentage of very bad offers. It is not necessary to develop a prediction algorithm 100% effective and it is not feasible in financial terms. Every sector/project has its “good-enough”: sometimes the priority is a quick response but in other cases the covering is the focus"
Practical Data Science: Building Minimum Viable Models - KDnuggets,"To find a MVM, it is necessary a constant dialogue between the areas that define the business goals and the data scientist.  It is no use the specialist working only two months with the data, since finding the MVM requires to pay attention to what data provide. Many times, the business areas require a very precise model with training data that is not enough, they are noisy or they do not adjust to the thought model.  Maybe it is better to reduce the scope of the model to the portion of the data where it better works and, in the future, expand the coverage of the model when the startup has better financial resources"
Practical Data Science: Building Minimum Viable Models - KDnuggets,"More than 70% of data science project’s efforts consist on data-junk: collection and cleaning of data. And the time for modeling, experimenting and communicating results is too short. So that MVM model comes to accelerate the knowledge extraction process from a “lean” perspective"
Data Science Basics: An Introduction to Ensemble Learners - KDnuggets,"I only visited 3 garages in my example, but you could imagine that accuracy would likely increase if I had visited tens or hundreds of garages, especially if my car's problem was one of a very complex nature. This holds true for bagging, and the bagged classifier often is significantly more accurate than single constituent classifiers. Also note that the type of constituent classifier used are inconsequential; the resulting model can be made up of any single classifier type"
Data Science Basics: An Introduction to Ensemble Learners - KDnuggets,"Thinking again of my car problem, perhaps I had been to one particular garage numerous times in the past, and trusted their diagnosis slightly more than others. Also suppose that I was not a fan of previous interactions with the dealership, and that I trusted their insight less. The weights I assigned would be reflective"
Data Science Basics: An Introduction to Ensemble Learners - KDnuggets,"A combiner algorithm is then trained to make ultimate predictions using the predictions of other algorithms. This combiner can be any ensemble technique, but logistic regression is often found to be an adequate and simple algorithm to perform this combining. Along with classification, stacking can also be employed in unsupervised learning tasks such as density estimation"
Data Science Basics: An Introduction to Ensemble Learners - KDnuggets,"An easy mistake for data science newcomers to make is to underestimate the complexity of the algorithm landscape, thinking that a decision tree is a decision tree, a neural network is a neural network, etc. Besides overlooking the fact that there is significant difference between various specific implementations of algorithm families (look at the vast amount of research being done in neural networks over the past few years for explicit evidence), the idea that various algorithms can be used in tandem (via ensemble or intervening meta-learners) to accomplish greater accuracy in a given task, or even to tackle tasks which, alone, would not be solvable, does not even register. Any form of recent ""artificial intelligence"" you can think of takes this approach. But that's a different conversation completely"
